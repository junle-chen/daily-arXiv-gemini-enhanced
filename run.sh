#!/bin/bash

# --- è„šæœ¬è®¾ç½® ---
# set -e: ä»»ä½•å‘½ä»¤å¤±è´¥ï¼Œè„šæœ¬å°†ç«‹å³åœæ­¢æ‰§è¡Œã€‚è¿™å¯¹äºè°ƒè¯•è‡³å…³é‡è¦ï¼
set -e

echo "ğŸš€ Starting Daily ArXiv Update Workflow..."

# --- 1. å®šä¹‰å˜é‡ ---
# è¿™æ ·åšå¯ä»¥è®©è„šæœ¬æ›´æ˜“äºé˜…è¯»å’Œç»´æŠ¤
today=$(date -u "+%Y-%m-%d")
RAW_JSONL_FILE="data/${today}.jsonl"
UNIQUE_JSONL_FILE="data/${today}_unique.jsonl"
# å‡è®¾ä½ çš„ AI å¢å¼ºè„šæœ¬ä¼šä½¿ç”¨è¿™ä¸ªåå­—
ENHANCED_JSONL_FILE="data/${today}_unique_AI_enhanced_Chinese.jsonl" 
# æœ€ç»ˆçš„ Markdown æ–‡ä»¶å
FINAL_MD_FILE="data/${today}.md"

# --- 2. è¿è¡Œ Scrapy çˆ¬è™« ---
# å§‹ç»ˆä»é¡¹ç›®æ ¹ç›®å½•è°ƒç”¨ï¼Œå¹¶æ˜ç¡®æŒ‡å®šè·¯å¾„
echo "--- Step 1: Crawling data from ArXiv ---"
(cd daily_arxiv && scrapy crawl arxiv -o ../${RAW_JSONL_FILE})
echo "âœ… Raw data saved to ${RAW_JSONL_FILE}"

# --- 3. è¿è¡Œå»é‡è„šæœ¬ ---
# æ˜ç¡®æŒ‡å®šè¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶
echo "--- Step 2: Deduplicating data ---"
python deduplicate.py ${RAW_JSONL_FILE} -o ${UNIQUE_JSONL_FILE}
echo "âœ… Unique data saved to ${UNIQUE_JSONL_FILE}"

# --- 4. (å¯é€‰) è¿è¡Œ AI å¢å¼ºè„šæœ¬ ---
# ç¡®ä¿å®ƒçš„è¾“å…¥æ˜¯å»é‡åçš„æ–‡ä»¶
# echo "--- Step 3: Enhancing data with AI ---"
# python ai/enhance.py --data ${UNIQUE_JSONL_FILE}
# echo "âœ… AI enhancement complete."
# # å¦‚æœä½ è¿è¡Œäº† AI å¢å¼ºï¼Œåç»­æ­¥éª¤çš„è¾“å…¥æ–‡ä»¶å°±éœ€è¦æ”¹å˜
# INPUT_FOR_MD=${ENHANCED_JSONL_FILE}

# --- 5. è¿è¡Œ Markdown ç”Ÿæˆè„šæœ¬ ---
# å¦‚æœä½ æ²¡æœ‰ AI å¢å¼ºæ­¥éª¤ï¼Œå°±ç”¨å»é‡åçš„æ–‡ä»¶
INPUT_FOR_MD=${UNIQUE_JSONL_FILE}

echo "--- Step 4: Converting JSONL to Markdown ---"
# ä»æ ¹ç›®å½•è°ƒç”¨ï¼Œå¹¶ä½¿ç”¨ç›¸å¯¹äºæ ¹ç›®å½•çš„è·¯å¾„
python to_md/convert.py --data ${INPUT_FOR_MD}
echo "âœ… Markdown report generated at ${FINAL_MD_FILE}"

# --- 6. æ›´æ–°ä¸» README æ–‡ä»¶ ---
echo "--- Step 5: Updating main README.md ---"
python update_readme.py
echo "âœ… README.md updated."

echo "ğŸ‰ Workflow finished successfully!"
