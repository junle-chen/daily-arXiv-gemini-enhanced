#!/bin/bash
set -e

echo "ğŸš€ Starting Daily ArXiv Update Workflow..."

# --- 1. ä½¿ç”¨åŒ—äº¬æ—¶åŒºè·å–æ­£ç¡®çš„å½“å¤©æ—¥æœŸ ---
today=$(TZ=Asia/Shanghai date "+%Y-%m-%d")
# åœ¨ macOS ä¸Šç”¨ä¸åŒçš„æ–¹æ³•è·å–æ˜¨å¤©çš„æ—¥æœŸ
if [[ "$OSTYPE" == "darwin"* ]]; then
    # macOS æ–¹å¼
    yesterday=$(TZ=Asia/Shanghai date -v -1d "+%Y-%m-%d")
else
    # Linux æ–¹å¼
    yesterday=$(TZ=Asia/Shanghai date -d "yesterday" "+%Y-%m-%d")
fi
echo "âœ… Workflow date set to: ${today} (Asia/Shanghai)"

# --- 2. å®šä¹‰æ‰€æœ‰æ–‡ä»¶å ---
RAW_JSONL_FILE="data/${today}.jsonl"
UNIQUE_JSONL_FILE="data/${today}_unique.jsonl"
YESTERDAY_UNIQUE_FILE="data/${yesterday}_unique.jsonl"
ENHANCED_JSONL_FILE="data/${today}_unique_AI_enhanced_Chinese.jsonl"
FINAL_MD_FILE="data/${today}.md"

# --- 3. è¿è¡Œ Scrapy çˆ¬è™« ---
echo "--- Step 1: Crawling data from ArXiv ---"
(cd daily_arxiv && scrapy crawl arxiv -o ../${RAW_JSONL_FILE})
echo "âœ… Raw data saved to ${RAW_JSONL_FILE}"

# --- 4. è¿è¡Œå»é‡è„šæœ¬ ---
echo "--- Step 2: Deduplicating raw data ---"
python deduplicate.py ${RAW_JSONL_FILE} -o ${UNIQUE_JSONL_FILE}
echo "âœ… Unique data saved to ${UNIQUE_JSONL_FILE}"

# --- æ–°å¢: ç­›é€‰è½¨è¿¹é¢„æµ‹å’Œå¤§æ¨¡å‹ç›¸å…³è®ºæ–‡ ---
TRAJECTORY_LLM_FILE="data/${today}_trajectory_llm.jsonl"
echo "--- Step 2.1: Filtering papers related to trajectory prediction and large models using LLM ---"
# ä½¿ç”¨ä¸AIå¢å¼ºç›¸åŒçš„æ¨¡å‹
MODEL_NAME=${MODEL_NAME:-"gemini-2.0-flash"}
python filter_papers.py --data ${UNIQUE_JSONL_FILE} -o ${TRAJECTORY_LLM_FILE} --model ${MODEL_NAME} --threshold 0.6
echo "âœ… Filtered data saved to ${TRAJECTORY_LLM_FILE}"

# --- 5. æ–°å¢ï¼šæ™ºèƒ½æ¯”è¾ƒä»Šå¤©å’Œæ˜¨å¤©çš„å†…å®¹ (å¿½ç•¥è¡Œåº) ---
echo "--- Step 3: Checking for new content (ignoring line order) ---"
if [ -f "$YESTERDAY_UNIQUE_FILE" ]; then
    # æå–ã€æ’åºå¹¶æ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶çš„ ID é›†åˆ
    # diff <(command1) <(command2) æ˜¯ä¸€ç§é«˜çº§ç”¨æ³•ï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªå‘½ä»¤çš„è¾“å‡º
    # grep -o '"id": "[^"]*"' ä¼šåªæå–å‡º id å­—æ®µ
    # sort ä¼šå¯¹æå–å‡ºçš„ id æ’åº
    # å¦‚æœä¸¤ä¸ªæ’åºåçš„ id åˆ—è¡¨æ²¡æœ‰å·®å¼‚ï¼Œdiff å‘½ä»¤çš„è¾“å‡ºå°±æ˜¯ç©ºçš„
    if [ -z "$(diff <(grep -o '"id": "[^"]*"' "$UNIQUE_JSONL_FILE" | sort) <(grep -o '"id": "[^"]*"' "$YESTERDAY_UNIQUE_FILE" | sort))" ]; then
        echo "â„¹ï¸  No new papers found. The set of papers is the same as yesterday. Exiting workflow."
        rm "$RAW_JSONL_FILE" "$UNIQUE_JSONL_FILE"
        exit 0
    else
        echo "âœ… New content found. Proceeding with the workflow."
    fi
else
    echo "â„¹ï¸  Yesterday's file not found. Assuming first run."
fi

# --- 6. è¿è¡Œ AI å¢å¼ºè„šæœ¬ ---
echo "--- Step 4: Enhancing data with AI ---"
# ç¡®ä¿å®ƒçš„è¾“å…¥æ˜¯å»é‡åçš„æ–‡ä»¶
python ai/enhance.py --data ${UNIQUE_JSONL_FILE}
echo "âœ… AI enhancement complete. Output is ${ENHANCED_JSONL_FILE}"

# --- æ–°å¢: ä¸ºè½¨è¿¹é¢„æµ‹å’Œå¤§æ¨¡å‹æ•°æ®ç”Ÿæˆå¢å¼ºå†…å®¹ ---
TRAJECTORY_LLM_ENHANCED_FILE="data/${today}_trajectory_llm_AI_enhanced_Chinese.jsonl"
TRAJECTORY_LLM_MD_FILE="data/${today}_trajectory_llm.md"
echo "--- Step 4.1: Enhancing trajectory prediction and large model data with AI ---"
python ai/enhance.py --data ${TRAJECTORY_LLM_FILE}
echo "âœ… AI enhancement for trajectory prediction papers complete. Output is ${TRAJECTORY_LLM_ENHANCED_FILE}"

# --- 7. è¿è¡Œ Markdown ç”Ÿæˆè„šæœ¬ ---
echo "--- Step 5: Converting JSONL to Markdown ---"
python to_md/convert.py --data ${ENHANCED_JSONL_FILE}
echo "âœ… Markdown report generated at ${FINAL_MD_FILE}"

# --- æ–°å¢: ä¸ºè½¨è¿¹é¢„æµ‹å’Œå¤§æ¨¡å‹æ•°æ®ç”ŸæˆMarkdown ---
echo "--- Step 5.1: Converting trajectory prediction and large model JSONL to Markdown ---"
python to_md/convert.py --data ${TRAJECTORY_LLM_ENHANCED_FILE} --output ${TRAJECTORY_LLM_MD_FILE}
echo "âœ… Trajectory prediction and large model Markdown report generated at ${TRAJECTORY_LLM_MD_FILE}"

# --- 8. æ›´æ–°ä¸» README æ–‡ä»¶ ---
echo "--- Step 6: Updating main README.md ---"
python update_readme.py
echo "âœ… README.md updated."

echo "ğŸ‰ Workflow finished successfully!"
