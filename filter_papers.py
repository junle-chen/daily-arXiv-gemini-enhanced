#!/usr/bin/env python
# filepath: /Users/junle/Code/daily-arXiv-gemini-enhanced/filter_papers.py
import argparse
import json
import os
import sys
import time
import random
import signal
import threading
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
)
from google.api_core.exceptions import (
    ResourceExhausted,
    InternalServerError,
    ServiceUnavailable,
)
from langchain_google_genai import ChatGoogleGenerativeAI
import api_manager
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="Filter papers based on relevance to trajectory prediction and large models using LLM analysis."
    )
    parser.add_argument(
        "--data",
        type=str,
        required=True,
        help="Path to the JSONL data file containing papers.",
    )
    parser.add_argument(
        "--output",
        "-o",
        type=str,
        help="Path to the output filtered JSONL file. If not provided, a default name will be generated.",
    )
    parser.add_argument(
        "--model",
        "-m",
        type=str,
        default="gemini-2.0-flash",
        help="LLM model to use for relevance detection.",
    )
    parser.add_argument(
        "--threshold",
        "-t",
        type=float,
        default=0.7,
        help="Confidence threshold for relevance (0.0-1.0).",
    )
    return parser.parse_args()


def main():
    args = parse_args()

    # Configure environment for optimal API usage in GitHub Actions
    api_manager.setup_environment()

    if not os.path.exists(args.data):
        print(f"âŒ Error: Input file not found at '{args.data}'", file=sys.stderr)
        return

    # å¦‚æœç”¨æˆ·æ²¡æœ‰æä¾›è¾“å‡ºæ–‡ä»¶åï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª
    if args.output:
        output_path = args.output
    else:
        # ä¾‹å¦‚: 'data/2025-06-07_unique.jsonl' -> 'data/2025-06-07_trajectory_llm.jsonl'
        base, ext = os.path.splitext(args.data)
        output_path = f"{base}_trajectory_llm{ext}"

    # åˆå§‹åŒ–LLMæ¨¡å‹
    try:
        llm = ChatGoogleGenerativeAI(model=args.model)
        print(f"â„¹ï¸ Using {args.model} for relevance detection", file=sys.stderr)
    except Exception as e:
        print(f"âŒ Error: Could not initialize LLM model: {e}", file=sys.stderr)
        return

    # å®šä¹‰å¸¦æœ‰é‡è¯•æœºåˆ¶å’Œè¶…æ—¶æ§åˆ¶çš„LLMè°ƒç”¨å‡½æ•°
    @retry(
        reraise=True,
        stop=stop_after_attempt(5),  # æœ€å¤šå°è¯•5æ¬¡
        wait=wait_exponential(multiplier=1, min=4, max=60),  # æŒ‡æ•°é€€é¿ç­–ç•¥
        retry=retry_if_exception_type(
            (
                ResourceExhausted,
                InternalServerError,
                ServiceUnavailable,
                APITimeoutError,
            )
        ),
    )
    def invoke_with_retry(title, summary, timeout_seconds=120):
        """ä½¿ç”¨è¶…æ—¶ä¿æŠ¤è°ƒç”¨LLM API"""
        print(f"ğŸ“¤ å‘é€è¯·æ±‚åˆ°æ¨¡å‹ (æ ‡é¢˜: '{title[:30]}...')", file=sys.stderr)

        # ä½¿ç”¨çº¿ç¨‹è¶…æ—¶æœºåˆ¶
        def invoke_llm():
            return llm.invoke(prompt_template.format(title=title, summary=summary))

        result = [None]
        error = [None]

        def worker():
            try:
                result[0] = invoke_llm()
            except Exception as e:
                error[0] = e

        thread = threading.Thread(target=worker)
        thread.daemon = True
        start_time = time.time()
        thread.start()
        thread.join(timeout_seconds)
        elapsed = time.time() - start_time

        if thread.is_alive():
            print(f"âš ï¸ APIè°ƒç”¨è¶…æ—¶ ({timeout_seconds}ç§’)", file=sys.stderr)
            raise APITimeoutError(f"APIè°ƒç”¨è¶…æ—¶ï¼ˆè¶…è¿‡{timeout_seconds}ç§’ï¼‰")
        if error[0] is not None:
            print(f"âš ï¸ APIè°ƒç”¨é”™è¯¯: {error[0]}", file=sys.stderr)
            raise error[0]

        print(f"âœ“ APIå“åº”æˆåŠŸ (ç”¨æ—¶: {elapsed:.1f}ç§’)", file=sys.stderr)
        return result[0]

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    system_template = """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡è¿‡æ»¤å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ä¸€ç¯‡è®ºæ–‡æ˜¯å¦ä¸è½¨è¿¹é¢„æµ‹ï¼ˆtrajectory predictionï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼‰ç›¸å…³ã€‚
    
    è®ºæ–‡å†…å®¹å¯èƒ½æ¶‰åŠä»¥ä¸‹ä¸»é¢˜ï¼š
    1. è½¨è¿¹é¢„æµ‹ï¼šè¡Œäººè½¨è¿¹é¢„æµ‹ã€è½¦è¾†è½¨è¿¹é¢„æµ‹ã€ç§»åŠ¨ç‰©ä½“è·¯å¾„è§„åˆ’ã€åŠ¨ä½œé¢„æµ‹ç­‰
    2. å¤§æ¨¡å‹ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€åŸºç¡€æ¨¡å‹ï¼ˆfoundation modelsï¼‰ã€å¤§è§„æ¨¡AIæ¨¡å‹ç­‰
    
    è¯·ä»”ç»†åˆ†æè®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦ï¼Œåˆ¤æ–­å…¶ä¸ä¸Šè¿°ä¸»é¢˜çš„ç›¸å…³æ€§ã€‚å¦‚æœè®ºæ–‡åŒæ—¶æ¶‰åŠè½¨è¿¹é¢„æµ‹å’Œå¤§æ¨¡å‹ï¼Œæˆ–è€…å°†è¿™ä¸¤ä¸ªé¢†åŸŸç»“åˆèµ·æ¥çš„å·¥ä½œï¼Œç›¸å…³æ€§ä¼šæ›´é«˜ã€‚"""

    human_template = """åˆ†æä»¥ä¸‹è®ºæ–‡ï¼Œåˆ¤æ–­å…¶ä¸è½¨è¿¹é¢„æµ‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç›¸å…³æ€§ï¼š
    
    æ ‡é¢˜ï¼š{title}
    æ‘˜è¦ï¼š{summary}
    
    è¯·è¾“å‡ºä¸€ä¸ªJSONæ ¼å¼çš„å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    1. relevance_score: 0.0-1.0ä¹‹é—´çš„æ•°å­—ï¼Œè¡¨ç¤ºç›¸å…³æ€§ç¨‹åº¦ï¼ˆ1.0è¡¨ç¤ºé«˜åº¦ç›¸å…³ï¼‰
    2. explanation: ç®€çŸ­è§£é‡Šä¸ºä»€ä¹ˆè¿™ç¯‡è®ºæ–‡ç›¸å…³æˆ–ä¸ç›¸å…³
    3. keywords: æå–çš„ä¸è½¨è¿¹é¢„æµ‹æˆ–å¤§æ¨¡å‹ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨
    
    åªè¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡æœ¬ã€‚"""

    prompt_template = ChatPromptTemplate.from_messages(
        [
            SystemMessagePromptTemplate.from_template(system_template),
            HumanMessagePromptTemplate.from_template(template=human_template),
        ]
    )

    filtered_papers = []
    total_papers = 0
    processed_papers = 0

    # è¯»å–è®ºæ–‡æ•°æ®
    papers = []
    with open(args.data, "r", encoding="utf-8") as f_in:
        for line in f_in:
            try:
                papers.append(json.loads(line))
            except json.JSONDecodeError:
                print(
                    f"âš ï¸ Warning: Skipping malformed JSON: {line.strip()}",
                    file=sys.stderr,
                )

    total_papers = len(papers)
    print(
        f"â„¹ï¸ Processing {total_papers} papers for relevance analysis...", file=sys.stderr
    )

    for paper in papers:
        processed_papers += 1

        # æå–æ ‡é¢˜å’Œæ‘˜è¦
        title = paper.get("title", "")
        summary = paper.get("summary", "")

        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„å†…å®¹åˆ†æï¼Œåˆ™è·³è¿‡
        if not title or not summary:
            continue

        try:
            # ä½¿ç”¨APIç®¡ç†å™¨æ·»åŠ æ™ºèƒ½å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
            api_manager.smart_delay()  # ä½¿ç”¨é»˜è®¤å‚æ•°ä»¥æœ€ä¼˜åŒ–APIä½¿ç”¨ç‡
            print(
                f"Processing paper {processed_papers}/{total_papers}: {title[:50]}...",
                file=sys.stderr,
            )

            # ä½¿ç”¨å¸¦é‡è¯•æœºåˆ¶çš„å‡½æ•°è°ƒç”¨LLMåˆ†æè®ºæ–‡å†…å®¹
            try:
                response = invoke_with_retry(title, summary)
            except ResourceExhausted as e:
                # ç‰¹åˆ«å¤„ç†APIé…é¢è¶…é™é”™è¯¯
                if "GenerateRequestsPerDayPerProjectPerModel-FreeTier" in str(e):
                    print(
                        f"âš ï¸ å·²è¾¾åˆ°æ¯æ—¥APIé…é¢é™åˆ¶ (1000æ¬¡/å¤©)ï¼Œåœæ­¢å¤„ç†æ›´å¤šè®ºæ–‡",
                        file=sys.stderr,
                    )
                    # å¦‚æœå·²ç»å¤„ç†äº†è¶³å¤Ÿçš„è®ºæ–‡ï¼Œå¯ä»¥ç»§ç»­å·¥ä½œæµ
                    if len(filtered_papers) > 0:
                        break
                    else:
                        raise e
                else:
                    # å…¶ä»–APIé€Ÿç‡é”™è¯¯ç”±retryå¤„ç†
                    raise e

            # è§£æå“åº”
            try:
                # å°è¯•ä»LLMå“åº”ä¸­æå–JSONéƒ¨åˆ†
                result = response.content
                if isinstance(result, str):
                    # åˆ é™¤å¯èƒ½çš„Markdownä»£ç å—æ ‡è®°
                    result = result.replace("```json", "").replace("```", "").strip()

                analysis = json.loads(result)

                # æ£€æŸ¥ç›¸å…³æ€§åˆ†æ•°æ˜¯å¦è¶…è¿‡é˜ˆå€¼
                if analysis.get("relevance_score", 0) >= args.threshold:
                    # æ·»åŠ åˆ†æç»“æœåˆ°è®ºæ–‡æ•°æ®
                    paper["relevance_analysis"] = analysis
                    filtered_papers.append(paper)

                    # è¾“å‡ºè¿›åº¦å’Œç»“æœ
                    print(
                        f"âœ“ [{processed_papers}/{total_papers}] ç›¸å…³è®ºæ–‡: {title} (åˆ†æ•°: {analysis.get('relevance_score')})",
                        file=sys.stderr,
                    )
                else:
                    print(
                        f"Ã— [{processed_papers}/{total_papers}] ä¸ç›¸å…³: {title}",
                        file=sys.stderr,
                    )

            except json.JSONDecodeError:
                # å¦‚æœæ— æ³•è§£æJSONï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
                content = (title + " " + summary).lower()
                if any(
                    kw in content
                    for kw in [
                        "trajectory",
                        "predict",
                        "forecasting",
                        "motion",
                        "path",
                        "llm",
                        "large model",
                        "foundation model",
                    ]
                ):
                    paper["relevance_analysis"] = {
                        "relevance_score": 0.6,
                        "explanation": "åŸºäºå…³é”®è¯åŒ¹é…",
                        "keywords": ["backup_method"],
                    }
                    filtered_papers.append(paper)

        except Exception as e:
            print(f"âš ï¸ Error analyzing paper '{title}': {e}", file=sys.stderr)

    # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
    filtered_papers.sort(
        key=lambda p: p.get("relevance_analysis", {}).get("relevance_score", 0),
        reverse=True,
    )

    # å°†è¿‡æ»¤åçš„è®ºæ–‡å†™å…¥è¾“å‡ºæ–‡ä»¶
    with open(output_path, "w", encoding="utf-8") as f_out:
        for paper in filtered_papers:
            f_out.write(json.dumps(paper) + "\n")

    print(
        f"â„¹ï¸ å¤„ç†å®Œæˆ {total_papers} ç¯‡è®ºæ–‡. æ‰¾åˆ° {len(filtered_papers)} ç¯‡ä¸è½¨è¿¹é¢„æµ‹å’Œå¤§æ¨¡å‹ç›¸å…³çš„è®ºæ–‡.",
        file=sys.stderr,
    )
    print(f"âœ… æˆåŠŸä¿å­˜è¿‡æ»¤åçš„è®ºæ–‡åˆ°: {output_path}", file=sys.stderr)


# æ·»åŠ è¶…æ—¶å¤„ç†ç±»å’Œå‡½æ•°
class APITimeoutError(Exception):
    """APIè°ƒç”¨è¶…æ—¶å¼‚å¸¸"""

    pass


def timeout_handler(signum, frame):
    """å¤„ç†è¶…æ—¶ä¿¡å·"""
    raise APITimeoutError("APIè°ƒç”¨è¶…æ—¶")


def call_with_timeout(func, args=(), kwargs={}, timeout_seconds=60):
    """ä½¿ç”¨çº¿ç¨‹è¶…æ—¶æœºåˆ¶è°ƒç”¨å‡½æ•°ï¼ˆé€‚ç”¨äºæ‰€æœ‰å¹³å°ï¼‰"""
    result = [None]
    error = [None]

    def worker():
        try:
            result[0] = func(*args, **kwargs)
        except Exception as e:
            error[0] = e

    thread = threading.Thread(target=worker)
    thread.daemon = True
    thread.start()
    thread.join(timeout_seconds)

    if thread.is_alive():
        return None, APITimeoutError(f"APIè°ƒç”¨è¶…æ—¶ï¼ˆè¶…è¿‡{timeout_seconds}ç§’ï¼‰")
    if error[0] is not None:
        return None, error[0]
    return result[0], None


if __name__ == "__main__":
    main()
