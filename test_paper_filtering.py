#!/usr/bin/env python
"""
æµ‹è¯• Gemini API æ˜¯å¦å¯ä»¥æ­£ç¡®å¤„ç†è®ºæ–‡è¿‡æ»¤ä»»åŠ¡
"""
import os
import sys
import json
import time
import signal
import threading
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
import api_manager


class TimeoutError(Exception):
    """è¶…æ—¶é”™è¯¯"""
    pass


def timeout_handler(signum, frame):
    """å¤„ç†è¶…æ—¶ä¿¡å·"""
    raise TimeoutError("APIè°ƒç”¨è¶…æ—¶")


def test_paper_filtering(timeout_seconds=120):
    """ä½¿ç”¨è¶…æ—¶æœºåˆ¶æµ‹è¯•è®ºæ–‡è¿‡æ»¤APIåŠŸèƒ½"""
    # è®¾ç½®è¶…æ—¶å¤„ç†å™¨
    if hasattr(signal, 'SIGALRM'):
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(timeout_seconds)
    
    try:
        success = run_paper_filtering_test()
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)  # å–æ¶ˆè¶…æ—¶è®¡æ—¶å™¨
        return success
    except TimeoutError as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False
    except Exception as e:
        if hasattr(signal, 'SIGALRM'):
            signal.alarm(0)  # å–æ¶ˆè¶…æ—¶è®¡æ—¶å™¨
        print(f"âŒ æµ‹è¯•å¤±è´¥: {type(e).__name__}: {e}")
        return False


def run_paper_filtering_test():
    """æµ‹è¯•è®ºæ–‡è¿‡æ»¤åŠŸèƒ½"""
    print("ğŸ” å¼€å§‹æµ‹è¯•è®ºæ–‡è¿‡æ»¤åŠŸèƒ½...")
    
    # é…ç½®ç¯å¢ƒ
    api_manager.setup_environment()
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model_name = os.environ.get("GEMINI_MODEL", "gemini-2.0-flash")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    try:
        llm = ChatGoogleGenerativeAI(model=model_name)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        raise
        
    # åˆ›å»ºä¸filter_papers.pyç›¸åŒçš„æç¤ºæ¨¡æ¿
    system_template = """ä½ æ˜¯ä¸€ä¸ªå­¦æœ¯è®ºæ–‡è¿‡æ»¤å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­ä¸€ç¯‡è®ºæ–‡æ˜¯å¦ä¸è½¨è¿¹é¢„æµ‹ï¼ˆtrajectory predictionï¼‰å’Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼‰ç›¸å…³ã€‚
    
    è®ºæ–‡å†…å®¹å¯èƒ½æ¶‰åŠä»¥ä¸‹ä¸»é¢˜ï¼š
    1. è½¨è¿¹é¢„æµ‹ï¼šè¡Œäººè½¨è¿¹é¢„æµ‹ã€è½¦è¾†è½¨è¿¹é¢„æµ‹ã€ç§»åŠ¨ç‰©ä½“è·¯å¾„è§„åˆ’ã€åŠ¨ä½œé¢„æµ‹ç­‰
    2. å¤§æ¨¡å‹ï¼šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€åŸºç¡€æ¨¡å‹ï¼ˆfoundation modelsï¼‰ã€å¤§è§„æ¨¡AIæ¨¡å‹ç­‰
    
    è¯·ä»”ç»†åˆ†æè®ºæ–‡æ ‡é¢˜å’Œæ‘˜è¦ï¼Œåˆ¤æ–­å…¶ä¸ä¸Šè¿°ä¸»é¢˜çš„ç›¸å…³æ€§ã€‚å¦‚æœè®ºæ–‡åŒæ—¶æ¶‰åŠè½¨è¿¹é¢„æµ‹å’Œå¤§æ¨¡å‹ï¼Œæˆ–è€…å°†è¿™ä¸¤ä¸ªé¢†åŸŸç»“åˆèµ·æ¥çš„å·¥ä½œï¼Œç›¸å…³æ€§ä¼šæ›´é«˜ã€‚"""

    human_template = """åˆ†æä»¥ä¸‹è®ºæ–‡ï¼Œåˆ¤æ–­å…¶ä¸è½¨è¿¹é¢„æµ‹å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„ç›¸å…³æ€§ï¼š
    
    æ ‡é¢˜ï¼š{title}
    æ‘˜è¦ï¼š{summary}
    
    è¯·è¾“å‡ºä¸€ä¸ªJSONæ ¼å¼çš„å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
    1. relevance_score: 0.0-1.0ä¹‹é—´çš„æ•°å­—ï¼Œè¡¨ç¤ºç›¸å…³æ€§ç¨‹åº¦ï¼ˆ1.0è¡¨ç¤ºé«˜åº¦ç›¸å…³ï¼‰
    2. explanation: ç®€çŸ­è§£é‡Šä¸ºä»€ä¹ˆè¿™ç¯‡è®ºæ–‡ç›¸å…³æˆ–ä¸ç›¸å…³
    3. keywords: æå–çš„ä¸è½¨è¿¹é¢„æµ‹æˆ–å¤§æ¨¡å‹ç›¸å…³çš„å…³é”®è¯åˆ—è¡¨
    
    åªè¿”å›JSONå¯¹è±¡ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡æœ¬ã€‚"""
    
    prompt_template = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(system_template),
        HumanMessagePromptTemplate.from_template(human_template),
    ])
    
    # å‡†å¤‡ä¸€ä¸ªç›¸å…³çš„è®ºæ–‡æ ·ä¾‹
    test_paper = {
        "title": "Trajectory Prediction for Autonomous Vehicles using Large Language Models",
        "summary": "In this paper, we propose a novel method for predicting the trajectories of vehicles and pedestrians by leveraging large language models (LLMs). Our approach combines traditional motion prediction techniques with the contextual understanding capabilities of foundation models. We demonstrate that by incorporating LLMs into the prediction pipeline, we can achieve more accurate trajectory forecasting in complex urban environments. Experimental results show a 15% improvement over state-of-the-art methods on standard benchmarks."
    }
    
    print(f"ğŸ“ æµ‹è¯•è®ºæ–‡: {test_paper['title']}")
    print("ğŸ”„ è°ƒç”¨APIä¸­...")
    
    # æ·»åŠ å»¶è¿Ÿä»¥ç¡®ä¿ä¸ä¼šæœ‰é€Ÿç‡é™åˆ¶é—®é¢˜
    api_manager.smart_delay()
    
    # è°ƒç”¨API
    start_time = time.time()
    try:
        print("ğŸ“¤ æ­£åœ¨å‘é€APIè¯·æ±‚...")
        response = llm.invoke(prompt_template.format(title=test_paper["title"], summary=test_paper["summary"]))
        elapsed = time.time() - start_time
        
        print(f"â±ï¸ APIå“åº”æ—¶é—´: {elapsed:.2f}ç§’")
        print(f"ğŸ“„ åŸå§‹APIå“åº”: {response.content}")
        
        # å°è¯•è§£æJSONå“åº”
        try:
            content = response.content
            if isinstance(content, str):
                content = content.replace("```json", "").replace("```", "").strip()
            
            result = json.loads(content)
            print(f"âœ… æˆåŠŸè§£æJSONå“åº”")
            print(f"ğŸ“Š ç›¸å…³æ€§å¾—åˆ†: {result.get('relevance_score', 'N/A')}")
            print(f"ğŸ“ è§£é‡Š: {result.get('explanation', 'N/A')}")
            print(f"ğŸ”‘ å…³é”®è¯: {result.get('keywords', [])}")
            
            # ä¿å­˜ç»“æœ
            with open("paper_filtering_test_result.json", "w", encoding="utf-8") as f:
                json.dump({
                    "paper": test_paper,
                    "response": result,
                    "elapsed_seconds": elapsed
                }, f, ensure_ascii=False, indent=2)
            
            print("âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ° paper_filtering_test_result.json")
            return True
            
        except json.JSONDecodeError as e:
            print(f"âŒ æ— æ³•è§£æJSONå“åº”: {e}")
            print(f"åŸå§‹å“åº”: {response.content}")
            raise
            
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"âŒ APIè°ƒç”¨å¤±è´¥ (ç»è¿‡ {elapsed:.2f}ç§’): {type(e).__name__}: {e}")
        raise


def run_test_with_thread_timeout(timeout_seconds=120):
    """ä½¿ç”¨çº¿ç¨‹è¶…æ—¶æœºåˆ¶è¿›è¡Œæµ‹è¯•ï¼ˆé€‚ç”¨äºWindowsç­‰ä¸æ”¯æŒä¿¡å·çš„å¹³å°ï¼‰"""
    result = {"success": None, "error": None}
    
    def worker():
        try:
            success = run_paper_filtering_test()
            result["success"] = success
        except Exception as e:
            result["error"] = e
    
    thread = threading.Thread(target=worker)
    thread.daemon = True
    thread.start()
    thread.join(timeout_seconds)
    
    if thread.is_alive():
        print("âŒ æµ‹è¯•å¤±è´¥: APIè°ƒç”¨è¶…æ—¶")
        return False
    elif result["error"]:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {type(result['error']).__name__}: {result['error']}")
        return False
    elif result["success"]:
        return True
    else:
        print("âŒ æµ‹è¯•å¤±è´¥: æœªçŸ¥é”™è¯¯")
        return False


if __name__ == "__main__":
    print("ğŸš€ è®ºæ–‡è¿‡æ»¤ API æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    timeout_sec = 120
    if len(sys.argv) > 1:
        try:
            timeout_sec = int(sys.argv[1])
        except ValueError:
            pass
    
    print(f"â±ï¸ è®¾ç½®è¶…æ—¶æ—¶é—´: {timeout_sec} ç§’")
    
    # æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©ä¸åŒçš„è¶…æ—¶å¤„ç†æ–¹å¼
    if hasattr(signal, 'SIGALRM'):  # Unix/Mac
        test_paper_filtering(timeout_sec)
    else:  # Windows
        run_test_with_thread_timeout(timeout_sec)
