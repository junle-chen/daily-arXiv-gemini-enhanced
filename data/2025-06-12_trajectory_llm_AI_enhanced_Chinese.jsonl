{"id": "2506.08532", "pdf": "https://arxiv.org/pdf/2506.08532", "abs": "https://arxiv.org/abs/2506.08532", "authors": ["Yanwei Gong", "Xiaolin Chang"], "title": "Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness", "categories": ["cs.AI"], "comment": null, "summary": "The rapid growth of the low-altitude economy has driven the widespread\nadoption of unmanned aerial vehicles (UAVs). This growing deployment presents\nnew challenges for UAV trajectory planning in complex urban environments.\nHowever, existing studies often overlook key factors, such as urban airspace\nconstraints and economic efficiency, which are essential in low-altitude\neconomy contexts. Deep reinforcement learning (DRL) is regarded as a promising\nsolution to these issues, while its practical adoption remains limited by low\nlearning efficiency. To overcome this limitation, we propose a novel UAV\ntrajectory planning framework that combines DRL with large language model (LLM)\nreasoning to enable safe, compliant, and economically viable path planning.\nExperimental results demonstrate that our method significantly outperforms\nexisting baselines across multiple metrics, including data collection rate,\ncollision avoidance, successful landing, regulatory compliance, and energy\nefficiency. These results validate the effectiveness of our approach in\naddressing UAV trajectory planning key challenges under constraints of the\nlow-altitude economy networking.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u53ca\u4e86UAV\u8f68\u8ff9\u89c4\u5212\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408DRL\u548cLLM\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["UAV trajectory planning", "trajectory planning", "large language model", "LLM", "deep reinforcement learning", "DRL"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DRL\u548cLLM\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u7ea6\u675f\u4e0b\u7684\u89c4\u5212\u6548\u7387\u548c\u6548\u679c\u3002", "motivation": "\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u5feb\u901f\u589e\u957f\u63a8\u52a8\u4e86\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u8fd9\u79cd\u65e5\u76ca\u589e\u957f\u7684\u90e8\u7f72\u4e3a\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u4e86\u5173\u952e\u56e0\u7d20\uff0c\u5982\u57ce\u5e02\u7a7a\u57df\u9650\u5236\u548c\u7ecf\u6d4e\u6548\u7387\uff0c\u8fd9\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u80cc\u666f\u4e0b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DRL\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u65b0\u578b\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u5b89\u5168\u3001\u5408\u89c4\u548c\u7ecf\u6d4e\u53ef\u884c\u7684\u8def\u5f84\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6536\u96c6\u7387\u3001\u907f\u78b0\u3001\u6210\u529f\u7740\u9646\u3001\u6cd5\u89c4\u9075\u4ece\u6027\u548c\u80fd\u6e90\u6548\u7387\u7b49\u591a\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6536\u96c6\u7387\u3001\u907f\u78b0\u3001\u6210\u529f\u7740\u9646\u3001\u6cd5\u89c4\u9075\u4ece\u6027\u548c\u80fd\u6e90\u6548\u7387\u7b49\u591a\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u89e3\u51b3\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u7ea6\u675f\u4e0b\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u5173\u952e\u6311\u6218\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "summary_zh": "\u4f4e\u7a7a\u7ecf\u6d4e\u7684\u5feb\u901f\u53d1\u5c55\u63a8\u52a8\u4e86\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u65e5\u76ca\u589e\u957f\u7684\u65e0\u4eba\u673a\u90e8\u7f72\u7ed9\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u4e86\u57ce\u5e02\u7a7a\u57df\u9650\u5236\u548c\u7ecf\u6d4e\u6548\u7387\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u800c\u8fd9\u4e9b\u56e0\u7d20\u5728\u4f4e\u7a7a\u7ecf\u6d4e\u80cc\u666f\u4e0b\u81f3\u5173\u91cd\u8981\u3002\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u88ab\u8ba4\u4e3a\u662f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u4e00\u4e2a\u6709\u5e0c\u671b\u7684\u65b9\u6848\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u5230\u5b66\u4e60\u6548\u7387\u4f4e\u7684\u9650\u5236\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DRL\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u65b0\u578b\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u5b89\u5168\u3001\u5408\u89c4\u548c\u7ecf\u6d4e\u53ef\u884c\u7684\u8def\u5f84\u89c4\u5212\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u6570\u636e\u6536\u96c6\u7387\u3001\u907f\u78b0\u3001\u6210\u529f\u7740\u9646\u3001\u6cd5\u89c4\u9075\u4ece\u6027\u548c\u80fd\u6e90\u6548\u7387\u7b49\u591a\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u89e3\u51b3\u4f4e\u7a7a\u7ecf\u6d4e\u7f51\u7edc\u7ea6\u675f\u4e0b\u7684\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u5173\u952e\u6311\u6218\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.08228", "pdf": "https://arxiv.org/pdf/2506.08228", "abs": "https://arxiv.org/abs/2506.08228", "authors": ["Mustafa Baniodeh", "Kratarth Goel", "Scott Ettinger", "Carlos Fuertes", "Ari Seff", "Tim Shen", "Cole Gulino", "Chenjie Yang", "Ghassen Jerfel", "Dokook Choe", "Rui Wang", "Vinutha Kallem", "Sergio Casas", "Rami Al-Rfou", "Benjamin Sapp", "Dragomir Anguelov"], "title": "Scaling Laws of Motion Forecasting and Planning -- A Technical Report", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "We study the empirical scaling laws of a family of encoder-decoder\nautoregressive transformer models on the task of joint motion forecasting and\nplanning in the autonomous driving domain. Using a 500 thousand hours driving\ndataset, we demonstrate that, similar to language modeling, model performance\nimproves as a power-law function of the total compute budget, and we observe a\nstrong correlation between model training loss and model evaluation metrics.\nMost interestingly, closed-loop metrics also improve with scaling, which has\nimportant implications for the suitability of open-loop metrics for model\ndevelopment and hill climbing. We also study the optimal scaling of the number\nof transformer parameters and the training data size for a training\ncompute-optimal model. We find that as the training compute budget grows,\noptimal scaling requires increasing the model size 1.5x as fast as the dataset\nsize. We also study inference-time compute scaling, where we observe that\nsampling and clustering the output of smaller models makes them competitive\nwith larger models, up to a crossover point beyond which a larger models\nbecomes more inference-compute efficient. Overall, our experimental results\ndemonstrate that optimizing the training and inference-time scaling properties\nof motion forecasting and planning models is a key lever for improving their\nperformance to address a wide variety of driving scenarios. Finally, we briefly\nstudy the utility of training on general logged driving data of other agents to\nimprove the performance of the ego-agent, an important research area to address\nthe scarcity of robotics data for large capacity models training.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e2d\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u7684\u81ea\u56de\u5f52Transformer\u6a21\u578b\u7684\u7ecf\u9a8c\u7f29\u653e\u89c4\u5f8b\u3002\u5b83\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528Transformer\u6a21\u578b\uff08\u4e00\u79cd\u5927\u6a21\u578b\u67b6\u6784\uff09\u8fdb\u884c\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u8bad\u7ec3\u6570\u636e\u5927\u5c0f\u4ee5\u53ca\u8ba1\u7b97\u8d44\u6e90\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["motion forecasting", "motion planning", "transformer models", "scaling laws", "autonomous driving", "large capacity models"]}, "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u4f18\u5316\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u6a21\u578b\u7684\u8bad\u7ec3\u53ca\u63a8\u7406\u7f29\u653e\u662f\u63d0\u5347\u6027\u80fd\u7684\u5173\u952e\uff0c\u4e14\u4f7f\u7528\u901a\u7528\u9a7e\u9a76\u6570\u636e\u8bad\u7ec3\u80fd\u63d0\u5347\u81ea\u6211\u667a\u80fd\u4f53\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e2d\u8054\u5408\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u7684\u6a21\u578b\u6027\u80fd\u6269\u5c55\u89c4\u5f8b\u3002", "method": "\u4f7f\u7528\u4e00\u4e2a\u5305\u542b50\u4e07\u5c0f\u65f6\u9a7e\u9a76\u6570\u636e\u96c6\u7684encoder-decoder\u81ea\u56de\u5f52transformer\u6a21\u578b\u3002", "result": "\u6a21\u578b\u6027\u80fd\u968f\u7740\u603b\u8ba1\u7b97\u9884\u7b97\u7684\u589e\u52a0\u800c\u5448\u5e42\u5f8b\u51fd\u6570\u5173\u7cfb\u63d0\u9ad8\uff0c\u5e76\u4e14\u6a21\u578b\u8bad\u7ec3\u635f\u5931\u548c\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u95ed\u73af\u6307\u6807\u4e5f\u968f\u7740\u7f29\u653e\u800c\u63d0\u9ad8\u3002\u968f\u7740\u8bad\u7ec3\u8ba1\u7b97\u9884\u7b97\u7684\u589e\u957f\uff0c\u6700\u4f73\u7f29\u653e\u9700\u8981\u4ee5\u6570\u636e\u96c6\u5927\u5c0f\u76841.5\u500d\u901f\u5ea6\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u3002\u8f83\u5c0f\u6a21\u578b\u7684\u8f93\u51fa\u8fdb\u884c\u91c7\u6837\u548c\u805a\u7c7b\u4f7f\u5176\u4e0e\u8f83\u5927\u7684\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u76f4\u5230\u8d85\u8fc7\u4ea4\u53c9\u70b9\uff0c\u8f83\u5927\u7684\u6a21\u578b\u53d8\u5f97\u66f4\u6709\u6548\u7387\u3002", "conclusion": "\u4f18\u5316\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u7279\u6027\u662f\u63d0\u9ad8\u5176\u6027\u80fd\u4ee5\u89e3\u51b3\u5404\u79cd\u9a7e\u9a76\u573a\u666f\u7684\u5173\u952e\u624b\u6bb5\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u901a\u7528\u65e5\u5fd7\u9a7e\u9a76\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u53ef\u4ee5\u63d0\u9ad8\u81ea\u6211\u667a\u80fd\u4f53\u7684\u6027\u80fd\u3002", "summary_zh": "\u6211\u4eec\u7814\u7a76\u4e86\u4e00\u7cfb\u5217encoder-decoder\u81ea\u56de\u5f52transformer\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u8054\u5408\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u7684\u7ecf\u9a8c\u7f29\u653e\u89c4\u5f8b\u3002\u4f7f\u7528\u4e00\u4e2a\u5305\u542b50\u4e07\u5c0f\u65f6\u9a7e\u9a76\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\uff0c\u4e0e\u8bed\u8a00\u5efa\u6a21\u7c7b\u4f3c\uff0c\u6a21\u578b\u6027\u80fd\u968f\u7740\u603b\u8ba1\u7b97\u9884\u7b97\u7684\u589e\u52a0\u800c\u5448\u5e42\u5f8b\u51fd\u6570\u5173\u7cfb\u63d0\u9ad8\uff0c\u5e76\u4e14\u6211\u4eec\u89c2\u5bdf\u5230\u6a21\u578b\u8bad\u7ec3\u635f\u5931\u548c\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u4e4b\u95f4\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u6700\u6709\u8da3\u7684\u662f\uff0c\u95ed\u73af\u6307\u6807\u4e5f\u968f\u7740\u7f29\u653e\u800c\u63d0\u9ad8\uff0c\u8fd9\u5bf9\u4e8e\u5f00\u653e\u73af\u8def\u6307\u6807\u5728\u6a21\u578b\u5f00\u53d1\u548c\u722c\u5761\u4e2d\u7684\u9002\u7528\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86transformer\u53c2\u6570\u6570\u91cf\u548c\u8bad\u7ec3\u6570\u636e\u5927\u5c0f\u5bf9\u4e8e\u8bad\u7ec3\u8ba1\u7b97\u6700\u4f73\u6a21\u578b\u7684\u6700\u4f73\u7f29\u653e\u6bd4\u4f8b\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u968f\u7740\u8bad\u7ec3\u8ba1\u7b97\u9884\u7b97\u7684\u589e\u957f\uff0c\u6700\u4f73\u7f29\u653e\u9700\u8981\u4ee5\u6570\u636e\u96c6\u5927\u5c0f\u76841.5\u500d\u901f\u5ea6\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u63a8\u7406\u65f6\u8ba1\u7b97\u7f29\u653e\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u5bf9\u8f83\u5c0f\u6a21\u578b\u7684\u8f93\u51fa\u8fdb\u884c\u91c7\u6837\u548c\u805a\u7c7b\u4f7f\u5176\u4e0e\u8f83\u5927\u7684\u6a21\u578b\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u76f4\u5230\u8d85\u8fc7\u4ea4\u53c9\u70b9\uff0c\u8f83\u5927\u7684\u6a21\u578b\u53d8\u5f97\u66f4\u6709\u6548\u7387\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u7279\u6027\u662f\u63d0\u9ad8\u5176\u6027\u80fd\u4ee5\u89e3\u51b3\u5404\u79cd\u9a7e\u9a76\u573a\u666f\u7684\u5173\u952e\u624b\u6bb5\u3002\u6700\u540e\uff0c\u6211\u4eec\u7b80\u8981\u5730\u7814\u7a76\u4e86\u4f7f\u7528\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u901a\u7528\u65e5\u5fd7\u9a7e\u9a76\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u4ee5\u63d0\u9ad8\u81ea\u6211\u667a\u80fd\u4f53\u7684\u6027\u80fd\u7684\u6548\u7528\uff0c\u8fd9\u662f\u89e3\u51b3\u7528\u4e8e\u5927\u5bb9\u91cf\u6a21\u578b\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u6570\u636e\u7a00\u7f3a\u6027\u7684\u4e00\u4e2a\u91cd\u8981\u7814\u7a76\u9886\u57df\u3002"}}
{"id": "2506.08052", "pdf": "https://arxiv.org/pdf/2506.08052", "abs": "https://arxiv.org/abs/2506.08052", "authors": ["Yongkang Li", "Kaixin Xiong", "Xiangyu Guo", "Fang Li", "Sixu Yan", "Gangwei Xu", "Lijun Zhou", "Long Chen", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "ReCogDrive: A Reinforced Cognitive Framework for End-to-End Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Although end-to-end autonomous driving has made remarkable progress, its\nperformance degrades significantly in rare and long-tail scenarios. Recent\napproaches attempt to address this challenge by leveraging the rich world\nknowledge of Vision-Language Models (VLMs), but these methods suffer from\nseveral limitations: (1) a significant domain gap between the pre-training data\nof VLMs and real-world driving data, (2) a dimensionality mismatch between the\ndiscrete language space and the continuous action space, and (3) imitation\nlearning tends to capture the average behavior present in the dataset, which\nmay be suboptimal even dangerous. In this paper, we propose ReCogDrive, an\nautonomous driving system that integrates VLMs with diffusion planner, which\nadopts a three-stage paradigm for training. In the first stage, we use a\nlarge-scale driving question-answering datasets to train the VLMs, mitigating\nthe domain discrepancy between generic content and real-world driving\nscenarios. In the second stage, we employ a diffusion-based planner to perform\nimitation learning, mapping representations from the latent language space to\ncontinuous driving actions. Finally, we fine-tune the diffusion planner using\nreinforcement learning with NAVSIM non-reactive simulator, enabling the model\nto generate safer, more human-like driving trajectories. We evaluate our\napproach on the planning-oriented NAVSIM benchmark, achieving a PDMS of 89.6\nand setting a new state-of-the-art that surpasses the previous vision-only SOTA\nby 5.6 PDMS.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on autonomous driving, which inherently involves trajectory prediction. It utilizes Vision-Language Models (VLMs), which are a type of large model, and a diffusion planner to generate driving trajectories. The use of VLMs and the focus on generating trajectories for autonomous driving makes it relevant to both trajectory prediction and large language models.", "keywords": ["trajectory prediction", "autonomous driving", "Vision-Language Models", "VLMs", "diffusion planner", "reinforcement learning", "driving trajectories"]}, "AI": {"tldr": "ReCogDrive\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u6269\u6563\u89c4\u5212\u5668\uff0c\u5e76\u5728\u4e09\u9636\u6bb5\u8bad\u7ec3\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u5728\u957f\u5c3e\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0e\u5b9e\u9645\u9a7e\u9a76\u6570\u636e\u5b58\u5728\u9886\u57df\u5dee\u5f02\uff1b\u79bb\u6563\u8bed\u8a00\u7a7a\u95f4\u4e0e\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u5b58\u5728\u7ef4\u5ea6\u4e0d\u5339\u914d\uff1b\u6a21\u4eff\u5b66\u4e60\u503e\u5411\u4e8e\u6355\u6349\u6b21\u4f18\u884c\u4e3a\u3002", "method": "ReCogDrive\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5305\u62ec\uff1a\u4f7f\u7528\u9a7e\u9a76\u95ee\u7b54\u6570\u636e\u96c6\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\uff0c\u4ee5\u53ca\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6269\u6563\u89c4\u5212\u5668\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cReCogDrive\u7684PDMS\u8fbe\u523089.6\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u4f73\u89c6\u89c9\u65b9\u68485.6 PDMS\u3002", "conclusion": "\u901a\u8fc7\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u8bc4\u4f30\uff0cReCogDrive\u8fbe\u5230\u4e8689.6\u7684PDMS\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u7684\u89c6\u89c9SOTA 5.6 PDMS\uff0c\u786e\u7acb\u4e86\u65b0\u7684\u6280\u672f\u6c34\u5e73\u3002", "summary_zh": "\u5c3d\u7ba1\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u7f55\u89c1\u548c\u957f\u5c3e\u573a\u666f\u4e2d\uff0c\u5176\u6027\u80fd\u4f1a\u663e\u8457\u4e0b\u964d\u3002\u6700\u8fd1\u7684\u65b9\u6cd5\u8bd5\u56fe\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u4e30\u5bcc\u7684\u4e16\u754c\u77e5\u8bc6\u6765\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u51e0\u4e2a\u5c40\u9650\u6027\uff1a\uff081\uff09VLM\u7684\u9884\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u9a7e\u9a76\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u9886\u57df\u5dee\u8ddd\uff1b\uff082\uff09\u79bb\u6563\u8bed\u8a00\u7a7a\u95f4\u4e0e\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e4b\u95f4\u5b58\u5728\u7ef4\u5ea6\u4e0d\u5339\u914d\uff1b\uff083\uff09\u6a21\u4eff\u5b66\u4e60\u503e\u5411\u4e8e\u6355\u6349\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u5e73\u5747\u884c\u4e3a\uff0c\u8fd9\u53ef\u80fd\u5e76\u975e\u6700\u4f18\u751a\u81f3\u5371\u9669\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86ReCogDrive\uff0c\u4e00\u79cd\u96c6\u6210\u4e86VLM\u4e0e\u6269\u6563\u89c4\u5212\u5668\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u6211\u4eec\u4f7f\u7528\u5927\u89c4\u6a21\u9a7e\u9a76\u95ee\u7b54\u6570\u636e\u96c6\u6765\u8bad\u7ec3VLM\uff0c\u4ece\u800c\u7f13\u89e3\u901a\u7528\u5185\u5bb9\u4e0e\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u5f02\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u91c7\u7528\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u5668\u6765\u6267\u884c\u6a21\u4eff\u5b66\u4e60\uff0c\u5c06\u6765\u81ea\u6f5c\u5728\u8bed\u8a00\u7a7a\u95f4\u7684\u8868\u793a\u6620\u5c04\u5230\u8fde\u7eed\u9a7e\u9a76\u52a8\u4f5c\u3002\u6700\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548cNAVSIM\u975e\u53cd\u5e94\u5f0f\u6a21\u62df\u5668\u6765\u5fae\u8c03\u6269\u6563\u89c4\u5212\u5668\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u751f\u6210\u66f4\u5b89\u5168\u3001\u66f4\u50cf\u4eba\u7c7b\u7684\u9a7e\u9a76\u8f68\u8ff9\u3002\u6211\u4eec\u5728\u9762\u5411\u89c4\u5212\u7684NAVSIM\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8689.6\u7684PDMS\uff0c\u5e76\u521b\u9020\u4e86\u8d85\u8d8a\u4e4b\u524d\u4ec5\u89c6\u89c9SOTA 5.6 PDMS\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2506.08541", "pdf": "https://arxiv.org/pdf/2506.08541", "abs": "https://arxiv.org/abs/2506.08541", "authors": ["Qi Yan", "Brian Zhang", "Yutong Zhang", "Daniel Yang", "Joshua White", "Di Chen", "Jiachao Liu", "Langechuan Liu", "Binnan Zhuang", "Shaoshuai Shi", "Renjie Liao"], "title": "TrajFlow: Multi-modal Motion Prediction via Flow Matching", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Efficient and accurate motion prediction is crucial for ensuring safety and\ninformed decision-making in autonomous driving, particularly under dynamic\nreal-world conditions that necessitate multi-modal forecasts. We introduce\nTrajFlow, a novel flow matching-based motion prediction framework that\naddresses the scalability and efficiency challenges of existing generative\ntrajectory prediction methods. Unlike conventional generative approaches that\nemploy i.i.d. sampling and require multiple inference passes to capture diverse\noutcomes, TrajFlow predicts multiple plausible future trajectories in a single\npass, significantly reducing computational overhead while maintaining coherence\nacross predictions. Moreover, we propose a ranking loss based on the\nPlackett-Luce distribution to improve uncertainty estimation of predicted\ntrajectories. Additionally, we design a self-conditioning training technique\nthat reuses the model's own predictions to construct noisy inputs during a\nsecond forward pass, thereby improving generalization and accelerating\ninference. Extensive experiments on the large-scale Waymo Open Motion Dataset\n(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across\nvarious key metrics, underscoring its effectiveness for safety-critical\nautonomous driving applications. The code and other details are available on\nthe project website https://traj-flow.github.io/.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper focuses on trajectory prediction, specifically motion prediction for autonomous driving. While it doesn't directly involve large language models, the core topic of multi-modal motion prediction falls under the broader scope of trajectory prediction. The use of flow matching is a relevant technical approach within this domain.", "keywords": ["trajectory prediction", "motion prediction", "autonomous driving", "multi-modal", "flow matching"]}, "AI": {"tldr": "TrajFlow\u901a\u8fc7\u5355\u6b21\u9884\u6d4b\u591a\u4e2a\u8f68\u8ff9\u3001\u5f15\u5165\u6392\u5e8f\u635f\u5931\u548c\u81ea\u6761\u4ef6\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u9884\u6d4b\u3002", "motivation": "\u5728\u52a8\u6001\u771f\u5b9e\u4e16\u754c\u7684\u6761\u4ef6\u4e0b\uff0c\u9ad8\u6548\u51c6\u786e\u7684\u8fd0\u52a8\u9884\u6d4b\u5bf9\u4e8e\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u660e\u667a\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u591a\u6a21\u5f0f\u9884\u6d4b\u7684\u60c5\u51b5\u4e0b\u3002", "method": "TrajFlow\u9884\u6d4b\u591a\u4e2a\u5408\u7406\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u5e76\u5728Plackett-Luce\u5206\u5e03\u7684\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u6392\u5e8f\u635f\u5931\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u6761\u4ef6\u8bad\u7ec3\u6280\u672f\u3002", "result": "TrajFlow\u5728Waymo Open Motion Dataset (WOMD) \u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5404\u79cd\u5173\u952e\u6307\u6807\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TrajFlow\u5728Waymo Open Motion Dataset\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "summary_zh": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u9ad8\u6548\u51c6\u786e\u7684\u8fd0\u52a8\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u591a\u6a21\u6001\u9884\u6d4b\u7684\u52a8\u6001\u771f\u5b9e\u573a\u666f\u4e0b\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u8fd0\u52a8\u9884\u6d4b\u6846\u67b6TrajFlow\uff0c\u5b83\u89e3\u51b3\u4e86\u73b0\u6709\u751f\u6210\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u6311\u6218\u3002\u4e0e\u4f20\u7edf\u7684\u751f\u6210\u65b9\u6cd5\u4e0d\u540c\uff0cTrajFlow\u901a\u8fc7\u5355\u6b21\u9884\u6d4b\u591a\u4e2a\u5408\u7406\u7684\u672a\u6765\u8f68\u8ff9\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9884\u6d4b\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePlackett-Luce\u5206\u5e03\u7684\u6392\u5e8f\u635f\u5931\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u8f68\u8ff9\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u6761\u4ef6\u8bad\u7ec3\u6280\u672f\uff0c\u8be5\u6280\u672f\u5728\u7b2c\u4e8c\u6b21\u524d\u5411\u4f20\u64ad\u671f\u95f4\u91cd\u590d\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u7684\u9884\u6d4b\u6765\u6784\u5efa\u566a\u58f0\u8f93\u5165\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u5e76\u52a0\u901f\u63a8\u7406\u3002\u5728Waymo Open Motion Dataset (WOMD) \u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTrajFlow\u5728\u5404\u79cd\u5173\u952e\u6307\u6807\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u7a81\u663e\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u578b\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.09042", "pdf": "https://arxiv.org/pdf/2506.09042", "abs": "https://arxiv.org/abs/2506.09042", "authors": ["Xuanchi Ren", "Yifan Lu", "Tianshi Cao", "Ruiyuan Gao", "Shengyu Huang", "Amirmojtaba Sabour", "Tianchang Shen", "Tobias Pfaff", "Jay Zhangjie Wu", "Runjian Chen", "Seung Wook Kim", "Jun Gao", "Laura Leal-Taixe", "Mike Chen", "Sanja Fidler", "Huan Ling"], "title": "Cosmos-Drive-Dreams: Scalable Synthetic Driving Data Generation with World Foundation Models", "categories": ["cs.CV"], "comment": "Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao: Equal contribution.\n  Only the core contributors are listed. The full list of contributors can be\n  found in Appendix A of this paper", "summary": "Collecting and annotating real-world data for safety-critical physical AI\nsystems, such as Autonomous Vehicle (AV), is time-consuming and costly. It is\nespecially challenging to capture rare edge cases, which play a critical role\nin training and testing of an AV system. To address this challenge, we\nintroduce the Cosmos-Drive-Dreams - a synthetic data generation (SDG) pipeline\nthat aims to generate challenging scenarios to facilitate downstream tasks such\nas perception and driving policy training. Powering this pipeline is\nCosmos-Drive, a suite of models specialized from NVIDIA Cosmos world foundation\nmodel for the driving domain and are capable of controllable, high-fidelity,\nmulti-view, and spatiotemporally consistent driving video generation. We\nshowcase the utility of these models by applying Cosmos-Drive-Dreams to scale\nthe quantity and diversity of driving datasets with high-fidelity and\nchallenging scenarios. Experimentally, we demonstrate that our generated data\nhelps in mitigating long-tail distribution problems and enhances generalization\nin downstream tasks such as 3D lane detection, 3D object detection and driving\npolicy learning. We open source our pipeline toolkit, dataset and model weights\nthrough the NVIDIA's Cosmos platform.\n  Project page: https://research.nvidia.com/labs/toronto-ai/cosmos_drive_dreams", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e16\u754c\u57fa\u7840\u6a21\u578b\uff08World Foundation Models\uff09\u751f\u6210\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\u7684\u5408\u6210\u6570\u636e\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u6d4b\u8bd5\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u5305\u62ec\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\uff08driving policy learning\uff09\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8bba\u6587\u91cd\u70b9\u5728\u4e8e\u4f7f\u7528\u5927\u89c4\u6a21AI\u6a21\u578b\uff08\u4e16\u754c\u57fa\u7840\u6a21\u578b\uff09\u6765\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u95ee\u9898\u3002", "keywords": ["foundation models", "driving policy learning", "autonomous vehicle", "synthetic data generation"]}, "AI": {"tldr": "Cosmos-Drive-Dreams \u662f\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u65e8\u5728\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u9a7e\u9a76\u573a\u666f\uff0c\u4ee5\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66 (AV) \u7b49\u5b89\u5168\u5173\u952e\u7269\u7406 AI \u7cfb\u7edf\u6536\u96c6\u548c\u6807\u6ce8\u771f\u5b9e\u6570\u636e\u65e2\u8017\u65f6\u53c8\u6602\u8d35\uff0c\u5c24\u5176\u96be\u4ee5\u6355\u83b7\u7a00\u6709\u8fb9\u7f18\u6848\u4f8b\uff0c\u800c\u8fd9\u4e9b\u6848\u4f8b\u5728 AV \u7cfb\u7edf\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002", "method": "Cosmos-Drive-Dreams \u63d0\u51fa\u4e86\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210 (SDG) \u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u4f7f\u7528 NVIDIA Cosmos \u4e16\u754c\u57fa\u7840\u6a21\u578b\u5b9a\u5236\u7684 Cosmos-Drive \u6a21\u578b\u5957\u4ef6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u3001\u591a\u89c6\u89d2\u548c\u65f6\u7a7a\u4e00\u81f4\u7684\u9a7e\u9a76\u89c6\u9891\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCosmos-Drive-Dreams \u751f\u6210\u7684\u6570\u636e\u6709\u52a9\u4e8e\u7f13\u89e3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5e76\u589e\u5f3a 3D \u8f66\u9053\u68c0\u6d4b\u30013D \u76ee\u6807\u68c0\u6d4b\u548c\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5408\u6210\u6570\u636e\uff0cCosmos-Drive-Dreams \u7f13\u89e3\u4e86\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5e76\u589e\u5f3a\u4e86 3D \u8f66\u9053\u68c0\u6d4b\u30013D \u76ee\u6807\u68c0\u6d4b\u548c\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "summary_zh": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66 (AV) \u7b49\u5b89\u5168\u5173\u952e\u7269\u7406 AI \u7cfb\u7edf\u6536\u96c6\u548c\u6807\u6ce8\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u65e2\u8017\u65f6\u53c8\u6602\u8d35\u3002\u6355\u83b7\u7f55\u89c1\u7684\u8fb9\u7f18\u60c5\u51b5\u5c24\u5176\u5177\u6709\u6311\u6218\u6027\uff0c\u8fd9\u4e9b\u60c5\u51b5\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u6311\u6218\uff0c\u6211\u4eec\u63a8\u51fa\u4e86 Cosmos-Drive-Dreams\u2014\u2014\u4e00\u4e2a\u5408\u6210\u6570\u636e\u751f\u6210 (SDG) \u6d41\u7a0b\uff0c\u65e8\u5728\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\uff0c\u4ee5\u4fc3\u8fdb\u611f\u77e5\u548c\u9a7e\u9a76\u7b56\u7565\u8bad\u7ec3\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002Cosmos-Drive \u4e3a\u8be5\u6d41\u7a0b\u63d0\u4f9b\u652f\u6301\uff0c\u5b83\u662f\u4e00\u5957\u4e13\u95e8\u4ece NVIDIA Cosmos \u4e16\u754c\u57fa\u7840\u6a21\u578b\u4e3a\u9a7e\u9a76\u9886\u57df\u5b9a\u5236\u7684\u6a21\u578b\uff0c\u80fd\u591f\u751f\u6210\u53ef\u63a7\u3001\u9ad8\u4fdd\u771f\u3001\u591a\u89c6\u89d2\u548c\u65f6\u7a7a\u4e00\u81f4\u7684\u9a7e\u9a76\u89c6\u9891\u3002\u6211\u4eec\u901a\u8fc7\u5e94\u7528 Cosmos-Drive-Dreams \u6765\u6269\u5c55\u5177\u6709\u9ad8\u4fdd\u771f\u548c\u6311\u6218\u6027\u573a\u666f\u7684\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u6570\u91cf\u548c\u591a\u6837\u6027\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u7684\u6548\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u751f\u6210\u7684\u6570\u636e\u6709\u52a9\u4e8e\u7f13\u89e3\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u5e76\u589e\u5f3a 3D \u8f66\u9053\u68c0\u6d4b\u30013D \u76ee\u6807\u68c0\u6d4b\u548c\u9a7e\u9a76\u7b56\u7565\u5b66\u4e60\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6211\u4eec\u901a\u8fc7 NVIDIA \u7684 Cosmos \u5e73\u53f0\u5f00\u6e90\u4e86\u6211\u4eec\u7684\u6d41\u7a0b\u5de5\u5177\u5305\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u6743\u91cd\u3002"}}
{"id": "2506.08149", "pdf": "https://arxiv.org/pdf/2506.08149", "abs": "https://arxiv.org/abs/2506.08149", "authors": ["Hang Wang", "Dechen Gao", "Junshan Zhang"], "title": "Ego-centric Learning of Communicative World Models for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "We study multi-agent reinforcement learning (MARL) for tasks in complex\nhigh-dimensional environments, such as autonomous driving. MARL is known to\nsuffer from the \\textit{partial observability} and \\textit{non-stationarity}\nissues. To tackle these challenges, information sharing is often employed,\nwhich however faces major hurdles in practice, including overwhelming\ncommunication overhead and scalability concerns. By making use of generative AI\nembodied in world model together with its latent representation, we develop\n{\\it CALL}, \\underline{C}ommunic\\underline{a}tive Wor\\underline{l}d\nMode\\underline{l}, for MARL, where 1) each agent first learns its world model\nthat encodes its state and intention into low-dimensional latent representation\nwith smaller memory footprint, which can be shared with other agents of\ninterest via lightweight communication; and 2) each agent carries out\nego-centric learning while exploiting lightweight information sharing to enrich\nher world model, and then exploits its generalization capacity to improve\nprediction for better planning. We characterize the gain on the prediction\naccuracy from the information sharing and its impact on performance gap.\nExtensive experiments are carried out on the challenging local trajectory\nplanning tasks in the CARLA platform to demonstrate the performance gains of\nusing \\textit{CALL}.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on multi-agent reinforcement learning for autonomous driving, specifically addressing trajectory planning. While it doesn't explicitly mention large language models, it utilizes a 'world model' with generative AI and latent representations for prediction, which shares some conceptual similarities with large models. The connection to trajectory prediction is strong, but the connection to large language models is more indirect, hence the score.", "keywords": ["trajectory planning", "autonomous driving", "world model", "multi-agent reinforcement learning", "prediction"]}, "AI": {"tldr": "CALL\uff1a\u4e00\u79cd\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u901a\u4fe1\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4fe1\u606f\u5171\u4eab\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u590d\u6742\u9ad8\u7ef4\u73af\u5883\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u9762\u4e34\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\uff0c\u4fe1\u606f\u5171\u4eab\u53d7\u5230\u901a\u4fe1\u5f00\u9500\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u7684\u963b\u788d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCALL\u7684MARL\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e16\u754c\u6a21\u578b\u53ca\u5176\u6f5c\u5728\u8868\u793a\u8fdb\u884c\u901a\u4fe1\u3002", "result": "\u901a\u8fc7\u4fe1\u606f\u5171\u4eab\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u7f29\u5c0f\u4e86\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "\u5728CARLA\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528CALL\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002", "summary_zh": "\u6211\u4eec\u7814\u7a76\u4e86\u590d\u6742\u9ad8\u7ef4\u73af\u5883\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\uff09\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u3002\u4f17\u6240\u5468\u77e5\uff0cMARL \u5b58\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u975e\u5e73\u7a33\u6027\u95ee\u9898\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u901a\u5e38\u91c7\u7528\u4fe1\u606f\u5171\u4eab\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u9762\u4e34\u4e3b\u8981\u969c\u788d\uff0c\u5305\u62ec\u5de8\u5927\u7684\u901a\u4fe1\u5f00\u9500\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002\u901a\u8fc7\u5229\u7528\u4e16\u754c\u6a21\u578b\u4e2d\u5305\u542b\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u53ca\u5176\u6f5c\u5728\u8868\u793a\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u7528\u4e8e MARL \u7684\u901a\u4fe1\u4e16\u754c\u6a21\u578b CALL\uff0c\u5176\u4e2d 1) \u6bcf\u4e2a\u667a\u80fd\u4f53\u9996\u5148\u5b66\u4e60\u5176\u4e16\u754c\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u5176\u72b6\u6001\u548c\u610f\u56fe\u7f16\u7801\u4e3a\u5177\u6709\u8f83\u5c0f\u5185\u5b58\u5360\u7528\u7684\u4f4e\u7ef4\u6f5c\u5728\u8868\u793a\uff0c\u53ef\u4ee5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u901a\u4fe1\u4e0e\u611f\u5174\u8da3\u7684\u5176\u4ed6\u667a\u80fd\u4f53\u5171\u4eab\uff1b2) \u6bcf\u4e2a\u667a\u80fd\u4f53\u5728\u8fdb\u884c\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u5b66\u4e60\u65f6\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u4fe1\u606f\u5171\u4eab\u6765\u4e30\u5bcc\u5176\u4e16\u754c\u6a21\u578b\uff0c\u7136\u540e\u5229\u7528\u5176\u6cdb\u5316\u80fd\u529b\u6765\u6539\u8fdb\u9884\u6d4b\uff0c\u4ece\u800c\u66f4\u597d\u5730\u8fdb\u884c\u89c4\u5212\u3002\u6211\u4eec\u63cf\u8ff0\u4e86\u4fe1\u606f\u5171\u4eab\u5bf9\u9884\u6d4b\u7cbe\u5ea6\u7684\u63d0\u5347\u53ca\u5176\u5bf9\u6027\u80fd\u5dee\u8ddd\u7684\u5f71\u54cd\u3002\u5728 CARLA \u5e73\u53f0\u4e0a\u5177\u6709\u6311\u6218\u6027\u7684\u672c\u5730\u8f68\u8ff9\u89c4\u5212\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u4ee5\u8bc1\u660e\u4f7f\u7528 CALL \u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2506.08021", "pdf": "https://arxiv.org/pdf/2506.08021", "abs": "https://arxiv.org/abs/2506.08021", "authors": ["Weihao Zou", "Weibing Feng", "Pin Wu"], "title": "FlowBERT: Prompt-tuned BERT for variable flow field prediction", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "This study proposes a universal flow field prediction framework based on\nknowledge transfer\n  from large language model (LLM), addressing the high computational costs of\ntraditional\n  computational fluid dynamics (CFD) methods and the limited cross-condition\ntransfer capability\n  of existing deep learning models. The framework innovatively integrates\nProper Orthogonal\n  Decomposition (POD) dimensionality reduction with fine-tuning strategies for\npretrained LLM,\n  where POD facilitates compressed representation of flow field features while\nthe fine-tuned model\n  learns to encode system dynamics in state space. To enhance the model's\nadaptability to flow field\n  data, we specifically designed fluid dynamics-oriented text templates that\nimprove predictive\n  performance through enriched contextual semantic information. Experimental\nresults demonstrate\n  that our framework outperforms conventional Transformer models in few-shot\nlearning scenarios while\n  exhibiting exceptional generalization across various inflow conditions and\nairfoil geometries.\n  Ablation studies reveal the contributions of key components in the FlowBERT\narchitecture. Compared\n  to traditional Navier-Stokes equation solvers requiring hours of computation,\nour approach reduces\n  prediction time to seconds while maintaining over 90% accuracy. The developed\nknowledge transfer\n  paradigm establishes a new direction for rapid fluid dynamics prediction,\nwith potential\n  applications extending to aerodynamic optimization, flow control, and other\nengineering domains.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper uses a large language model (BERT) for flow field prediction, which, while not directly trajectory prediction, shares similarities in predicting dynamic systems. The use of LLMs and the prediction of a dynamic field contribute to the relevance.", "keywords": ["Large Language Model", "LLM", "BERT", "Flow field prediction", "knowledge transfer"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u8fc1\u79fb\u7684\u901a\u7528\u6d41\u573a\u9884\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u8fdb\u884c\u5feb\u901f\u6d41\u4f53\u52a8\u529b\u5b66\u9884\u6d4b\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66(CFD)\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6709\u9650\u7684\u8de8\u6761\u4ef6\u8fc1\u79fb\u80fd\u529b\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u521b\u65b0\u6027\u5730\u5c06Proper Orthogonal Decomposition (POD)\u964d\u7ef4\u4e0e\u9884\u8bad\u7ec3LLM\u7684\u5fae\u8c03\u7b56\u7565\u76f8\u7ed3\u5408\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684Transformer\u6a21\u578b\uff0c\u540c\u65f6\u5728\u5404\u79cd\u6d41\u5165\u6761\u4ef6\u548c\u7ffc\u578b\u51e0\u4f55\u5f62\u72b6\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e0e\u9700\u8981\u6570\u5c0f\u65f6\u8ba1\u7b97\u7684\u4f20\u7edfNavier-Stokes\u65b9\u7a0b\u6c42\u89e3\u5668\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u9884\u6d4b\u65f6\u95f4\u7f29\u77ed\u5230\u51e0\u79d2\uff0c\u540c\u65f6\u4fdd\u630190%\u4ee5\u4e0a\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6\u4e3a\u5feb\u901f\u6d41\u4f53\u52a8\u529b\u5b66\u9884\u6d4b\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u5177\u6709\u5e94\u7528\u4e8e\u6c14\u52a8\u4f18\u5316\u3001\u6d41\u52a8\u63a7\u5236\u548c\u5176\u4ed6\u5de5\u7a0b\u9886\u57df\u7684\u6f5c\u529b\u3002", "summary_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u77e5\u8bc6\u8fc1\u79fb\u7684\u901a\u7528\u6d41\u573a\u9884\u6d4b\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66(CFD)\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u6709\u9650\u8de8\u6761\u4ef6\u8fc1\u79fb\u80fd\u529b\u95ee\u9898\u3002\u8be5\u6846\u67b6\u521b\u65b0\u6027\u5730\u5c06Proper Orthogonal Decomposition (POD)\u964d\u7ef4\u4e0e\u9884\u8bad\u7ec3LLM\u7684\u5fae\u8c03\u7b56\u7565\u76f8\u7ed3\u5408\uff0c\u5176\u4e2dPOD\u6709\u52a9\u4e8e\u538b\u7f29\u8868\u793a\u6d41\u573a\u7279\u5f81\uff0c\u800c\u5fae\u8c03\u540e\u7684\u6a21\u578b\u5b66\u4e60\u5728\u72b6\u6001\u7a7a\u95f4\u4e2d\u7f16\u7801\u7cfb\u7edf\u52a8\u529b\u5b66\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u5bf9\u6d41\u573a\u6570\u636e\u7684\u9002\u5e94\u6027\uff0c\u6211\u4eec\u4e13\u95e8\u8bbe\u8ba1\u4e86\u9762\u5411\u6d41\u4f53\u52a8\u529b\u5b66\u7684\u6587\u672c\u6a21\u677f\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u8bed\u4e49\u4fe1\u606f\u6765\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edf\u7684Transformer\u6a21\u578b\uff0c\u540c\u65f6\u5728\u5404\u79cd\u6d41\u5165\u6761\u4ef6\u548c\u7ffc\u578b\u51e0\u4f55\u5f62\u72b6\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86FlowBERT\u67b6\u6784\u4e2d\u5173\u952e\u7ec4\u4ef6\u7684\u8d21\u732e\u3002\u4e0e\u9700\u8981\u6570\u5c0f\u65f6\u8ba1\u7b97\u7684\u4f20\u7edfNavier-Stokes\u65b9\u7a0b\u6c42\u89e3\u5668\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u9884\u6d4b\u65f6\u95f4\u7f29\u77ed\u5230\u51e0\u79d2\uff0c\u540c\u65f6\u4fdd\u630190%\u4ee5\u4e0a\u7684\u7cbe\u5ea6\u3002\u6240\u5f00\u53d1\u7684\u77e5\u8bc6\u8f6c\u79fb\u8303\u5f0f\u4e3a\u5feb\u901f\u6d41\u4f53\u52a8\u529b\u5b66\u9884\u6d4b\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u65b9\u5411\uff0c\u5176\u6f5c\u5728\u5e94\u7528\u53ef\u6269\u5c55\u5230\u6c14\u52a8\u4f18\u5316\u3001\u6d41\u52a8\u63a7\u5236\u548c\u5176\u4ed6\u5de5\u7a0b\u9886\u57df\u3002"}}
{"id": "2506.08957", "pdf": "https://arxiv.org/pdf/2506.08957", "abs": "https://arxiv.org/abs/2506.08957", "authors": ["Yash Ranjan", "Rahul Sengupta", "Anand Rangarajan", "Sanjay Ranka"], "title": "IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Traffic simulators are widely used to study the operational efficiency of\nroad infrastructure, but their rule-based approach limits their ability to\nmimic real-world driving behavior. Traffic intersections are critical\ncomponents of the road infrastructure, both in terms of safety risk (nearly 28%\nof fatal crashes and 58% of nonfatal crashes happen at intersections) as well\nas the operational efficiency of a road corridor. This raises an important\nquestion: can we create a data-driven simulator that can mimic the macro- and\nmicro-statistics of the driving behavior at a traffic intersection? Deep\nGenerative Modeling-based trajectory prediction models provide a good starting\npoint to model the complex dynamics of vehicles at an intersection. But they\nare not tested in a \"live\" micro-simulation scenario and are not evaluated on\ntraffic engineering-related metrics. In this study, we propose traffic\nengineering-related metrics to evaluate generative trajectory prediction models\nand provide a simulation-in-the-loop pipeline to do so. We also provide a\nmulti-headed self-attention-based trajectory prediction model that incorporates\nthe signal information, which outperforms our previous models on the evaluation\nmetrics.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4e8e\u4ea4\u901a\u8def\u53e3\u7684\u591a\u8f66\u8f86\u9a7e\u9a76\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u4e86\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u8f68\u8ff9\u9884\u6d4b\u662f\u5176\u6838\u5fc3\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u56e0\u6b64\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "traffic simulation", "multi-vehicle driving", "self-attention"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u901a\u4eff\u771f\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u66f4\u4f18\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6a21\u62df\u8def\u53e3\u4ea4\u901a\u884c\u4e3a\u3002", "motivation": "\u4ea4\u901a\u4eff\u771f\u5668\u5728\u7814\u7a76\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u7684\u8fd0\u8425\u6548\u7387\u65b9\u9762\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u9650\u5236\u4e86\u5b83\u4eec\u6a21\u4eff\u771f\u5b9e\u9a7e\u9a76\u884c\u4e3a\u7684\u80fd\u529b\u3002\u4ea4\u901a\u8def\u53e3\u662f\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u65e0\u8bba\u662f\u5728\u5b89\u5168\u98ce\u9669\uff08\u8fd1 28% \u7684\u81f4\u547d\u78b0\u649e\u548c 58% \u7684\u975e\u81f4\u547d\u78b0\u649e\u53d1\u751f\u5728\u8def\u53e3\uff09\u8fd8\u662f\u9053\u8def\u8d70\u5eca\u7684\u8fd0\u8425\u6548\u7387\u65b9\u9762\u3002\u8fd9\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u95ee\u9898\uff1a\u6211\u4eec\u80fd\u5426\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u4eff\u771f\u5668\uff0c\u53ef\u4ee5\u6a21\u4eff\u4ea4\u901a\u8def\u53e3\u9a7e\u9a76\u884c\u4e3a\u7684\u5b8f\u89c2\u548c\u5fae\u89c2\u7edf\u8ba1\u6570\u636e\uff1f", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u878d\u5408\u4e86\u4fe1\u53f7\u4fe1\u606f\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4ea4\u901a\u5de5\u7a0b\u76f8\u5173\u6307\u6807\u4e0a\u4f18\u4e8e\u4ee5\u5f80\u6a21\u578b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u4ea4\u901a\u5de5\u7a0b\u76f8\u5173\u6307\u6807\u4e0a\u4f18\u4e8e\u4ee5\u5f80\u6a21\u578b\u3002", "summary_zh": "\u4ea4\u901a\u4eff\u771f\u5668\u88ab\u5e7f\u6cdb\u7528\u4e8e\u7814\u7a76\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u7684\u8fd0\u8425\u6548\u7387\uff0c\u4f46\u5b83\u4eec\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u9650\u5236\u4e86\u5b83\u4eec\u6a21\u4eff\u771f\u5b9e\u9a7e\u9a76\u884c\u4e3a\u7684\u80fd\u529b\u3002\u4ea4\u901a\u8def\u53e3\u662f\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u65e0\u8bba\u662f\u5728\u5b89\u5168\u98ce\u9669\uff08\u8fd1 28% \u7684\u81f4\u547d\u78b0\u649e\u548c 58% \u7684\u975e\u81f4\u547d\u78b0\u649e\u53d1\u751f\u5728\u8def\u53e3\uff09\u8fd8\u662f\u9053\u8def\u8d70\u5eca\u7684\u8fd0\u8425\u6548\u7387\u65b9\u9762\u3002\u8fd9\u5c31\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cd\u8981\u7684\u95ee\u9898\uff1a\u6211\u4eec\u80fd\u5426\u521b\u5efa\u4e00\u4e2a\u6570\u636e\u9a71\u52a8\u7684\u4eff\u771f\u5668\uff0c\u53ef\u4ee5\u6a21\u4eff\u4ea4\u901a\u8def\u53e3\u9a7e\u9a76\u884c\u4e3a\u7684\u5b8f\u89c2\u548c\u5fae\u89c2\u7edf\u8ba1\u6570\u636e\uff1f\u57fa\u4e8e\u6df1\u5ea6\u751f\u6210\u5efa\u6a21\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u4e3a\u6a21\u62df\u8def\u53e3\u8f66\u8f86\u7684\u590d\u6742\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u4e2a\u826f\u597d\u7684\u8d77\u70b9\u3002\u4f46\u5b83\u4eec\u6ca1\u6709\u5728\u201c\u5b9e\u65f6\u201d\u5fae\u89c2\u4eff\u771f\u573a\u666f\u4e2d\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4e5f\u6ca1\u6709\u5728\u4ea4\u901a\u5de5\u7a0b\u76f8\u5173\u7684\u6307\u6807\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u5728\u8fd9\u9879\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4ea4\u901a\u5de5\u7a0b\u76f8\u5173\u7684\u6307\u6807\u6765\u8bc4\u4f30\u751f\u6210\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a simulation-in-the-loop \u7ba1\u9053\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u878d\u5408\u4e86\u4fe1\u53f7\u4fe1\u606f\uff0c\u5e76\u5728\u8bc4\u4f30\u6307\u6807\u4e0a\u4f18\u4e8e\u6211\u4eec\u4e4b\u524d\u7684\u6a21\u578b\u3002"}}
{"id": "2506.08694", "pdf": "https://arxiv.org/pdf/2506.08694", "abs": "https://arxiv.org/abs/2506.08694", "authors": ["Mohammadreza Salehi", "Shashanka Venkataramanan", "Ioana Simion", "Efstratios Gavves", "Cees G. M. Snoek", "Yuki M Asano"], "title": "MoSiC: Optimal-Transport Motion Trajectory for Dense Self-Supervised Learning", "categories": ["cs.CV"], "comment": "preprint", "summary": "Dense self-supervised learning has shown great promise for learning pixel-\nand patch-level representations, but extending it to videos remains challenging\ndue to the complexity of motion dynamics. Existing approaches struggle as they\nrely on static augmentations that fail under object deformations, occlusions,\nand camera movement, leading to inconsistent feature learning over time. We\npropose a motion-guided self-supervised learning framework that clusters dense\npoint tracks to learn spatiotemporally consistent representations. By\nleveraging an off-the-shelf point tracker, we extract long-range motion\ntrajectories and optimize feature clustering through a momentum-encoder-based\noptimal transport mechanism. To ensure temporal coherence, we propagate cluster\nassignments along tracked points, enforcing feature consistency across views\ndespite viewpoint changes. Integrating motion as an implicit supervisory\nsignal, our method learns representations that generalize across frames,\nimproving robustness in dynamic scenes and challenging occlusion scenarios. By\ninitializing from strong image-pretrained models and leveraging video data for\ntraining, we improve state-of-the-art by 1% to 6% on six image and video\ndatasets and four evaluation benchmarks. The implementation is publicly\navailable at our GitHub repository: https://github.com/SMSD75/MoSiC/tree/main", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on learning spatiotemporally consistent representations using motion trajectories, specifically point tracks. It uses optimal transport to cluster these trajectories. While it doesn't explicitly use Large Language Models, the use of motion trajectories and the goal of learning representations for video understanding align with the trajectory prediction domain. The initialization from image-pretrained models also hints at leveraging pre-trained knowledge, a common practice with large models, although not necessarily LLMs.", "keywords": ["motion trajectory", "trajectory", "motion dynamics", "point tracks", "self-supervised learning", "video", "representation learning"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u5bc6\u96c6\u70b9\u8f68\u8ff9\u6765\u5b66\u4e60\u65f6\u7a7a\u4e00\u81f4\u7684\u89c6\u9891\u8868\u5f81\u3002", "motivation": "\u73b0\u6709\u7684\u5bc6\u96c6\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u589e\u5f3a\uff0c\u8fd9\u5728\u5bf9\u8c61\u53d8\u5f62\u3001\u906e\u6321\u548c\u76f8\u673a\u8fd0\u52a8\u4e0b\u4f1a\u5931\u6548\uff0c\u5bfc\u81f4\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u4e0d\u4e00\u81f4\u7684\u7279\u5f81\u5b66\u4e60\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5176\u5728\u89c6\u9891\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u805a\u7c7b\u5bc6\u96c6\u70b9\u8f68\u8ff9\u6765\u5b66\u4e60\u65f6\u7a7a\u4e00\u81f4\u7684\u8868\u5f81\u3002\u5229\u7528\u73b0\u6210\u7684\u70b9\u8ddf\u8e2a\u5668\u63d0\u53d6\u957f\u7a0b\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u52a8\u91cf\u7f16\u7801\u5668\u7684\u6700\u4f18\u4f20\u8f93\u673a\u5236\u4f18\u5316\u7279\u5f81\u805a\u7c7b\u3002\u4e3a\u4e86\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u8be5\u65b9\u6cd5\u6cbf\u8ddf\u8e2a\u70b9\u4f20\u64ad\u805a\u7c7b\u5206\u914d\uff0c\u4ece\u800c\u5728\u89c6\u89d2\u53d8\u5316\u65f6\u4fdd\u6301\u8de8\u89c6\u89d2\u7684\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728\u516d\u4e2a\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u96c6\u4ee5\u53ca\u56db\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06state-of-the-art\u7ed3\u679c\u63d0\u9ad8\u4e861%\u52306%\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u8fd0\u52a8\u4fe1\u606f\u4f5c\u4e3a\u9690\u5f0f\u76d1\u7763\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u5b66\u4e60\u5230\u7684\u8868\u5f81\u5728\u5e27\u95f4\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5728\u52a8\u6001\u573a\u666f\u548c\u906e\u6321\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86state-of-the-art\u7684\u7ed3\u679c\u3002", "summary_zh": "\u7a20\u5bc6\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u5b66\u4e60\u50cf\u7d20\u548cpatch\u7ea7\u522b\u7684\u8868\u5f81\u65b9\u9762\u663e\u793a\u51fa\u4e86\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u662f\u7531\u4e8e\u8fd0\u52a8\u52a8\u6001\u7684\u590d\u6742\u6027\uff0c\u5c06\u5176\u6269\u5c55\u5230\u89c6\u9891\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u589e\u5f3a\uff0c\u8fd9\u5728\u5bf9\u8c61\u53d8\u5f62\u3001\u906e\u6321\u548c\u76f8\u673a\u8fd0\u52a8\u4e0b\u4f1a\u5931\u6548\uff0c\u5bfc\u81f4\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u4e0d\u4e00\u81f4\u7684\u7279\u5f81\u5b66\u4e60\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u5f15\u5bfc\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u805a\u7c7b\u5bc6\u96c6\u70b9\u8f68\u8ff9\u6765\u5b66\u4e60\u65f6\u7a7a\u4e00\u81f4\u7684\u8868\u5f81\u3002\u901a\u8fc7\u5229\u7528\u73b0\u6210\u7684\u70b9\u8ddf\u8e2a\u5668\uff0c\u6211\u4eec\u63d0\u53d6\u957f\u7a0b\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u52a8\u91cf\u7f16\u7801\u5668\u7684\u6700\u4f18\u4f20\u8f93\u673a\u5236\u4f18\u5316\u7279\u5f81\u805a\u7c7b\u3002\u4e3a\u4e86\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u6211\u4eec\u6cbf\u8ddf\u8e2a\u70b9\u4f20\u64ad\u805a\u7c7b\u5206\u914d\uff0c\u4ece\u800c\u5728\u89c6\u89d2\u53d8\u5316\u65f6\u4fdd\u6301\u8de8\u89c6\u89d2\u7684\u7279\u5f81\u4e00\u81f4\u6027\u3002\u901a\u8fc7\u6574\u5408\u8fd0\u52a8\u4f5c\u4e3a\u9690\u5f0f\u76d1\u7763\u4fe1\u53f7\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b66\u4e60\u5230\u7684\u8868\u5f81\u5728\u5e27\u95f4\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5728\u52a8\u6001\u573a\u666f\u548c\u906e\u6321\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u4ece\u5f3a\u5927\u7684\u56fe\u50cf\u9884\u8bad\u7ec3\u6a21\u578b\u521d\u59cb\u5316\u5e76\u5229\u7528\u89c6\u9891\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u6211\u4eec\u5728\u516d\u4e2a\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u96c6\u4ee5\u53ca\u56db\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u5c06\u6700\u5148\u8fdb\u6c34\u5e73\u63d0\u9ad8\u4e861%\u52306%\u3002\u8be5\u5b9e\u73b0\u53ef\u5728\u6211\u4eec\u7684GitHub\u5b58\u50a8\u5e93\u4e2d\u516c\u5f00\u83b7\u5f97\uff1ahttps://github.com/SMSD75/MoSiC/tree/main"}}
{"id": "2506.08098", "pdf": "https://arxiv.org/pdf/2506.08098", "abs": "https://arxiv.org/abs/2506.08098", "authors": ["Akash Vishwakarma", "Hojin Lee", "Mohith Suresh", "Priyam Shankar Sharma", "Rahul Vishwakarma", "Sparsh Gupta", "Yuvraj Anupam Chauhan"], "title": "Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph", "categories": ["cs.AI"], "comment": null, "summary": "The emergence of capable large language model (LLM) based agents necessitates\nmemory architectures that transcend mere data storage, enabling continuous\nlearning, nuanced reasoning, and dynamic adaptation. Current memory systems\noften grapple with fundamental limitations in structural flexibility, temporal\nawareness, and the ability to synthesize higher-level insights from raw\ninteraction data. This paper introduces Cognitive Weave, a novel memory\nframework centered around a multi-layered spatio-temporal resonance graph\n(STRG). This graph manages information as semantically rich insight particles\n(IPs), which are dynamically enriched with resonance keys, signifiers, and\nsituational imprints via a dedicated semantic oracle interface (SOI). These IPs\nare interconnected through typed relational strands, forming an evolving\nknowledge tapestry. A key component of Cognitive Weave is the cognitive\nrefinement process, an autonomous mechanism that includes the synthesis of\ninsight aggregates (IAs) condensed, higher-level knowledge structures derived\nfrom identified clusters of related IPs. We present comprehensive experimental\nresults demonstrating Cognitive Weave's marked enhancement over existing\napproaches in long-horizon planning tasks, evolving question-answering\nscenarios, and multi-session dialogue coherence. The system achieves a notable\n34% average improvement in task completion rates and a 42% reduction in mean\nquery latency when compared to state-of-the-art baselines. Furthermore, this\npaper explores the ethical considerations inherent in such advanced memory\nsystems, discusses the implications for long-term memory in LLMs, and outlines\npromising future research trajectories.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bb0\u5fc6\u67b6\u6784\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u63d0\u5230\u4e86\u957f\u671f\u89c4\u5212\uff08long-horizon planning tasks\uff09\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u76f8\u5173\u3002\u8bba\u6587\u91cd\u70b9\u5728\u4e8eLLM\u7684\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u3002", "keywords": ["large language model", "LLMs", "long-horizon planning tasks", "memory architectures"]}, "AI": {"tldr": "\u8ba4\u77e5\u7f16\u7ec7\uff1a\u4e00\u79cd\u57fa\u4e8e\u591a\u5c42\u65f6\u7a7a\u5171\u632f\u56fe\u7684\u65b0\u578b\u8bb0\u5fc6\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\u5728\u7ed3\u6784\u7075\u6d3b\u6027\u3001\u65f6\u95f4\u611f\u77e5\u4ee5\u53ca\u4ece\u539f\u59cb\u4ea4\u4e92\u6570\u636e\u4e2d\u7efc\u5408\u66f4\u9ad8\u5c42\u6b21\u89c1\u89e3\u7684\u80fd\u529b\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bb0\u5fc6\u6846\u67b6\u201c\u8ba4\u77e5\u7f16\u7ec7\u201d\uff0c\u5b83\u56f4\u7ed5\u4e00\u4e2a\u591a\u5c42\u65f6\u7a7a\u5171\u632f\u56fe (STRG) \u6784\u5efa\uff0c\u8be5\u56fe\u5c06\u4fe1\u606f\u7ba1\u7406\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6d1e\u5bdf\u7c92\u5b50 (IP)\uff0c\u5e76\u901a\u8fc7\u4e13\u7528\u7684\u8bed\u4e49\u9884\u8a00\u63a5\u53e3 (SOI) \u52a8\u6001\u5730\u7528\u5171\u632f\u952e\u3001\u6307\u793a\u7b26\u548c\u60c5\u5883\u5370\u8bb0\u4e30\u5bcc\u8fd9\u4e9b\u7c92\u5b50\u3002", "result": "\u5728\u957f\u7a0b\u89c4\u5212\u4efb\u52a1\u4e2d\uff0c\u8ba4\u77e5\u7f16\u7ec7\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u5e73\u5747\u63d0\u9ad8 34%\uff0c\u67e5\u8be2\u5ef6\u8fdf\u51cf\u5c11 42%\u3002", "conclusion": "\u8ba4\u77e5\u7f16\u7ec7\u5728\u957f\u7a0b\u89c4\u5212\u3001\u6f14\u8fdb\u5f0f\u95ee\u7b54\u548c\u591a\u4f1a\u8bdd\u5bf9\u8bdd\u8fde\u8d2f\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u5e73\u5747\u63d0\u9ad8 34%\uff0c\u67e5\u8be2\u5ef6\u8fdf\u51cf\u5c11 42%\u3002", "summary_zh": "\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u667a\u80fd\u4ee3\u7406\u7684\u51fa\u73b0\uff0c\u9700\u8981\u8d85\u8d8a\u5355\u7eaf\u6570\u636e\u5b58\u50a8\u7684\u8bb0\u5fc6\u67b6\u6784\uff0c\u4ee5\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3001\u7ec6\u81f4\u63a8\u7406\u548c\u52a8\u6001\u9002\u5e94\u3002\u5f53\u524d\u7684\u8bb0\u5fc6\u7cfb\u7edf\u5728\u7ed3\u6784\u7075\u6d3b\u6027\u3001\u65f6\u95f4\u611f\u77e5\u4ee5\u53ca\u4ece\u539f\u59cb\u4ea4\u4e92\u6570\u636e\u4e2d\u7efc\u5408\u66f4\u9ad8\u5c42\u6b21\u89c1\u89e3\u7684\u80fd\u529b\u65b9\u9762\u901a\u5e38\u9762\u4e34\u6839\u672c\u6027\u5c40\u9650\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bb0\u5fc6\u6846\u67b6\u201c\u8ba4\u77e5\u7f16\u7ec7\u201d\uff0c\u5b83\u56f4\u7ed5\u4e00\u4e2a\u591a\u5c42\u65f6\u7a7a\u5171\u632f\u56fe (STRG) \u6784\u5efa\u3002\u8be5\u56fe\u5c06\u4fe1\u606f\u7ba1\u7406\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u6d1e\u5bdf\u7c92\u5b50 (IP)\uff0c\u8fd9\u4e9b\u7c92\u5b50\u901a\u8fc7\u4e13\u7528\u7684\u8bed\u4e49\u9884\u8a00\u63a5\u53e3 (SOI) \u52a8\u6001\u5730\u7528\u5171\u632f\u952e\u3001\u6307\u793a\u7b26\u548c\u60c5\u5883\u5370\u8bb0\u4e30\u5bcc\u3002\u8fd9\u4e9b IP \u901a\u8fc7\u7c7b\u578b\u5316\u7684\u5173\u7cfb\u94fe\u76f8\u4e92\u8fde\u63a5\uff0c\u5f62\u6210\u4e00\u4e2a\u4e0d\u65ad\u6f14\u53d8\u7684\u77e5\u8bc6\u4f53\u7cfb\u3002\u8ba4\u77e5\u7f16\u7ec7\u7684\u4e00\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\u662f\u8ba4\u77e5\u6539\u8fdb\u8fc7\u7a0b\uff0c\u8fd9\u662f\u4e00\u79cd\u81ea\u4e3b\u673a\u5236\uff0c\u5305\u62ec\u7efc\u5408\u6d1e\u5bdf\u805a\u5408 (IA)\uff0c\u8fd9\u662f\u4e00\u79cd\u4ece\u5df2\u8bc6\u522b\u7684\u76f8\u5173 IP \u96c6\u7fa4\u4e2d\u63d0\u53d6\u7684\u7cbe\u7b80\u7684\u3001\u66f4\u9ad8\u5c42\u6b21\u7684\u77e5\u8bc6\u7ed3\u6784\u3002\u6211\u4eec\u5c55\u793a\u4e86\u5168\u9762\u7684\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u8ba4\u77e5\u7f16\u7ec7\u5728\u957f\u7a0b\u89c4\u5212\u4efb\u52a1\u3001\u6f14\u8fdb\u5f0f\u95ee\u7b54\u573a\u666f\u548c\u591a\u4f1a\u8bdd\u5bf9\u8bdd\u8fde\u8d2f\u6027\u65b9\u9762\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u663e\u7740\u589e\u5f3a\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u7cfb\u7edf\u5728\u4efb\u52a1\u5b8c\u6210\u7387\u65b9\u9762\u5e73\u5747\u63d0\u9ad8\u4e86 34%\uff0c\u67e5\u8be2\u5ef6\u8fdf\u5e73\u5747\u51cf\u5c11\u4e86 42%\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u63a2\u8ba8\u4e86\u8fd9\u79cd\u5148\u8fdb\u8bb0\u5fc6\u7cfb\u7edf\u4e2d\u56fa\u6709\u7684\u4f26\u7406\u8003\u91cf\uff0c\u8ba8\u8bba\u4e86\u5176\u5bf9 LLM \u4e2d\u957f\u671f\u8bb0\u5fc6\u7684\u5f71\u54cd\uff0c\u5e76\u6982\u8ff0\u4e86\u6709\u5e0c\u671b\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2506.08185", "pdf": "https://arxiv.org/pdf/2506.08185", "abs": "https://arxiv.org/abs/2506.08185", "authors": ["Huixin Zhan", "Jason H. Moore"], "title": "Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surgeons exhibit distinct operating styles due to differences in training,\nexperience, and motor behavior - yet current AI systems often ignore this\npersonalization signal. We propose a novel approach to model fine-grained,\nsurgeon-specific fingerprinting in robotic surgery using a discrete diffusion\nframework integrated with a vision-language-action (VLA) pipeline. Our method\nformulates gesture prediction as a structured sequence denoising task,\nconditioned on multimodal inputs including endoscopic video, surgical intent\nlanguage, and a privacy-aware embedding of surgeon identity and skill.\nPersonalized surgeon fingerprinting is encoded through natural language prompts\nusing third-party language models, allowing the model to retain individual\nbehavioral style without exposing explicit identity. We evaluate our method on\nthe JIGSAWS dataset and demonstrate that it accurately reconstructs gesture\nsequences while learning meaningful motion fingerprints unique to each surgeon.\nTo quantify the privacy implications of personalization, we perform membership\ninference attacks and find that more expressive embeddings improve task\nperformance but simultaneously increase susceptibility to identity leakage.\nThese findings demonstrate that while personalized embeddings improve\nperformance, they also increase vulnerability to identity leakage, revealing\nthe importance of balancing personalization with privacy risk in surgical\nmodeling. Code is available at:\nhttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper relates to both trajectory prediction and large language models to some extent. It uses a vision-language-action framework to predict surgical gestures, which can be viewed as a form of action/trajectory prediction. It also incorporates language models to encode surgeon style via natural language prompts. However, the primary focus is on surgeon fingerprinting and privacy, not trajectory prediction or large language models themselves. The connection to trajectory prediction is through gesture prediction as a structured sequence, and the connection to large language models is through their use in encoding surgeon style.", "keywords": ["gesture prediction", "vision-language-action", "language models", "sequence denoising", "action prediction"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u79bb\u6563\u6269\u6563\u6846\u67b6\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7ba1\u9053\uff0c\u5728\u673a\u5668\u4eba\u624b\u672f\u4e2d\u5bf9\u5916\u79d1\u533b\u751f\u7279\u5b9a\u6307\u7eb9\u8fdb\u884c\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u4fdd\u62a4\u4e4b\u95f4\u8fdb\u884c\u4e86\u6743\u8861\u3002", "motivation": "\u7531\u4e8e\u8bad\u7ec3\u3001\u7ecf\u9a8c\u548c\u8fd0\u52a8\u884c\u4e3a\u7684\u5dee\u5f02\uff0c\u5916\u79d1\u533b\u751f\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u624b\u672f\u98ce\u683c - \u4f46\u5f53\u524d\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u901a\u5e38\u5ffd\u7565\u8fd9\u79cd\u4e2a\u6027\u5316\u4fe1\u53f7\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e0e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u7ba1\u9053\u96c6\u6210\u7684\u79bb\u6563\u6269\u6563\u6846\u67b6\uff0c\u5728\u673a\u5668\u4eba\u624b\u672f\u4e2d\u5bf9\u7ec6\u7c92\u5ea6\u7684\u3001\u5916\u79d1\u533b\u751f\u7279\u5b9a\u7684\u6307\u7eb9\u8fdb\u884c\u5efa\u6a21\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u624b\u52bf\u9884\u6d4b\u516c\u5f0f\u5316\u4e3a\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5e8f\u5217\u53bb\u566a\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u4ee5\u591a\u6a21\u6001\u8f93\u5165\u4e3a\u6761\u4ef6\uff0c\u5305\u62ec\u5185\u7aa5\u955c\u89c6\u9891\u3001\u624b\u672f\u610f\u56fe\u8bed\u8a00\u4ee5\u53ca\u5916\u79d1\u533b\u751f\u8eab\u4efd\u548c\u6280\u80fd\u7684\u9690\u79c1\u611f\u77e5\u5d4c\u5165\u3002", "result": "\u6211\u4eec\u5728JIGSAWS\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u5b83\u53ef\u4ee5\u51c6\u786e\u5730\u91cd\u5efa\u624b\u52bf\u5e8f\u5217\uff0c\u540c\u65f6\u5b66\u4e60\u6bcf\u4e2a\u5916\u79d1\u533b\u751f\u72ec\u7279\u7684\u6709\u610f\u4e49\u7684\u8fd0\u52a8\u6307\u7eb9\u3002", "conclusion": "\u4e2a\u6027\u5316\u5d4c\u5165\u867d\u7136\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u8eab\u4efd\u6cc4\u9732\u7684\u98ce\u9669\uff0c\u63ed\u793a\u4e86\u5728\u624b\u672f\u5efa\u6a21\u4e2d\u5e73\u8861\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u98ce\u9669\u7684\u91cd\u8981\u6027\u3002", "summary_zh": "\u7531\u4e8e\u5916\u79d1\u533b\u751f\u7684\u8bad\u7ec3\u3001\u7ecf\u9a8c\u548c\u8fd0\u52a8\u884c\u4e3a\u5b58\u5728\u5dee\u5f02\uff0c\u4ed6\u4eec\u7684\u624b\u672f\u98ce\u683c\u4e5f\u5404\u4e0d\u76f8\u540c\uff0c\u4f46\u76ee\u524d\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5f80\u5f80\u5ffd\u7565\u4e86\u8fd9\u79cd\u4e2a\u6027\u5316\u7684\u4fe1\u53f7\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4e0e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u7ba1\u9053\u96c6\u6210\u7684\u79bb\u6563\u6269\u6563\u6846\u67b6\uff0c\u5728\u673a\u5668\u4eba\u624b\u672f\u4e2d\u5bf9\u5916\u79d1\u533b\u751f\u7279\u6709\u7684\u7cbe\u7ec6\u6307\u7eb9\u8fdb\u884c\u5efa\u6a21\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5c06\u624b\u52bf\u9884\u6d4b\u6784\u5efa\u4e3a\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u5e8f\u5217\u53bb\u566a\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u4ee5\u591a\u6a21\u6001\u8f93\u5165\u4e3a\u6761\u4ef6\uff0c\u5305\u62ec\u5185\u7aa5\u955c\u89c6\u9891\u3001\u624b\u672f\u610f\u56fe\u8bed\u8a00\u4ee5\u53ca\u5916\u79d1\u533b\u751f\u8eab\u4efd\u548c\u6280\u80fd\u7684\u9690\u79c1\u611f\u77e5\u5d4c\u5165\u3002\u4e2a\u6027\u5316\u7684\u5916\u79d1\u533b\u751f\u6307\u7eb9\u901a\u8fc7\u4f7f\u7528\u7b2c\u4e09\u65b9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8fdb\u884c\u7f16\u7801\uff0c\u4ece\u800c\u4f7f\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u66b4\u9732\u660e\u786e\u8eab\u4efd\u7684\u60c5\u51b5\u4e0b\u4fdd\u7559\u4e2a\u4f53\u884c\u4e3a\u98ce\u683c\u3002\u6211\u4eec\u5728JIGSAWS\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5730\u91cd\u5efa\u624b\u52bf\u5e8f\u5217\uff0c\u540c\u65f6\u5b66\u4e60\u6bcf\u4e2a\u5916\u79d1\u533b\u751f\u72ec\u7279\u7684\u6709\u610f\u4e49\u7684\u8fd0\u52a8\u6307\u7eb9\u3002\u4e3a\u4e86\u91cf\u5316\u4e2a\u6027\u5316\u7684\u9690\u79c1\u5f71\u54cd\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u53d1\u73b0\u66f4\u5177\u8868\u73b0\u529b\u7684\u5d4c\u5165\u53ef\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\uff0c\u4f46\u540c\u65f6\u4e5f\u589e\u52a0\u4e86\u8eab\u4efd\u6cc4\u9732\u7684\u53ef\u80fd\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0c\u867d\u7136\u4e2a\u6027\u5316\u5d4c\u5165\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u4f46\u4e5f\u589e\u52a0\u4e86\u8eab\u4efd\u6cc4\u9732\u7684\u98ce\u9669\uff0c\u63ed\u793a\u4e86\u5728\u624b\u672f\u5efa\u6a21\u4e2d\u5e73\u8861\u4e2a\u6027\u5316\u4e0e\u9690\u79c1\u98ce\u9669\u7684\u91cd\u8981\u6027\u3002\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u53d6\uff1ahttps://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting\u3002"}}
{"id": "2506.08344", "pdf": "https://arxiv.org/pdf/2506.08344", "abs": "https://arxiv.org/abs/2506.08344", "authors": ["Ne\u015fet \u00dcnver Akmandor", "Sarvesh Prajapati", "Mark Zolotas", "Ta\u015fk\u0131n Pad\u0131r"], "title": "Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted to the 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Traditional motion planning methods for robots with many degrees-of-freedom,\nsuch as mobile manipulators, are often computationally prohibitive for\nreal-world settings. In this paper, we propose a novel multi-model motion\nplanning pipeline, termed Re4MPC, which computes trajectories using Nonlinear\nModel Predictive Control (NMPC). Re4MPC generates trajectories in a\ncomputationally efficient manner by reactively selecting the model, cost, and\nconstraints of the NMPC problem depending on the complexity of the task and\nrobot state. The policy for this reactive decision-making is learned via a Deep\nReinforcement Learning (DRL) framework. We introduce a mathematical formulation\nto integrate NMPC into this DRL framework. To validate our methodology and\ndesign choices, we evaluate DRL training and test outcomes in a physics-based\nsimulation involving a mobile manipulator. Experimental results demonstrate\nthat Re4MPC is more computationally efficient and achieves higher success rates\nin reaching end-effector goals than the NMPC baseline, which computes\nwhole-body trajectories without our learning mechanism.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on motion planning using Nonlinear Model Predictive Control (NMPC) and Deep Reinforcement Learning (DRL). While it doesn't directly involve Large Language Models, the use of DRL connects it to the broader field of large AI models. The mention of motion planning and trajectory generation indicates relevance to trajectory prediction, albeit in a robotics context rather than human/vehicle trajectory prediction. The 'multi-model motion planning' aspect also strengthens the trajectory prediction link.", "keywords": ["motion planning", "trajectory", "Nonlinear Model Predictive Control", "Deep Reinforcement Learning"]}, "AI": {"tldr": "Re4MPC\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u53cd\u5e94\u6027\u5730\u9009\u62e9NMPC\u95ee\u9898\u7684\u6a21\u578b\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u751f\u6210\u8f68\u8ff9\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u591a\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u8ba1\u7b97\u91cf\u8fc7\u5927\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u578b\u8fd0\u52a8\u89c4\u5212\u6d41\u7a0bRe4MPC\uff0c\u5b83\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u8ba1\u7b97\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6846\u67b6\u5b66\u4e60\u53cd\u5e94\u51b3\u7b56\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRe4MPC\u6bd4NMPC\u57fa\u7ebf\u5728\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u4e14\u5728\u8fbe\u5230\u672b\u7aef\u6267\u884c\u5668\u76ee\u6807\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002", "conclusion": "Re4MPC\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8eNMPC\u57fa\u7ebf\u3002", "summary_zh": "\u9488\u5bf9\u591a\u81ea\u7531\u5ea6\u673a\u5668\u4eba\uff08\u5982\u79fb\u52a8\u673a\u68b0\u81c2\uff09\u7684\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u91cf\u8fc7\u5927\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5b9e\u9645\u73af\u5883\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u578b\u8fd0\u52a8\u89c4\u5212\u6d41\u7a0bRe4MPC\uff0c\u8be5\u6d41\u7a0b\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u8ba1\u7b97\u8f68\u8ff9\u3002Re4MPC\u901a\u8fc7\u6839\u636e\u4efb\u52a1\u548c\u673a\u5668\u4eba\u72b6\u6001\u7684\u590d\u6742\u6027\uff0c\u53cd\u5e94\u6027\u5730\u9009\u62e9NMPC\u95ee\u9898\u7684\u6a21\u578b\u3001\u6210\u672c\u548c\u7ea6\u675f\uff0c\u4ece\u800c\u9ad8\u6548\u5730\u751f\u6210\u8f68\u8ff9\u3002\u8fd9\u79cd\u53cd\u5e94\u6027\u51b3\u7b56\u7684\u7b56\u7565\u662f\u901a\u8fc7\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6846\u67b6\u5b66\u4e60\u7684\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u6570\u5b66\u516c\u5f0f\uff0c\u5c06NMPC\u96c6\u6210\u5230\u8fd9\u4e2aDRL\u6846\u67b6\u4e2d\u3002\u4e3a\u4e86\u9a8c\u8bc1\u6211\u4eec\u7684\u65b9\u6cd5\u548c\u8bbe\u8ba1\u9009\u62e9\uff0c\u6211\u4eec\u5728\u4e00\u4e2a\u6d89\u53ca\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u57fa\u4e8e\u7269\u7406\u7684\u4eff\u771f\u73af\u5883\u4e2d\u8bc4\u4f30\u4e86DRL\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7ed3\u679c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRe4MPC\u6bd4NMPC\u57fa\u7ebf\u5728\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\uff0c\u5e76\u4e14\u5728\u8fbe\u5230\u672b\u7aef\u6267\u884c\u5668\u76ee\u6807\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002NMPC\u57fa\u7ebf\u5728\u6ca1\u6709\u6211\u4eec\u7684\u5b66\u4e60\u673a\u5236\u7684\u60c5\u51b5\u4e0b\u8ba1\u7b97\u5168\u8eab\u8f68\u8ff9\u3002"}}
{"id": "2506.08440", "pdf": "https://arxiv.org/pdf/2506.08440", "abs": "https://arxiv.org/abs/2506.08440", "authors": ["Zengjue Chen", "Runliang Niu", "He Kong", "Qi Wang"], "title": "TGRPO :Fine-tuning Vision-Language-Action Model via Trajectory-wise Group Relative Policy Optimization", "categories": ["cs.RO"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) model have demonstrated\nstrong generalization capabilities across diverse scenes, tasks, and robotic\nplatforms when pretrained at large-scale datasets. However, these models still\nrequire task-specific fine-tuning in novel environments, a process that relies\nalmost exclusively on supervised fine-tuning (SFT) using static trajectory\ndatasets. Such approaches neither allow robot to interact with environment nor\ndo they leverage feedback from live execution. Also, their success is\ncritically dependent on the size and quality of the collected trajectories.\nReinforcement learning (RL) offers a promising alternative by enabling\nclosed-loop interaction and aligning learned policies directly with task\nobjectives. In this work, we draw inspiration from the ideas of GRPO and\npropose the Trajectory-wise Group Relative Policy Optimization (TGRPO) method.\nBy fusing step-level and trajectory-level advantage signals, this method\nimproves GRPO's group-level advantage estimation, thereby making the algorithm\nmore suitable for online reinforcement learning training of VLA. Experimental\nresults on ten manipulation tasks from the libero-object benchmark demonstrate\nthat TGRPO consistently outperforms various baseline methods, capable of\ngenerating more robust and efficient policies across multiple tested scenarios.\nOur source codes are available at: https://github.com/hahans/TGRPO", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on fine-tuning a Vision-Language-Action model for robotic manipulation tasks using reinforcement learning. While it involves a VLA model (related to Large Language Models through vision-language pre-training), the primary focus is on action and policy optimization within a trajectory context. The connection to trajectory prediction is indirect, as it focuses on generating trajectories rather than predicting them. The relevance is derived from the use of a vision-language-action model, which builds upon large language models.", "keywords": ["Vision-Language-Action model", "VLA", "reinforcement learning", "trajectory", "policy optimization"]}, "AI": {"tldr": "TGRPO\u901a\u8fc7\u878d\u5408\u6b65\u7ea7\u548c\u8f68\u8ff9\u7ea7\u4f18\u52bf\u4fe1\u53f7\uff0c\u6539\u8fdb\u4e86VLA\u6a21\u578b\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u7fa4\u4f53\u7ea7\u4f18\u52bf\u4f30\u8ba1\uff0c\u4ece\u800c\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "VLA\u6a21\u578b\u9700\u8981\u5728\u65b0\u73af\u5883\u4e2d\u8fdb\u884c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5fae\u8c03\uff0c\u5e76\u4e14\u4e25\u91cd\u4f9d\u8d56\u4e8e\u9759\u6001\u8f68\u8ff9\u6570\u636e\u96c6\u7684\u5927\u5c0f\u548c\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u65e0\u6cd5\u4e0e\u73af\u5883\u4ea4\u4e92\u6216\u5229\u7528\u5b9e\u65f6\u6267\u884c\u7684\u53cd\u9988\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f68\u8ff9\u5f0f\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08TGRPO\uff09\u65b9\u6cd5\uff0c\u878d\u5408\u4e86\u6b65\u7ea7\u548c\u8f68\u8ff9\u7ea7\u4f18\u52bf\u4fe1\u53f7\uff0c\u4ece\u800c\u6539\u8fdb\u4e86GRPO\u7684\u7fa4\u4f53\u7ea7\u4f18\u52bf\u4f30\u8ba1\u3002", "result": "\u5728libero-object\u57fa\u51c6\u6d4b\u8bd5\u7684\u5341\u4e2a\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTGRPO\u59cb\u7ec8\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TGRPO\u5728\u591a\u4e2a\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u66f4\u7a33\u5065\u548c\u9ad8\u6548\u7684\u7b56\u7565\u3002", "summary_zh": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u8868\u660e\uff0c\u5f53\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u65f6\uff0c\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u7684\u573a\u666f\u3001\u4efb\u52a1\u548c\u673a\u5668\u4eba\u5e73\u53f0\u4e2d\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ecd\u7136\u9700\u8981\u5728\u65b0\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5fae\u8c03\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u51e0\u4e4e\u5b8c\u5168\u4f9d\u8d56\u4e8e\u4f7f\u7528\u9759\u6001\u8f68\u8ff9\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u3002\u8fd9\u4e9b\u65b9\u6cd5\u65e2\u4e0d\u5141\u8bb8\u673a\u5668\u4eba\u4e0e\u73af\u5883\u4ea4\u4e92\uff0c\u4e5f\u4e0d\u5229\u7528\u5b9e\u65f6\u6267\u884c\u7684\u53cd\u9988\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u7684\u6210\u529f\u8fd8\u4e25\u91cd\u4f9d\u8d56\u4e8e\u6536\u96c6\u5230\u7684\u8f68\u8ff9\u7684\u5927\u5c0f\u548c\u8d28\u91cf\u3002\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u901a\u8fc7\u5b9e\u73b0\u95ed\u73af\u4ea4\u4e92\u5e76\u5c06\u5b66\u4e60\u5230\u7684\u7b56\u7565\u76f4\u63a5\u4e0e\u4efb\u52a1\u76ee\u6807\u5bf9\u9f50\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4eceGRPO\u7684\u601d\u60f3\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5e76\u63d0\u51fa\u4e86\u8f68\u8ff9\u5f0f\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08TGRPO\uff09\u65b9\u6cd5\u3002\u901a\u8fc7\u878d\u5408\u6b65\u7ea7\u548c\u8f68\u8ff9\u7ea7\u4f18\u52bf\u4fe1\u53f7\uff0c\u8be5\u65b9\u6cd5\u6539\u8fdb\u4e86GRPO\u7684\u7fa4\u4f53\u7ea7\u4f18\u52bf\u4f30\u8ba1\uff0c\u4ece\u800c\u4f7f\u8be5\u7b97\u6cd5\u66f4\u9002\u5408VLA\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002\u5728libero-object\u57fa\u51c6\u6d4b\u8bd5\u7684\u5341\u4e2a\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTGRPO\u59cb\u7ec8\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u66f4\u7a33\u5065\u548c\u9ad8\u6548\u7684\u7b56\u7565\u3002\u6211\u4eec\u7684\u6e90\u4ee3\u7801\u53ef\u5728\u4ee5\u4e0b\u7f51\u5740\u83b7\u5f97\uff1ahttps://github.com/hahans/TGRPO"}}
{"id": "2506.08459", "pdf": "https://arxiv.org/pdf/2506.08459", "abs": "https://arxiv.org/abs/2506.08459", "authors": ["Juanran Wang", "Marc R. Schlichting", "Harrison Delecki", "Mykel J. Kochenderfer"], "title": "Diffusion Models for Safety Validation of Autonomous Driving Systems", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Safety validation of autonomous driving systems is extremely challenging due\nto the high risks and costs of real-world testing as well as the rarity and\ndiversity of potential failures. To address these challenges, we train a\ndenoising diffusion model to generate potential failure cases of an autonomous\nvehicle given any initial traffic state. Experiments on a four-way intersection\nproblem show that in a variety of scenarios, the diffusion model can generate\nrealistic failure samples while capturing a wide variety of potential failures.\nOur model does not require any external training dataset, can perform training\nand inference with modest computing resources, and does not assume any prior\nknowledge of the system under test, with applicability to safety validation for\ntraffic intersections.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses using diffusion models (which can be considered a type of generative model related to large models) for generating failure cases in autonomous driving systems. Autonomous driving inherently involves trajectory prediction. Therefore, while it doesn't explicitly use Large Language Models, it utilizes a large generative model for a task closely tied to trajectory prediction, making it moderately relevant.", "keywords": ["diffusion models", "autonomous driving systems", "safety validation", "failure cases", "trajectory prediction"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53bb\u566a\u6269\u6563\u6a21\u578b\u7684\u81ea\u52a8\u9a7e\u9a76\u6545\u969c\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b89\u5168\u9a8c\u8bc1\u7684\u96be\u9898\u3002", "motivation": "\u7531\u4e8e\u73b0\u5b9e\u4e16\u754c\u6d4b\u8bd5\u7684\u9ad8\u98ce\u9669\u548c\u6210\u672c\u4ee5\u53ca\u6f5c\u5728\u6545\u969c\u7684\u7f55\u89c1\u6027\u548c\u591a\u6837\u6027\uff0c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u9a8c\u8bc1\u6781\u5177\u6311\u6218\u6027\u3002", "method": "\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u751f\u6210\u7ed9\u5b9a\u4efb\u4f55\u521d\u59cb\u4ea4\u901a\u72b6\u6001\u7684\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u6f5c\u5728\u6545\u969c\u6848\u4f8b\u3002", "result": "\u5728\u56db\u5411\u4ea4\u53c9\u53e3\u95ee\u9898\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e2d\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u771f\u5b9e\u7684\u6545\u969c\u6837\u672c\uff0c\u540c\u65f6\u6355\u83b7\u5404\u79cd\u6f5c\u5728\u6545\u969c\u3002", "conclusion": "\u8be5\u6269\u6563\u6a21\u578b\u65e0\u9700\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u4f7f\u7528\u9002\u5ea6\u7684\u8ba1\u7b97\u8d44\u6e90\u6267\u884c\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u5e76\u4e14\u4e0d\u5047\u8bbe\u4efb\u4f55\u5173\u4e8e\u88ab\u6d4b\u7cfb\u7edf\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9002\u7528\u4e8e\u4ea4\u901a\u8def\u53e3\u7684\u5b89\u5168\u9a8c\u8bc1\u3002", "summary_zh": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff08ADS\uff09\u7684\u5b89\u5168\u9a8c\u8bc1\u6781\u5177\u6311\u6218\uff0c\u8fd9\u662f\u7531\u4e8e\u771f\u5b9e\u8def\u51b5\u6d4b\u8bd5\u5177\u6709\u9ad8\u98ce\u9669\u548c\u9ad8\u6210\u672c\uff0c\u5e76\u4e14\u6f5c\u5728\u7684\u5931\u6548\u60c5\u51b5\u975e\u5e38\u5c11\u89c1\u548c\u591a\u6837\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u5728\u7ed9\u5b9a\u4efb\u4f55\u521d\u59cb\u4ea4\u901a\u72b6\u6001\u4e0b\u751f\u6210\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AV\uff09\u7684\u6f5c\u5728\u5931\u6548\u6848\u4f8b\u3002\u5728\u56db\u5c94\u8def\u53e3\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e0b\uff0c\u8be5\u6269\u6563\u6a21\u578b\u80fd\u591f\u751f\u6210\u771f\u5b9e\u7684\u5931\u6548\u6837\u672c\uff0c\u540c\u65f6\u6355\u6349\u5404\u79cd\u5404\u6837\u7684\u6f5c\u5728\u5931\u6548\u60c5\u51b5\u3002\u6211\u4eec\u7684\u6a21\u578b\u4e0d\u9700\u8981\u4efb\u4f55\u5916\u90e8\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u4f7f\u7528\u9002\u5ea6\u7684\u8ba1\u7b97\u8d44\u6e90\u6267\u884c\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u5e76\u4e14\u4e0d\u5047\u8bbe\u4efb\u4f55\u5173\u4e8e\u88ab\u6d4b\u7cfb\u7edf\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9002\u7528\u4e8e\u4ea4\u901a\u8def\u53e3\u7684\u5b89\u5168\u9a8c\u8bc1\u3002"}}
{"id": "2506.08851", "pdf": "https://arxiv.org/pdf/2506.08851", "abs": "https://arxiv.org/abs/2506.08851", "authors": ["Sepehr Samavi", "Garvish Bhutani", "Florian Shkurti", "Angela P. Schoellig"], "title": "Deploying SICNav in the Field: Safe and Interactive Crowd Navigation using MPC and Bilevel Optimization", "categories": ["cs.RO"], "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics\n  (non-archival)", "summary": "Safe and efficient navigation in crowded environments remains a critical\nchallenge for robots that provide a variety of service tasks such as food\ndelivery or autonomous wheelchair mobility. Classical robot crowd navigation\nmethods decouple human motion prediction from robot motion planning, which\nneglects the closed-loop interactions between humans and robots. This lack of a\nmodel for human reactions to the robot plan (e.g. moving out of the way) can\ncause the robot to get stuck. Our proposed Safe and Interactive Crowd\nNavigation (SICNav) method is a bilevel Model Predictive Control (MPC)\nframework that combines prediction and planning into one optimization problem,\nexplicitly modeling interactions among agents. In this paper, we present a\nsystems overview of the crowd navigation platform we use to deploy SICNav in\npreviously unseen indoor and outdoor environments. We provide a preliminary\nanalysis of the system's operation over the course of nearly 7 km of autonomous\nnavigation over two hours in both indoor and outdoor environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4eba\u7fa4\u5bfc\u822a\u4e2d\u7684\u5b89\u5168\u548c\u4ea4\u4e92\u95ee\u9898\uff0c\u4f7f\u7528\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\uff0c\u6d89\u53ca\u5230\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u4f46\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e0d\u7b97\u9ad8\u3002", "keywords": ["trajectory prediction", "human motion prediction", "MPC", "crowd navigation"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b89\u5168\u548c\u4ea4\u4e92\u5f0f\u7684\u62e5\u6324\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4e92\u52a8\uff0c\u5b9e\u73b0\u4e86\u5728\u62e5\u6324\u73af\u5883\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u5728\u62e5\u6324\u73af\u5883\u4e2d\u5b89\u5168\u9ad8\u6548\u7684\u5bfc\u822a\u4ecd\u7136\u662f\u673a\u5668\u4eba\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u7684\u673a\u5668\u4eba\u62e5\u6324\u5bfc\u822a\u65b9\u6cd5\u5c06\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u5206\u79bb\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u95ed\u73af\u4e92\u52a8\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u5bb9\u6613\u9677\u5165\u56f0\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5b89\u5168\u548c\u4ea4\u4e92\u5f0f\u7684\u62e5\u6324\u5bfc\u822a\u3002", "result": "\u5728\u5ba4\u5185\u548c\u5ba4\u5916\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u8fd17\u516c\u91cc\u7684\u81ea\u4e3b\u5bfc\u822a\u5b9e\u9a8c\uff0c\u521d\u6b65\u5206\u6790\u4e86\u7cfb\u7edf\u7684\u8fd0\u884c\u60c5\u51b5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b89\u5168\u548c\u4ea4\u4e92\u5f0f\u7684\u62e5\u6324\u5bfc\u822a\uff08SICNav\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u548c\u89c4\u5212\u7ed3\u5408\u5230\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u663e\u5f0f\u5730\u5efa\u6a21\u4e86\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002", "summary_zh": "\u5728\u62e5\u6324\u73af\u5883\u4e2d\u5b89\u5168\u9ad8\u6548\u7684\u5bfc\u822a\u4ecd\u7136\u662f\u673a\u5668\u4eba\u9762\u4e34\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u8fd9\u4e9b\u673a\u5668\u4eba\u6267\u884c\u5404\u79cd\u670d\u52a1\u4efb\u52a1\uff0c\u4f8b\u5982\u98df\u7269\u8fd0\u9001\u6216\u81ea\u4e3b\u8f6e\u6905\u79fb\u52a8\u3002\u7ecf\u5178\u7684\u673a\u5668\u4eba\u62e5\u6324\u5bfc\u822a\u65b9\u6cd5\u5c06\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u5206\u79bb\uff0c\u5ffd\u7565\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u95ed\u73af\u4e92\u52a8\u3002\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u8ba1\u5212\u7684\u53cd\u5e94\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u8ba9\u8def\uff09\uff0c\u53ef\u80fd\u5bfc\u81f4\u673a\u5668\u4eba\u9677\u5165\u56f0\u5883\u3002\u6211\u4eec\u63d0\u51fa\u7684\u5b89\u5168\u548c\u4ea4\u4e92\u5f0f\u62e5\u6324\u5bfc\u822a\uff08SICNav\uff09\u65b9\u6cd5\u662f\u4e00\u4e2a\u53cc\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u5b83\u5c06\u9884\u6d4b\u548c\u89c4\u5212\u7ed3\u5408\u5230\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u663e\u5f0f\u5730\u5efa\u6a21\u4e86\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4e92\u52a8\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5bf9\u62e5\u6324\u5bfc\u822a\u5e73\u53f0\u8fdb\u884c\u4e86\u7cfb\u7edf\u6982\u8ff0\uff0c\u6211\u4eec\u4f7f\u7528\u8be5\u5e73\u53f0\u5728\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u5ba4\u5185\u548c\u5ba4\u5916\u73af\u5883\u4e2d\u90e8\u7f72SICNav\u3002\u6211\u4eec\u5bf9\u7cfb\u7edf\u5728\u5ba4\u5185\u548c\u5ba4\u5916\u73af\u5883\u4e2d\u8d85\u8fc72\u5c0f\u65f6\u7684\u8fd17\u516c\u91cc\u81ea\u4e3b\u5bfc\u822a\u8fc7\u7a0b\u4e2d\u7684\u8fd0\u884c\u60c5\u51b5\u8fdb\u884c\u4e86\u521d\u6b65\u5206\u6790\u3002"}}
{"id": "2506.08512", "pdf": "https://arxiv.org/pdf/2506.08512", "abs": "https://arxiv.org/abs/2506.08512", "authors": ["Zhiyi Zhu", "Xiaoyu Wu", "Zihao Liu", "Linlin Yang"], "title": "MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Temporal Grounding (VTG), which aims to localize video clips\ncorresponding to natural language queries, is a fundamental yet challenging\ntask in video understanding. Existing Transformer-based methods often suffer\nfrom redundant attention and suboptimal multi-modal alignment. To address these\nlimitations, we propose MLVTG, a novel framework that integrates two key\nmodules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba\nblocks as a backbone instead of Transformers to model temporal dependencies and\nextract robust video representations for multi-modal alignment. LLMRefiner\nleverages the specific frozen layer of a pre-trained Large Language Model (LLM)\nto implicitly transfer semantic priors, enhancing multi-modal alignment without\nfine-tuning. This dual alignment strategy, temporal modeling via structured\nstate-space dynamics and semantic purification via textual priors, enables more\nprecise localization. Extensive experiments on QVHighlights, Charades-STA, and\nTVSum demonstrate that MLVTG achieves state-of-the-art performance and\nsignificantly outperforms existing baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Video Temporal Grounding, which is related to predicting or understanding temporal sequences in videos. It utilizes a Large Language Model (LLM) component for semantic purification. While not directly addressing trajectory prediction, the temporal aspect and use of LLMs make it moderately relevant.", "keywords": ["Large Language Model", "LLM", "video understanding", "temporal grounding", "multi-modal alignment"]}, "AI": {"tldr": "MLVTG\u901a\u8fc7MambaAligner\u548cLLMRefiner\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u5197\u4f59\u6ce8\u610f\u529b\u548c\u6b21\u4f18\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u3002", "method": "MLVTG\u96c6\u6210\u4e86MambaAligner\u548cLLMRefiner\u4e24\u4e2a\u5173\u952e\u6a21\u5757\u3002MambaAligner\u4f7f\u7528\u5806\u53e0\u7684Vision Mamba\u5757\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u4ee3\u66ffTransformer\u6765\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u5e76\u63d0\u53d6\u9c81\u68d2\u7684\u89c6\u9891\u8868\u793a\u4ee5\u8fdb\u884c\u591a\u6a21\u6001\u5bf9\u9f50\u3002LLMRefiner\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7279\u5b9a\u51bb\u7ed3\u5c42\u6765\u9690\u5f0f\u4f20\u9012\u8bed\u4e49\u5148\u9a8c\uff0c\u4ece\u800c\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u800c\u65e0\u9700\u5fae\u8c03\u3002", "result": "\u5728QVHighlights\u3001Charades-STA\u548cTVSum\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMLVTG\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u3002", "conclusion": "MLVTG\u901a\u8fc7MambaAligner\u548cLLMRefiner\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "summary_zh": "\u89c6\u9891\u65f6\u5e8f\u5b9a\u4f4d\uff08VTG\uff09\u65e8\u5728\u5b9a\u4f4d\u4e0e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u76f8\u5bf9\u5e94\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u662f\u89c6\u9891\u7406\u89e3\u4e2d\u4e00\u9879\u57fa\u7840\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684\u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u5197\u4f59\u6ce8\u610f\u529b\u548c\u6b21\u4f18\u7684\u591a\u6a21\u6001\u5bf9\u9f50\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6MLVTG\uff0c\u5b83\u96c6\u6210\u4e86\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1aMambaAligner\u548cLLMRefiner\u3002MambaAligner\u4f7f\u7528\u5806\u53e0\u7684Vision Mamba\u5757\u4f5c\u4e3a\u4e3b\u5e72\uff0c\u4ee3\u66ffTransformer\u6765\u5efa\u6a21\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\u5e76\u63d0\u53d6\u9c81\u68d2\u7684\u89c6\u9891\u8868\u793a\u4ee5\u8fdb\u884c\u591a\u6a21\u6001\u5bf9\u9f50\u3002LLMRefiner\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7279\u5b9a\u51bb\u7ed3\u5c42\u6765\u9690\u5f0f\u4f20\u9012\u8bed\u4e49\u5148\u9a8c\uff0c\u4ece\u800c\u589e\u5f3a\u591a\u6a21\u6001\u5bf9\u9f50\uff0c\u800c\u65e0\u9700\u5fae\u8c03\u3002\u8fd9\u79cd\u53cc\u91cd\u5bf9\u9f50\u7b56\u7565\uff0c\u5373\u901a\u8fc7\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u52a8\u529b\u5b66\u8fdb\u884c\u65f6\u95f4\u5efa\u6a21\uff0c\u4ee5\u53ca\u901a\u8fc7\u6587\u672c\u5148\u9a8c\u8fdb\u884c\u8bed\u4e49\u63d0\u7eaf\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u5b9a\u4f4d\u3002\u5728QVHighlights\u3001Charades-STA\u548cTVSum\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMLVTG\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2506.08963", "pdf": "https://arxiv.org/pdf/2506.08963", "abs": "https://arxiv.org/abs/2506.08963", "authors": ["Yash Ranjan", "Rahul Sengupta", "Anand Rangarajan", "Sanjay Ranka"], "title": "Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics", "categories": ["cs.AI"], "comment": null, "summary": "Traffic Intersections are vital to urban road networks as they regulate the\nmovement of people and goods. However, they are regions of conflicting\ntrajectories and are prone to accidents. Deep Generative models of traffic\ndynamics at signalized intersections can greatly help traffic authorities\nbetter understand the efficiency and safety aspects. At present, models are\nevaluated on computational metrics that primarily look at trajectory\nreconstruction errors. They are not evaluated online in a `live'\nmicrosimulation scenario. Further, these metrics do not adequately consider\ntraffic engineering-specific concerns such as red-light violations, unallowed\nstoppage, etc. In this work, we provide a comprehensive analytics tool to\ntrain, run, and evaluate models with metrics that give better insights into\nmodel performance from a traffic engineering point of view. We train a\nstate-of-the-art multi-vehicle trajectory forecasting model on a large dataset\ncollected by running a calibrated scenario of a real-world urban intersection.\nWe then evaluate the performance of the prediction models, online in a\nmicrosimulator, under unseen traffic conditions. We show that despite using\nideally-behaved trajectories as input, and achieving low trajectory\nreconstruction errors, the generated trajectories show behaviors that break\ntraffic rules. We introduce new metrics to evaluate such undesired behaviors\nand present our results.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u4ea4\u901a\u8def\u53e3\u7684\u8f66\u8f86\u8f68\u8ff9\u751f\u6210\u6a21\u578b\u8bc4\u4f30\u3002\u867d\u7136\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u751f\u6210\u6a21\u578b\uff0c\u4f46\u5e76\u672a\u63d0\u53ca\u6216\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u76f8\u5173\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u3002", "keywords": ["trajectory prediction", "vehicle trajectory", "traffic intersection", "generative models", "trajectory forecasting"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ea4\u901a\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ea4\u901a\u9884\u6d4b\u6a21\u578b\u5728\u5fae\u89c2\u6a21\u62df\u4e2d\u4ea7\u751f\u7684\u8fdd\u53cd\u4ea4\u901a\u89c4\u5219\u7684\u884c\u4e3a\u3002", "motivation": "\u4ea4\u901a\u8def\u53e3\u662f\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5b83\u4eec\u4e5f\u662f\u8f68\u8ff9\u51b2\u7a81\u548c\u5bb9\u6613\u53d1\u751f\u4e8b\u6545\u7684\u533a\u57df\u3002\u4ea4\u901a\u4fe1\u53f7\u706f\u63a7\u5236\u7684\u4ea4\u53c9\u53e3\u4ea4\u901a\u52a8\u6001\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u6781\u5927\u5730\u5e2e\u52a9\u4ea4\u901a\u90e8\u95e8\u66f4\u597d\u5730\u7406\u89e3\u6548\u7387\u548c\u5b89\u5168\u95ee\u9898\u3002\u76ee\u524d\uff0c\u6a21\u578b\u4e3b\u8981\u5728\u8ba1\u7b97\u6307\u6807\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u91cd\u5efa\u8bef\u5dee\u3002\u4ed6\u4eec\u6ca1\u6709\u5728\u201c\u5b9e\u65f6\u201d\u5fae\u89c2\u6a21\u62df\u573a\u666f\u4e2d\u8fdb\u884c\u5728\u7ebf\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6307\u6807\u6ca1\u6709\u5145\u5206\u8003\u8651\u4ea4\u901a\u5de5\u7a0b\u7684\u5177\u4f53\u95ee\u9898\uff0c\u4f8b\u5982\u95ef\u7ea2\u706f\u3001\u4e0d\u5141\u8bb8\u505c\u8f66\u7b49\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7efc\u5408\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u8bad\u7ec3\u3001\u8fd0\u884c\u548c\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u80fd\u591f\u66f4\u597d\u6d1e\u5bdf\u6a21\u578b\u6027\u80fd\u7684\u6307\u6807\u3002", "result": "\u5728\u5fae\u89c2\u6a21\u62df\u5668\u4e2d\uff0c\u5728\u672a\u77e5\u7684\u4ea4\u901a\u6761\u4ef6\u4e0b\uff0c\u5728\u7ebf\u8bc4\u4f30\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u4f7f\u7528\u4e86\u7406\u60f3\u5316\u7684\u8f68\u8ff9\u4f5c\u4e3a\u8f93\u5165\u5e76\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u8f68\u8ff9\u91cd\u5efa\u8bef\u5dee\uff0c\u4f46\u751f\u6210\u7684\u8f68\u8ff9\u663e\u793a\u51fa\u8fdd\u53cd\u4ea4\u901a\u89c4\u5219\u7684\u884c\u4e3a\u3002\u5f15\u5165\u4e86\u65b0\u7684\u6307\u6807\u6765\u8bc4\u4f30\u8fd9\u79cd\u4e0d\u826f\u884c\u4e3a\uff0c\u5e76\u5c55\u793a\u4e86\u7ed3\u679c\u3002", "conclusion": "\u5c3d\u7ba1\u4f7f\u7528\u4e86\u7406\u60f3\u5316\u7684\u8f68\u8ff9\u4f5c\u4e3a\u8f93\u5165\u5e76\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u8f68\u8ff9\u91cd\u5efa\u8bef\u5dee\uff0c\u4f46\u751f\u6210\u7684\u8f68\u8ff9\u663e\u793a\u51fa\u8fdd\u53cd\u4ea4\u901a\u89c4\u5219\u7684\u884c\u4e3a\u3002", "summary_zh": "\u4ea4\u901a\u8def\u53e3\u5bf9\u4e8e\u57ce\u5e02\u9053\u8def\u7f51\u7edc\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u4eec\u8c03\u8282\u7740\u4eba\u5458\u548c\u8d27\u7269\u7684\u6d41\u52a8\u3002\u7136\u800c\uff0c\u5b83\u4eec\u4e5f\u662f\u8f68\u8ff9\u51b2\u7a81\u7684\u533a\u57df\uff0c\u5bb9\u6613\u53d1\u751f\u4e8b\u6545\u3002\u4fe1\u53f7\u4ea4\u53c9\u53e3\u4ea4\u901a\u52a8\u6001\u7684\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u53ef\u4ee5\u6781\u5927\u5730\u5e2e\u52a9\u4ea4\u901a\u90e8\u95e8\u66f4\u597d\u5730\u7406\u89e3\u6548\u7387\u548c\u5b89\u5168\u65b9\u9762\u3002\u76ee\u524d\uff0c\u6a21\u578b\u5728\u8ba1\u7b97\u6307\u6807\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u91cd\u5efa\u8bef\u5dee\u3002\u5b83\u4eec\u6ca1\u6709\u5728\u201c\u5b9e\u65f6\u201d\u5fae\u89c2\u6a21\u62df\u573a\u666f\u4e2d\u8fdb\u884c\u5728\u7ebf\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6307\u6807\u6ca1\u6709\u5145\u5206\u8003\u8651\u4ea4\u901a\u5de5\u7a0b\u7684\u5177\u4f53\u95ee\u9898\uff0c\u4f8b\u5982\u95ef\u7ea2\u706f\u3001\u4e0d\u5141\u8bb8\u505c\u8f66\u7b49\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7efc\u5408\u5206\u6790\u5de5\u5177\u6765\u8bad\u7ec3\u3001\u8fd0\u884c\u548c\u8bc4\u4f30\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u80fd\u591f\u66f4\u597d\u6d1e\u5bdf\u6a21\u578b\u6027\u80fd\u7684\u6307\u6807\uff0c\u4ece\u4ea4\u901a\u5de5\u7a0b\u7684\u89d2\u5ea6\u6765\u770b\u3002\u6211\u4eec\u5728\u4e00\u4e2a\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6570\u636e\u96c6\u662f\u901a\u8fc7\u8fd0\u884c\u771f\u5b9e\u57ce\u5e02\u4ea4\u53c9\u53e3\u7684\u6821\u51c6\u573a\u666f\u6536\u96c6\u7684\u3002\u7136\u540e\uff0c\u5728\u5fae\u89c2\u6a21\u62df\u5668\u4e2d\uff0c\u5728\u672a\u77e5\u7684\u4ea4\u901a\u6761\u4ef6\u4e0b\uff0c\u5728\u7ebf\u8bc4\u4f30\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u8868\u660e\uff0c\u5c3d\u7ba1\u4f7f\u7528\u4e86\u7406\u60f3\u5316\u7684\u8f68\u8ff9\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u5b9e\u73b0\u4e86\u8f83\u4f4e\u7684\u8f68\u8ff9\u91cd\u5efa\u8bef\u5dee\uff0c\u4f46\u751f\u6210\u7684\u8f68\u8ff9\u663e\u793a\u51fa\u8fdd\u53cd\u4ea4\u901a\u89c4\u5219\u7684\u884c\u4e3a\u3002\u6211\u4eec\u5f15\u5165\u4e86\u65b0\u7684\u6307\u6807\u6765\u8bc4\u4f30\u8fd9\u79cd\u4e0d\u826f\u884c\u4e3a\uff0c\u5e76\u5c55\u793a\u4e86\u6211\u4eec\u7684\u7ed3\u679c\u3002"}}
{"id": "2506.08553", "pdf": "https://arxiv.org/pdf/2506.08553", "abs": "https://arxiv.org/abs/2506.08553", "authors": ["Agnese Taluzzi", "Davide Gesualdi", "Riccardo Santambrogio", "Chiara Plizzari", "Francesca Palermo", "Simone Mentasti", "Matteo Matteucci"], "title": "From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge", "categories": ["cs.CV"], "comment": "Technical report for the HD-EPIC VQA Challenge 2025 (1st place)", "summary": "This report presents SceneNet and KnowledgeNet, our approaches developed for\nthe HD-EPIC VQA Challenge 2025. SceneNet leverages scene graphs generated with\na multi-modal large language model (MLLM) to capture fine-grained object\ninteractions, spatial relationships, and temporally grounded events. In\nparallel, KnowledgeNet incorporates ConceptNet's external commonsense knowledge\nto introduce high-level semantic connections between entities, enabling\nreasoning beyond directly observable visual evidence. Each method demonstrates\ndistinct strengths across the seven categories of the HD-EPIC benchmark, and\ntheir combination within our framework results in an overall accuracy of 44.21%\non the challenge, highlighting its effectiveness for complex egocentric VQA\ntasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a multi-modal large language model (MLLM) to generate scene graphs, which are then used for VQA. While not directly related to trajectory prediction, the use of scene graphs and understanding object interactions could be relevant for future work in predicting agent behavior in a scene. The mention of a multi-modal large language model also increases the relevance.", "keywords": ["large language model", "MLLM", "scene graphs", "object interactions"]}, "AI": {"tldr": "SceneNet\u548cKnowledgeNet\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\u4e0e\u5e38\u8bc6\u77e5\u8bc6\uff0c\u5728HD-EPIC VQA\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u8be5\u62a5\u544a\u65e8\u5728\u89e3\u51b3HD-EPIC VQA\u6311\u6218\u8d5b\u4e2d\u7684\u590d\u6742\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u4efb\u52a1\u3002", "method": "SceneNet\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u573a\u666f\u56fe\uff0c\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u5bf9\u8c61\u4ea4\u4e92\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u65f6\u95f4\u5b9a\u4f4d\u4e8b\u4ef6\uff1bKnowledgeNet\u7ed3\u5408ConceptNet\u7684\u5916\u90e8\u5e38\u8bc6\u77e5\u8bc6\uff0c\u5f15\u5165\u5b9e\u4f53\u4e4b\u95f4\u7684\u9ad8\u5c42\u8bed\u4e49\u8fde\u63a5\u3002", "result": "SceneNet\u548cKnowledgeNet\u5728HD-EPIC\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e03\u4e2a\u7c7b\u522b\u4e2d\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u4f18\u52bf\uff0c\u5e76\u4e14\u5b83\u4eec\u5728\u6846\u67b6\u4e2d\u7684\u7ed3\u5408\u5728\u6311\u6218\u8d5b\u4e2d\u5b9e\u73b0\u4e8644.21%\u7684\u603b\u4f53\u51c6\u786e\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684SceneNet\u548cKnowledgeNet\u65b9\u6cd5\u5728HD-EPIC VQA\u6311\u6218\u8d5b\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u7ed3\u5408\u4e24\u8005\u5728\u590d\u6742\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2VQA\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8644.21%\u7684\u51c6\u786e\u7387\u3002", "summary_zh": "\u672c\u62a5\u544a\u4ecb\u7ecd\u4e86SceneNet\u548cKnowledgeNet\uff0c\u8fd9\u662f\u6211\u4eec\u4e3aHD-EPIC VQA 2025\u6311\u6218\u8d5b\u5f00\u53d1\u7684\u4e24\u79cd\u65b9\u6cd5\u3002SceneNet\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u7684\u573a\u666f\u56fe\u6765\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u5bf9\u8c61\u4ea4\u4e92\u3001\u7a7a\u95f4\u5173\u7cfb\u548c\u65f6\u95f4\u5b9a\u4f4d\u4e8b\u4ef6\u3002\u540c\u65f6\uff0cKnowledgeNet\u7ed3\u5408ConceptNet\u7684\u5916\u90e8\u5e38\u8bc6\u77e5\u8bc6\uff0c\u5f15\u5165\u5b9e\u4f53\u4e4b\u95f4\u7684\u9ad8\u5c42\u8bed\u4e49\u8fde\u63a5\uff0c\u4ece\u800c\u5b9e\u73b0\u8d85\u8d8a\u76f4\u63a5\u89c2\u5bdf\u5230\u7684\u89c6\u89c9\u8bc1\u636e\u7684\u63a8\u7406\u3002\u6bcf\u79cd\u65b9\u6cd5\u5728HD-EPIC\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e03\u4e2a\u7c7b\u522b\u4e2d\u90fd\u8868\u73b0\u51fa\u72ec\u7279\u7684\u4f18\u52bf\uff0c\u5e76\u4e14\u5b83\u4eec\u5728\u6211\u4eec\u7684\u6846\u67b6\u4e2d\u7684\u7ed3\u5408\u5728\u6311\u6218\u8d5b\u4e2d\u5b9e\u73b0\u4e8644.21%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u7a81\u663e\u4e86\u5176\u5728\u590d\u6742\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2VQA\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.09049", "pdf": "https://arxiv.org/pdf/2506.09049", "abs": "https://arxiv.org/abs/2506.09049", "authors": ["Li Kang", "Xiufeng Song", "Heng Zhou", "Yiran Qin", "Jie Yang", "Xiaohong Liu", "Philip Torr", "Lei Bai", "Zhenfei Yin"], "title": "VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning", "categories": ["cs.AI", "cs.CV", "cs.RO"], "comment": "Project page: https://faceong.github.io/VIKI-R/", "summary": "Coordinating multiple embodied agents in dynamic environments remains a core\nchallenge in artificial intelligence, requiring both perception-driven\nreasoning and scalable cooperation strategies. While recent works have\nleveraged large language models (LLMs) for multi-agent planning, a few have\nbegun to explore vision-language models (VLMs) for visual reasoning. However,\nthese VLM-based approaches remain limited in their support for diverse\nembodiment types. In this work, we introduce VIKI-Bench, the first hierarchical\nbenchmark tailored for embodied multi-agent cooperation, featuring three\nstructured levels: agent activation, task planning, and trajectory perception.\nVIKI-Bench includes diverse robot embodiments, multi-view visual observations,\nand structured supervision signals to evaluate reasoning grounded in visual\ninputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a\ntwo-stage framework that fine-tunes a pretrained vision-language model (VLM)\nusing Chain-of-Thought annotated demonstrations, followed by reinforcement\nlearning under multi-level reward signals. Our extensive experiments show that\nVIKI-R significantly outperforms baselines method across all task levels.\nFurthermore, we show that reinforcement learning enables the emergence of\ncompositional cooperation patterns among heterogeneous agents. Together,\nVIKI-Bench and VIKI-R offer a unified testbed and method for advancing\nmulti-agent, visual-driven cooperation in embodied AI systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u5e76\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u548c\u8f68\u8ff9\u9884\u6d4b\u90fd\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u4efb\u52a1\u89c4\u5212\u548c\u8f68\u8ff9\u611f\u77e5\u5c42\u9762\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002", "keywords": ["large language models", "vision-language models", "multi-agent", "trajectory perception"]}, "AI": {"tldr": "VIKI-Bench\u548cVIKI-R\u4e3a\u63a8\u8fdb\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u591a\u667a\u80fd\u4f53\u3001\u89c6\u89c9\u9a71\u52a8\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u65b9\u6cd5\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u534f\u8c03\u591a\u4e2a\u5177\u8eab\u667a\u80fd\u4f53\u4ecd\u7136\u662f\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u611f\u77e5\u9a71\u52a8\u7684\u63a8\u7406\u548c\u53ef\u6269\u5c55\u7684\u534f\u4f5c\u7b56\u7565\u3002\u73b0\u6709\u7684\u57fa\u4e8eVLM\u7684\u65b9\u6cd5\u5728\u652f\u6301\u591a\u6837\u5316\u7684\u5177\u8eab\u7c7b\u578b\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVIKI-R\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u601d\u7ef4\u94fe\u6ce8\u91ca\u6f14\u793a\u5bf9\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u5728\u591a\u7ea7\u5956\u52b1\u4fe1\u53f7\u4e0b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "VIKI-R\u5728\u6240\u6709\u4efb\u52a1\u7ea7\u522b\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u5f02\u6784\u667a\u80fd\u4f53\u4e4b\u95f4\u7ec4\u5408\u534f\u4f5c\u6a21\u5f0f\u7684\u51fa\u73b0\u3002", "conclusion": "VIKI-R\u5728\u6240\u6709\u4efb\u52a1\u7ea7\u522b\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u5f02\u6784\u667a\u80fd\u4f53\u4e4b\u95f4\u7ec4\u5408\u534f\u4f5c\u6a21\u5f0f\u7684\u51fa\u73b0\u3002", "summary_zh": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u534f\u8c03\u591a\u4e2a\u5177\u8eab\u667a\u80fd\u4f53\u4ecd\u7136\u662f\u4eba\u5de5\u667a\u80fd\u9886\u57df\u7684\u6838\u5fc3\u6311\u6218\uff0c\u8fd9\u9700\u8981\u611f\u77e5\u9a71\u52a8\u7684\u63a8\u7406\u548c\u53ef\u6269\u5c55\u7684\u534f\u4f5c\u7b56\u7565\u3002\u867d\u7136\u6700\u8fd1\u7684\u5de5\u4f5c\u5df2\u7ecf\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u89c4\u5212\uff0c\u4f46\u4e5f\u6709\u4e00\u4e9b\u5de5\u4f5c\u5f00\u59cb\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u57fa\u4e8eVLM\u7684\u65b9\u6cd5\u5728\u652f\u6301\u591a\u6837\u5316\u7684\u5177\u8eab\u7c7b\u578b\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86VIKI-Bench\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4e3a\u5177\u8eab\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u91cf\u8eab\u5b9a\u5236\u7684\u5206\u5c42\u57fa\u51c6\uff0c\u5177\u6709\u4e09\u4e2a\u7ed3\u6784\u5316\u7ea7\u522b\uff1a\u667a\u80fd\u4f53\u6fc0\u6d3b\u3001\u4efb\u52a1\u89c4\u5212\u548c\u8f68\u8ff9\u611f\u77e5\u3002VIKI-Bench\u5305\u62ec\u4e0d\u540c\u7684\u673a\u5668\u4eba\u5177\u8eab\u3001\u591a\u89c6\u89d2\u89c6\u89c9\u89c2\u5bdf\u548c\u7ed3\u6784\u5316\u76d1\u7763\u4fe1\u53f7\uff0c\u4ee5\u8bc4\u4f30\u57fa\u4e8e\u89c6\u89c9\u8f93\u5165\u7684\u63a8\u7406\u3002\u4e3a\u4e86\u8bc1\u660eVIKI-Bench\u7684\u6548\u7528\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVIKI-R\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u601d\u7ef4\u94fe\u6ce8\u91ca\u6f14\u793a\u5bf9\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u7136\u540e\u5728\u591a\u7ea7\u5956\u52b1\u4fe1\u53f7\u4e0b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u3002\u6211\u4eec\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cVIKI-R\u5728\u6240\u6709\u4efb\u52a1\u7ea7\u522b\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8868\u660e\uff0c\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u5f02\u6784\u667a\u80fd\u4f53\u4e4b\u95f4\u7ec4\u5408\u534f\u4f5c\u6a21\u5f0f\u7684\u51fa\u73b0\u3002\u603b\u4e4b\uff0cVIKI-Bench\u548cVIKI-R\u4e3a\u63a8\u8fdb\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u4e2d\u591a\u667a\u80fd\u4f53\u3001\u89c6\u89c9\u9a71\u52a8\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d4b\u8bd5\u5e73\u53f0\u548c\u65b9\u6cd5\u3002"}}
{"id": "2506.08566", "pdf": "https://arxiv.org/pdf/2506.08566", "abs": "https://arxiv.org/abs/2506.08566", "authors": ["Yibo Cui", "Liang Xie", "Yu Zhao", "Jiawei Sun", "Erwei Yin"], "title": "Generating Vision-Language Navigation Instructions Incorporated Fine-Grained Alignment Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Navigation (VLN) enables intelligent agents to navigate\nenvironments by integrating visual perception and natural language\ninstructions, yet faces significant challenges due to the scarcity of\nfine-grained cross-modal alignment annotations. Existing datasets primarily\nfocus on global instruction-trajectory matching, neglecting\nsub-instruction-level and entity-level alignments critical for accurate\nnavigation action decision-making. To address this limitation, we propose\nFCA-NIG, a generative framework that automatically constructs navigation\ninstructions with dual-level fine-grained cross-modal annotations. In this\nframework, an augmented trajectory is first divided into sub-trajectories,\nwhich are then processed through GLIP-based landmark detection, crafted\ninstruction construction, OFA-Speaker based R2R-like instruction generation,\nand CLIP-powered entity selection, generating sub-instruction-trajectory pairs\nwith entity-landmark annotations. Finally, these sub-pairs are aggregated to\nform a complete instruction-trajectory pair. The framework generates the\nFCA-R2R dataset, the first large-scale augmentation dataset featuring precise\nsub-instruction-sub-trajectory and entity-landmark alignments. Extensive\nexperiments demonstrate that training with FCA-R2R significantly improves the\nperformance of multiple state-of-the-art VLN agents, including SF, EnvDrop,\nRecBERT, and HAMT. Incorporating sub-instruction-trajectory alignment enhances\nagents' state awareness and decision accuracy, while entity-landmark alignment\nfurther boosts navigation performance and generalization. These results\nhighlight the effectiveness of FCA-NIG in generating high-quality, scalable\ntraining data without manual annotation, advancing fine-grained cross-modal\nlearning in complex navigation tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper is related to navigation, which can be considered a form of trajectory prediction. It also uses and mentions models like GLIP, OFA, and CLIP, which can be considered large models, though not necessarily Large Language Models in the traditional sense. The core focus is on generating better training data for VLN, but the underlying task involves predicting navigation trajectories based on instructions.", "keywords": ["navigation", "trajectory", "GLIP", "OFA", "CLIP", "Vision-Language Navigation", "instruction-trajectory"]}, "AI": {"tldr": "FCA-NIG\u6846\u67b6\u81ea\u52a8\u751f\u6210\u5177\u6709\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u6ce8\u91ca\u7684\u5bfc\u822a\u6307\u4ee4\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4f7f\u667a\u80fd\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u611f\u77e5\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6765\u5bfc\u822a\u73af\u5883\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6ce8\u91ca\u800c\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5168\u5c40\u6307\u4ee4-\u8f68\u8ff9\u5339\u914d\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u4e8e\u51c6\u786e\u7684\u5bfc\u822a\u52a8\u4f5c\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u7684\u5b50\u6307\u4ee4\u7ea7\u522b\u548c\u5b9e\u4f53\u7ea7\u522b\u7684\u5bf9\u9f50\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86FCA-NIG\uff0c\u8fd9\u662f\u4e00\u4e2a\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u81ea\u52a8\u6784\u5efa\u5177\u6709\u53cc\u5c42\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u6ce8\u91ca\u7684\u5bfc\u822a\u6307\u4ee4\u3002\u5728\u8be5\u6846\u67b6\u4e2d\uff0c\u9996\u5148\u5c06\u589e\u5f3a\u7684\u8f68\u8ff9\u5206\u6210\u5b50\u8f68\u8ff9\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8eGLIP\u7684\u5730\u6807\u68c0\u6d4b\u3001\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\u6784\u5efa\u3001\u57fa\u4e8eOFA-Speaker\u7684R2R\u7c7b\u6307\u4ee4\u751f\u6210\u548cCLIP\u9a71\u52a8\u7684\u5b9e\u4f53\u9009\u62e9\u8fdb\u884c\u5904\u7406\uff0c\u4ece\u800c\u751f\u6210\u5e26\u6709\u5b9e\u4f53-\u5730\u6807\u6ce8\u91ca\u7684\u5b50\u6307\u4ee4-\u8f68\u8ff9\u5bf9\u3002\u6700\u540e\uff0c\u5c06\u8fd9\u4e9b\u5b50\u5bf9\u805a\u5408\u4ee5\u5f62\u6210\u5b8c\u6574\u7684\u6307\u4ee4-\u8f68\u8ff9\u5bf9\u3002", "result": "\u8be5\u6846\u67b6\u751f\u6210\u4e86FCA-R2R\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5177\u6709\u7cbe\u786e\u7684\u5b50\u6307\u4ee4-\u5b50\u8f68\u8ff9\u548c\u5b9e\u4f53-\u5730\u6807\u5bf9\u9f50\u7684\u5927\u89c4\u6a21\u589e\u5f3a\u6570\u636e\u96c6\u3002", "conclusion": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528FCA-R2R\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u4e2a\u6700\u5148\u8fdb\u7684VLN\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5305\u62ecSF\u3001EnvDrop\u3001RecBERT\u548cHAMT\u3002\u7ed3\u5408\u5b50\u6307\u4ee4-\u8f68\u8ff9\u5bf9\u9f50\u589e\u5f3a\u4e86\u4ee3\u7406\u7684\u72b6\u6001\u611f\u77e5\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u800c\u5b9e\u4f53-\u5730\u6807\u5bf9\u9f50\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5bfc\u822a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86FCA-NIG\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u65e0\u9700\u624b\u52a8\u6ce8\u91ca\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5b66\u4e60\u3002", "summary_zh": "\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4f7f\u667a\u80fd\u4ee3\u7406\u80fd\u591f\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u611f\u77e5\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6765\u5bfc\u822a\u73af\u5883\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6ce8\u91ca\u800c\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7684\u6570\u636e\u96c6\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5168\u5c40\u6307\u4ee4-\u8f68\u8ff9\u5339\u914d\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u4e8e\u51c6\u786e\u7684\u5bfc\u822a\u52a8\u4f5c\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u7684\u5b50\u6307\u4ee4\u7ea7\u522b\u548c\u5b9e\u4f53\u7ea7\u522b\u7684\u5bf9\u9f50\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FCA-NIG\uff0c\u8fd9\u662f\u4e00\u4e2a\u751f\u6210\u6846\u67b6\uff0c\u53ef\u4ee5\u81ea\u52a8\u6784\u5efa\u5177\u6709\u53cc\u5c42\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u6ce8\u91ca\u7684\u5bfc\u822a\u6307\u4ee4\u3002\u5728\u8be5\u6846\u67b6\u4e2d\uff0c\u9996\u5148\u5c06\u589e\u5f3a\u7684\u8f68\u8ff9\u5206\u6210\u5b50\u8f68\u8ff9\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8eGLIP\u7684\u5730\u6807\u68c0\u6d4b\u3001\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\u6784\u5efa\u3001\u57fa\u4e8eOFA-Speaker\u7684R2R\u7c7b\u6307\u4ee4\u751f\u6210\u548cCLIP\u9a71\u52a8\u7684\u5b9e\u4f53\u9009\u62e9\u8fdb\u884c\u5904\u7406\uff0c\u4ece\u800c\u751f\u6210\u5e26\u6709\u5b9e\u4f53-\u5730\u6807\u6ce8\u91ca\u7684\u5b50\u6307\u4ee4-\u8f68\u8ff9\u5bf9\u3002\u6700\u540e\uff0c\u5c06\u8fd9\u4e9b\u5b50\u5bf9\u805a\u5408\u4ee5\u5f62\u6210\u5b8c\u6574\u7684\u6307\u4ee4-\u8f68\u8ff9\u5bf9\u3002\u8be5\u6846\u67b6\u751f\u6210\u4e86FCA-R2R\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u5177\u6709\u7cbe\u786e\u7684\u5b50\u6307\u4ee4-\u5b50\u8f68\u8ff9\u548c\u5b9e\u4f53-\u5730\u6807\u5bf9\u9f50\u7684\u5927\u89c4\u6a21\u589e\u5f3a\u6570\u636e\u96c6\u3002\u5927\u91cf\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528FCA-R2R\u8bad\u7ec3\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u591a\u4e2a\u6700\u5148\u8fdb\u7684VLN\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u5305\u62ecSF\u3001EnvDrop\u3001RecBERT\u548cHAMT\u3002\u7ed3\u5408\u5b50\u6307\u4ee4-\u8f68\u8ff9\u5bf9\u9f50\u589e\u5f3a\u4e86\u4ee3\u7406\u7684\u72b6\u6001\u611f\u77e5\u548c\u51b3\u7b56\u51c6\u786e\u6027\uff0c\u800c\u5b9e\u4f53-\u5730\u6807\u5bf9\u9f50\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u5bfc\u822a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u51fa\u4e86FCA-NIG\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u65e0\u9700\u624b\u52a8\u6ce8\u91ca\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u5b66\u4e60\u3002"}}
{"id": "2506.08704", "pdf": "https://arxiv.org/pdf/2506.08704", "abs": "https://arxiv.org/abs/2506.08704", "authors": ["Xiaohan Zhang", "Sitong Wang", "Yushen Yan", "Yi Yang", "Mingda Xu", "Qi Liu"], "title": "TraGraph-GS: Trajectory Graph-based Gaussian Splatting for Arbitrary Large-Scale Scene Rendering", "categories": ["cs.CV"], "comment": null, "summary": "High-quality novel view synthesis for large-scale scenes presents a\nchallenging dilemma in 3D computer vision. Existing methods typically partition\nlarge scenes into multiple regions, reconstruct a 3D representation using\nGaussian splatting for each region, and eventually merge them for novel view\nrendering. They can accurately render specific scenes, yet they do not\ngeneralize effectively for two reasons: (1) rigid spatial partition techniques\nstruggle with arbitrary camera trajectories, and (2) the merging of regions\nresults in Gaussian overlap to distort texture details. To address these\nchallenges, we propose TraGraph-GS, leveraging a trajectory graph to enable\nhigh-precision rendering for arbitrarily large-scale scenes. We present a\nspatial partitioning method for large-scale scenes based on graphs, which\nincorporates a regularization constraint to enhance the rendering of textures\nand distant objects, as well as a progressive rendering strategy to mitigate\nartifacts caused by Gaussian overlap. Experimental results demonstrate its\nsuperior performance both on four aerial and four ground datasets and highlight\nits remarkable efficiency: our method achieves an average improvement of 1.86\ndB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to\nstate-of-the-art approaches.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on rendering large-scale scenes using Gaussian Splatting and trajectory graphs. While it involves trajectories, the core contribution is in rendering and 3D reconstruction, not trajectory prediction. There is no mention of large language models.", "keywords": ["trajectory graph", "rendering"]}, "AI": {"tldr": "TraGraph-GS\u5229\u7528\u8f68\u8ff9\u56fe\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u573a\u666f\u7684\u9ad8\u7cbe\u5ea6\u6e32\u67d3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65b0\u89c6\u89d2\u5408\u6210\u7684\u8d28\u91cf\u3002", "motivation": "Existing novel view synthesis methods for large-scale scenes struggle with arbitrary camera trajectories and Gaussian overlap issues when merging regions, leading to poor generalization and distorted texture details.", "method": "The authors propose a graph-based spatial partitioning method with a regularization constraint for texture and distant object rendering, along with a progressive rendering strategy to reduce Gaussian overlap artifacts.", "result": "Experimental results show that TraGraph-GS achieves an average improvement of 1.86 dB in PSNR on aerial datasets and 1.62 dB on ground datasets compared to state-of-the-art approaches.", "conclusion": "The paper introduces TraGraph-GS, a novel approach using trajectory graphs for high-precision rendering of large-scale scenes, demonstrating significant improvements in PSNR compared to existing methods on both aerial and ground datasets.", "summary_zh": "\u9488\u5bf9\u5927\u89c4\u6a21\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u65b0\u89c6\u89d2\u5408\u6210\u662f 3D \u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u96be\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5927\u578b\u573a\u666f\u5212\u5206\u4e3a\u591a\u4e2a\u533a\u57df\uff0c\u4f7f\u7528\u9ad8\u65af\u6e85\u5c04\u4e3a\u6bcf\u4e2a\u533a\u57df\u91cd\u5efa 3D \u8868\u793a\uff0c\u5e76\u6700\u7ec8\u5408\u5e76\u5b83\u4eec\u4ee5\u8fdb\u884c\u65b0\u89c6\u89d2\u6e32\u67d3\u3002\u5b83\u4eec\u53ef\u4ee5\u51c6\u786e\u5730\u6e32\u67d3\u7279\u5b9a\u573a\u666f\uff0c\u4f46\u7531\u4e8e\u4e24\u4e2a\u539f\u56e0\uff0c\u5b83\u4eec\u4e0d\u80fd\u6709\u6548\u5730\u6cdb\u5316\uff1a\uff081\uff09\u521a\u6027\u7a7a\u95f4\u5206\u5272\u6280\u672f\u96be\u4ee5\u5904\u7406\u4efb\u610f\u76f8\u673a\u8f68\u8ff9\uff0c\u4ee5\u53ca\uff082\uff09\u533a\u57df\u7684\u5408\u5e76\u5bfc\u81f4\u9ad8\u65af\u91cd\u53e0\uff0c\u4ece\u800c\u626d\u66f2\u7eb9\u7406\u7ec6\u8282\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 TraGraph-GS\uff0c\u5b83\u5229\u7528\u8f68\u8ff9\u56fe\u6765\u5b9e\u73b0\u4efb\u610f\u5927\u89c4\u6a21\u573a\u666f\u7684\u9ad8\u7cbe\u5ea6\u6e32\u67d3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u5927\u89c4\u6a21\u573a\u666f\u7a7a\u95f4\u5212\u5206\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6b63\u5219\u5316\u7ea6\u675f\uff0c\u4ee5\u589e\u5f3a\u7eb9\u7406\u548c\u8fdc\u5904\u7269\u4f53\u7684\u6e32\u67d3\uff0c\u4ee5\u53ca\u4e00\u79cd\u6e10\u8fdb\u5f0f\u6e32\u67d3\u7b56\u7565\uff0c\u4ee5\u51cf\u8f7b\u7531\u9ad8\u65af\u91cd\u53e0\u5f15\u8d77\u7684\u4f2a\u5f71\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5b83\u5728\u56db\u4e2a\u822a\u7a7a\u6570\u636e\u96c6\u548c\u56db\u4e2a\u5730\u9762\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u7a81\u51fa\u4e86\u5176\u5353\u8d8a\u7684\u6548\u7387\uff1a\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u822a\u7a7a\u6570\u636e\u96c6\u4e0a\u7684 PSNR \u5e73\u5747\u63d0\u9ad8\u4e86 1.86 dB\uff0c\u5728\u5730\u9762\u6570\u636e\u96c6\u4e0a\u7684 PSNR \u5e73\u5747\u63d0\u9ad8\u4e86 1.62 dB\u3002"}}
{"id": "2506.08729", "pdf": "https://arxiv.org/pdf/2506.08729", "abs": "https://arxiv.org/abs/2506.08729", "authors": ["Dieuwertje Alblas", "Patryk Rygiel", "Julian Suk", "Kaj O. Kappe", "Marieke Hofman", "Christoph Brune", "Kak Khee Yeung", "Jelmer M. Wolterink"], "title": "Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the\nabdominal aorta. AAAs may rupture, with a survival rate of only 20\\%. Current\nclinical guidelines recommend elective surgical repair when the maximum AAA\ndiameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet\nthese criteria are periodically monitored, with surveillance intervals based on\nthe maximum AAA diameter. However, this diameter does not take into account the\ncomplex relation between the 3D AAA shape and its growth, making standardized\nintervals potentially unfit. Personalized AAA growth predictions could improve\nmonitoring strategies. We propose to use an SE(3)-symmetric transformer model\nto predict AAA growth directly on the vascular model surface enriched with\nlocal, multi-physical features. In contrast to other works which have\nparameterized the AAA shape, this representation preserves the vascular\nsurface's anatomical structure and geometric fidelity. We train our model using\na longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24\nAAA patients at irregularly sampled intervals. After training, our model\npredicts AAA growth to the next scan moment with a median diameter error of\n1.18 mm. We further demonstrate our model's utility to identify whether a\npatient will become eligible for elective repair within two years (acc = 0.93).\nFinally, we evaluate our model's generalization on an external validation set\nconsisting of 25 CTAs from 7 AAA patients from a different hospital. Our\nresults show that local directional AAA growth prediction from the vascular\nsurface is feasible and may contribute to personalized surveillance strategies.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on predicting the growth of abdominal aortic aneurysms (AAAs) using a transformer model. While it doesn't explicitly deal with trajectory prediction in the traditional sense (e.g., predicting the path of a moving object), it does predict the 'trajectory' of the AAA's growth over time. The use of a transformer model, although not a large language model, indicates a connection to modern deep learning architectures. The model predicts a growth trajectory based on local features. It does not directly involve large language models.", "keywords": ["growth prediction", "transformer model", "prediction"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e SE(3) \u5bf9\u79f0\u53d8\u6362\u6a21\u578b\u7684 AAA \u751f\u957f\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u76f4\u63a5\u5728\u8840\u7ba1\u6a21\u578b\u8868\u9762\u4e0a\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u53d6\u5f97\u4e86\u826f\u597d\u7684\u9884\u6d4b\u6548\u679c\uff0c\u6709\u671b\u6539\u5584\u4e2a\u6027\u5316\u76d1\u6d4b\u7b56\u7565\u3002", "motivation": "\u8179\u4e3b\u52a8\u8109\u7624 (AAA) \u662f\u8179\u4e3b\u52a8\u8109\u7684\u8fdb\u884c\u6027\u5c40\u7076\u6027\u6269\u5f20\u3002AAA \u53ef\u80fd\u4f1a\u7834\u88c2\uff0c\u5b58\u6d3b\u7387\u4ec5\u4e3a 20%\u3002\u76ee\u524d\u7684\u4e34\u5e8a\u6307\u5357\u5efa\u8bae\uff0c\u5f53\u7537\u6027\u6700\u5927 AAA \u76f4\u5f84\u8d85\u8fc7 55 \u6beb\u7c73\u6216\u5973\u6027\u8d85\u8fc7 50 \u6beb\u7c73\u65f6\uff0c\u8fdb\u884c\u9009\u62e9\u6027\u624b\u672f\u4fee\u590d\u3002\u4e0d\u7b26\u5408\u8fd9\u4e9b\u6807\u51c6\u7684\u60a3\u8005\u4f1a\u5b9a\u671f\u63a5\u53d7\u76d1\u6d4b\uff0c\u76d1\u6d4b\u95f4\u9694\u57fa\u4e8e\u6700\u5927 AAA \u76f4\u5f84\u3002\u7136\u800c\uff0c\u8be5\u76f4\u5f84\u6ca1\u6709\u8003\u8651\u5230 3D AAA \u5f62\u72b6\u4e0e\u5176\u751f\u957f\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4f7f\u5f97\u6807\u51c6\u5316\u95f4\u9694\u53ef\u80fd\u4e0d\u5408\u9002\u3002\u4e2a\u6027\u5316\u7684 AAA \u751f\u957f\u9884\u6d4b\u53ef\u4ee5\u6539\u5584\u76d1\u6d4b\u7b56\u7565\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4f7f\u7528 SE(3) \u5bf9\u79f0\u53d8\u6362\u6a21\u578b\u6765\u76f4\u63a5\u9884\u6d4b\u8840\u7ba1\u6a21\u578b\u8868\u9762\u7684 AAA \u751f\u957f\uff0c\u8be5\u8868\u9762\u5bcc\u542b\u5c40\u90e8\u591a\u7269\u7406\u7279\u5f81\u3002\u4e0e\u53c2\u6570\u5316 AAA \u5f62\u72b6\u7684\u5176\u4ed6\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8fd9\u79cd\u8868\u793a\u4fdd\u7559\u4e86\u8840\u7ba1\u8868\u9762\u7684\u89e3\u5256\u7ed3\u6784\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "result": "\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u9884\u6d4b\u4e0b\u4e00\u6b21\u626b\u63cf\u65f6\u523b\u7684 AAA \u751f\u957f\uff0c\u4e2d\u503c\u76f4\u5f84\u8bef\u5dee\u4e3a 1.18 \u6beb\u7c73\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u8bc6\u522b\u60a3\u8005\u662f\u5426\u5c06\u5728\u4e24\u5e74\u5185\u6709\u8d44\u683c\u63a5\u53d7\u9009\u62e9\u6027\u4fee\u590d\u65b9\u9762\u7684\u6548\u7528\uff08acc = 0.93\uff09\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u5916\u90e8\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8be5\u9a8c\u8bc1\u96c6\u7531\u6765\u81ea\u4e0d\u540c\u533b\u9662\u7684 7 \u540d AAA \u60a3\u8005\u7684 25 \u6b21 CTA \u626b\u63cf\u7ec4\u6210\u3002", "conclusion": "\u5c40\u90e8\u65b9\u5411\u6027 AAA \u4ece\u8840\u7ba1\u8868\u9762\u7684\u751f\u957f\u9884\u6d4b\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u80fd\u6709\u52a9\u4e8e\u4e2a\u6027\u5316\u76d1\u6d4b\u7b56\u7565\u3002", "summary_zh": "\u8179\u4e3b\u52a8\u8109\u7624 (AAA) \u662f\u8179\u4e3b\u52a8\u8109\u7684\u8fdb\u884c\u6027\u5c40\u90e8\u6269\u5f20\u3002AAA \u53ef\u80fd\u4f1a\u7834\u88c2\uff0c\u5b58\u6d3b\u7387\u4ec5\u4e3a 20%\u3002\u76ee\u524d\u7684\u4e34\u5e8a\u6307\u5357\u5efa\u8bae\uff0c\u5f53\u7537\u6027\u6700\u5927 AAA \u76f4\u5f84\u8d85\u8fc7 55 \u6beb\u7c73\u6216\u5973\u6027\u8d85\u8fc7 50 \u6beb\u7c73\u65f6\uff0c\u8fdb\u884c\u9009\u62e9\u6027\u624b\u672f\u4fee\u590d\u3002\u4e0d\u7b26\u5408\u8fd9\u4e9b\u6807\u51c6\u7684\u60a3\u8005\u4f1a\u5b9a\u671f\u63a5\u53d7\u76d1\u6d4b\uff0c\u76d1\u6d4b\u95f4\u9694\u57fa\u4e8e\u6700\u5927 AAA \u76f4\u5f84\u3002\u7136\u800c\uff0c\u8be5\u76f4\u5f84\u6ca1\u6709\u8003\u8651\u5230 3D AAA \u5f62\u72b6\u4e0e\u5176\u751f\u957f\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4f7f\u5f97\u6807\u51c6\u5316\u95f4\u9694\u53ef\u80fd\u4e0d\u5408\u9002\u3002\u4e2a\u6027\u5316\u7684 AAA \u751f\u957f\u9884\u6d4b\u53ef\u4ee5\u6539\u5584\u76d1\u6d4b\u7b56\u7565\u3002\u6211\u4eec\u63d0\u51fa\u4f7f\u7528 SE(3) \u5bf9\u79f0\u53d8\u6362\u6a21\u578b\u6765\u76f4\u63a5\u9884\u6d4b\u8840\u7ba1\u6a21\u578b\u8868\u9762\u7684 AAA \u751f\u957f\uff0c\u8be5\u8868\u9762\u5bcc\u542b\u5c40\u90e8\u591a\u7269\u7406\u7279\u5f81\u3002\u4e0e\u53c2\u6570\u5316 AAA \u5f62\u72b6\u7684\u5176\u4ed6\u5de5\u4f5c\u76f8\u6bd4\uff0c\u8fd9\u79cd\u8868\u793a\u4fdd\u7559\u4e86\u8840\u7ba1\u8868\u9762\u7684\u89e3\u5256\u7ed3\u6784\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002\u6211\u4eec\u4f7f\u7528 24 \u540d AAA \u60a3\u8005\u7684 113 \u6b21\u8ba1\u7b97\u673a\u65ad\u5c42\u626b\u63cf\u8840\u7ba1\u9020\u5f71 (CTA) \u626b\u63cf\u7684\u7eb5\u5411\u6570\u636e\u96c6\uff0c\u4ee5\u4e0d\u89c4\u5219\u91c7\u6837\u95f4\u9694\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u3002\u7ecf\u8fc7\u8bad\u7ec3\uff0c\u6211\u4eec\u7684\u6a21\u578b\u53ef\u4ee5\u9884\u6d4b\u4e0b\u4e00\u6b21\u626b\u63cf\u65f6\u523b\u7684 AAA \u751f\u957f\uff0c\u4e2d\u503c\u76f4\u5f84\u8bef\u5dee\u4e3a 1.18 \u6beb\u7c73\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u8bc6\u522b\u60a3\u8005\u662f\u5426\u5c06\u5728\u4e24\u5e74\u5185\u6709\u8d44\u683c\u63a5\u53d7\u9009\u62e9\u6027\u4fee\u590d\u65b9\u9762\u7684\u6548\u7528\uff08acc = 0.93\uff09\u3002\u6700\u540e\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5728\u5916\u90e8\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8be5\u9a8c\u8bc1\u96c6\u7531\u6765\u81ea\u4e0d\u540c\u533b\u9662\u7684 7 \u540d AAA \u60a3\u8005\u7684 25 \u6b21 CTA \u626b\u63cf\u7ec4\u6210\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5c40\u90e8\u65b9\u5411\u6027 AAA \u4ece\u8840\u7ba1\u8868\u9762\u7684\u751f\u957f\u9884\u6d4b\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ef\u80fd\u6709\u52a9\u4e8e\u4e2a\u6027\u5316\u76d1\u6d4b\u7b56\u7565\u3002"}}
{"id": "2506.08441", "pdf": "https://arxiv.org/pdf/2506.08441", "abs": "https://arxiv.org/abs/2506.08441", "authors": ["Anh N. Nhu", "Sanghyun Son", "Ming Lin"], "title": "Time-Aware World Model for Adaptive Prediction and Control", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Paper accepted to ICML 2025", "summary": "In this work, we introduce the Time-Aware World Model (TAWM), a model-based\napproach that explicitly incorporates temporal dynamics. By conditioning on the\ntime-step size, {\\Delta}t, and training over a diverse range of {\\Delta}t\nvalues -- rather than sampling at a fixed time-step -- TAWM learns both high-\nand low-frequency task dynamics across diverse control problems. Grounded in\nthe information-theoretic insight that the optimal sampling rate depends on a\nsystem's underlying dynamics, this time-aware formulation improves both\nperformance and data efficiency. Empirical evaluations show that TAWM\nconsistently outperforms conventional models across varying observation rates\nin a variety of control tasks, using the same number of training samples and\niterations. Our code can be found online at:\ngithub.com/anh-nn01/Time-Aware-World-Model.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65f6\u95f4\u611f\u77e5\u7684\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u9884\u6d4b\u548c\u63a7\u5236\u3002 \u867d\u7136\u5b83\u6ca1\u6709\u660e\u786e\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u5173\u6ce8\u7684\u662f\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\uff0c\u5176\u4e2d\u9884\u6d4b\u7cfb\u7edf\u672a\u6765\u72b6\u6001\u7684\u80fd\u529b\u662f\u6838\u5fc3\u3002 \"World Model\" \u672c\u8eab\u7684\u6982\u5ff5\u4e5f\u7ecf\u5e38\u4e0e\u5927\u578b\u6a21\u578b\u7684\u5e94\u7528\u76f8\u5173\u3002 \u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e0d\u662f\u76f4\u63a5\u76f8\u5173\u3002", "keywords": ["World Model", "prediction", "control", "temporal dynamics"]}, "AI": {"tldr": "TAWM\u901a\u8fc7\u663e\u5f0f\u5730\u7ed3\u5408\u65f6\u95f4\u52a8\u6001\uff0c\u5b66\u4e60\u4e86\u5404\u79cd\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u9ad8\u9891\u548c\u4f4e\u9891\u4efb\u52a1\u52a8\u6001\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u578b\u901a\u5e38\u4ee5\u56fa\u5b9a\u7684\u65f6\u95f4\u6b65\u957f\u8fdb\u884c\u91c7\u6837\uff0c\u5ffd\u7565\u4e86\u7cfb\u7edf\u5e95\u5c42\u52a8\u6001\u5bf9\u6700\u4f73\u91c7\u6837\u7387\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u65f6\u95f4\u611f\u77e5\u4e16\u754c\u6a21\u578b(TAWM)\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u95f4\u611f\u77e5\u4e16\u754c\u6a21\u578b(TAWM)\uff0c\u8be5\u6a21\u578b\u663e\u5f0f\u5730\u7ed3\u5408\u4e86\u65f6\u95f4\u52a8\u6001\u3002\u901a\u8fc7\u8c03\u8282\u65f6\u95f4\u6b65\u957f{\\Delta}t\uff0c\u5e76\u5728\u4e0d\u540c\u7684{\\Delta}t\u503c\u8303\u56f4\u5185\u8fdb\u884c\u8bad\u7ec3\uff0cTAWM\u5b66\u4e60\u4e86\u5404\u79cd\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u9ad8\u9891\u548c\u4f4e\u9891\u4efb\u52a1\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTAWM\u5728\u5404\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u76f8\u540c\u6570\u91cf\u7684\u8bad\u7ec3\u6837\u672c\u548c\u8fed\u4ee3\u6b21\u6570\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u89c2\u5bdf\u7387\u4e0b\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u65f6\u95f4\u611f\u77e5\u4e16\u754c\u6a21\u578b(TAWM)\u5728\u5404\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u4f7f\u7528\u76f8\u540c\u6570\u91cf\u7684\u8bad\u7ec3\u6837\u672c\u548c\u8fed\u4ee3\u6b21\u6570\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u89c2\u5bdf\u7387\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "summary_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65f6\u95f4\u611f\u77e5\u4e16\u754c\u6a21\u578b(TAWM)\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5b83\u663e\u5f0f\u5730\u7ed3\u5408\u4e86\u65f6\u95f4\u52a8\u6001\u3002\u901a\u8fc7\u8c03\u8282\u65f6\u95f4\u6b65\u957f{\\Delta}t\uff0c\u5e76\u5728\u4e0d\u540c\u7684{\\Delta}t\u503c\u8303\u56f4\u5185\u8fdb\u884c\u8bad\u7ec3\uff0cTAWM\u5b66\u4e60\u4e86\u5404\u79cd\u63a7\u5236\u95ee\u9898\u4e2d\u7684\u9ad8\u9891\u548c\u4f4e\u9891\u4efb\u52a1\u52a8\u6001\u3002\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6d1e\u5bdf\uff0c\u5373\u6700\u4f73\u91c7\u6837\u7387\u53d6\u51b3\u4e8e\u7cfb\u7edf\u7684\u5e95\u5c42\u52a8\u6001\uff0c\u8fd9\u79cd\u65f6\u95f4\u611f\u77e5\u516c\u5f0f\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cTAWM\u5728\u5404\u79cd\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528\u76f8\u540c\u6570\u91cf\u7684\u8bad\u7ec3\u6837\u672c\u548c\u8fed\u4ee3\u6b21\u6570\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u7684\u89c2\u5bdf\u7387\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002"}}
{"id": "2506.08817", "pdf": "https://arxiv.org/pdf/2506.08817", "abs": "https://arxiv.org/abs/2506.08817", "authors": ["Shuyi Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Lingfeng Zhang", "Pengwei Wang", "Zhongyuan Wang", "Hongxuan Ma", "Shanghang Zhang"], "title": "Video-CoT: A Comprehensive Dataset for Spatiotemporal Understanding of Videos Based on Chain-of-Thought", "categories": ["cs.CV"], "comment": null, "summary": "Video content comprehension is essential for various applications, ranging\nfrom video analysis to interactive systems. Despite advancements in large-scale\nvision-language models (VLMs), these models often struggle to capture the\nnuanced, spatiotemporal details essential for thorough video analysis. To\naddress this gap, we introduce Video-CoT, a groundbreaking dataset designed to\nenhance spatiotemporal understanding using Chain-of-Thought (CoT)\nmethodologies. Video-CoT contains 192,000 fine-grained spa-tiotemporal\nquestion-answer pairs and 23,000 high-quality CoT-annotated samples, providing\na solid foundation for evaluating spatiotemporal understanding in video\ncomprehension. Additionally, we provide a comprehensive benchmark for assessing\nthese tasks, with each task featuring 750 images and tailored evaluation\nmetrics. Our extensive experiments reveal that current VLMs face significant\nchallenges in achieving satisfactory performance, high-lighting the\ndifficulties of effective spatiotemporal understanding. Overall, the Video-CoT\ndataset and benchmark open new avenues for research in multimedia understanding\nand support future innovations in intelligent systems requiring advanced video\nanalysis capabilities. By making these resources publicly available, we aim to\nencourage further exploration in this critical area. Project\nwebsite:https://video-cot.github.io/ .", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on spatiotemporal understanding of videos using Chain-of-Thought methodologies and large-scale vision-language models (VLMs). While it doesn't directly address trajectory prediction, the spatiotemporal understanding aspect is relevant, and it explicitly mentions VLMs, connecting it to large language models. The connection is not strong, but present.", "keywords": ["spatiotemporal understanding", "large-scale vision-language models", "VLMs", "Chain-of-Thought"]}, "AI": {"tldr": "Video-CoT\u6570\u636e\u96c6\u901a\u8fc7\u63d0\u4f9b\u5927\u89c4\u6a21\u65f6\u7a7a\u95ee\u7b54\u5bf9\u548cCoT\u6807\u6ce8\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u9891\u7406\u89e3\u4e2d\u6a21\u578b\u7684\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "Large-scale vision-language models (VLMs) often struggle to capture the nuanced, spatiotemporal details essential for thorough video analysis.", "method": "The authors introduce Video-CoT, a dataset with 192,000 spatiotemporal question-answer pairs and 23,000 CoT-annotated samples, along with a benchmark for evaluation.", "result": "Experiments show that current VLMs face significant challenges in achieving satisfactory performance on the Video-CoT dataset.", "conclusion": "Current VLMs struggle with spatiotemporal understanding, highlighting the need for improved models.", "summary_zh": "\u4e3a\u4e86\u63d0\u5347\u89c6\u9891\u5206\u6790\u4e2d\u65f6\u7a7a\u7406\u89e3\u7684\u80fd\u529b\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86Video-CoT\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b192,000\u4e2a\u7cbe\u7ec6\u7684\u65f6\u7a7a\u95ee\u7b54\u5bf9\u548c23,000\u4e2a\u9ad8\u8d28\u91cf\u7684CoT\u6807\u6ce8\u6837\u672c\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8fd9\u4e9b\u4efb\u52a1\uff0c\u6bcf\u4e2a\u4efb\u52a1\u5305\u542b750\u5f20\u56fe\u50cf\u548c\u5b9a\u5236\u7684\u8bc4\u4f30\u6307\u6807\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u7684VLMs\u5728\u5b9e\u73b0\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u65b9\u9762\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u7a81\u663e\u4e86\u6709\u6548\u65f6\u7a7a\u7406\u89e3\u7684\u56f0\u96be\u3002Video-CoT\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4e3a\u591a\u5a92\u4f53\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u9014\u5f84\uff0c\u5e76\u652f\u6301\u672a\u6765\u9700\u8981\u9ad8\u7ea7\u89c6\u9891\u5206\u6790\u529f\u80fd\u7684\u667a\u80fd\u7cfb\u7edf\u521b\u65b0\u3002"}}
{"id": "2506.08902", "pdf": "https://arxiv.org/pdf/2506.08902", "abs": "https://arxiv.org/abs/2506.08902", "authors": ["Chongyi Zheng", "Seohong Park", "Sergey Levine", "Benjamin Eysenbach"], "title": "Intention-Conditioned Flow Occupancy Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large-scale pre-training has fundamentally changed how machine learning\nresearch is done today: large foundation models are trained once, and then can\nbe used by anyone in the community (including those without data or compute\nresources to train a model from scratch) to adapt and fine-tune to specific\ntasks. Applying this same framework to reinforcement learning (RL) is appealing\nbecause it offers compelling avenues for addressing core challenges in RL,\nincluding sample efficiency and robustness. However, there remains a\nfundamental challenge to pre-train large models in the context of RL: actions\nhave long-term dependencies, so training a foundation model that reasons across\ntime is important. Recent advances in generative AI have provided new tools for\nmodeling highly complex distributions. In this paper, we build a probabilistic\nmodel to predict which states an agent will visit in the temporally distant\nfuture (i.e., an occupancy measure) using flow matching. As large datasets are\noften constructed by many distinct users performing distinct tasks, we include\nin our model a latent variable capturing the user intention. This intention\nincreases the expressivity of our model, and enables adaptation with\ngeneralized policy improvement. We call our proposed method\nintention-conditioned flow occupancy models (InFOM). Comparing with alternative\nmethods for pre-training, our experiments on $36$ state-based and $4$\nimage-based benchmark tasks demonstrate that the proposed method achieves $1.8\n\\times$ median improvement in returns and increases success rates by $36\\%$.\nWebsite: https://chongyi-zheng.github.io/infom Code:\nhttps://github.com/chongyi-zheng/infom", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses pre-training large models in the context of reinforcement learning, which can be relevant to trajectory prediction as it models future states. The abstract mentions 'large foundation models' and 'generative AI', indicating a connection to large language models, although not directly used for trajectory prediction in this specific context. The prediction of which states an agent will visit in the future is conceptually related to trajectory prediction.", "keywords": ["large foundation models", "generative AI", "reinforcement learning", "occupancy measure", "intention-conditioned"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a InFOM \u7684\u610f\u56fe\u6761\u4ef6\u6d41\u5360\u7528\u6a21\u578b\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u9884\u8bad\u7ec3\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u72b6\u6001\u548c\u5229\u7528\u7528\u6237\u610f\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56de\u62a5\u548c\u6210\u529f\u7387\u3002", "motivation": "Applying large-scale pre-training to reinforcement learning can address core challenges like sample efficiency and robustness, but pre-training models that reason across time remains a challenge.", "method": "The paper builds a probabilistic model using flow matching to predict future states, incorporating a latent variable to capture user intention and enable adaptation with generalized policy improvement.", "result": "Experiments on 36 state-based and 4 image-based benchmark tasks demonstrate that InFOM achieves 1.8x median improvement in returns and increases success rates by 36%.", "conclusion": "The proposed InFOM method achieves significant improvements in returns and success rates compared to alternative pre-training methods on a range of benchmark tasks.", "summary_zh": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4ece\u6839\u672c\u4e0a\u6539\u53d8\u4e86\u5f53\u4eca\u673a\u5668\u5b66\u4e60\u7684\u7814\u7a76\u65b9\u5f0f\uff1a\u5927\u578b\u57fa\u7840\u6a21\u578b\u7ecf\u8fc7\u4e00\u6b21\u8bad\u7ec3\u540e\uff0c\u793e\u533a\u4e2d\u7684\u4efb\u4f55\u4eba\u90fd\u53ef\u4ee5\u4f7f\u7528\u5b83\u4eec\uff08\u5305\u62ec\u90a3\u4e9b\u6ca1\u6709\u6570\u636e\u6216\u8ba1\u7b97\u8d44\u6e90\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b\u7684\u4eba\uff09\u6765\u9002\u5e94\u548c\u5fae\u8c03\u7279\u5b9a\u4efb\u52a1\u3002\u5c06\u8fd9\u79cd\u76f8\u540c\u7684\u6846\u67b6\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60 (RL) \u5f88\u6709\u5438\u5f15\u529b\uff0c\u56e0\u4e3a\u5b83\u4e3a\u89e3\u51b3 RL \u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff08\u5305\u62ec\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\uff09\u63d0\u4f9b\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u9014\u5f84\u3002\u7136\u800c\uff0c\u5728 RL \u7684\u80cc\u666f\u4e0b\u9884\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u4ecd\u7136\u5b58\u5728\u4e00\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\uff1a\u52a8\u4f5c\u5177\u6709\u957f\u671f\u4f9d\u8d56\u6027\uff0c\u56e0\u6b64\u8bad\u7ec3\u4e00\u4e2a\u80fd\u591f\u8de8\u65f6\u95f4\u63a8\u7406\u7684\u57fa\u7840\u6a21\u578b\u975e\u5e38\u91cd\u8981\u3002\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u5efa\u6a21\u9ad8\u5ea6\u590d\u6742\u7684\u5206\u5e03\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u4e00\u4e2a\u6982\u7387\u6a21\u578b\uff0c\u4ee5\u4f7f\u7528\u6d41\u5339\u914d\u9884\u6d4b\u667a\u80fd\u4f53\u5728\u9065\u8fdc\u7684\u5c06\u6765\u5c06\u8bbf\u95ee\u54ea\u4e9b\u72b6\u6001\uff08\u5373\uff0c\u5360\u7528\u7387\u5ea6\u91cf\uff09\u3002\u7531\u4e8e\u5927\u578b\u6570\u636e\u96c6\u901a\u5e38\u7531\u6267\u884c\u4e0d\u540c\u4efb\u52a1\u7684\u8bb8\u591a\u4e0d\u540c\u7528\u6237\u6784\u5efa\uff0c\u56e0\u6b64\u6211\u4eec\u5728\u6a21\u578b\u4e2d\u5305\u542b\u4e00\u4e2a\u6f5c\u5728\u53d8\u91cf\uff0c\u7528\u4e8e\u6355\u83b7\u7528\u6237\u610f\u56fe\u3002\u8fd9\u79cd\u610f\u56fe\u63d0\u9ad8\u4e86\u6211\u4eec\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u80fd\u591f\u901a\u8fc7\u5e7f\u4e49\u7b56\u7565\u6539\u8fdb\u8fdb\u884c\u9002\u5e94\u3002\u6211\u4eec\u79f0\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u610f\u56fe\u6761\u4ef6\u6d41\u5360\u7528\u6a21\u578b (InFOM)\u3002\u4e0e\u66ff\u4ee3\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u5728 36 \u4e2a\u57fa\u4e8e\u72b6\u6001\u548c 4 \u4e2a\u57fa\u4e8e\u56fe\u50cf\u7684\u57fa\u51c6\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86 1.8 \u500d\u7684\u56de\u62a5\u4e2d\u4f4d\u6570\u6539\u8fdb\uff0c\u5e76\u5c06\u6210\u529f\u7387\u63d0\u9ad8\u4e86 36%\u3002"}}
{"id": "2506.09007", "pdf": "https://arxiv.org/pdf/2506.09007", "abs": "https://arxiv.org/abs/2506.09007", "authors": ["Sophia Tang", "Yinuo Zhang", "Alexander Tong", "Pranam Chatterjee"], "title": "Branched Schr\u00f6dinger Bridge Matching", "categories": ["cs.LG"], "comment": null, "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on predicting intermediate trajectories between distributions, which is related to trajectory prediction. Although it doesn't explicitly mention large language models, the problem it addresses is relevant to generative modeling, a field where large models are increasingly used. The mention of multi-path surface navigation is also suggestive of applications in robotics and autonomous driving, where trajectory prediction is crucial.", "keywords": ["trajectory prediction", "generative modeling", "Schr\u00f6dinger Bridge Matching", "multi-path surface navigation"]}, "AI": {"tldr": "BranchSBM \u5b66\u4e60\u5206\u652f\u859b\u5b9a\u8c14\u6865\uff0c\u80fd\u591f\u8868\u793a\u7fa4\u4f53\u6c34\u5e73\u53d1\u6563\u5230\u591a\u4e2a\u7ec8\u7aef\u5206\u5e03\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u83b7\u5206\u652f\u6216\u53d1\u6563\u6f14\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5982\u6d41\u5339\u914d\u548cSchr\"odinger Bridge Matching\uff0c\u901a\u8fc7\u5efa\u6a21\u5355\u4e2a\u968f\u673a\u8def\u5f84\u6765\u6709\u6548\u5730\u5b66\u4e60\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u6620\u5c04\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u672c\u8d28\u4e0a\u4ec5\u9650\u4e8e\u5355\u5cf0\u8f6c\u6362\uff0c\u65e0\u6cd5\u6355\u83b7\u4ece\u5171\u540c\u8d77\u6e90\u5230\u591a\u4e2a\u4e0d\u540c\u7ed3\u679c\u7684\u5206\u652f\u6216\u53d1\u6563\u6f14\u5316\u3002", "method": "BranchSBM\u53c2\u6570\u5316\u591a\u4e2a\u968f\u65f6\u95f4\u53d8\u5316\u7684\u901f\u5ea6\u573a\u548c\u589e\u957f\u8fc7\u7a0b\uff0c\u4ece\u800c\u80fd\u591f\u8868\u793a\u7fa4\u4f53\u6c34\u5e73\u53d1\u6563\u5230\u591a\u4e2a\u7ec8\u7aef\u5206\u5e03\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86BranchSBM\u4e0d\u4ec5\u66f4\u5177\u8868\u73b0\u529b\uff0c\u800c\u4e14\u5bf9\u4e8e\u6d89\u53ca\u591a\u8def\u5f84\u8868\u9762\u5bfc\u822a\u3001\u6a21\u62df\u6765\u81ea\u540c\u8d28\u7956\u7ec6\u80de\u72b6\u6001\u7684\u7ec6\u80de\u547d\u8fd0\u5206\u53c9\u4ee5\u53ca\u6a21\u62df\u5bf9\u6270\u52a8\u7684\u4e0d\u540c\u7ec6\u80de\u53cd\u5e94\u7684\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "BranchSBM\u4e0d\u4ec5\u66f4\u5177\u8868\u73b0\u529b\uff0c\u800c\u4e14\u5bf9\u4e8e\u6d89\u53ca\u591a\u8def\u5f84\u8868\u9762\u5bfc\u822a\u3001\u6a21\u62df\u6765\u81ea\u540c\u8d28\u7956\u7ec6\u80de\u72b6\u6001\u7684\u7ec6\u80de\u547d\u8fd0\u5206\u53c9\u4ee5\u53ca\u6a21\u62df\u5bf9\u6270\u52a8\u7684\u4e0d\u540c\u7ec6\u80de\u53cd\u5e94\u7684\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "summary_zh": "\u9884\u6d4b\u521d\u59cb\u5206\u5e03\u548c\u76ee\u6807\u5206\u5e03\u4e4b\u95f4\u7684\u4e2d\u95f4\u8f68\u8ff9\u662f\u751f\u6210\u5efa\u6a21\u4e2d\u7684\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\u3002\u73b0\u6709\u7684\u65b9\u6cd5\uff0c\u5982\u6d41\u5339\u914d\u548cSchr\"odinger Bridge Matching\uff0c\u901a\u8fc7\u5efa\u6a21\u5355\u4e2a\u968f\u673a\u8def\u5f84\u6765\u6709\u6548\u5730\u5b66\u4e60\u4e24\u4e2a\u5206\u5e03\u4e4b\u95f4\u7684\u6620\u5c04\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u672c\u8d28\u4e0a\u4ec5\u9650\u4e8e\u5355\u5cf0\u8f6c\u6362\uff0c\u65e0\u6cd5\u6355\u83b7\u4ece\u5171\u540c\u8d77\u6e90\u5230\u591a\u4e2a\u4e0d\u540c\u7ed3\u679c\u7684\u5206\u652f\u6216\u53d1\u6563\u6f14\u5316\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86Branched Schr\"odinger Bridge Matching (BranchSBM)\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b66\u4e60\u5206\u652fSchr\"odinger bridge\u7684\u65b0\u6846\u67b6\u3002BranchSBM\u53c2\u6570\u5316\u591a\u4e2a\u968f\u65f6\u95f4\u53d8\u5316\u7684\u901f\u5ea6\u573a\u548c\u589e\u957f\u8fc7\u7a0b\uff0c\u4ece\u800c\u80fd\u591f\u8868\u793a\u7fa4\u4f53\u6c34\u5e73\u53d1\u6563\u5230\u591a\u4e2a\u7ec8\u7aef\u5206\u5e03\u3002\u6211\u4eec\u8bc1\u660e\u4e86BranchSBM\u4e0d\u4ec5\u66f4\u5177\u8868\u73b0\u529b\uff0c\u800c\u4e14\u5bf9\u4e8e\u6d89\u53ca\u591a\u8def\u5f84\u8868\u9762\u5bfc\u822a\u3001\u6a21\u62df\u6765\u81ea\u540c\u8d28\u7956\u7ec6\u80de\u72b6\u6001\u7684\u7ec6\u80de\u547d\u8fd0\u5206\u53c9\u4ee5\u53ca\u6a21\u62df\u5bf9\u6270\u52a8\u7684\u4e0d\u540c\u7ec6\u80de\u53cd\u5e94\u7684\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.09046", "pdf": "https://arxiv.org/pdf/2506.09046", "abs": "https://arxiv.org/abs/2506.09046", "authors": ["Xiaowen Ma", "Chenyang Lin", "Yao Zhang", "Volker Tresp", "Yunpu Ma"], "title": "Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": null, "summary": "Leveraging multiple Large Language Models(LLMs) has proven effective for\naddressing complex, high-dimensional tasks, but current approaches often rely\non static, manually engineered multi-agent configurations. To overcome these\nconstraints, we present the Agentic Neural Network(ANN), a framework that\nconceptualizes multi-agent collaboration as a layered neural network\narchitecture. In this design, each agent operates as a node, and each layer\nforms a cooperative \"team\" focused on a specific subtask. Agentic Neural\nNetwork follows a two-phase optimization strategy: (1) Forward Phase-Drawing\ninspiration from neural network forward passes, tasks are dynamically\ndecomposed into subtasks, and cooperative agent teams with suitable aggregation\nmethods are constructed layer by layer. (2) Backward Phase-Mirroring\nbackpropagation, we refine both global and local collaboration through\niterative feedback, allowing agents to self-evolve their roles, prompts, and\ncoordination. This neuro-symbolic approach enables ANN to create new or\nspecialized agent teams post-training, delivering notable gains in accuracy and\nadaptability. Across four benchmark datasets, ANN surpasses leading multi-agent\nbaselines under the same configurations, showing consistent performance\nimprovements. Our findings indicate that ANN provides a scalable, data-driven\nframework for multi-agent systems, combining the collaborative capabilities of\nLLMs with the efficiency and flexibility of neural network principles. We plan\nto open-source the entire framework.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on multi-agent systems leveraging large language models (LLMs) for task decomposition and collaboration. While it doesn't directly address trajectory prediction, the concept of agents evolving and coordinating could potentially be applied to trajectory prediction scenarios, especially in multi-agent settings like autonomous driving or crowd navigation. The use of LLMs is a key element.", "keywords": ["Large Language Models", "LLMs", "multi-agent systems", "agent collaboration"]}, "AI": {"tldr": "Agentic Neural Network (ANN) \u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6784\u5efa\u4e3a\u795e\u7ecf\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u89e3\u4efb\u52a1\u548c\u8fed\u4ee3\u4f18\u5316\u534f\u4f5c\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u7684\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9759\u6001\u7684\u3001\u624b\u52a8\u8bbe\u8ba1\u7684\u591a\u667a\u80fd\u4f53\u914d\u7f6e\uff0c\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "Agentic Neural Network (ANN) \u9075\u5faa\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff1a(1) \u524d\u5411\u9636\u6bb5\uff1a\u4efb\u52a1\u88ab\u52a8\u6001\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u9010\u5c42\u6784\u5efa\u5177\u6709\u5408\u9002\u805a\u5408\u65b9\u6cd5\u7684\u534f\u4f5c\u667a\u80fd\u4f53\u56e2\u961f\u3002(2) \u540e\u5411\u9636\u6bb5\uff1a\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u6765\u4f18\u5316\u5168\u5c40\u548c\u5c40\u90e8\u534f\u4f5c\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u5176\u89d2\u8272\u3001\u63d0\u793a\u548c\u534f\u8c03\u3002", "result": "Agentic Neural Network (ANN) \u80fd\u591f\u521b\u5efa\u65b0\u7684\u6216\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u56e2\u961f\uff0c\u4ece\u800c\u5728\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cANN \u5728\u76f8\u540c\u7684\u914d\u7f6e\u4e0b\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u3002", "conclusion": "Agentic Neural Network (ANN) \u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u8868\u660e ANN \u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\u3002", "summary_zh": "\u5229\u7528\u591a\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u89e3\u51b3\u590d\u6742\u7684\u9ad8\u7ef4\u4efb\u52a1\uff0c\u4f46\u76ee\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9759\u6001\u7684\u3001\u624b\u52a8\u8bbe\u8ba1\u7684\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Agentic Neural Network\uff08ANN\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5c06\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6982\u5ff5\u5316\u4e3a\u5206\u5c42\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6846\u67b6\u3002\u5728\u8fd9\u79cd\u8bbe\u8ba1\u4e2d\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4f5c\u4e3a\u4e00\u4e2a\u8282\u70b9\u8fd0\u884c\uff0c\u6bcf\u4e00\u5c42\u5f62\u6210\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u7279\u5b9a\u5b50\u4efb\u52a1\u7684\u534f\u4f5c\u201c\u56e2\u961f\u201d\u3002Agentic Neural Network \u9075\u5faa\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff1a\uff081\uff09\u524d\u5411\u9636\u6bb5\u2014\u2014\u4ece\u795e\u7ecf\u7f51\u7edc\u524d\u5411\u4f20\u9012\u4e2d\u83b7\u5f97\u7075\u611f\uff0c\u4efb\u52a1\u88ab\u52a8\u6001\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u9010\u5c42\u6784\u5efa\u5177\u6709\u5408\u9002\u805a\u5408\u65b9\u6cd5\u7684\u534f\u4f5c\u667a\u80fd\u4f53\u56e2\u961f\u3002\uff082\uff09\u540e\u5411\u9636\u6bb5\u2014\u2014\u955c\u50cf\u53cd\u5411\u4f20\u64ad\uff0c\u6211\u4eec\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u6765\u4f18\u5316\u5168\u5c40\u548c\u5c40\u90e8\u534f\u4f5c\uff0c\u5141\u8bb8\u667a\u80fd\u4f53\u81ea\u6211\u8fdb\u5316\u5176\u89d2\u8272\u3001\u63d0\u793a\u548c\u534f\u8c03\u3002\u8fd9\u79cd\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u4f7f ANN \u80fd\u591f\u5728\u8bad\u7ec3\u540e\u521b\u5efa\u65b0\u7684\u6216\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u56e2\u961f\uff0c\u4ece\u800c\u5728\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cANN \u5728\u76f8\u540c\u7684\u914d\u7f6e\u4e0b\u8d85\u8d8a\u4e86\u9886\u5148\u7684\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u663e\u793a\u51fa\u6301\u7eed\u7684\u6027\u80fd\u6539\u8fdb\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cANN \u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86 LLM \u7684\u534f\u4f5c\u80fd\u529b\u548c\u795e\u7ecf\u7f51\u7edc\u539f\u7406\u7684\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002\u6211\u4eec\u8ba1\u5212\u5f00\u6e90\u6574\u4e2a\u6846\u67b6\u3002"}}
{"id": "2506.08433", "pdf": "https://arxiv.org/pdf/2506.08433", "abs": "https://arxiv.org/abs/2506.08433", "authors": ["Hern\u00e1n Maina", "Nicol\u00e1s Wolovick", "Luciana Benotti"], "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "A shorter version of this work was accepted as a two-page abstract\n  for presentation at the Widening Natural Language Processing (WiNLP) 2023\n  Workshop. That version was not publicly released, and this is the first\n  public version of the work", "summary": "Training Large Language Models (LLMs) is costly in terms of energy, hardware,\nand annotated data, often resulting in a positionality rooted in predominant\ncultures and values (Santy et al., 2023). Domain adaptation has emerged as a\npromising strategy to better align models with diverse cultural and value\ncontexts (Hershcovich et al., 2022), but its computational cost remains a\nsignificant barrier, particularly for research groups lacking access to\nlarge-scale infrastructure. In this paper, we evaluate how the use of different\nnumerical precisions and data parallelization strategies impacts both training\nspeed (as a proxy to energy and hardware consumption) and model accuracy, with\nthe goal of facilitating domain adaptation in low-resource environments. Our\nfindings are relevant to any setting where energy efficiency, accessibility, or\nlimited hardware availability are key concerns.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on domain adaptation for Large Language Models (LLMs) and addresses the computational cost associated with training them, especially in low-resource environments. While it doesn't directly deal with trajectory prediction, the mention of LLMs gives it a moderate relevance.", "keywords": ["Large Language Models", "LLMs", "domain adaptation"]}, "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u6570\u503c\u7cbe\u5ea6\u548c\u6570\u636e\u5e76\u884c\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u7ecf\u6d4e\u9ad8\u6548\u5730\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u9886\u57df\u81ea\u9002\u5e94\u662f\u4f7f\u6a21\u578b\u9002\u5e94\u4e0d\u540c\u6587\u5316\u548c\u4ef7\u503c\u89c2\u73af\u5883\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u7b56\u7565\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u663e\u8457\u7684\u969c\u788d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7f3a\u4e4f\u5927\u578b\u57fa\u7840\u8bbe\u65bd\u7684\u7814\u7a76\u56e2\u961f\u3002", "method": "\u8bc4\u4f30\u4e86\u4e0d\u540c\u6570\u503c\u7cbe\u5ea6\u548c\u6570\u636e\u5e76\u884c\u7b56\u7565\u5bf9\u8bad\u7ec3\u901f\u5ea6\u548c\u6a21\u578b\u51c6\u786e\u7387\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8c03\u6574\u6570\u503c\u7cbe\u5ea6\u548c\u6570\u636e\u5e76\u884c\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u5730\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u8c03\u6574\u6570\u503c\u7cbe\u5ea6\u548c\u6570\u636e\u5e76\u884c\u7b56\u7565\uff0c\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u6709\u6548\u5730\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u51c6\u786e\u7387\u3002", "summary_zh": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u80fd\u6e90\u3001\u786c\u4ef6\u548c\u6807\u6ce8\u6570\u636e\u65b9\u9762\u7684\u6210\u672c\u5f88\u9ad8\uff0c\u8fd9\u901a\u5e38\u5bfc\u81f4\u4e00\u79cd\u6839\u690d\u4e8e\u4e3b\u8981\u6587\u5316\u548c\u4ef7\u503c\u89c2\u7684\u5b9a\u4f4d\uff08Santy et al., 2023\uff09\u3002\u9886\u57df\u81ea\u9002\u5e94\u5df2\u7ecf\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u4f7f\u6a21\u578b\u4e0e\u4e0d\u540c\u7684\u6587\u5316\u548c\u4ef7\u503c\u89c2\u73af\u5883\u5bf9\u9f50\uff08Hershcovich et al., 2022\uff09\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u4ecd\u7136\u662f\u4e00\u4e2a\u663e\u8457\u7684\u969c\u788d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7f3a\u4e4f\u5927\u578b\u57fa\u7840\u8bbe\u65bd\u7684\u7814\u7a76\u56e2\u961f\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u8bc4\u4f30\u4e86\u4f7f\u7528\u4e0d\u540c\u7684\u6570\u503c\u7cbe\u5ea6\u548c\u6570\u636e\u5e76\u884c\u5316\u7b56\u7565\u5982\u4f55\u5f71\u54cd\u8bad\u7ec3\u901f\u5ea6\uff08\u4f5c\u4e3a\u80fd\u6e90\u548c\u786c\u4ef6\u6d88\u8017\u7684\u4ee3\u8868\uff09\u548c\u6a21\u578b\u51c6\u786e\u7387\uff0c\u76ee\u7684\u662f\u4fc3\u8fdb\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u9886\u57df\u81ea\u9002\u5e94\u3002\u6211\u4eec\u7684\u53d1\u73b0\u4e0e\u80fd\u6e90\u6548\u7387\u3001\u53ef\u8bbf\u95ee\u6027\u6216\u6709\u9650\u7684\u786c\u4ef6\u53ef\u7528\u6027\u662f\u5173\u952e\u8003\u8651\u56e0\u7d20\u7684\u4efb\u4f55\u8bbe\u7f6e\u76f8\u5173\u3002"}}
