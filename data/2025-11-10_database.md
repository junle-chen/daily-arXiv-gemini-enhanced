# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-11-10

## 目录

- [人工智能 (Artificial Intelligence) (1)](#cs-ai)
- [计算语言学 (Computation and Language) (2)](#cs-cl)
- [计算机视觉 (Computer Vision) (1)](#cs-cv)
- [cs.DB (2)](#cs-db)
- [cs.IR (1)](#cs-ir)
- [机器学习 (Machine Learning) (1)](#cs-lg)
- [cs.MA (1)](#cs-ma)

## 人工智能 (Artificial Intelligence) [cs.AI]
### [1] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm, Cornelius Wolff, Madelon Hulsebos*

Main category: cs.AI

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction, where the responsibility of query specification is shared among the user and the system. We develop a principled framework distinguishing cooperative queries, i.e., queries that yield a resolvable interpretation, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. Our framework and analysis of queries shifts the perspective from fixing ambiguity to embracing cooperation in resolving queries. This reflection enables more informed design and evaluation for natural language interfaces for tabular data, for which we outline implications and directions for future research.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.04584) | **Categories:** cs.AI, cs.CL, cs.DB, cs.HC

---


## 计算语言学 (Computation and Language) [cs.CL]
### [1] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed, Md Mubtasim Ahasan, Jahir Sadik Monon, Muntasir Wahed, M Ashraful Amin, A K M Mahbubur Rahman, Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Text-to-SQL systems provide a natural language interface that can enable even laymen to access information stored in databases. However, existing Large Language Models (LLM) struggle with SQL generation from natural instructions due to large schema sizes and complex reasoning. Prior work often focuses on complex, somewhat impractical pipelines using flagship models, while smaller, efficient models remain overlooked. In this work, we explore three multi-agent LLM pipelines, with systematic performance benchmarking across a range of small to large open-source models: (1) Multi-agent discussion pipeline, where agents iteratively critique and refine SQL queries, and a judge synthesizes the final answer; (2) Planner-Coder pipeline, where a thinking model planner generates stepwise SQL generation plans and a coder synthesizes queries; and (3) Coder-Aggregator pipeline, where multiple coders independently generate SQL queries, and a reasoning agent selects the best query. Experiments on the Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small model performance, with up to 10.6% increase in Execution Accuracy for Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines, the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest score of 56.4%. Codes are available at https://github.com/treeDweller98/bappa-sql.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.04153) | **Categories:** cs.CL, cs.AI, cs.DB, cs.MA

---

### [2] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar, Purvi Chaurasia, Sanchit Kabra, Ananya Srivastava, Vivek Gupta, Chandan K. Reddy*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform tables, underrepresenting the complexity of real-world data and giving an incomplete view of Large Language Models' (LLMs) reasoning abilities. Real tables are long, heterogeneous, and domain-specific, mixing structured fields with free text and requiring multi-hop reasoning across thousands of tokens. To address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from 2031 real-world tables spanning two domains: i) RB-Science (NSF grant records) and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates LLMs jointly across scale, heterogeneity, domain specificity, and reasoning complexity. Experiments with open-source and proprietary models show that LLMs struggle with heterogeneous schemas and complex multi-hop inference, revealing persistent weaknesses in current architectures and prompting strategies. RUST-BENCH establishes a challenging new testbed for advancing tabular reasoning research.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.04491) | **Categories:** cs.CL, cs.AI, cs.DB, cs.IR, cs.LG

---


## 计算机视觉 (Computer Vision) [cs.CV]
### [1] [Improving Diagnostic Performance on Small and Imbalanced Datasets Using Class-Based Input Image Composition](https://arxiv.org/abs/2511.03891)
*Hlali Azzeddine, Majid Ben Yakhlef, Soulaiman El Hazzat*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Small, imbalanced datasets and poor input image quality can lead to high false predictions rates with deep learning models. This paper introduces Class-Based Image Composition, an approach that allows us to reformulate training inputs through a fusion of multiple images of the same class into combined visual composites, named Composite Input Images (CoImg). That enhances the intra-class variance and improves the valuable information density per training sample and increases the ability of the model to distinguish between subtle disease patterns. Our method was evaluated on the Optical Coherence Tomography Dataset for Image-Based Deep Learning Methods (OCTDL) (Kulyabin et al., 2024), which contains 2,064 high-resolution optical coherence tomography (OCT) scans of the human retina, representing seven distinct diseases with a significant class imbalance. We constructed a perfectly class-balanced version of this dataset, named Co-OCTDL, where each scan is resented as a 3x1 layout composite image. To assess the effectiveness of this new representation, we conducted a comparative analysis between the original dataset and its variant using a VGG16 model. A fair comparison was ensured by utilizing the identical model architecture and hyperparameters for all experiments. The proposed approach markedly improved diagnostic results.The enhanced Dataset achieved near-perfect accuracy (99.6%) with F1-score (0.995) and AUC (0.9996), compared to a baseline model trained on raw dataset. The false prediction rate was also significantly lower, this demonstrates that the method can producehigh-quality predictions even for weak datasets affected by class imbalance or small sample size.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.03891) | **Categories:** cs.CV, cs.AI, cs.DB

---


## cs.DB [cs.DB]
### [1] [GPU-Based Floating-point Adaptive Lossless Compression](https://arxiv.org/abs/2511.04140)
*Zheng Li, Weiyan Wang, Ruiyuan Li, Chao Chen, Xianlei Long, Linjiang Zheng, Quanqing Xu, Chuanhui Yang*

Main category: cs.DB

TL;DR: Falcon提出了一种基于GPU的浮点自适应无损压缩框架，通过轻量级异步流水线、精确快速的浮点到整数转换方法和自适应稀疏位平面无损编码策略，实现了更高的压缩比和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 物联网和高性能计算等领域产生大量的浮点时间序列数据，对其进行无损压缩至关重要，并且利用GPU的大规模并行性可以实现前所未有的吞吐量。然而，设计高性能的GPU无损压缩器面临异构数据移动瓶颈、精度保持转换复杂性和异常引起的稀疏性降低这三个关键挑战。

Method: Falcon框架首先引入轻量级异步流水线，隐藏CPU和GPU之间数据传输的I/O延迟。然后，提出了一种具有理论保证的精确快速浮点到整数转换方法，消除了浮点运算引起的误差。此外，设计了一种自适应稀疏位平面无损编码策略，减少了由异常值引起的稀疏性。

Result: 在12个不同的数据集上进行的大量实验表明，Falcon的压缩比比最先进的基于CPU的方法提高了9.1%，压缩吞吐量比最快的基于GPU的竞争对手高2.43倍，解压缩吞吐量高2.4倍。

Conclusion: Falcon框架通过解决异构数据移动瓶颈、精度保持转换复杂性和异常引起的稀疏性降低这三个关键挑战，实现了更高的压缩比和吞吐量，为物联网和高性能计算等领域的海量浮点时间序列数据压缩提供了一种有效的解决方案。

Abstract: 物联网（IoT）和高性能计算（HPC）等领域会产生大量的浮点时间序列数据。压缩这些数据并保持其绝对保真度至关重要，而利用现代GPU的大规模并行性为前所未有的吞吐量提供了途径。然而，设计这种高性能的基于GPU的无损压缩器面临三个关键挑战：1) 异构数据移动瓶颈，2) 精度保持转换复杂性，以及 3) 异常引起的稀疏性降低。为了应对这些挑战，本文提出了一种基于GPU的浮点自适应无损压缩框架 Falcon。具体来说，Falcon 首先引入了一个轻量级的异步流水线，它隐藏了 CPU 和 GPU 之间数据传输期间的 I/O 延迟。然后，我们提出了一种具有理论保证的精确快速的浮点到整数转换方法，消除了浮点运算引起的误差。此外，我们设计了一种自适应稀疏位平面无损编码策略，减少了由异常值引起的稀疏性。在 12 个不同的数据集上进行的大量实验表明，我们的压缩比比最先进的基于 CPU 的方法提高了 9.1%，压缩吞吐量比最快的基于 GPU 的竞争对手高 2.43 倍，解压缩吞吐量高 2.4 倍。

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.04140) | **Categories:** cs.DB, cs.DS

---

### [2] [EntroGD: Efficient Compression and Accurate Direct Analytics on Compressed Data](https://arxiv.org/abs/2511.04148)
*Xiaobo Zhao, Daniel E. Lucani*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Generalized Deduplication (GD) enables lossless compression with direct analytics on compressed data by dividing data into \emph{bases} and \emph{deviations} and performing dictionary encoding on the former. However, GD algorithms face scalability challenges for high-dimensional data. For example, the GreedyGD algorithm relies on an iterative bit-selection process across $d$-dimensional data resulting in $O(nd^2)$ complexity for $n$ data rows to select bits to be used as bases and deviations. Although the $n$ data rows can be reduced during training at the expense of performance, highly dimensional data still experiences a marked loss in performance. This paper introduces EntroGD, an entropy-guided GD framework that reduces complexity of the bit-selection algorithm to $O(nd)$. EntroGD operates considers a two-step process. First, it generates condensed samples to preserve analytic fidelity. Second, it applies entropy-guided bit selection to maximize compression efficiency. Across 18 datasets of varying types and dimensionalities, EntroGD achieves compression performance comparable to GD-based and universal compressors, while reducing configuration time by up to 53.5$\times$ over GreedyGD and accelerating clustering by up to 31.6$\times$ over the original data with negligible accuracy loss by performing analytics on the condensed samples, which are much fewer than original samples. Thus, EntroGD provides an efficient and scalable solution to performing analytics directly on compressed data.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.04148) | **Categories:** cs.DB

---


## cs.IR [cs.IR]
### [1] [Coordination-Free Lane Partitioning for Convergent ANN Search](https://arxiv.org/abs/2511.04221)
*Carl Kugblenu, Petri Vuorimaa*

Main category: cs.IR

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Production vector search systems often fan out each query across parallel lanes (threads, replicas, or shards) to meet latency service-level objectives (SLOs). In practice, these lanes rediscover the same candidates, so extra compute does not increase coverage. We present a coordination-free lane partitioner that turns duplication into complementary work at the same cost and deadline. For each query we (1) build a deterministic candidate pool sized to the total top-k budget, (2) apply a per-query pseudorandom permutation, and (3) assign each lane a disjoint slice of positions. Lanes then return different results by construction, with no runtime coordination.   At equal cost with four lanes (total candidate budget 64), on SIFT1M (1M SIFT feature vectors) with Hierarchical Navigable Small World graphs (HNSW) recall@10 rises from 0.249 to 0.999 while lane overlap falls from nearly 100% to 0%. On MS MARCO (8.8M passages) with HNSW, hit@10 improves from 0.200 to 0.601 and Mean Reciprocal Rank at 10 (MRR@10) from 0.133 to 0.330. For inverted file (IVF) indexes we see smaller but consistent gains (for example, +11% on MS MARCO) by de-duplicating list routing. A microbenchmark shows planner overhead of ~37 microseconds per query (mean at the main setting) with linear growth in the number of merged candidates.   These results yield a simple operational guideline: size the per-query pool to the total budget, deterministically partition positions across lanes, and turn redundant fan-out into complementary coverage without changing budget or deadline.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.04221) | **Categories:** cs.IR, cs.DB

---


## 机器学习 (Machine Learning) [cs.LG]
### [1] [Learning Filter-Aware Distance Metrics for Nearest Neighbor Search with Multiple Filters](https://arxiv.org/abs/2511.04073)
*Ananya Sutradhar, Suryansh Gupta, Ravishankar Krishnaswamy, Haiyang Xu, Aseem Rastogi, Gopal Srinivasa*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Filtered Approximate Nearest Neighbor (ANN) search retrieves the closest vectors for a query vector from a dataset. It enforces that a specified set of discrete labels $S$ for the query must be included in the labels of each retrieved vector. Existing graph-based methods typically incorporate filter awareness by assigning fixed penalties or prioritizing nodes based on filter satisfaction. However, since these methods use fixed, data in- dependent penalties, they often fail to generalize across datasets with diverse label and vector distributions. In this work, we propose a principled alternative that learns the optimal trade-off between vector distance and filter match directly from the data, rather than relying on fixed penalties. We formulate this as a constrained linear optimization problem, deriving weights that better reflect the underlying filter distribution and more effectively address the filtered ANN search problem. These learned weights guide both the search process and index construction, leading to graph structures that more effectively capture the underlying filter distribution and filter semantics. Our experiments demonstrate that adapting the distance function to the data significantly im- proves accuracy by 5-10% over fixed-penalty methods, providing a more flexible and generalizable framework for the filtered ANN search problem.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.04073) | **Categories:** cs.LG, cs.DB, cs.IR

---


## cs.MA [cs.MA]
### [1] [OptiMA: A Transaction-Based Framework with Throughput Optimization for Very Complex Multi-Agent Systems](https://arxiv.org/abs/2511.03761)
*Umut Çalıkyılmaz, Nitin Nayak, Jinghua Groppe, Sven Groppe*

Main category: cs.MA

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: In recent years, the research of multi-agent systems has taken a direction to explore larger and more complex models to fulfill sophisticated tasks. We point out two possible pitfalls that might be caused by increasing complexity; susceptibilities to faults, and performance bottlenecks. To prevent the former threat, we propose a transaction-based framework to design very complex multi-agent systems (VCMAS). To address the second threat, we offer to integrate transaction scheduling into the proposed framework. We implemented both of these ideas to develop the OptiMA framework and show that it is able to facilitate the execution of VCMAS with more than a hundred agents. We also demonstrate the effect of transaction scheduling on such a system by showing improvements up to more than 16\%. Furthermore, we also performed a theoretical analysis on the transaction scheduling problem and provided practical tools that can be used for future research on it.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.03761) | **Categories:** cs.MA, cs.AI, cs.DB

---
