{"id": "2509.21592", "pdf": "https://arxiv.org/pdf/2509.21592", "abs": "https://arxiv.org/abs/2509.21592", "authors": ["Gabrijel Boduljak", "Laurynas Karazija", "Iro Laina", "Christian Rupprecht", "Andrea Vedaldi"], "title": "What Happens Next? Anticipating Future Motion by Generating Point Trajectories", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We consider the problem of forecasting motion from a single image, i.e.,\npredicting how objects in the world are likely to move, without the ability to\nobserve other parameters such as the object velocities or the forces applied to\nthem. We formulate this task as conditional generation of dense trajectory\ngrids with a model that closely follows the architecture of modern video\ngenerators but outputs motion trajectories instead of pixels. This approach\ncaptures scene-wide dynamics and uncertainty, yielding more accurate and\ndiverse predictions than prior regressors and generators. We extensively\nevaluate our method on simulated data, demonstrate its effectiveness on\ndownstream applications such as robotics, and show promising accuracy on\nreal-world intuitive physics datasets. Although recent state-of-the-art video\ngenerators are often regarded as world models, we show that they struggle with\nforecasting motion from a single image, even in simple physical scenarios such\nas falling blocks or mechanical object interactions, despite fine-tuning on\nsuch data. We show that this limitation arises from the overhead of generating\npixels rather than directly modeling motion.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u7269\u4f53\u8fd0\u52a8\u8f68\u8ff9\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u751f\u6210\u8f68\u8ff9\u3001\u9884\u6d4b\u8fd0\u52a8\u7b49\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\u3002\u867d\u7136\u63d0\u5230\u4e86video generators\uff0c\u4f46\u5e76\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u800c\u662f\u4fa7\u91cd\u4e8e\u89c6\u89c9\u9884\u6d4b\u548c\u8fd0\u52a8\u5efa\u6a21\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "trajectory prediction", "\u8fd0\u52a8\u9884\u6d4b", "motion forecasting", "conditional generation", "motion trajectories", "video generators"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u7269\u4f53\u8fd0\u52a8\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u751f\u6210\u5bc6\u96c6\u8f68\u8ff9\u7f51\u683c\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u548c\u591a\u6837\u7684\u9884\u6d4b\u3002", "motivation": "\u89e3\u51b3\u4ec5\u901a\u8fc7\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u7269\u4f53\u8fd0\u52a8\u8f68\u8ff9\u7684\u95ee\u9898\uff0c\u65e0\u9700\u7269\u4f53\u901f\u5ea6\u6216\u4f5c\u7528\u529b\u7b49\u5176\u4ed6\u53c2\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6761\u4ef6\u751f\u6210\u5bc6\u96c6\u8f68\u8ff9\u7f51\u683c\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u67b6\u6784\u4e0e\u73b0\u4ee3\u89c6\u9891\u751f\u6210\u5668\u7c7b\u4f3c\uff0c\u4f46\u8f93\u51fa\u7684\u662f\u8fd0\u52a8\u8f68\u8ff9\u800c\u4e0d\u662f\u50cf\u7d20\u3002", "result": "\u5728\u6a21\u62df\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u4eba\u7b49\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u76f4\u89c2\u7269\u7406\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u826f\u597d\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\uff0c\u76f4\u63a5\u5efa\u6a21\u8fd0\u52a8\u6bd4\u751f\u6210\u50cf\u7d20\u66f4\u80fd\u6709\u6548\u9884\u6d4b\u5355\u5f20\u56fe\u50cf\u4e2d\u7684\u7269\u4f53\u8fd0\u52a8\u3002", "summary_zh": "\u672c\u6587\u7814\u7a76\u4e86\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u8fd0\u52a8\u7684\u95ee\u9898\uff0c\u5373\u9884\u6d4b\u4e16\u754c\u4e2d\u7684\u7269\u4f53\u53ef\u80fd\u5982\u4f55\u79fb\u52a8\uff0c\u800c\u65e0\u9700\u89c2\u5bdf\u7269\u4f53\u901f\u5ea6\u6216\u65bd\u52a0\u5728\u7269\u4f53\u4e0a\u7684\u529b\u7b49\u5176\u4ed6\u53c2\u6570\u3002 \u6211\u4eec\u5c06\u6b64\u4efb\u52a1\u5b9a\u4e49\u4e3a\u5bc6\u96c6\u8f68\u8ff9\u7f51\u683c\u7684\u6761\u4ef6\u751f\u6210\uff0c\u8be5\u6a21\u578b\u4e0e\u73b0\u4ee3\u89c6\u9891\u751f\u6210\u5668\u7684\u67b6\u6784\u975e\u5e38\u76f8\u4f3c\uff0c\u4f46\u8f93\u51fa\u7684\u662f\u8fd0\u52a8\u8f68\u8ff9\u800c\u4e0d\u662f\u50cf\u7d20\u3002 \u8fd9\u79cd\u65b9\u6cd5\u6355\u6349\u4e86\u573a\u666f\u8303\u56f4\u5185\u7684\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u4ea7\u751f\u6bd4\u5148\u524d\u7684\u56de\u5f52\u5668\u548c\u751f\u6210\u5668\u66f4\u51c6\u786e\u548c\u591a\u6837\u7684\u9884\u6d4b\u3002 \u6211\u4eec\u5728\u6a21\u62df\u6570\u636e\u4e0a\u5e7f\u6cdb\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u673a\u5668\u4eba\u7b49\u4e0b\u6e38\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u76f4\u89c2\u7269\u7406\u6570\u636e\u96c6\u4e0a\u663e\u793a\u51fa\u826f\u597d\u7684\u51c6\u786e\u6027\u3002 \u5c3d\u7ba1\u6700\u8fd1\u6700\u5148\u8fdb\u7684\u89c6\u9891\u751f\u6210\u5668\u901a\u5e38\u88ab\u8ba4\u4e3a\u662f\u4e16\u754c\u6a21\u578b\uff0c\u4f46\u6211\u4eec\u8868\u660e\uff0c\u5373\u4f7f\u5728\u7b80\u5355\u7684\u7269\u7406\u573a\u666f\uff08\u4f8b\u5982\u6389\u843d\u7684\u5757\u6216\u673a\u68b0\u7269\u4f53\u4ea4\u4e92\uff09\u4e2d\uff0c\u5b83\u4eec\u4e5f\u5f88\u96be\u4ece\u5355\u4e2a\u56fe\u50cf\u9884\u6d4b\u8fd0\u52a8\uff0c\u5c3d\u7ba1\u5bf9\u6b64\u7c7b\u6570\u636e\u8fdb\u884c\u4e86\u5fae\u8c03\u3002 \u6211\u4eec\u8868\u660e\uff0c\u8fd9\u79cd\u9650\u5236\u662f\u7531\u4e8e\u751f\u6210\u50cf\u7d20\u800c\u4e0d\u662f\u76f4\u63a5\u5efa\u6a21\u8fd0\u52a8\u7684\u5f00\u9500\u9020\u6210\u7684\u3002"}}
{"id": "2509.21873", "pdf": "https://arxiv.org/pdf/2509.21873", "abs": "https://arxiv.org/abs/2509.21873", "authors": ["Nishant Doshi"], "title": "Improved Vehicle Maneuver Prediction using Game Theoretic Priors", "categories": ["cs.RO"], "comment": null, "summary": "Conventional maneuver prediction methods use some sort of classification\nmodel on temporal trajectory data to predict behavior of agents over a set time\nhorizon. Despite of having the best precision and recall, these models cannot\npredict a lane change accurately unless they incorporate information about the\nentire scene. Level-k game theory can leverage the human-like hierarchical\nreasoning to come up with the most rational decisions each agent can make in a\ngroup. This can be leveraged to model interactions between different vehicles\nin presence of each other and hence compute the most rational decisions each\nagent would make. The result of game theoretic evaluation can be used as a\n\"prior\" or combined with a traditional motion-based classification model to\nachieve more accurate predictions. The proposed approach assumes that the\nstates of the vehicles around the target lead vehicle are known. The module\nwill output the most rational maneuver prediction of the target vehicle based\non an online optimization solution. These predictions are instrumental in\ndecision making systems like Adaptive Cruise Control (ACC) or Traxen's\niQ-Cruise further improving the resulting fuel savings.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86\u535a\u5f08\u8bba\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u8f68\u8ff9\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b", "\u52a8\u4f5c\u9884\u6d4b", "\u673a\u52a8\u9884\u6d4b", "\u535a\u5f08\u8bba"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u535a\u5f08\u8bba\u548c\u4f20\u7edf\u8fd0\u52a8\u6a21\u578b\u7684\u8f66\u8f86\u884c\u4e3a\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u884c\u4e3a\u9884\u6d4b\u65b9\u6cd5\u5728\u9884\u6d4b\u53d8\u9053\u884c\u4e3a\u65f6\uff0c\u9700\u8981\u573a\u666f\u4fe1\u606f\uff0c\u5e76\u4e14\u6ca1\u6709\u8003\u8651\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002", "method": "\u5229\u7528Level-k\u535a\u5f08\u8bba\u5bf9\u8f66\u8f86\u95f4\u7684\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5c06\u535a\u5f08\u8bba\u7684\u8f93\u51fa\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u4e0e\u4f20\u7edf\u8fd0\u52a8\u6a21\u578b\u7ed3\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u8f66\u8f86\u7684\u5408\u7406\u884c\u4e3a\u3002", "conclusion": "\u8be5\u9884\u6d4b\u7ed3\u679c\u53ef\u7528\u4e8e\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\u7b49\u51b3\u7b56\u7cfb\u7edf\uff0c\u4ece\u800c\u63d0\u9ad8\u71c3\u6cb9\u6548\u7387\u3002", "summary_zh": "\u4f20\u7edf\u7684\u673a\u52a8\u9884\u6d4b\u65b9\u6cd5\u4f7f\u7528\u67d0\u79cd\u5206\u7c7b\u6a21\u578b\u5bf9\u65f6\u95f4\u8f68\u8ff9\u6570\u636e\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u9884\u6d4b\u667a\u80fd\u4f53\u5728\u4e00\u5b9a\u65f6\u95f4\u8303\u56f4\u5185\u7684\u884c\u4e3a\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u6700\u4f73\u7684\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\uff0c\u4f46\u9664\u975e\u5b83\u4eec\u5305\u542b\u6709\u5173\u6574\u4e2a\u573a\u666f\u7684\u4fe1\u606f\uff0c\u5426\u5219\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u53d8\u9053\u884c\u4e3a\u3002Level-k\u535a\u5f08\u8bba\u53ef\u4ee5\u5229\u7528\u7c7b\u4eba\u5206\u5c42\u63a8\u7406\u6765\u5f97\u51fa\u6bcf\u4e2a\u667a\u80fd\u4f53\u5728\u4e00\u7ec4\u4e2d\u53ef\u4ee5\u505a\u51fa\u7684\u6700\u5408\u7406\u7684\u51b3\u7b56\u3002\u8fd9\u53ef\u4ee5\u7528\u4e8e\u6a21\u62df\u4e0d\u540c\u8f66\u8f86\u5728\u5f7c\u6b64\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u7684\u4ea4\u4e92\uff0c\u4ece\u800c\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u667a\u80fd\u4f53\u5c06\u505a\u51fa\u7684\u6700\u5408\u7406\u7684\u51b3\u7b56\u3002\u535a\u5f08\u8bba\u8bc4\u4f30\u7684\u7ed3\u679c\u53ef\u4ee5\u7528\u4f5c\u201c\u5148\u9a8c\u201d\u6216\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u8fd0\u52a8\u7684\u5206\u7c7b\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\uff0c\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5047\u8bbe\u76ee\u6807\u5f15\u5bfc\u8f66\u8f86\u5468\u56f4\u7684\u8f66\u8f86\u72b6\u6001\u662f\u5df2\u77e5\u7684\u3002\u8be5\u6a21\u5757\u5c06\u57fa\u4e8e\u5728\u7ebf\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u8f93\u51fa\u76ee\u6807\u8f66\u8f86\u7684\u6700\u5408\u7406\u7684\u673a\u52a8\u9884\u6d4b\u3002\u8fd9\u4e9b\u9884\u6d4b\u6709\u52a9\u4e8e\u8bf8\u5982\u81ea\u9002\u5e94\u5de1\u822a\u63a7\u5236\uff08ACC\uff09\u6216Traxen\u7684iQ-Cruise\u7b49\u51b3\u7b56\u7cfb\u7edf\uff0c\u4ece\u800c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u71c3\u6cb9\u6548\u7387\u3002"}}
{"id": "2509.21799", "pdf": "https://arxiv.org/pdf/2509.21799", "abs": "https://arxiv.org/abs/2509.21799", "authors": ["Hongze Mi", "Yibo Feng", "Wenjie Lu", "Yuqi Wang", "Jinyuan Li", "Song Cao", "He Cui", "Tengfei Tian", "Xuelin Zhang", "Haotian Luo", "Di Sun", "Naiqiang Tan", "Gang Pan"], "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents", "categories": ["cs.AI"], "comment": null, "summary": "Graphical User Interface (GUI) agents aim to automate a wide spectrum of\nhuman tasks by emulating user interaction. Despite rapid advancements, current\napproaches are hindered by several critical challenges: data bottleneck in\nend-to-end training, high cost of delayed error detection, and risk of\ncontradictory guidance. Inspired by the human cognitive loop of Thinking,\nAlignment, and Reflection, we present D-Artemis -- a novel deliberative\nframework in this paper. D-Artemis leverages a fine-grained, app-specific tip\nretrieval mechanism to inform its decision-making process. It also employs a\nproactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC)\nCheck module and Action Correction Agent (ACA) work in concert to mitigate the\nrisk of execution failures. A post-execution Status Reflection Agent (SRA)\ncompletes the cognitive loop, enabling strategic learning from experience.\nCrucially, D-Artemis enhances the capabilities of general-purpose Multimodal\nlarge language models (MLLMs) for GUI tasks without the need for training on\ncomplex trajectory datasets, demonstrating strong generalization. D-Artemis\nestablishes new state-of-the-art (SOTA) results across both major benchmarks,\nachieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2.\nExtensive ablation studies further demonstrate the significant contribution of\neach component to the framework.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on GUI agents and uses multimodal large language models (MLLMs) to automate user interaction. While it doesn't directly deal with trajectory prediction in the traditional sense (e.g., predicting the path of a moving object), the concept of automating a sequence of actions within a GUI environment can be viewed as a form of trajectory prediction in a discrete action space. The use of MLLMs also makes it relevant to the large language model theme.", "keywords": ["large language models", "MLLMs", "GUI agents", "automation", "trajectory (implied - sequence of GUI actions)"]}, "AI": {"tldr": "D-Artemis\u662f\u4e00\u4e2a\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u5faa\u73af\u7684GUI\u4ee3\u7406\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u63d0\u793a\u68c0\u7d22\u3001\u9884\u6267\u884c\u5bf9\u9f50\u548c\u72b6\u6001\u53cd\u601d\u6765\u63d0\u9ad8\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728GUI\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u5728\u590d\u6742\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709GUI\u4ee3\u7406\u65b9\u6cd5\u5b58\u5728\u7aef\u5230\u7aef\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u74f6\u9888\u3001\u5ef6\u8fdf\u9519\u8bef\u68c0\u6d4b\u7684\u9ad8\u6210\u672c\u4ee5\u53ca\u77db\u76fe\u6307\u5bfc\u7684\u98ce\u9669\u3002", "method": "D-Artemis\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u7ec6\u7c92\u5ea6\u7684\u5e94\u7528\u7279\u5b9a\u63d0\u793a\u68c0\u7d22\u673a\u5236\u3001Thought-Action Consistency (TAC)\u68c0\u67e5\u6a21\u5757\u548cAction Correction Agent (ACA)\u7ec4\u6210\u7684\u9884\u6267\u884c\u5bf9\u9f50\u9636\u6bb5\uff0c\u4ee5\u53ca\u6267\u884c\u540e\u7684Status Reflection Agent (SRA)\u3002", "result": "D-Artemis\u5728AndroidWorld\u548cScreenSpot-V2\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u65b0\u7684state-of-the-art (SOTA) \u7ed3\u679c\uff0c\u6210\u529f\u7387\u5206\u522b\u4e3a75.8%\u548c96.8%\u3002", "conclusion": "D-Artemis\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u5faa\u73af\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728GUI\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u5728\u590d\u6742\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\u3002", "summary_zh": "\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u4ee3\u7406\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u7528\u6237\u4ea4\u4e92\u6765\u81ea\u52a8\u5316\u5404\u79cd\u4eba\u7c7b\u4efb\u52a1\u3002\u5c3d\u7ba1\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u5f53\u524d\u7684\u65b9\u6cd5\u53d7\u5230\u51e0\u4e2a\u5173\u952e\u6311\u6218\u7684\u963b\u788d\uff1a\u7aef\u5230\u7aef\u8bad\u7ec3\u4e2d\u7684\u6570\u636e\u74f6\u9888\u3001\u5ef6\u8fdf\u9519\u8bef\u68c0\u6d4b\u7684\u9ad8\u6210\u672c\u4ee5\u53ca\u77db\u76fe\u6307\u5bfc\u7684\u98ce\u9669\u3002\u53d7\u5230\u4eba\u7c7b\u8ba4\u77e5\u5faa\u73af\u2014\u2014\u601d\u8003\u3001\u5bf9\u9f50\u548c\u53cd\u601d\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5728\u672c\u6587\u4e2d\u63d0\u51fa\u4e86D-Artemis\u2014\u2014\u4e00\u79cd\u65b0\u9896\u7684\u5ba1\u8bae\u6846\u67b6\u3002D-Artemis\u5229\u7528\u7ec6\u7c92\u5ea6\u7684\u3001\u7279\u5b9a\u4e8e\u5e94\u7528\u7a0b\u5e8f\u7684\u63d0\u793a\u68c0\u7d22\u673a\u5236\u6765\u544a\u77e5\u5176\u51b3\u7b56\u8fc7\u7a0b\u3002\u5b83\u8fd8\u91c7\u7528\u4e86\u4e00\u4e2a\u4e3b\u52a8\u7684\u9884\u6267\u884c\u5bf9\u9f50\u9636\u6bb5\uff0c\u5176\u4e2d\u601d\u60f3-\u884c\u52a8\u4e00\u81f4\u6027\uff08TAC\uff09\u68c0\u67e5\u6a21\u5757\u548c\u884c\u52a8\u6821\u6b63\u4ee3\u7406\uff08ACA\uff09\u534f\u540c\u5de5\u4f5c\uff0c\u4ee5\u964d\u4f4e\u6267\u884c\u5931\u8d25\u7684\u98ce\u9669\u3002\u6267\u884c\u540e\u7684\u72b6\u6001\u53cd\u601d\u4ee3\u7406\uff08SRA\uff09\u5b8c\u6210\u4e86\u8ba4\u77e5\u5faa\u73af\uff0c\u4ece\u800c\u80fd\u591f\u4ece\u7ecf\u9a8c\u4e2d\u8fdb\u884c\u6218\u7565\u5b66\u4e60\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0cD-Artemis\u589e\u5f3a\u4e86\u901a\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728GUI\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u800c\u65e0\u9700\u5728\u590d\u6742\u7684\u8f68\u8ff9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ece\u800c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002D-Artemis\u5728\u4e24\u4e2a\u4e3b\u8981\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\uff08SOTA\uff09\u7ed3\u679c\uff0c\u5728AndroidWorld\u4e0a\u5b9e\u73b0\u4e8675.8%\u7684\u6210\u529f\u7387\uff0c\u5728ScreenSpot-V2\u4e0a\u5b9e\u73b0\u4e8696.8%\u7684\u6210\u529f\u7387\u3002\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u6bcf\u4e2a\u7ec4\u4ef6\u5bf9\u6846\u67b6\u7684\u91cd\u5927\u8d21\u732e\u3002"}}
{"id": "2509.21842", "pdf": "https://arxiv.org/pdf/2509.21842", "abs": "https://arxiv.org/abs/2509.21842", "authors": ["Yansong Ning", "Rui Liu", "Jun Wang", "Kai Chen", "Wei Li", "Jun Fang", "Kan Zheng", "Naiqiang Tan", "Hao Liu"], "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents", "categories": ["cs.AI"], "comment": "Under review", "summary": "Travel planning (TP) agent has recently worked as an emerging building block\nto interact with external tools and resources for travel itinerary generation,\nensuring enjoyable user experience. Despite its benefits, existing studies rely\non hand craft prompt and fixed agent workflow, hindering more flexible and\nautonomous TP agent. This paper proposes DeepTravel, an end to end agentic\nreinforcement learning framework for building autonomous travel planning agent,\ncapable of autonomously planning, executing tools, and reflecting on tool\nresponses to explore, verify, and refine intermediate actions in multi step\nreasoning. To achieve this, we first construct a robust sandbox environment by\ncaching transportation, accommodation and POI data, facilitating TP agent\ntraining without being constrained by real world APIs limitations (e.g.,\ninconsistent outputs). Moreover, we develop a hierarchical reward modeling\nsystem, where a trajectory level verifier first checks spatiotemporal\nfeasibility and filters unsatisfied travel itinerary, and then the turn level\nverifier further validate itinerary detail consistency with tool responses,\nenabling efficient and precise reward service. Finally, we propose the reply\naugmented reinforcement learning method that enables TP agent to periodically\nreplay from a failures experience buffer, emerging notable agentic capacity. We\ndeploy trained TP agent on DiDi Enterprise Solutions App and conduct\ncomprehensive online and offline evaluations, demonstrating that DeepTravel\nenables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing\nfrontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on travel planning using reinforcement learning and large language models. While not directly trajectory prediction, travel planning implicitly involves trajectory/route generation. The use of LLMs (specifically Qwen3 32B) is explicitly mentioned, making it relevant to the prompt.", "keywords": ["travel planning", "reinforcement learning", "large language models", "LLMs", "Qwen3", "trajectory"]}, "AI": {"tldr": "DeepTravel\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684agent\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u81ea\u4e3b\u65c5\u884c\u89c4\u5212agent\uff0c\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u6267\u884c\u5de5\u5177\u4ee5\u53ca\u53cd\u601d\u5de5\u5177\u54cd\u5e94\u3002", "motivation": "\u73b0\u6709\u65c5\u884c\u89c4\u5212agent\u4f9d\u8d56\u4e8e\u624b\u5de5prompt\u548c\u56fa\u5b9a\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u81ea\u4e3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86DeepTravel\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u9c81\u68d2\u7684sandbox\u73af\u5883\u3001\u4e00\u4e2a\u5206\u5c42\u5956\u52b1\u5efa\u6a21\u7cfb\u7edf\u548c\u4e00\u4e2areply\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "DeepTravel\u4f7f\u5c0f\u5c3a\u5bf8LLM\uff08\u4f8b\u5982Qwen3 32B\uff09\u5728\u65c5\u884c\u89c4\u5212\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u524d\u6cbfLLM\uff0c\u4f8b\u5982OpenAI o1\u3001o3\u548cDeepSeek R1\u3002", "conclusion": "DeepTravel\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u65c5\u884c\u89c4\u5212agent\u7684\u81ea\u4e3b\u6027\u548c\u6027\u80fd\u3002", "summary_zh": "\u65c5\u884c\u89c4\u5212 (TP) \u4ee3\u7406\u6700\u8fd1\u5df2\u6210\u4e3a\u4e00\u4e2a\u65b0\u5174\u7684\u6784\u5efa\u5757\uff0c\u7528\u4e8e\u4e0e\u5916\u90e8\u5de5\u5177\u548c\u8d44\u6e90\u4ea4\u4e92\u4ee5\u751f\u6210\u65c5\u884c\u884c\u7a0b\uff0c\u4ece\u800c\u786e\u4fdd\u6109\u5feb\u7684\u7528\u6237\u4f53\u9a8c\u3002 \u5c3d\u7ba1\u5b83\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u4e8e\u624b\u5de5\u5236\u4f5c\u7684\u63d0\u793a\u548c\u56fa\u5b9a\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\uff0c\u963b\u788d\u4e86\u66f4\u7075\u6d3b\u548c\u81ea\u4e3b\u7684TP\u4ee3\u7406\u3002 \u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aefagent\u5f3a\u5316\u5b66\u4e60\u6846\u67b6DeepTravel\uff0c\u7528\u4e8e\u6784\u5efa\u81ea\u4e3b\u65c5\u884c\u89c4\u5212agent\uff0c\u8be5agent\u80fd\u591f\u81ea\u4e3b\u89c4\u5212\u3001\u6267\u884c\u5de5\u5177\u4ee5\u53ca\u53cd\u601d\u5de5\u5177\u54cd\u5e94\uff0c\u4ece\u800c\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u63a2\u7d22\u3001\u9a8c\u8bc1\u548c\u5b8c\u5584\u4e2d\u95f4\u64cd\u4f5c\u3002 \u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u6211\u4eec\u9996\u5148\u901a\u8fc7\u7f13\u5b58\u4ea4\u901a\u3001\u4f4f\u5bbf\u548cPOI\u6570\u636e\u6765\u6784\u5efa\u4e00\u4e2a\u5f3a\u5927\u7684sandbox\u73af\u5883\uff0c\u4ece\u800c\u4fc3\u8fdbTP\u4ee3\u7406\u8bad\u7ec3\uff0c\u800c\u4e0d\u53d7\u73b0\u5b9e\u4e16\u754cAPI\u9650\u5236\uff08\u4f8b\u5982\uff0c\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\uff09\u7684\u7ea6\u675f\u3002 \u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u5206\u5c42\u5956\u52b1\u5efa\u6a21\u7cfb\u7edf\uff0c\u5176\u4e2d\u8f68\u8ff9\u7ea7\u522b\u7684\u9a8c\u8bc1\u5668\u9996\u5148\u68c0\u67e5\u65f6\u7a7a\u53ef\u884c\u6027\u5e76\u8fc7\u6ee4\u4e0d\u6ee1\u8db3\u7684\u65c5\u884c\u884c\u7a0b\uff0c\u7136\u540eturn\u7ea7\u522b\u7684\u9a8c\u8bc1\u5668\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u884c\u7a0b\u7ec6\u8282\u4e0e\u5de5\u5177\u54cd\u5e94\u7684\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u800c\u7cbe\u786e\u7684\u5956\u52b1\u670d\u52a1\u3002 \u6700\u540e\uff0c\u6211\u4eec\u63d0\u51fa\u4e86reply\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7fTP\u4ee3\u7406\u80fd\u591f\u5b9a\u671f\u4ece\u5931\u8d25\u7ecf\u9a8c\u7f13\u51b2\u533a\u4e2d\u8fdb\u884creplay\uff0c\u4ece\u800c\u4ea7\u751f\u663e\u7740\u7684agent\u80fd\u529b\u3002 \u6211\u4eec\u5728DiDi Enterprise Solutions App\u4e0a\u90e8\u7f72\u4e86\u7ecf\u8fc7\u8bad\u7ec3\u7684TP\u4ee3\u7406\uff0c\u5e76\u8fdb\u884c\u4e86\u5168\u9762\u7684\u7ebf\u4e0a\u548c\u7ebf\u4e0b\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eDeepTravel\u4f7f\u5c0f\u5c3a\u5bf8LLM\uff08\u4f8b\u5982Qwen3 32B\uff09\u5728\u65c5\u884c\u8ba1\u5212\u4efb\u52a1\u4e2d\u663e\u7740\u4f18\u4e8e\u73b0\u6709\u7684\u524d\u6cbfLLM\uff0c\u4f8b\u5982OpenAI o1\u3001o3\u548cDeepSeek R1\u3002"}}
{"id": "2509.22642", "pdf": "https://arxiv.org/pdf/2509.22642", "abs": "https://arxiv.org/abs/2509.22642", "authors": ["Xiaowei Chi", "Peidong Jia", "Chun-Kai Fan", "Xiaozhu Ju", "Weishi Mi", "Kevin Zhang", "Zhiyuan Qin", "Wanxin Tian", "Kuangzhi Ge", "Hao Li", "Zezhong Qian", "Anthony Chen", "Qiang Zhou", "Yueru Jia", "Jiaming Liu", "Yong Dai", "Qingpo Wuwu", "Chengyu Bai", "Yu-Kai Wang", "Ying Li", "Lizhang Chen", "Yong Bao", "Zhiyuan Jiang", "Jiacheng Zhu", "Kai Tang", "Ruichuan An", "Yulin Luo", "Qiuxuan Feng", "Siyuan Zhou", "Chi-min Chan", "Chengkai Hou", "Wei Xue", "Sirui Han", "Yike Guo", "Shanghang Zhang", "Jian Tang"], "title": "WoW: Towards a World omniscient World model Through Embodied Interaction", "categories": ["cs.RO", "cs.CV", "cs.MM"], "comment": null, "summary": "Humans develop an understanding of intuitive physics through active\ninteraction with the world. This approach is in stark contrast to current video\nmodels, such as Sora, which rely on passive observation and therefore struggle\nwith grasping physical causality. This observation leads to our central\nhypothesis: authentic physical intuition of the world model must be grounded in\nextensive, causally rich interactions with the real world. To test this\nhypothesis, we present WoW, a 14-billion-parameter generative world model\ntrained on 2 million robot interaction trajectories. Our findings reveal that\nthe model's understanding of physics is a probabilistic distribution of\nplausible outcomes, leading to stochastic instabilities and physical\nhallucinations. Furthermore, we demonstrate that this emergent capability can\nbe actively constrained toward physical realism by SOPHIA, where\nvision-language model agents evaluate the DiT-generated output and guide its\nrefinement by iteratively evolving the language instructions. In addition, a\nco-trained Inverse Dynamics Model translates these refined plans into\nexecutable robotic actions, thus closing the imagination-to-action loop. We\nestablish WoWBench, a new benchmark focused on physical consistency and causal\nreasoning in video, where WoW achieves state-of-the-art performance in both\nhuman and autonomous evaluation, demonstrating strong ability in physical\ncausality, collision dynamics, and object permanence. Our work provides\nsystematic evidence that large-scale, real-world interaction is a cornerstone\nfor developing physical intuition in AI. Models, data, and benchmarks will be\nopen-sourced.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper describes a generative world model (WoW) trained on robot interaction trajectories. While not explicitly trajectory prediction, it involves generating plausible outcomes based on interaction with the world, which is related. The model is a 14-billion-parameter model, which falls under the scope of large models. The use of a vision-language model agent (SOPHIA) also points to the use of large models. The mention of an Inverse Dynamics Model further connects it to trajectory generation/planning in robotics.", "keywords": ["world model", "robot interaction trajectories", "large language model", "foundation model", "generative model", "Inverse Dynamics Model", "vision-language model"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWoW\u7684\u751f\u6210\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u5728\u5927\u91cf\u673a\u5668\u4eba\u4ea4\u4e92\u8f68\u8ff9\u4e0a\u8bad\u7ec3\uff0c\u5e76\u7ed3\u5408SOPHIA\u548c\u9006\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u63d0\u5347\u4e86AI\u5728\u7269\u7406\u56e0\u679c\u5173\u7cfb\u65b9\u9762\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u6a21\u578b\uff08\u5982Sora\uff09\u4f9d\u8d56\u88ab\u52a8\u89c2\u5bdf\uff0c\u96be\u4ee5\u7406\u89e3\u7269\u7406\u56e0\u679c\u5173\u7cfb\uff1b\u4eba\u7c7b\u901a\u8fc7\u4e0e\u4e16\u754c\u7684\u79ef\u6781\u4e92\u52a8\u53d1\u5c55\u76f4\u89c2\u7269\u7406\u7406\u89e3\u3002\u8bba\u6587\u65e8\u5728\u9a8c\u8bc1\uff1a\u4e16\u754c\u6a21\u578b\u7684\u771f\u5b9e\u7269\u7406\u76f4\u89c9\u5fc5\u987b\u57fa\u4e8e\u4e0e\u771f\u5b9e\u4e16\u754c\u7684\u5e7f\u6cdb\u3001\u56e0\u679c\u4e30\u5bcc\u7684\u4ea4\u4e92\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a140\u4ebf\u53c2\u6570\u7684\u751f\u6210\u4e16\u754c\u6a21\u578bWoW\uff0c\u8be5\u6a21\u578b\u5728200\u4e07\u4e2a\u673a\u5668\u4eba\u4ea4\u4e92\u8f68\u8ff9\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u540c\u65f6\uff0c\u4f7f\u7528SOPHIA\uff08\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\uff09\u8bc4\u4f30DiT\u751f\u6210\u7684\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6f14\u5316\u8bed\u8a00\u6307\u4ee4\u6765\u6307\u5bfc\u6539\u8fdb\uff0c\u4ece\u800c\u7ea6\u675f\u6a21\u578b\u5411\u7269\u7406\u771f\u5b9e\u6027\u9760\u62e2\u3002\u6b64\u5916\uff0c\u8fd8\u8bad\u7ec3\u4e86\u4e00\u4e2a\u9006\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5c06\u6539\u8fdb\u540e\u7684\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWoW\u6a21\u578b\u5bf9\u7269\u7406\u7684\u7406\u89e3\u662f\u5408\u7406\u7ed3\u679c\u7684\u6982\u7387\u5206\u5e03\uff0c\u5bfc\u81f4\u968f\u673a\u4e0d\u7a33\u5b9a\u6027\u548c\u7269\u7406\u5e7b\u89c9\u3002\u901a\u8fc7SOPHIA\u7684\u7ea6\u675f\uff0c\u53ef\u4ee5\u4e3b\u52a8\u5730\u5c06\u6a21\u578b\u5f15\u5bfc\u5411\u7269\u7406\u771f\u5b9e\u6027\u3002WoW\u5728WoWBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7269\u7406\u56e0\u679c\u5173\u7cfb\u3001\u78b0\u649e\u52a8\u529b\u5b66\u548c\u7269\u4f53\u6301\u4e45\u6027\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u3002", "conclusion": "\u5927\u89c4\u6a21\u7684\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u662fAI\u53d1\u5c55\u7269\u7406\u76f4\u89c9\u7684\u57fa\u77f3\u3002\u8bba\u6587\u7684\u6a21\u578b\u3001\u6570\u636e\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u5f00\u6e90\u3002", "summary_zh": "\u4eba\u7c7b\u901a\u8fc7\u4e0e\u4e16\u754c\u7684\u79ef\u6781\u4e92\u52a8\u6765\u53d1\u5c55\u5bf9\u76f4\u89c2\u7269\u7406\u7684\u7406\u89e3\u3002\u8fd9\u4e0e\u5f53\u524d\u7684\u89c6\u9891\u6a21\u578b\uff08\u5982Sora\uff09\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u540e\u8005\u4f9d\u8d56\u4e8e\u88ab\u52a8\u89c2\u5bdf\uff0c\u56e0\u6b64\u96be\u4ee5\u638c\u63e1\u7269\u7406\u56e0\u679c\u5173\u7cfb\u3002\u8fd9\u4e00\u89c2\u5bdf\u7ed3\u679c\u5f15\u51fa\u4e86\u6211\u4eec\u7684\u4e2d\u5fc3\u5047\u8bbe\uff1a\u4e16\u754c\u6a21\u578b\u7684\u771f\u5b9e\u7269\u7406\u76f4\u89c9\u5fc5\u987b\u5efa\u7acb\u5728\u4e0e\u73b0\u5b9e\u4e16\u754c\u7684\u5927\u91cf\u3001\u56e0\u679c\u4e30\u5bcc\u7684\u4e92\u52a8\u4e4b\u4e0a\u3002\u4e3a\u4e86\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\uff0c\u6211\u4eec\u63d0\u51fa\u4e86WoW\uff0c\u4e00\u4e2a\u5728200\u4e07\u4e2a\u673a\u5668\u4eba\u4e92\u52a8\u8f68\u8ff9\u4e0a\u8bad\u7ec3\u7684140\u4ebf\u53c2\u6570\u7684\u751f\u6210\u4e16\u754c\u6a21\u578b\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5bf9\u7269\u7406\u7684\u7406\u89e3\u662f\u5408\u7406\u7ed3\u679c\u7684\u6982\u7387\u5206\u5e03\uff0c\u5bfc\u81f4\u968f\u673a\u4e0d\u7a33\u5b9a\u6027\u548c\u7269\u7406\u5e7b\u89c9\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b0\u5174\u80fd\u529b\u53ef\u4ee5\u901a\u8fc7SOPHIA\u4e3b\u52a8\u5730\u7ea6\u675f\u5230\u7269\u7406\u771f\u5b9e\u6027\uff0c\u5176\u4e2d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u667a\u80fd\u4f53\u8bc4\u4f30DiT\u751f\u6210\u7684\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6f14\u5316\u8bed\u8a00\u6307\u4ee4\u6765\u6307\u5bfc\u5176\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u5171\u540c\u8bad\u7ec3\u7684\u9006\u52a8\u529b\u5b66\u6a21\u578b\u5c06\u8fd9\u4e9b\u6539\u8fdb\u7684\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u52a8\u4f5c\uff0c\u4ece\u800c\u95ed\u5408\u4e86\u4ece\u60f3\u8c61\u5230\u884c\u52a8\u7684\u5faa\u73af\u3002\u6211\u4eec\u5efa\u7acb\u4e86WoWBench\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u89c6\u9891\u4e2d\u7684\u7269\u7406\u4e00\u81f4\u6027\u548c\u56e0\u679c\u63a8\u7406\uff0cWoW\u5728\u4eba\u7c7b\u548c\u81ea\u4e3b\u8bc4\u4f30\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7269\u7406\u56e0\u679c\u5173\u7cfb\u3001\u78b0\u649e\u52a8\u529b\u5b66\u548c\u7269\u4f53\u6301\u4e45\u6027\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u7684\u8bc1\u636e\uff0c\u8868\u660e\u5927\u89c4\u6a21\u7684\u771f\u5b9e\u4e16\u754c\u4e92\u52a8\u662f\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u7269\u7406\u76f4\u89c9\u7684\u57fa\u77f3\u3002\u6a21\u578b\u3001\u6570\u636e\u548c\u57fa\u51c6\u6d4b\u8bd5\u5c06\u5f00\u6e90\u3002"}}
{"id": "2509.22643", "pdf": "https://arxiv.org/pdf/2509.22643", "abs": "https://arxiv.org/abs/2509.22643", "authors": ["Wenkai Guo", "Guanxing Lu", "Haoyuan Deng", "Zhenyu Wu", "Yansong Tang", "Ziwei Wang"], "title": "VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search", "categories": ["cs.RO"], "comment": "9 pages", "summary": "Vision-Language-Action models (VLAs) achieve strong performance in general\nrobotic manipulation tasks by scaling imitation learning. However, existing\nVLAs are limited to predicting short-sighted next-action, which struggle with\nlong-horizon trajectory tasks due to incremental deviations. To address this\nproblem, we propose a plug-in framework named VLA-Reasoner that effectively\nempowers off-the-shelf VLAs with the capability of foreseeing future states via\ntest-time scaling. Specifically, VLA-Reasoner samples and rolls out possible\naction trajectories where involved actions are rationales to generate future\nstates via a world model, which enables VLA-Reasoner to foresee and reason\npotential outcomes and search for the optimal actions. We further leverage\nMonte Carlo Tree Search (MCTS) to improve search efficiency in large action\nspaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a\nconfidence sampling mechanism based on Kernel Density Estimation (KDE), to\nenable efficient exploration in MCTS without redundant VLA queries. We evaluate\nintermediate states in MCTS via an offline reward shaping strategy, to score\npredicted futures and correct deviations with long-term feedback. We conducted\nextensive experiments in both simulators and the real world, demonstrating that\nour proposed VLA-Reasoner achieves significant improvements over the\nstate-of-the-art VLAs. Our method highlights a potential pathway toward\nscalable test-time computation of robotic manipulation.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on improving Vision-Language-Action models (VLAs) for robotic manipulation, specifically addressing long-horizon trajectory tasks. It uses Monte Carlo Tree Search (MCTS) to foresee future states and find optimal actions, which relates to trajectory prediction. While the paper mentions VLAs, which are related to large models, the core focus is on action planning and trajectory optimization. The connection to large language models is somewhat indirect, as the VLA might utilize a large language model internally, but that isn't the main contribution.", "keywords": ["trajectory", "Vision-Language-Action models", "VLA", "Monte Carlo Tree Search", "MCTS", "action prediction", "robotic manipulation"]}, "AI": {"tldr": "VLA-Reasoner\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u4f7f\u73b0\u6210\u7684VLA\u5177\u5907\u4e86\u9884\u6d4b\u672a\u6765\u72b6\u6001\u7684\u80fd\u529b\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b(VLA)\u5728\u5904\u7406\u957f\u65f6\u7a0b\u8f68\u8ff9\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u9884\u6d4b\u7684\u77ed\u89c6\u6027\uff0c\u4f1a\u4ea7\u751f\u7d2f\u79ef\u504f\u5dee\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVLA-Reasoner\u7684\u63d2\u4ef6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u91c7\u6837\u548c\u63a8\u6f14\u53ef\u80fd\u7684\u52a8\u4f5c\u8f68\u8ff9\u6765\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u5e76\u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u63d0\u9ad8\u641c\u7d22\u6548\u7387\uff0c\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u6838\u5bc6\u5ea6\u4f30\u8ba1(KDE)\u7684\u7f6e\u4fe1\u5ea6\u62bd\u6837\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u79bb\u7ebf\u5956\u52b1\u5851\u9020\u7b56\u7565\u6765\u8bc4\u4f30\u4e2d\u95f4\u72b6\u6001\uff0c\u5e76\u7ea0\u6b63\u504f\u5dee\u3002", "result": "\u5728\u6a21\u62df\u5668\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5b9e\u9a8c\u4e2d\uff0cVLA-Reasoner\u76f8\u6bd4\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u53ef\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u6f5c\u5728\u9014\u5f84\u3002", "summary_zh": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b(VLA)\u901a\u8fc7\u6269\u5c55\u6a21\u4eff\u5b66\u4e60\u5728\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684VLA\u6a21\u578b\u4ec5\u9650\u4e8e\u9884\u6d4b\u77ed\u89c6\u7684\u4e0b\u4e00\u6b65\u52a8\u4f5c\uff0c\u7531\u4e8e\u589e\u91cf\u504f\u5dee\uff0c\u96be\u4ee5\u5904\u7406\u957f\u65f6\u7a0b\u8f68\u8ff9\u4efb\u52a1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aVLA-Reasoner\u7684\u63d2\u4ef6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\uff0c\u6709\u6548\u5730\u4f7f\u73b0\u6210\u7684VLA\u5177\u5907\u4e86\u9884\u6d4b\u672a\u6765\u72b6\u6001\u7684\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0cVLA-Reasoner\u91c7\u6837\u5e76\u63a8\u6f14\u53ef\u80fd\u7684\u52a8\u4f5c\u8f68\u8ff9\uff0c\u5176\u4e2d\u6d89\u53ca\u7684\u52a8\u4f5c\u662f\u4f7f\u7528\u4e16\u754c\u6a21\u578b\u751f\u6210\u672a\u6765\u72b6\u6001\u7684\u7406\u7531\uff0c\u8fd9\u4f7f\u5f97VLA-Reasoner\u80fd\u591f\u9884\u89c1\u548c\u63a8\u7406\u6f5c\u5728\u7684\u7ed3\u679c\uff0c\u5e76\u641c\u7d22\u6700\u4f73\u52a8\u4f5c\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5229\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u6765\u63d0\u9ad8\u5927\u578b\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u641c\u7d22\u6548\u7387\uff0c\u5176\u4e2d\u9010\u6b65\u7684VLA\u9884\u6d4b\u4e3a\u6839\u8282\u70b9\u63d0\u4f9b\u79cd\u5b50\u3002\u540c\u65f6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u6838\u5bc6\u5ea6\u4f30\u8ba1(KDE)\u7684\u7f6e\u4fe1\u5ea6\u62bd\u6837\u673a\u5236\uff0c\u4ee5\u5728MCTS\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\uff0c\u800c\u65e0\u9700\u5197\u4f59\u7684VLA\u67e5\u8be2\u3002\u6211\u4eec\u901a\u8fc7\u79bb\u7ebf\u5956\u52b1\u5851\u9020\u7b56\u7565\u8bc4\u4f30MCTS\u4e2d\u7684\u4e2d\u95f4\u72b6\u6001\uff0c\u4ee5\u8bc4\u4f30\u9884\u6d4b\u7684\u672a\u6765\u5e76\u4f7f\u7528\u957f\u671f\u53cd\u9988\u7ea0\u6b63\u504f\u5dee\u3002\u6211\u4eec\u5728\u6a21\u62df\u5668\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u63d0\u51fa\u7684VLA-Reasoner\u76f8\u6bd4\u6700\u5148\u8fdb\u7684VLA\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u7a81\u51fa\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u53ef\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7684\u6f5c\u5728\u9014\u5f84\u3002"}}
{"id": "2509.21523", "pdf": "https://arxiv.org/pdf/2509.21523", "abs": "https://arxiv.org/abs/2509.21523", "authors": ["Xiaofan Yu", "Yuwei Wu", "Katherine Mao", "Ye Tian", "Vijay Kumar", "Tajana Rosing"], "title": "DroneFL: Federated Learning for Multi-UAV Visual Target Tracking", "categories": ["cs.RO"], "comment": null, "summary": "Multi-robot target tracking is a fundamental problem that requires\ncoordinated monitoring of dynamic entities in applications such as precision\nagriculture, environmental monitoring, disaster response, and security\nsurveillance. While Federated Learning (FL) has the potential to enhance\nlearning across multiple robots without centralized data aggregation, its use\nin multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely\nunderexplored. Key challenges include limited onboard computational resources,\nsignificant data heterogeneity in FL due to varying targets and the fields of\nview, and the need for tight coupling between trajectory prediction and\nmulti-robot planning. In this paper, we introduce DroneFL, the first federated\nlearning framework specifically designed for efficient multi-UAV target\ntracking. We design a lightweight local model to predict target trajectories\nfrom sensor inputs, using a frozen YOLO backbone and a shallow transformer for\nefficient onboard training. The updated models are periodically aggregated in\nthe cloud for global knowledge sharing. To alleviate the data heterogeneity\nthat hinders FL convergence, DroneFL introduces a position-invariant model\narchitecture with altitude-based adaptive instance normalization. Finally, we\nfuse predictions from multiple UAVs in the cloud and generate optimal\ntrajectories that balance target prediction accuracy and overall tracking\nperformance. Our results show that DroneFL reduces prediction error by 6%-83%\nand tracking distance by 0.4%-4.6% compared to a distributed non-FL framework.\nIn terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has\non average just 1.56 KBps data rate to the cloud.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on federated learning for multi-UAV target tracking, which involves trajectory prediction. While it uses a transformer, it's a shallow transformer for efficiency and not a large language model. The connection to trajectory prediction is stronger than the connection to large language models.", "keywords": ["trajectory prediction", "federated learning", "transformer", "target tracking"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21593", "pdf": "https://arxiv.org/pdf/2509.21593", "abs": "https://arxiv.org/abs/2509.21593", "authors": ["Peng Luo", "Xiayin Lou", "Yu Zheng", "Zhuo Zheng", "Stefano Ermon"], "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models", "categories": ["cs.AI", "physics.soc-ph"], "comment": null, "summary": "Geospatial modeling provides critical solutions for pressing global\nchallenges such as sustainability and climate change. Existing large language\nmodel (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at\nevolving generic code but lack the domain knowledge and multi-step reasoning\nrequired for complex geospatial problems. We introduce GeoEvolve, a multi-agent\nLLM framework that couples evolutionary search with geospatial domain knowledge\nto automatically design and refine geospatial algorithms. GeoEvolve operates in\ntwo nested loops: an inner loop leverages a code evolver to generate and mutate\ncandidate solutions, while an outer agentic controller evaluates global elites\nand queries a GeoKnowRAG module -- a structured geospatial knowledge base that\ninjects theoretical priors from geography. This knowledge-guided evolution\nsteers the search toward theoretically meaningful and computationally efficient\nalgorithms. We evaluate GeoEvolve on two fundamental and classical tasks:\nspatial interpolation (kriging) and spatial uncertainty quantification\n(geospatial conformal prediction). Across these benchmarks, GeoEvolve\nautomatically improves and discovers new algorithms, incorporating geospatial\ntheory on top of classical models. It reduces spatial interpolation error\n(RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%.\nAblation studies confirm that domain-guided retrieval is essential for stable,\nhigh-quality evolution. These results demonstrate that GeoEvolve provides a\nscalable path toward automated, knowledge-driven geospatial modeling, opening\nnew opportunities for trustworthy and efficient AI-for-Science discovery.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5730\u7406\u7a7a\u95f4\u5efa\u6a21\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u4f7f\u7528\u4e86LLM\u8fdb\u884c\u7b97\u6cd5\u53d1\u73b0\u548c\u4f18\u5316\uff0c\u5e76\u4e14\u63d0\u5230\u4e86\u7a7a\u95f4\u6570\u636e\u5904\u7406\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLM", "geospatial modeling", "algorithm discovery"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21651", "pdf": "https://arxiv.org/pdf/2509.21651", "abs": "https://arxiv.org/abs/2509.21651", "authors": ["Abhishek Jindal", "Dmitry Kalashnikov", "Oscar Chang", "Divya Garikapati", "Anirudha Majumdar", "Pierre Sermanet", "Vikas Sindhwani"], "title": "Can AI Perceive Physical Danger and Intervene?", "categories": ["cs.AI"], "comment": null, "summary": "When AI interacts with the physical world -- as a robot or an assistive agent\n-- new safety challenges emerge beyond those of purely ``digital AI\". In such\ninteractions, the potential for physical harm is direct and immediate. How well\ndo state-of-the-art foundation models understand common-sense facts about\nphysical safety, e.g. that a box may be too heavy to lift, or that a hot cup of\ncoffee should not be handed to a child? In this paper, our contributions are\nthree-fold: first, we develop a highly scalable approach to continuous physical\nsafety benchmarking of Embodied AI systems, grounded in real-world injury\nnarratives and operational safety constraints. To probe multi-modal safety\nunderstanding, we turn these narratives and constraints into photorealistic\nimages and videos capturing transitions from safe to unsafe states, using\nadvanced generative models. Secondly, we comprehensively analyze the ability of\nmajor foundation models to perceive risks, reason about safety, and trigger\ninterventions; this yields multi-faceted insights into their deployment\nreadiness for safety-critical agentic applications. Finally, we develop a\npost-training paradigm to teach models to explicitly reason about\nembodiment-specific safety constraints provided through system instructions.\nThe resulting models generate thinking traces that make safety reasoning\ninterpretable and transparent, achieving state of the art performance in\nconstraint satisfaction evaluations. The benchmark will be released at\nhttps://asimov-benchmark.github.io/v2", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8AI\u5728\u7269\u7406\u4e16\u754c\u4e2d\u7684\u5b89\u5168\u611f\u77e5\u548c\u5e72\u9884\uff0c\u4f7f\u7528\u4e86\u5927\u578b\u57fa\u7840\u6a21\u578b\u6765\u7406\u89e3\u7269\u7406\u5b89\u5168\u5e38\u8bc6\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u7814\u7a76\u7684\u5177\u8eab\u667a\u80fd\uff08Embodied AI\uff09\u548c\u98ce\u9669\u611f\u77e5\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5728\u67d0\u4e9b\u5e94\u7528\u573a\u666f\u4e0b\u5b58\u5728\u5173\u8054\uff0c\u4f8b\u5982\u81ea\u52a8\u9a7e\u9a76\u3001\u673a\u5668\u4eba\u5bfc\u822a\u7b49\u3002\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u578b\u751f\u6210\u6a21\u578b\uff0c\u5e76\u5bf9\u5927\u578b\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u4e86\u8bc4\u4f30\u548c\u8bad\u7ec3\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002", "keywords": ["foundation models", "generative models", "Embodied AI", "physical safety"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21690", "pdf": "https://arxiv.org/pdf/2509.21690", "abs": "https://arxiv.org/abs/2509.21690", "authors": ["Muqun Hu", "Wenxi Chen", "Wenjing Li", "Falak Mandali", "Zijian He", "Renhong Zhang", "Praveen Krisna", "Katherine Christian", "Leo Benaharon", "Dizhi Ma", "Karthik Ramani", "Yan Gu"], "title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation", "categories": ["cs.RO"], "comment": null, "summary": "Humanoid table tennis (TT) demands rapid perception, proactive whole-body\nmotion, and agile footwork under strict timing -- capabilities that remain\ndifficult for unified controllers. We propose a reinforcement learning\nframework that maps ball-position observations directly to whole-body joint\ncommands for both arm striking and leg locomotion, strengthened by predictive\nsignals and dense, physics-guided rewards. A lightweight learned predictor, fed\nwith recent ball positions, estimates future ball states and augments the\npolicy's observations for proactive decision-making. During training, a\nphysics-based predictor supplies precise future states to construct dense,\ninformative rewards that lead to effective exploration. The resulting policy\nattains strong performance across varied serve ranges (hit rate $\\geq$ 96% and\nsuccess rate $\\geq$ 92%) in simulations. Ablation studies confirm that both the\nlearned predictor and the predictive reward design are critical for end-to-end\nlearning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute\njoints, the policy produces coordinated lateral and forward-backward footwork\nwith accurate, fast returns, suggesting a practical path toward versatile,\ncompetitive humanoid TT.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on humanoid table tennis, which involves predicting the ball's trajectory and the humanoid's movements. While it doesn't explicitly use or mention Large Language Models, the use of a learned predictor for future ball states connects to the broader theme of prediction in robotic systems. The prediction aspect is relevant to trajectory prediction, but the absence of LLMs lowers the overall relevance.", "keywords": ["prediction", "learned predictor", "future ball states", "reinforcement learning", "humanoid"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21928", "pdf": "https://arxiv.org/pdf/2509.21928", "abs": "https://arxiv.org/abs/2509.21928", "authors": ["Jialiang Li", "Wenzheng Wu", "Gaojing Zhang", "Yifan Han", "Wenzhao Lian"], "title": "SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Successfully solving long-horizon manipulation tasks remains a fundamental\nchallenge. These tasks involve extended action sequences and complex object\ninteractions, presenting a critical gap between high-level symbolic planning\nand low-level continuous control. To bridge this gap, two essential\ncapabilities are required: robust long-horizon task planning and effective\ngoal-conditioned manipulation. Existing task planning methods, including\ntraditional and LLM-based approaches, often exhibit limited generalization or\nsparse semantic reasoning. Meanwhile, image-conditioned control methods\nstruggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a\nnovel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon\nManipulation Tasks. SAGE utilizes semantic scene graphs as a structural\nrepresentation for scene states. A structural scene graph enables bridging\ntask-level semantic reasoning and pixel-level visuo-motor control. This also\nfacilitates the controllable synthesis of accurate, novel sub-goal images. SAGE\nconsists of two key components: (1) a scene graph-based task planner that uses\nVLMs and LLMs to parse the environment and reason about physically-grounded\nscene state transition sequences, and (2) a decoupled structural image editing\npipeline that controllably converts each target sub-goal graph into a\ncorresponding image through image inpainting and composition. Extensive\nexperiments have demonstrated that SAGE achieves state-of-the-art performance\non distinct long-horizon tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u573a\u666f\u56fe\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u957f\u65f6\u7a0b\u89c4\u5212\u7684\u6982\u5ff5\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u8bba\u6587\u4f7f\u7528\u4e86LLM\u8fdb\u884c\u73af\u5883\u89e3\u6790\u548c\u573a\u666f\u72b6\u6001\u8f6c\u6362\u63a8\u7406\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u76f8\u5173\u3002", "keywords": ["LLMs", "large language models", "long-horizon task planning", "scene graph"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21961", "pdf": "https://arxiv.org/pdf/2509.21961", "abs": "https://arxiv.org/abs/2509.21961", "authors": ["Lingguang Wang", "\u00d6mer \u015eahin Ta\u015f", "Marlon Steiner", "Christoph Stiller"], "title": "FlowDrive: moderated flow matching with data balancing for trajectory planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Learning-based planners are sensitive to the long-tailed distribution of\ndriving data. Common maneuvers dominate datasets, while dangerous or rare\nscenarios are sparse. This imbalance can bias models toward the frequent cases\nand degrade performance on critical scenarios. To tackle this problem, we\ncompare balancing strategies for sampling training data and find reweighting by\ntrajectory pattern an effective approach. We then present FlowDrive, a\nflow-matching trajectory planner that learns a conditional rectified flow to\nmap noise directly to trajectory distributions with few flow-matching steps. We\nfurther introduce moderated, in-the-loop guidance that injects small\nperturbation between flow steps to systematically increase trajectory diversity\nwhile remaining scene-consistent. On nuPlan and the interaction-focused\ninterPlan benchmarks, FlowDrive achieves state-of-the-art results among\nlearning-based planners and approaches methods with rule-based refinements.\nAfter adding moderated guidance and light post-processing (FlowDrive*), it\nachieves overall state-of-the-art performance across nearly all benchmark\nsplits.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory planning for driving scenarios, specifically using flow matching. While it doesn't explicitly use or mention large language models, the use of 'learning-based planners' and addressing data imbalance are areas where LLMs could potentially be applied in the future. The core focus is on trajectory prediction/planning.", "keywords": ["trajectory planning", "driving data", "flow matching", "learning-based planners"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21983", "pdf": "https://arxiv.org/pdf/2509.21983", "abs": "https://arxiv.org/abs/2509.21983", "authors": ["Sigmund Hennum H\u00f8eg", "Aksel Vaaler", "Chaoqi Liu", "Olav Egeland", "Yilun Du"], "title": "Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning", "categories": ["cs.RO", "cs.AI"], "comment": "10 pages, 11 figures. This work has been submitted to the IEEE for\n  possible publication. See https://sigmundhh.com/hybrid_diffusion/ for the\n  project website", "summary": "Constructing robots to accomplish long-horizon tasks is a long-standing\nchallenge within artificial intelligence. Approaches using generative methods,\nparticularly Diffusion Models, have gained attention due to their ability to\nmodel continuous robotic trajectories for planning and control. However, we\nshow that these models struggle with long-horizon tasks that involve complex\ndecision-making and, in general, are prone to confusing different modes of\nbehavior, leading to failure. To remedy this, we propose to augment continuous\ntrajectory generation by simultaneously generating a high-level symbolic plan.\nWe show that this requires a novel mix of discrete variable diffusion and\ncontinuous diffusion, which dramatically outperforms the baselines. In\naddition, we illustrate how this hybrid diffusion process enables flexible\ntrajectory synthesis, allowing us to condition synthesized actions on partial\nand complete symbolic conditions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u751f\u6210\u5f0f\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u5e76\u4e14\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u8bba\u6587\u6838\u5fc3\u5728\u4e8e\u8f68\u8ff9\u751f\u6210\u548c\u89c4\u5212\uff0c\u5e76\u7ed3\u5408\u4e86\u7b26\u53f7\u89c4\u5212\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory generation", "Diffusion Models", "planning", "robotic trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21986", "pdf": "https://arxiv.org/pdf/2509.21986", "abs": "https://arxiv.org/abs/2509.21986", "authors": ["Tomoya Yoshida", "Shuhei Kurita", "Taichi Nishimura", "Shinsuke Mori"], "title": "Developing Vision-Language-Action Model from Egocentric Videos", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Egocentric videos capture how humans manipulate objects and tools, providing\ndiverse motion cues for learning object manipulation. Unlike the costly,\nexpert-driven manual teleoperation commonly used in training\nVision-Language-Action models (VLAs), egocentric videos offer a scalable\nalternative. However, prior studies that leverage such videos for training\nrobot policies typically rely on auxiliary annotations, such as detailed\nhand-pose recordings. Consequently, it remains unclear whether VLAs can be\ntrained directly from raw egocentric videos. In this work, we address this\nchallenge by leveraging EgoScaler, a framework that extracts 6DoF object\nmanipulation trajectories from egocentric videos without requiring auxiliary\nrecordings. We apply EgoScaler to four large-scale egocentric video datasets\nand automatically refine noisy or incomplete trajectories, thereby constructing\na new large-scale dataset for VLA pre-training. Our experiments with a\nstate-of-the-art $\\pi_0$ architecture in both simulated and real-robot\nenvironments yield three key findings: (i) pre-training on our dataset improves\ntask success rates by over 20\\% compared to training from scratch, (ii) the\nperformance is competitive with that achieved using real-robot datasets, and\n(iii) combining our dataset with real-robot data yields further improvements.\nThese results demonstrate that egocentric videos constitute a promising and\nscalable resource for advancing VLA research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c6\u9891\u4e2d\u5b66\u4e60\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5e76\u5229\u7528\u63d0\u53d6\u7684\u7269\u4f53\u64cd\u4f5c\u8f68\u8ff9\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u63d0\u53d6\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u4f46\u4e0e\u7eaf\u7cb9\u7684\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\u6709\u6240\u533a\u522b\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u53ef\u4ee5\u770b\u4f5c\u4e0e\u5927\u6a21\u578b\u76f8\u5173\uff0c\u4f46\u91cd\u70b9\u4e0d\u5728\u4e8e\u5927\u6a21\u578b\u672c\u8eab\u3002", "keywords": ["trajectory", "Vision-Language-Action Models", "pre-training", "egocentric videos"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22093", "pdf": "https://arxiv.org/pdf/2509.22093", "abs": "https://arxiv.org/abs/2509.22093", "authors": ["Xiaohuan Pei", "Yuxing Chen", "Siyu Xu", "Yunke Wang", "Yuheng Shi", "Chang Xu"], "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robotic manipulation with Vision-Language-Action models requires efficient\ninference over long-horizon multi-modal context, where attention to dense\nvisual tokens dominates computational cost. Existing methods optimize inference\nspeed by reducing visual redundancy within VLA models, but they overlook the\nvarying redundancy across robotic manipulation stages. We observe that the\nvisual token redundancy is higher in coarse manipulation phase than in\nfine-grained operations, and is strongly correlated with the action dynamic.\nMotivated by this observation, we propose \\textbf{A}ction-aware\n\\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP}), a multi-modal pruning\nframework that integrates text-driven token selection with action-aware\ntrajectory gating. Our method introduces a gating mechanism that conditions the\npruning signal on recent action trajectories, using past motion windows to\nadaptively adjust token retention ratios in accordance with dynamics, thereby\nbalancing computational efficiency and perceptual precision across different\nmanipulation stages. Extensive experiments on the LIBERO suites and diverse\nreal-world scenarios demonstrate that our method significantly reduces FLOPs\nand action inference latency (\\textit{e.g.} $1.35 \\times$ speed up on\nOpenVLA-OFT) while maintaining competitive success rates (\\textit{e.g.} 25.8\\%\nimprovements with OpenVLA) compared to baselines, thereby providing a simple\nplug-in path to efficient robot policies that advances the efficiency and\nperformance frontier of robotic manipulation. Our project website is:\n\\href{https://vla-adp.github.io/}{ADP.com}.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8Vision-Language-Action\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u52a8\u4f5c\u611f\u77e5\u7684\u52a8\u6001\u526a\u679d\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201caction trajectories\u201d\u548c\u201cmotion windows\u201d\uff0c\u6697\u793a\u4e86\u5bf9\u8f68\u8ff9\u7684\u5229\u7528\u3002\u6b64\u5916\uff0c\u8be5\u8bba\u6587\u6d89\u53ca\u4e86\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff0c\u5c5e\u4e8e\u5927\u6a21\u578b\u7684\u4e00\u79cd\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language-Action models", "action trajectories", "motion windows", "robotic manipulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21981", "pdf": "https://arxiv.org/pdf/2509.21981", "abs": "https://arxiv.org/abs/2509.21981", "authors": ["Zhimin Wang", "Shaokang He", "Duo Wu", "Jinghe Wang", "Linjia Kang", "Jing Yu", "Zhi Wang"], "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Effective real-world multi-agent collaboration requires not only accurate\nplanning but also the ability to reason about collaborators' intents -- a\ncrucial capability for avoiding miscoordination and redundant communication\nunder partial observable environments. Due to their strong planning and\nreasoning capabilities, large language models (LLMs) have emerged as promising\nautonomous agents for collaborative task solving. However, existing\ncollaboration frameworks for LLMs overlook their reasoning potential for\ndynamic intent inference, and thus produce inconsistent plans and redundant\ncommunication, reducing collaboration efficiency. To bridge this gap, we\npropose CoBel-World, a novel framework that equips LLM agents with a\ncollaborative belief world -- an internal representation jointly modeling the\nphysical environment and collaborators' mental states. CoBel-World enables\nagents to parse open-world task knowledge into structured beliefs via a\nsymbolic belief language, and perform zero-shot Bayesian-style belief updates\nthrough LLM reasoning. This allows agents to proactively detect potential\nmiscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated\non challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World\nsignificantly reduces communication costs by 22-60% and improves task\ncompletion efficiency by 4-28% compared to the strongest baseline. Our results\nshow that explicit, intent-aware belief modeling is essential for efficient and\nhuman-like collaboration in LLM-based multi-agent systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-agent collaboration using Large Language Models (LLMs) to reason about collaborators' intents and build a collaborative belief world. While it doesn't directly address trajectory prediction, the embodied multi-agent setting and the need for planning implicitly relate to predicting future states and actions. The use of LLMs is a major component.", "keywords": ["Large Language Models", "LLMs", "multi-agent collaboration", "reasoning", "planning", "embodied agents"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22195", "pdf": "https://arxiv.org/pdf/2509.22195", "abs": "https://arxiv.org/abs/2509.22195", "authors": ["Asher J. Hancock", "Xindi Wu", "Lihan Zha", "Olga Russakovsky", "Anirudha Majumdar"], "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting", "categories": ["cs.RO"], "comment": null, "summary": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to\ncreate vision-language-action (VLA) models is a promising paradigm for training\ngeneralist policies, but it suffers from a fundamental tradeoff: learning to\nproduce actions often diminishes the VLM's foundational reasoning and\nmultimodal understanding, hindering generalization to novel scenarios,\ninstruction following, and semantic understanding. We argue that this\ncatastrophic forgetting is due to a distribution mismatch between the VLM's\ninternet-scale pretraining corpus and the robotics fine-tuning data. Inspired\nby this observation, we introduce VLM2VLA: a VLA training paradigm that first\nresolves this mismatch at the data level by representing low-level actions with\nnatural language. This alignment makes it possible to train VLAs solely with\nLow-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and\naverting catastrophic forgetting. As a result, the VLM can be fine-tuned on\nrobot teleoperation data without fundamentally altering the underlying\narchitecture and without expensive co-training on internet-scale VLM datasets.\nThrough extensive Visual Question Answering (VQA) studies and over 800\nreal-world robotics experiments, we demonstrate that VLM2VLA preserves the\nVLM's core capabilities, enabling zero-shot generalization to novel tasks that\nrequire open-world semantic reasoning and multilingual instruction following.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Vision-Language Models (VLMs) for robot teleoperation, specifically training Vision-Language-Action (VLA) models. While it doesn't directly address trajectory prediction, the actions generated by the VLA model could be interpreted as a form of action prediction, which is related to trajectory prediction. The paper heavily involves Large Language Models (VLMs) and the problem of catastrophic forgetting during fine-tuning, making it relevant to the 'Large Models' aspect. The connection to trajectory prediction is indirect, hence the score.", "keywords": ["Vision-Language Models", "VLMs", "Vision-Language-Action", "VLA", "Large Language Models", "robot teleoperation", "actions"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22199", "pdf": "https://arxiv.org/pdf/2509.22199", "abs": "https://arxiv.org/abs/2509.22199", "authors": ["Haoyun Li", "Ivan Zhang", "Runqi Ouyang", "Xiaofeng Wang", "Zheng Zhu", "Zhiqin Yang", "Zhentao Zhang", "Boyuan Wang", "Chaojun Ni", "Wenkang Qin", "Xinze Chen", "Yun Ye", "Guan Huang", "Zhenbo Song", "Xingang Wang"], "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision Language Action (VLA) models derive their generalization capability\nfrom diverse training data, yet collecting embodied robot interaction data\nremains prohibitively expensive. In contrast, human demonstration videos are\nfar more scalable and cost-efficient to collect, and recent studies confirm\ntheir effectiveness in training VLA models. However, a significant domain gap\npersists between human videos and robot-executed videos, including unstable\ncamera viewpoints, visual discrepancies between human hands and robotic arms,\nand differences in motion dynamics. To bridge this gap, we propose\nMimicDreamer, a framework that turns fast, low-cost human demonstrations into\nrobot-usable supervision by jointly aligning vision, viewpoint, and actions to\ndirectly support policy training. For visual alignment, we propose H2R Aligner,\na video diffusion model that generates high-fidelity robot demonstration videos\nby transferring motion from human manipulation footage. For viewpoint\nstabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos\nvia homography and inpaints occlusions and distortions caused by warping. For\naction alignment, we map human hand trajectories to the robot frame and apply a\nconstrained inverse kinematics solver to produce feasible, low-jitter joint\ncommands with accurate pose tracking. Empirically, VLA models trained purely on\nour synthesized human-to-robot videos achieve few-shot execution on real\nrobots. Moreover, scaling training with human data significantly boosts\nperformance compared to models trained solely on real robot data; our approach\nimproves the average success rate by 14.7\\% across six representative\nmanipulation tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on training Vision Language Action (VLA) models for robot control using human demonstrations. While it doesn't explicitly mention trajectory prediction or use a Large Language Model directly, it involves generating robot-usable supervision by aligning vision, viewpoint, and actions, which can be seen as related to action prediction and planning. The use of a video diffusion model aligns with the 'large model' aspect, although not specifically an LLM.", "keywords": ["Vision Language Action models", "robot interaction", "video diffusion model", "action alignment", "trajectory"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21552", "pdf": "https://arxiv.org/pdf/2509.21552", "abs": "https://arxiv.org/abs/2509.21552", "authors": ["Yu Zhao", "Wei-Ning Chen", "Huseyin Atahan Inan", "Samuel Kessler", "Lu Wang", "Lukas Wutschitz", "Fangkai Yang", "Chaoyun Zhang", "Pasquale Minervini", "Saravan Rajmohan", "Robert Sim"], "title": "Learning GUI Grounding with Spatial Reasoning from Visual Feedback", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Graphical User Interface (GUI) grounding is commonly framed as a coordinate\nprediction task -- given a natural language instruction, generate on-screen\ncoordinates for actions such as clicks and keystrokes. However, recent Vision\nLanguage Models (VLMs) often fail to predict accurate numeric coordinates when\nprocessing high-resolution GUI images with complex layouts. To address this\nissue, we reframe GUI grounding as an \\emph{interactive search task}, where the\nVLM generates actions to move a cursor in the GUI to locate UI elements. At\neach step, the model determines the target object, evaluates the spatial\nrelations between the cursor and the target, and moves the cursor closer to the\ntarget conditioned on the movement history. In this interactive process, the\nrendered cursor provides visual feedback to help the model align its\npredictions with the corresponding on-screen locations. We train our GUI\ngrounding model, GUI-Cursor, using multi-step online reinforcement learning\nwith a dense trajectory-based reward function. Our experimental results show\nthat GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy\nand achieves state-of-the-art results on ScreenSpot-v2 ($88.8\\% \\rightarrow\n93.9\\%$) and ScreenSpot-Pro ($26.8\\% \\rightarrow 56.5\\%$). Moreover, we observe\nthat GUI-Cursor learns to solve the problem within two steps for 95\\% of\ninstances and can adaptively conduct more steps on more difficult examples.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper is relevant because it uses a large vision-language model (Qwen2.5-VL-7B) to perform an interactive search task that involves generating a trajectory (cursor movement) to locate UI elements. While the trajectory is in the GUI space rather than physical space, the concept of trajectory-based reward function and multi-step online reinforcement learning relate to trajectory prediction.", "keywords": ["Large Language Models", "VLMs", "trajectory-based reward function", "interactive search task", "Qwen2.5-VL-7B"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22205", "pdf": "https://arxiv.org/pdf/2509.22205", "abs": "https://arxiv.org/abs/2509.22205", "authors": ["Ke Ye", "Jiaming Zhou", "Yuanfeng Qiu", "Jiayi Liu", "Shihui Zhou", "Kun-Yu Lin", "Junwei Liang"], "title": "From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment", "categories": ["cs.RO"], "comment": null, "summary": "Generalizing to long-horizon manipulation tasks in a zero-shot setting\nremains a central challenge in robotics. Current multimodal foundation based\napproaches, despite their capabilities, typically fail to decompose high-level\ncommands into executable action sequences from static visual input alone. To\naddress this challenge, we introduce Super-Mimic, a hierarchical framework that\nenables zero-shot robotic imitation by directly inferring procedural intent\nfrom unscripted human demonstration videos. Our framework is composed of two\nsequential modules. First, a Human Intent Translator (HIT) parses the input\nvideo using multimodal reasoning to produce a sequence of language-grounded\nsubtasks. These subtasks then condition a Future Dynamics Predictor (FDP),\nwhich employs a generative model that synthesizes a physically plausible video\nrollout for each step. The resulting visual trajectories are dynamics-aware,\nexplicitly modeling crucial object interactions and contact points to guide the\nlow-level controller. We validate this approach through extensive experiments\non a suite of long-horizon manipulation tasks, where Super-Mimic significantly\noutperforms state-of-the-art zero-shot methods by over 20\\%. These results\nestablish that coupling video-driven intent parsing with prospective dynamics\nmodeling is a highly effective strategy for developing general-purpose robotic\nsystems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses long-horizon manipulation tasks in robotics, which can be related to trajectory prediction. It also mentions multimodal foundation models (large models) and uses a generative model to predict future dynamics, indicating some relevance to both trajectory prediction and large language models, although the connection is not explicitly stated as using LLMs for trajectory prediction.", "keywords": ["long-horizon manipulation", "future dynamics prediction", "generative model", "multimodal foundation models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22296", "pdf": "https://arxiv.org/pdf/2509.22296", "abs": "https://arxiv.org/abs/2509.22296", "authors": ["Joseph Hunt", "Koyo Fujii", "Aly Magassouba", "Praminda Caleb-Solly"], "title": "Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm", "categories": ["cs.RO"], "comment": "ICSR 2025, 8 pages, 3 figures", "summary": "Hospital patient falls remain a critical and costly challenge worldwide.\nWhile conventional fall prevention systems typically rely on post-fall\ndetection or reactive alerts, they also often suffer from high false positive\nrates and fail to address the underlying patient needs that lead to bed-exit\nattempts. This paper presents a novel system architecture that leverages the\nInternet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction\nfor proactive and personalized patient assistance. The system integrates a\nprivacy-preserving thermal sensing model capable of real-time bed-exit\nprediction, with two coordinated robotic agents that respond dynamically based\non predicted intent and patient input. This orchestrated response could not\nonly reduce fall risk but also attend to the patient's underlying motivations\nfor movement, such as thirst, discomfort, or the need for assistance, before a\nhazardous situation arises. Our contributions with this pilot study are\nthree-fold: (1) a modular IoRT-based framework enabling distributed sensing,\nprediction, and multi-robot coordination; (2) a demonstration of low-resolution\nthermal sensing for accurate, privacy-preserving preemptive bed-exit detection;\nand (3) results from a user study and systematic error analysis that inform the\ndesign of situationally aware, multi-agent interactions in hospital settings.\nThe findings highlight how interactive and connected robotic systems can move\nbeyond passive monitoring to deliver timely, meaningful assistance, empowering\nsafer, more responsive care environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u7269\u8054\u7f51\u673a\u5668\u4eba\u8fdb\u884c\u60a3\u8005\u8dcc\u5012\u9884\u6d4b\u548c\u9884\u9632\u3002\u867d\u7136\u6d89\u53ca\u201cbed-exit prediction\u201d\uff0c\u5c5e\u4e8e\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u66f4\u4fa7\u91cd\u4e8e\u673a\u5668\u4eba\u7cfb\u7edf\u548c\u5e94\u7528\u573a\u666f\uff0c\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u6a21\u578b\u3002\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["prediction", "robot", "human-robot interaction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22441", "pdf": "https://arxiv.org/pdf/2509.22441", "abs": "https://arxiv.org/abs/2509.22441", "authors": ["Zhangyuan Wang", "Yunpeng Zhu", "Yuqi Yan", "Xiaoyuan Tian", "Xinhao Shao", "Meixuan Li", "Weikun Li", "Guangsheng Su", "Weicheng Cui", "Dixia Fan"], "title": "UnderwaterVLA: Dual-brain Vision-Language-Action architecture for Autonomous Underwater Navigation", "categories": ["cs.RO"], "comment": "This paper introduces the first VLA framework for AUVs, featuring a\n  dual-brain architecture and zero-data MPC for real-world underwater\n  navigation", "summary": "This paper presents UnderwaterVLA, a novel framework for autonomous\nunderwater navigation that integrates multimodal foundation models with\nembodied intelligence systems. Underwater operations remain difficult due to\nhydrodynamic disturbances, limited communication bandwidth, and degraded\nsensing in turbid waters. To address these challenges, we introduce three\ninnovations. First, a dual-brain architecture decouples high-level mission\nreasoning from low-level reactive control, enabling robust operation under\ncommunication and computational constraints. Second, we apply\nVision-Language-Action(VLA) models to underwater robotics for the first time,\nincorporating structured chain-of-thought reasoning for interpretable\ndecision-making. Third, a hydrodynamics-informed Model Predictive Control(MPC)\nscheme compensates for fluid effects in real time without costly task-specific\ntraining. Experimental results in field tests show that UnderwaterVLA reduces\nnavigation errors in degraded visual conditions while maintaining higher task\ncompletion by 19% to 27% over baseline. By minimizing reliance on\nunderwater-specific training data and improving adaptability across\nenvironments, UnderwaterVLA provides a scalable and cost-effective path toward\nthe next generation of intelligent AUVs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses autonomous underwater navigation, which can be considered a form of trajectory prediction/planning for underwater vehicles. It also uses Vision-Language-Action (VLA) models, which are related to large language models, and mentions chain-of-thought reasoning. While not directly focusing on trajectory prediction algorithms or LLMs in the traditional sense, it bridges the gap by incorporating VLA models into autonomous navigation.", "keywords": ["autonomous underwater navigation", "Vision-Language-Action models", "VLA", "Model Predictive Control", "MPC", "chain-of-thought reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22550", "pdf": "https://arxiv.org/pdf/2509.22550", "abs": "https://arxiv.org/abs/2509.22550", "authors": ["Xiaoyun Qiu", "Haichao Liu", "Yue Pan", "Jun Ma", "Xinhu Zheng"], "title": "An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment", "categories": ["cs.RO"], "comment": null, "summary": "In mixed-traffic environments, where autonomous vehicles (AVs) interact with\ndiverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous\nbehaviors make safe and efficient lane change maneuvers highly challenging.\nExisting methods often oversimplify these interactions by assuming uniform\npatterns. We propose an intention-driven lane change framework that integrates\ndriving-style recognition, cooperation-aware decision-making, and coordinated\nmotion planning. A deep learning classifier trained on the NGSIM dataset\nidentifies human driving styles in real time. A cooperation score with\nintrinsic and interactive components estimates surrounding drivers' intentions\nand quantifies their willingness to cooperate with the ego vehicle.\nDecision-making combines behavior cloning with inverse reinforcement learning\nto determine whether a lane change should be initiated. For trajectory\ngeneration, model predictive control is integrated with IRL-based intention\ninference to produce collision-free and socially compliant maneuvers.\nExperiments show that the proposed model achieves 94.2\\% accuracy and 94.3\\%\nF1-score, outperforming rule-based and learning-based baselines by 4-15\\% in\nlane change recognition. These results highlight the benefit of modeling\ninter-driver heterogeneity and demonstrate the potential of the framework to\nadvance context-aware and human-like autonomous driving in complex traffic\nenvironments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on lane change maneuvers in mixed-traffic environments, integrating intention recognition, decision-making, and trajectory planning. It uses deep learning for driving style recognition and model predictive control for trajectory generation, which are related to trajectory prediction. However, it doesn't directly involve large language models. The connection to trajectory prediction is moderate because it focuses on a specific application (lane change) and uses deep learning rather than explicitly focusing on general trajectory prediction methods.", "keywords": ["trajectory prediction", "lane change", "deep learning", "motion planning", "intention recognition"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22573", "pdf": "https://arxiv.org/pdf/2509.22573", "abs": "https://arxiv.org/abs/2509.22573", "authors": ["Farida Mohsen", "Ali Safa"], "title": "MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Efficiently detecting human intent to interact with ubiquitous robots is\ncrucial for effective human-robot interaction (HRI) and collaboration. Over the\npast decade, deep learning has gained traction in this field, with most\nexisting approaches relying on multimodal inputs, such as RGB combined with\ndepth (RGB-D), to classify time-sequence windows of sensory data as interactive\nor non-interactive. In contrast, we propose a novel RGB-only pipeline for\npredicting human interaction intent with frame-level precision, enabling faster\nrobot responses and improved service quality. A key challenge in intent\nprediction is the class imbalance inherent in real-world HRI datasets, which\ncan hinder the model's training and generalization. To address this, we\nintroduce MINT-RVAE, a synthetic sequence generation method, along with new\nloss functions and training strategies that enhance generalization on\nout-of-sample data. Our approach achieves state-of-the-art performance (AUROC:\n0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB\ninput and supporting precise frame onset prediction. Finally, to support future\nresearch, we openly release our new dataset with frame-level labeling of human\ninteraction intent.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4eba\u673a\u4ea4\u4e92\u4e2d\u4eba\u7c7b\u610f\u56fe\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86RGB\u56fe\u50cf\u6570\u636e\u3002\u867d\u7136\u63d0\u5230\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5904\u7406\u548c\u9884\u6d4b\uff0c\u4f46\u66f4\u4fa7\u91cd\u4e8e\u610f\u56fe\u5206\u7c7b\u800c\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\u3002\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u4e86\u52a8\u4f5c\u9884\u6d4b\u548c\u5e8f\u5217\u751f\u6210\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\u3002", "keywords": ["intention prediction", "human-robot interaction", "sequence generation", "action prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22460", "pdf": "https://arxiv.org/pdf/2509.22460", "abs": "https://arxiv.org/abs/2509.22460", "authors": ["Shichao Weng", "Zhiqiang Wang", "Yuhua Zhou", "Rui Lu", "Ting Liu", "Zhiyang Teng", "Xiaozhang Liu", "Hanmeng Liu"], "title": "GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation", "categories": ["cs.AI"], "comment": null, "summary": "Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large\nLanguage Models (MLLMs), requiring not only the joint interpretation of text\nand diagrams but also iterative visuospatial reasoning. While existing\napproaches process diagrams as static images, they lack the capacity for\ndynamic manipulation - a core aspect of human geometric reasoning involving\nauxiliary line construction and affine transformations. We present GeoSketch, a\nneural-symbolic framework that recasts geometric reasoning as an interactive\nperception-reasoning-action loop. GeoSketch integrates: (1) a Perception module\nthat abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning\nmodule that applies geometric theorems to decide the next deductive step, and\n(3) a Sketch Action module that executes operations such as drawing auxiliary\nlines or applying transformations, thereby updating the diagram in a closed\nloop. To train this agent, we develop a two-stage pipeline: supervised\nfine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement\nlearning with dense, symbolic rewards to enhance robustness and strategic\nexploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a\nhigh-quality set of 390 geometry problems requiring auxiliary construction or\naffine transformations. Experiments on strong MLLM baselines demonstrate that\nGeoSketch significantly improves stepwise reasoning accuracy and\nproblem-solving success over static perception methods. By unifying\nhierarchical decision-making, executable visual actions, and symbolic\nverification, GeoSketch advances multimodal reasoning from static\ninterpretation to dynamic, verifiable interaction, establishing a new\nfoundation for solving complex visuospatial problems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on geometric problem solving using a neural-symbolic approach with multimodal large language models (MLLMs). While it doesn't directly address trajectory prediction, it utilizes large language models for reasoning and involves manipulating geometric diagrams through actions like drawing lines and transformations, which shares some conceptual similarities with path planning and prediction in dynamic environments. The connection to trajectory prediction is not explicit, but the use of MLLMs and the manipulation of visual elements for reasoning contribute to the relevance.", "keywords": ["Large Language Models", "MLLMs", "geometric reasoning", "affine transformation", "perception-reasoning-action loop"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21715", "pdf": "https://arxiv.org/pdf/2509.21715", "abs": "https://arxiv.org/abs/2509.21715", "authors": ["Xu Yang", "Gady Agam"], "title": "Motion-Aware Transformer for Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multi-object tracking (MOT) in videos remains challenging due to complex\nobject motions and crowded scenes. Recent DETR-based frameworks offer\nend-to-end solutions but typically process detection and tracking queries\njointly within a single Transformer Decoder layer, leading to conflicts and\ndegraded association accuracy. We introduce the Motion-Aware Transformer\n(MATR), which explicitly predicts object movements across frames to update\ntrack queries in advance. By reducing query collisions, MATR enables more\nconsistent training and improves both detection and association. Extensive\nexperiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers\nsignificant gains across standard metrics. On DanceTrack, MATR improves HOTA by\nmore than 9 points over MOTR without additional data and reaches a new\nstate-of-the-art score of 71.3 with supplementary data. MATR also achieves\nstate-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6\nmHOTA) without relying on external datasets. These results demonstrate that\nexplicitly modeling motion within end-to-end Transformers offers a simple yet\nhighly effective approach to advancing multi-object tracking.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-object tracking (MOT) and explicitly models object movements across frames using a Transformer-based architecture. While it doesn't directly involve Large Language Models, the core idea of predicting motion and using Transformers connects it to the broader theme of trajectory prediction. Therefore, the relevance is moderate.", "keywords": ["multi-object tracking", "motion prediction", "Transformer", "trajectory prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.22652", "pdf": "https://arxiv.org/pdf/2509.22652", "abs": "https://arxiv.org/abs/2509.22652", "authors": ["E-Ro Nguyen", "Yichi Zhang", "Kanchana Ranasinghe", "Xiang Li", "Michael S. Ryoo"], "title": "Pixel Motion Diffusion is What We Need for Robot Control", "categories": ["cs.RO", "cs.CV"], "comment": "16 pages, 7 figures", "summary": "We present DAWN (Diffusion is All We Need for robot control), a unified\ndiffusion-based framework for language-conditioned robotic manipulation that\nbridges high-level motion intent and low-level robot action via structured\npixel motion representation. In DAWN, both the high-level and low-level\ncontrollers are modeled as diffusion processes, yielding a fully trainable,\nend-to-end system with interpretable intermediate motion abstractions. DAWN\nachieves state-of-the-art results on the challenging CALVIN benchmark,\ndemonstrating strong multi-task performance, and further validates its\neffectiveness on MetaWorld. Despite the substantial domain gap between\nsimulation and reality and limited real-world data, we demonstrate reliable\nreal-world transfer with only minimal finetuning, illustrating the practical\nviability of diffusion-based motion abstractions for robotic control. Our\nresults show the effectiveness of combining diffusion modeling with\nmotion-centric representations as a strong baseline for scalable and robust\nrobot learning. Project page: https://nero1342.github.io/DAWN/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robot control using diffusion models conditioned on language. While it doesn't explicitly deal with trajectory prediction in the traditional sense (e.g., predicting human trajectories), it does involve predicting pixel motion for robot manipulation, which can be considered a form of trajectory prediction in the pixel space. The paper also uses language conditioning, hinting at the potential use of large language models. Thus, the paper exhibits a moderate relevance to both trajectory prediction and large models.", "keywords": ["robot control", "diffusion models", "language-conditioned", "motion prediction", "pixel motion"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
