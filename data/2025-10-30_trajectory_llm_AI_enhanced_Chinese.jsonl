{"id": "2510.23636", "pdf": "https://arxiv.org/pdf/2510.23636", "abs": "https://arxiv.org/abs/2510.23636", "authors": ["Thaweerath Phisannupawong", "Joshua Julian Damanik", "Han-Lim Choi"], "title": "Flight Delay Prediction via Cross-Modality Adaptation of Large Language Models and Aircraft Trajectory Representation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint submitted to Aerospace Science and Technology (Elsevier) for\n  possible publication", "summary": "Flight delay prediction has become a key focus in air traffic management, as\ndelays highlight inefficiencies that impact overall network performance. This\npaper presents a lightweight large language model-based multimodal flight delay\nprediction, formulated from the perspective of air traffic controllers\nmonitoring aircraft delay after entering the terminal area. The approach\nintegrates trajectory representations with textual aeronautical information,\nincluding flight information, weather reports, and aerodrome notices, by\nadapting trajectory data into the language modality to capture airspace\nconditions. Experimental results show that the model consistently achieves\nsub-minute prediction error by effectively leveraging contextual information\nrelated to the sources of delay. The framework demonstrates that linguistic\nunderstanding, when combined with cross-modality adaptation of trajectory\ninformation, enhances delay prediction. Moreover, the approach shows\npracticality and scalability for real-world operations, supporting real-time\nupdates that refine predictions upon receiving new operational information.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "The paper explicitly combines flight trajectory prediction with large language models for flight delay prediction. It adapts trajectory data into the language modality and leverages contextual information related to delay sources. This strongly indicates a high relevance to both trajectory prediction and large language models.", "keywords": ["trajectory prediction", "large language models", "flight delay prediction", "aircraft trajectory", "cross-modality adaptation"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8f68\u8ff9\u8868\u793a\u548c\u6587\u672c\u822a\u7a7a\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u4e9a\u5206\u949f\u7ea7\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u5df2\u6210\u4e3a\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u7684\u5173\u952e\uff0c\u56e0\u4e3a\u5ef6\u8bef\u7a81\u51fa\u4e86\u5f71\u54cd\u6574\u4f53\u7f51\u7edc\u6027\u80fd\u7684\u4f4e\u6548\u7387\u3002", "method": "\u8be5\u65b9\u6cd5\u4ece\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u5458\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u76d1\u6d4b\u98de\u673a\u8fdb\u5165\u7ec8\u7aef\u533a\u57df\u540e\u7684\u5ef6\u8bef\u60c5\u51b5\uff0c\u901a\u8fc7\u5c06\u8f68\u8ff9\u6570\u636e\u8f6c\u6362\u4e3a\u8bed\u8a00\u6a21\u6001\u6765\u6355\u83b7\u7a7a\u57df\u6761\u4ef6\uff0c\u6574\u5408\u4e86\u8f68\u8ff9\u8868\u793a\u548c\u6587\u672c\u822a\u7a7a\u4fe1\u606f\uff0c\u5305\u62ec\u822a\u73ed\u4fe1\u606f\u3001\u5929\u6c14\u62a5\u544a\u548c\u673a\u573a\u516c\u544a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e0e\u5ef6\u8bef\u6765\u6e90\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u59cb\u7ec8\u5b9e\u73b0\u4e9a\u5206\u949f\u7ea7\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "\u8be5\u6846\u67b6\u8868\u660e\uff0c\u8bed\u8a00\u7406\u89e3\u4e0e\u8f68\u8ff9\u4fe1\u606f\u7684\u8de8\u6a21\u6001\u9002\u5e94\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5ef6\u8bef\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u663e\u793a\u51fa\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u652f\u6301\u63a5\u6536\u5230\u65b0\u7684\u64cd\u4f5c\u4fe1\u606f\u65f6\u8fdb\u884c\u5b9e\u65f6\u66f4\u65b0\uff0c\u4ece\u800c\u6539\u8fdb\u9884\u6d4b\u3002", "summary_zh": "\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u5df2\u6210\u4e3a\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u4e2d\u7684\u4e00\u4e2a\u91cd\u70b9\uff0c\u56e0\u4e3a\u5ef6\u8bef\u7a81\u663e\u4e86\u5f71\u54cd\u6574\u4f53\u7f51\u7edc\u6027\u80fd\u7684\u4f4e\u6548\u7387\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u5236\u5458\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u76d1\u6d4b\u98de\u673a\u8fdb\u5165\u7ec8\u7aef\u533a\u57df\u540e\u7684\u5ef6\u8bef\u60c5\u51b5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u8f68\u8ff9\u6570\u636e\u8f6c\u6362\u4e3a\u8bed\u8a00\u6a21\u6001\u6765\u6355\u83b7\u7a7a\u57df\u6761\u4ef6\uff0c\u6574\u5408\u4e86\u8f68\u8ff9\u8868\u793a\u548c\u6587\u672c\u822a\u7a7a\u4fe1\u606f\uff0c\u5305\u62ec\u822a\u73ed\u4fe1\u606f\u3001\u5929\u6c14\u62a5\u544a\u548c\u673a\u573a\u516c\u544a\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e0e\u5ef6\u8bef\u6765\u6e90\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u59cb\u7ec8\u5b9e\u73b0\u4e9a\u5206\u949f\u7ea7\u7684\u9884\u6d4b\u8bef\u5dee\u3002\u8be5\u6846\u67b6\u8868\u660e\uff0c\u8bed\u8a00\u7406\u89e3\u4e0e\u8f68\u8ff9\u4fe1\u606f\u7684\u8de8\u6a21\u6001\u9002\u5e94\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5ef6\u8bef\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u663e\u793a\u51fa\u5728\u5b9e\u9645\u64cd\u4f5c\u4e2d\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u652f\u6301\u63a5\u6536\u5230\u65b0\u7684\u64cd\u4f5c\u4fe1\u606f\u65f6\u8fdb\u884c\u5b9e\u65f6\u66f4\u65b0\uff0c\u4ece\u800c\u6539\u8fdb\u9884\u6d4b\u3002"}}
{"id": "2510.23824", "pdf": "https://arxiv.org/pdf/2510.23824", "abs": "https://arxiv.org/abs/2510.23824", "authors": ["Murad Ismayilov", "Edwin Meriaux", "Shuo Wen", "Gregory Dudek"], "title": "Decentralized Multi-Agent Goal Assignment for Path Planning using Large Language Models", "categories": ["cs.AI"], "comment": "Accepted at MIT URTC 2025", "summary": "Coordinating multiple autonomous agents in shared environments under\ndecentralized conditions is a long-standing challenge in robotics and\nartificial intelligence. This work addresses the problem of decentralized goal\nassignment for multi-agent path planning, where agents independently generate\nranked preferences over goals based on structured representations of the\nenvironment, including grid visualizations and scenario data. After this\nreasoning phase, agents exchange their goal rankings, and assignments are\ndetermined by a fixed, deterministic conflict-resolution rule (e.g., agent\nindex ordering), without negotiation or iterative coordination. We\nsystematically compare greedy heuristics, optimal assignment, and large\nlanguage model (LLM)-based agents in fully observable grid-world settings. Our\nresults show that LLM-based agents, when provided with well-designed prompts\nand relevant quantitative information, can achieve near-optimal makespans and\nconsistently outperform traditional heuristics. These findings underscore the\npotential of language models for decentralized goal assignment in multi-agent\npath planning and highlight the importance of information structure in such\nsystems.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper addresses multi-agent path planning using Large Language Models (LLMs) for decentralized goal assignment. While not directly focusing on trajectory prediction in the sense of predicting future trajectories, it uses LLMs to improve path planning which is closely related to trajectory generation. The paper explicitly mentions using LLMs and path planning, indicating a significant connection.", "keywords": ["path planning", "large language models", "LLMs", "multi-agent", "goal assignment"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24108", "pdf": "https://arxiv.org/pdf/2510.24108", "abs": "https://arxiv.org/abs/2510.24108", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Jingde Chen", "Nadine Chang", "Maying Shen", "Jingyu Song", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "ZTRS: Zero-Imitation End-to-end Autonomous Driving with Trajectory Scoring", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "End-to-end autonomous driving maps raw sensor inputs directly into\nego-vehicle trajectories to avoid cascading errors from perception modules and\nto leverage rich semantic cues. Existing frameworks largely rely on Imitation\nLearning (IL), which can be limited by sub-optimal expert demonstrations and\ncovariate shift during deployment. On the other hand, Reinforcement Learning\n(RL) has recently shown potential in scaling up with simulations, but is\ntypically confined to low-dimensional symbolic inputs (e.g. 3D objects and\nmaps), falling short of full end-to-end learning from raw sensor data. We\nintroduce ZTRS (Zero-Imitation End-to-End Autonomous Driving with Trajectory\nScoring), a framework that combines the strengths of both worlds: sensor inputs\nwithout losing information and RL training for robust planning. To the best of\nour knowledge, ZTRS is the first framework that eliminates IL entirely by only\nlearning from rewards while operating directly on high-dimensional sensor data.\nZTRS utilizes offline reinforcement learning with our proposed Exhaustive\nPolicy Optimization (EPO), a variant of policy gradient tailored for enumerable\nactions and rewards. ZTRS demonstrates strong performance across three\nbenchmarks: Navtest (generic real-world open-loop planning), Navhard (open-loop\nplanning in challenging real-world and synthetic scenarios), and HUGSIM\n(simulated closed-loop driving). Specifically, ZTRS achieves the\nstate-of-the-art result on Navhard and outperforms IL-based baselines on\nHUGSIM. Code will be available at https://github.com/woxihuanjiangguo/ZTRS.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on end-to-end autonomous driving with trajectory scoring, which is directly related to trajectory prediction. While it doesn't explicitly use large language models, the techniques employed (reinforcement learning for trajectory planning) are often investigated in conjunction with LLMs for more complex reasoning and decision-making in autonomous systems. The paper's focus on learning directly from sensor data and avoiding imitation learning is a relevant trend, potentially paving the way for future integration with LLMs for richer scene understanding.", "keywords": ["trajectory prediction", "autonomous driving", "reinforcement learning", "trajectory scoring", "end-to-end learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24166", "pdf": "https://arxiv.org/pdf/2510.24166", "abs": "https://arxiv.org/abs/2510.24166", "authors": ["Xin Yang", "Yuhang Zhang", "Wei Li", "Xin Lin", "Wenbin Zou", "Chen Xu"], "title": "UniPlanner: A Unified Motion Planning Framework for Autonomous Vehicle Decision-Making Systems via Multi-Dataset Integration", "categories": ["cs.AI"], "comment": null, "summary": "Motion planning is a critical component of autonomous vehicle decision-making\nsystems, directly determining trajectory safety and driving efficiency. While\ndeep learning approaches have advanced planning capabilities, existing methods\nremain confined to single-dataset training, limiting their robustness in\nplanning.\n  Through systematic analysis, we discover that vehicular trajectory\ndistributions and history-future correlations demonstrate remarkable\nconsistency across different datasets. Based on these findings, we propose\nUniPlanner, the first planning framework designed for multi-dataset integration\nin autonomous vehicle decision-making. UniPlanner achieves unified\ncross-dataset learning through three synergistic innovations.\n  First, the History-Future Trajectory Dictionary Network (HFTDN) aggregates\nhistory-future trajectory pairs from multiple datasets, using historical\ntrajectory similarity to retrieve relevant futures and generate cross-dataset\nplanning guidance.\n  Second, the Gradient-Free Trajectory Mapper (GFTM) learns robust\nhistory-future correlations from multiple datasets, transforming historical\ntrajectories into universal planning priors. Its gradient-free design ensures\nthe introduction of valuable priors while preventing shortcut learning, making\nthe planning knowledge safely transferable. Third, the Sparse-to-Dense (S2D)\nparadigm implements adaptive dropout to selectively suppress planning priors\nduring training for robust learning, while enabling full prior utilization\nduring inference to maximize planning performance.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u63a2\u7d22\u4e86\u591a\u6570\u636e\u96c6\u96c6\u6210\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u89c4\u5212\u80fd\u529b\uff0c\u8fd9\u4e0e\u5229\u7528\u5927\u6570\u636e\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u601d\u60f3\u6709\u4e00\u5b9a\u5173\u8054\u3002\u5173\u952e\u8bcd\u8868\u660e\u4e86\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u5f3a\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "motion planning", "autonomous vehicle", "trajectory"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24152", "pdf": "https://arxiv.org/pdf/2510.24152", "abs": "https://arxiv.org/abs/2510.24152", "authors": ["Aodi Wu", "Xubo Luo"], "title": "Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "RoboSense Challenge with IROS 2025", "summary": "This technical report presents our solution for the RoboSense Challenge at\nIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous driving\nscene understanding across perception, prediction, planning, and corruption\ndetection tasks. We propose a systematic framework built on four core\ncomponents. First, a Mixture-of-Prompts router classifies questions and\ndispatches them to task-specific expert prompts, eliminating interference\nacross diverse question types. Second, task-specific prompts embed explicit\ncoordinate systems, spatial reasoning rules, role-playing,\nChain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored to\neach task. Third, a visual assembly module composes multi-view images with\nobject crops, magenta markers, and adaptive historical frames based on question\nrequirements. Fourth, we configure model inference parameters (temperature,\ntop-p, message roles) per task to optimize output quality. Implemented on\nQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (clean\ndata) and 72.85% on Phase-2 (corrupted data), demonstrating that structured\nprompting and spatial grounding substantially enhance VLM performance on\nsafety-critical autonomous driving tasks. Code and prompt are available at\nhttps://github.com/wuaodi/UCAS-CSU-phase2.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u5173\u6ce8\u4e8e\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u95ee\u9898\u3002\u867d\u7136\u4e3b\u8981\u4fa7\u91cd\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4e5f\u63d0\u5230\u4e86\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u4f7f\u7528\u4e86Qwen2.5-VL-72B\u8fd9\u6837\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language Models", "autonomous driving", "prediction", "Large Language Models", "Qwen2.5-VL-72B", "spatial reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23691", "pdf": "https://arxiv.org/pdf/2510.23691", "abs": "https://arxiv.org/abs/2510.23691", "authors": ["Zihao Wang", "Xujing Li", "Yining Ye", "Junjie Fang", "Haoming Wang", "Longxiang Liu", "Shihao Liang", "Junting Lu", "Zhiyong Wu", "Jiazhan Feng", "Wanjun Zhong", "Zili Li", "Yu Wang", "Yu Miao", "Bo Zhou", "Yuanfan Li", "Hao Wang", "Zhongkai Zhao", "Faming Wu", "Zhengxuan Jiang", "Weihao Tan", "Heyuan Yao", "Shi Yan", "Xiangyang Li", "Yitao Liang", "Yujia Qin", "Guang Shi"], "title": "Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents", "categories": ["cs.AI"], "comment": null, "summary": "We present Game-TARS, a generalist game agent trained with a unified,\nscalable action space anchored to human-aligned native keyboard-mouse inputs.\nUnlike API- or GUI-based approaches, this paradigm enables large-scale\ncontinual pre-training across heterogeneous domains, including OS, web, and\nsimulation games. Game-TARS is pre-trained on over 500B tokens with diverse\ntrajectories and multimodal data. Key techniques include a decaying continual\nloss to reduce causal confusion and an efficient Sparse-Thinking strategy that\nbalances reasoning depth and inference cost. Experiments show that Game-TARS\nachieves about 2 times the success rate over the previous sota model on\nopen-world Minecraft tasks, is close to the generality of fresh humans in\nunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet\nin FPS benchmarks. Scaling results on training-time and test-time confirm that\nthe unified action space sustains improvements when scaled to cross-game and\nmultimodal data. Our results demonstrate that simple, scalable action\nrepresentations combined with large-scale pre-training provide a promising path\ntoward generalist agents with broad computer-use abilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses a generalist game agent trained with large-scale pre-training, which falls under the scope of large language models and foundation models. The agent learns from diverse trajectories in games, implicitly involving trajectory prediction. While not explicitly focusing on trajectory prediction as a primary task, the agent's ability to navigate and interact within game environments necessitates some level of trajectory understanding and prediction.", "keywords": ["Large Language Models", "Foundation Models", "Pre-training", "Trajectories", "Game Agent"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23763", "pdf": "https://arxiv.org/pdf/2510.23763", "abs": "https://arxiv.org/abs/2510.23763", "authors": ["Siyin Wang", "Jinlan Fu", "Feihong Liu", "Xinzhe He", "Huangxuan Wu", "Junhao Shi", "Kexin Huang", "Zhaoye Fei", "Jingjing Gong", "Zuxuan Wu", "Yugang Jiang", "See-Kiong Ng", "Tat-Seng Chua", "Xipeng Qiu"], "title": "RoboOmni: Proactive Robot Manipulation in Omni-modal Context", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid\nprogress in Vision-Language-Action (VLA) models for robotic manipulation.\nAlthough effective in many scenarios, current approaches largely rely on\nexplicit instructions, whereas in real-world interactions, humans rarely issue\ninstructions directly. Effective collaboration requires robots to infer user\nintentions proactively. In this work, we introduce cross-modal contextual\ninstructions, a new setting where intent is derived from spoken dialogue,\nenvironmental sounds, and visual cues rather than explicit commands. To address\nthis new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor\nframework based on end-to-end omni-modal LLMs that unifies intention\nrecognition, interaction confirmation, and action execution. RoboOmni fuses\nauditory and visual signals spatiotemporally for robust intention recognition,\nwhile supporting direct speech interaction. To address the absence of training\ndata for proactive intention recognition in robotic manipulation, we build\nOmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640\nbackgrounds, and six contextual instruction types. Experiments in simulation\nand real-world settings show that RoboOmni surpasses text- and ASR-based\nbaselines in success rate, inference speed, intention recognition, and\nproactive assistance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7528\u6237\u610f\u56fe\u65b9\u9762\u7684\u5e94\u7528\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u673a\u5668\u4eba\u64cd\u4f5c\u901a\u5e38\u9700\u8981\u5bf9\u7269\u4f53\u8f68\u8ff9\u8fdb\u884c\u9884\u6d4b\u548c\u89c4\u5212\u3002\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "MLLMs", "robotic manipulation", "intention recognition"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23630", "pdf": "https://arxiv.org/pdf/2510.23630", "abs": "https://arxiv.org/abs/2510.23630", "authors": ["Ninghui Feng", "Yiyan Qi"], "title": "NUM2EVENT: Interpretable Event Reasoning from Numerical time-series", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently demonstrated impressive multimodal\nreasoning capabilities, yet their understanding of purely numerical time-series\nsignals remains limited. Existing approaches mainly focus on forecasting or\ntrend description, without uncovering the latent events that drive numerical\nchanges or explaining the reasoning process behind them. In this work, we\nintroduce the task of number-to-event reasoning and decoding, which aims to\ninfer interpretable structured events from numerical inputs, even when current\ntext is unavailable. To address the data scarcity and semantic alignment\nchallenges, we propose a reasoning-aware framework that integrates an\nagent-guided event extractor (AGE), a marked multivariate Hawkes-based\nsynthetic generator (EveDTS), and a two-stage fine-tuning pipeline combining a\ntime-series encoder with a structured decoder. Our model explicitly reasons\nover numerical changes, generates intermediate explanations, and outputs\nstructured event hypotheses. Experiments on multi-domain datasets show that our\nmethod substantially outperforms strong LLM baselines in event-level precision\nand recall. These results suggest a new direction for bridging quantitative\nreasoning and semantic understanding, enabling LLMs to explain and predict\nevents directly from numerical dynamics.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u4e0a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6570\u503c\u6570\u636e\u63a8\u65ad\u4e8b\u4ef6\u7684\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u548c\u4e8b\u4ef6\u9884\u6d4b\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u4e5f\u6709\u5e94\u7528\u3002\u540c\u65f6\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large language models", "LLMs", "time-series", "reasoning", "event prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23632", "pdf": "https://arxiv.org/pdf/2510.23632", "abs": "https://arxiv.org/abs/2510.23632", "authors": ["Guozhong Li", "Muhannad Alhumaidi", "Spiros Skiadopoulos", "Panos Kalnis"], "title": "LLMComp: A Language Modeling Paradigm for Error-Bounded Scientific Data Compression", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The rapid growth of high-resolution scientific simulations and observation\nsystems is generating massive spatiotemporal datasets, making efficient,\nerror-bounded compression increasingly important. Meanwhile, decoder-only large\nlanguage models (LLMs) have demonstrated remarkable capabilities in modeling\ncomplex sequential data. In this paper, we propose LLMCOMP, a novel lossy\ncompression paradigm that leverages decoder-only large LLMs to model scientific\ndata. LLMCOMP first quantizes 3D fields into discrete tokens, arranges them via\nZ-order curves to preserve locality, and applies coverage-guided sampling to\nenhance training efficiency. An autoregressive transformer is then trained with\nspatial-temporal embeddings to model token transitions. During compression, the\nmodel performs top-k prediction, storing only rank indices and fallback\ncorrections to ensure strict error bounds. Experiments on multiple reanalysis\ndatasets show that LLMCOMP consistently outperforms state-of-the-art\ncompressors, achieving up to 30% higher compression ratios under strict error\nbounds. These results highlight the potential of LLMs as general-purpose\ncompressors for high-fidelity scientific data.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Large Language Models (LLMs) for scientific data compression, specifically spatiotemporal data. While it doesn't directly address trajectory prediction, the modeling of spatiotemporal data is relevant. The use of LLMs is a core aspect of the paper, increasing its relevance.", "keywords": ["Large Language Models", "LLMs", "spatiotemporal data"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24052", "pdf": "https://arxiv.org/pdf/2510.24052", "abs": "https://arxiv.org/abs/2510.24052", "authors": ["Jongsuk Kim", "Jaeyoung Lee", "Gyojin Han", "Dongjae Lee", "Minki Jeong", "Junmo Kim"], "title": "SynAD: Enhancing Real-World End-to-End Autonomous Driving Models through Synthetic Data Integration", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent advancements in deep learning and the availability of high-quality\nreal-world driving datasets have propelled end-to-end autonomous driving.\nDespite this progress, relying solely on real-world data limits the variety of\ndriving scenarios for training. Synthetic scenario generation has emerged as a\npromising solution to enrich the diversity of training data; however, its\napplication within E2E AD models remains largely unexplored. This is primarily\ndue to the absence of a designated ego vehicle and the associated sensor\ninputs, such as camera or LiDAR, typically provided in real-world scenarios. To\naddress this gap, we introduce SynAD, the first framework designed to enhance\nreal-world E2E AD models using synthetic data. Our method designates the agent\nwith the most comprehensive driving information as the ego vehicle in a\nmulti-agent synthetic scenario. We further project path-level scenarios onto\nmaps and employ a newly developed Map-to-BEV Network to derive bird's-eye-view\nfeatures without relying on sensor inputs. Finally, we devise a training\nstrategy that effectively integrates these map-based synthetic data with real\ndriving data. Experimental results demonstrate that SynAD effectively\nintegrates all components and notably enhances safety performance. By bridging\nsynthetic scenario generation and E2E AD, SynAD paves the way for more\ncomprehensive and robust autonomous driving models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on end-to-end autonomous driving models and uses synthetic data to improve their performance. While it does not directly mention large language models, the use of synthetic data to train models for autonomous driving and the focus on trajectory prediction (implied by autonomous driving) gives it a moderate relevance. The paper discusses path-level scenarios, which are related to trajectory prediction.", "keywords": ["autonomous driving", "synthetic data", "trajectory prediction", "end-to-end"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23882", "pdf": "https://arxiv.org/pdf/2510.23882", "abs": "https://arxiv.org/abs/2510.23882", "authors": ["Adil Rasheed", "Oscar Ravik", "Omer San"], "title": "Hybrid Modeling, Sim-to-Real Reinforcement Learning, and Large Language Model Driven Control for Digital Twins", "categories": ["cs.AI"], "comment": null, "summary": "This work investigates the use of digital twins for dynamical system modeling\nand control, integrating physics-based, data-driven, and hybrid approaches with\nboth traditional and AI-driven controllers. Using a miniature greenhouse as a\ntest platform, four predictive models Linear, Physics-Based Modeling (PBM),\nLong Short Term Memory (LSTM), and Hybrid Analysis and Modeling (HAM) are\ndeveloped and compared under interpolation and extrapolation scenarios. Three\ncontrol strategies Model Predictive Control (MPC), Reinforcement Learning (RL),\nand Large Language Model (LLM) based control are also implemented to assess\ntrade-offs in precision, adaptability, and implementation effort. Results show\nthat in modeling HAM provides the most balanced performance across accuracy,\ngeneralization, and computational efficiency, while LSTM achieves high\nprecision at greater resource cost. Among controllers, MPC delivers robust and\npredictable performance, RL demonstrates strong adaptability, and LLM-based\ncontrollers offer flexible human-AI interaction when coupled with predictive\ntools.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses the use of Large Language Models (LLMs) for control, which is relevant to the 'large language model' aspect. It also uses predictive models like LSTM, which can be related to trajectory prediction, though the application in this paper (greenhouse control) is not directly trajectory prediction.  The use of Model Predictive Control (MPC) is also a weak link to trajectory prediction. The connection to trajectory prediction is indirect, thus the moderate relevance score.", "keywords": ["Large Language Model", "LLM", "Reinforcement Learning", "LSTM", "Model Predictive Control", "MPC", "predictive models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24069", "pdf": "https://arxiv.org/pdf/2510.24069", "abs": "https://arxiv.org/abs/2510.24069", "authors": ["Sangmin Kim", "Hajun Kim", "Gijeong Kim", "Min-Gyu Kim", "Hae-Won Park"], "title": "Dynamically-Consistent Trajectory Optimization for Legged Robots via Contact Point Decomposition", "categories": ["cs.RO"], "comment": "8 pages, 4 figures, IEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT\n  VERSION. ACCEPTED OCTOBER, 2025", "summary": "To generate reliable motion for legged robots through trajectory\noptimization, it is crucial to simultaneously compute the robot's path and\ncontact sequence, as well as accurately consider the dynamics in the problem\nformulation. In this paper, we present a phase-based trajectory optimization\nthat ensures the feasibility of translational dynamics and friction cone\nconstraints throughout the entire trajectory. Specifically, our approach\nleverages the superposition properties of linear differential equations to\ndecouple the translational dynamics for each contact point, which operates\nunder different phase sequences. Furthermore, we utilize the differentiation\nmatrix of B{\\'e}zier polynomials to derive an analytical relationship between\nthe robot's position and force, thereby ensuring the consistent satisfaction of\ntranslational dynamics. Additionally, by exploiting the convex closure property\nof B{\\'e}zier polynomials, our method ensures compliance with friction cone\nconstraints. Using the aforementioned approach, the proposed trajectory\noptimization framework can generate dynamically reliable motions with various\ngait sequences for legged robots. We validate our framework using a quadruped\nrobot model, focusing on the feasibility of dynamics and motion generation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u817f\u5f0f\u673a\u5668\u4eba\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\uff0c\u4f46\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u4f18\u5316\uff0c\u4f46\u4f18\u5316\u65b9\u6cd5\u548c\u76ee\u6807\u90fd\u96c6\u4e2d\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u7ea6\u675f\u4e0a\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory optimization", "legged robots", "motion generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23989", "pdf": "https://arxiv.org/pdf/2510.23989", "abs": "https://arxiv.org/abs/2510.23989", "authors": ["Shangde Gao", "Zelin Xu", "Zhe Jiang"], "title": "Learning Individual Movement Shifts After Urban Disruptions with Social Infrastructure Reliance", "categories": ["cs.AI"], "comment": null, "summary": "Shifts in individual movement patterns following disruptive events can reveal\nchanging demands for community resources. However, predicting such shifts\nbefore disruptive events remains challenging for several reasons. First,\nmeasures are lacking for individuals' heterogeneous social infrastructure\nresilience (SIR), which directly influences their movement patterns, and\ncommonly used features are often limited or unavailable at scale, e.g.,\nsociodemographic characteristics. Second, the complex interactions between\nindividual movement patterns and spatial contexts have not been sufficiently\ncaptured. Third, individual-level movement may be spatially sparse and not\nwell-suited to traditional decision-making methods for movement predictions.\nThis study incorporates individuals' SIR into a conditioned deep learning model\nto capture the complex relationships between individual movement patterns and\nlocal spatial context using large-scale, sparse individual-level data. Our\nexperiments demonstrate that incorporating individuals' SIR and spatial context\ncan enhance the model's ability to predict post-event individual movement\npatterns. The conditioned model can capture the divergent shifts in movement\npatterns among individuals who exhibit similar pre-event patterns but differ in\nSIR.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u57ce\u5e02\u5e72\u6270\u540e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u7684\u53d8\u5316\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u867d\u7136\u6d89\u53ca\u79fb\u52a8\u6a21\u5f0f\u9884\u6d4b\uff08\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff09\uff0c\u4f46\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u6216\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u793e\u4f1a\u57fa\u7840\u8bbe\u65bd\u4f9d\u8d56 (SIR) \u88ab\u7528\u4f5c\u5f71\u54cd\u56e0\u7d20\uff0c\u5e76\u88ab\u7eb3\u5165\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u80fd\u529b\u3002", "keywords": ["trajectory prediction", "movement prediction", "deep learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24118", "pdf": "https://arxiv.org/pdf/2510.24118", "abs": "https://arxiv.org/abs/2510.24118", "authors": ["Haotian Zhou", "Xiaole Wang", "He Li", "Fusheng Sun", "Shengyu Guo", "Guolei Qi", "Jianghuan Xu", "Huijing Zhao"], "title": "LagMemo: Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Navigating to a designated goal using visual information is a fundamental\ncapability for intelligent robots. Most classical visual navigation methods are\nrestricted to single-goal, single-modality, and closed set goal settings. To\naddress the practical demands of multi-modal, open-vocabulary goal queries and\nmulti-goal visual navigation, we propose LagMemo, a navigation system that\nleverages a language 3D Gaussian Splatting memory. During exploration, LagMemo\nconstructs a unified 3D language memory. With incoming task goals, the system\nqueries the memory, predicts candidate goal locations, and integrates a local\nperception-based verification mechanism to dynamically match and validate goals\nduring navigation. For fair and rigorous evaluation, we curate GOAT-Core, a\nhigh-quality core split distilled from GOAT-Bench tailored to multi-modal\nopen-vocabulary multi-goal visual navigation. Experimental results show that\nLagMemo's memory module enables effective multi-modal open-vocabulary goal\nlocalization, and that LagMemo outperforms state-of-the-art methods in\nmulti-goal visual navigation. Project page:\nhttps://weekgoodday.github.io/lagmemo", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper explores visual navigation using a language-based 3D memory. While not directly focused on trajectory prediction, the navigation task inherently involves predicting future movements. The use of language models for understanding goals and the creation of a 3D language memory connects it to large language models. Therefore, it has moderate relevance.", "keywords": ["visual navigation", "language models", "goal localization", "3D memory"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24085", "pdf": "https://arxiv.org/pdf/2510.24085", "abs": "https://arxiv.org/abs/2510.24085", "authors": ["Md. Shihab Uddin", "Md Nazmus Shakib", "Rahul Bhadani"], "title": "Modeling Electric Vehicle Car-Following Behavior: Classical vs Machine Learning Approach", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "The increasing adoption of electric vehicles (EVs) necessitates an\nunderstanding of their driving behavior to enhance traffic safety and develop\nsmart driving systems. This study compares classical and machine learning\nmodels for EV car following behavior. Classical models include the Intelligent\nDriver Model (IDM), Optimum Velocity Model (OVM), Optimal Velocity Relative\nVelocity (OVRV), and a simplified CACC model, while the machine learning\napproach employs a Random Forest Regressor. Using a real world dataset of an EV\nfollowing an internal combustion engine (ICE) vehicle under varied driving\nconditions, we calibrated classical model parameters by minimizing the RMSE\nbetween predictions and real data. The Random Forest model predicts\nacceleration using spacing, speed, and gap type as inputs. Results demonstrate\nthe Random Forest's superior accuracy, achieving RMSEs of 0.0046 (medium gap),\n0.0016 (long gap), and 0.0025 (extra long gap). Among physics based models,\nCACC performed best, with an RMSE of 2.67 for long gaps. These findings\nhighlight the machine learning model's performance across all scenarios. Such\nmodels are valuable for simulating EV behavior and analyzing mixed autonomy\ntraffic dynamics in EV integrated environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on modeling electric vehicle car-following behavior using both classical and machine learning approaches. While it doesn't directly involve Large Language Models, it does relate to trajectory prediction through modeling vehicle movement and predicting acceleration. The use of machine learning models like Random Forest for this purpose contributes to its relevance, although the absence of LLMs lowers the score.", "keywords": ["trajectory prediction", "car-following behavior", "machine learning", "acceleration prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23656", "pdf": "https://arxiv.org/pdf/2510.23656", "abs": "https://arxiv.org/abs/2510.23656", "authors": ["Fuqiang Liu", "Weiping Ding", "Luis Miranda-Moreno", "Lijun Sun"], "title": "Error Adjustment Based on Spatiotemporal Correlation Fusion for Traffic Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages, 7 figures, 3 tables", "summary": "Deep neural networks (DNNs) play a significant role in an increasing body of\nresearch on traffic forecasting due to their effectively capturing\nspatiotemporal patterns embedded in traffic data. A general assumption of\ntraining the said forecasting models via mean squared error estimation is that\nthe errors across time steps and spatial positions are uncorrelated. However,\nthis assumption does not really hold because of the autocorrelation caused by\nboth the temporality and spatiality of traffic data. This gap limits the\nperformance of DNN-based forecasting models and is overlooked by current\nstudies. To fill up this gap, this paper proposes Spatiotemporally\nAutocorrelated Error Adjustment (SAEA), a novel and general framework designed\nto systematically adjust autocorrelated prediction errors in traffic\nforecasting. Unlike existing approaches that assume prediction errors follow a\nrandom Gaussian noise distribution, SAEA models these errors as a\nspatiotemporal vector autoregressive (VAR) process to capture their intrinsic\ndependencies. First, it explicitly captures both spatial and temporal error\ncorrelations by a coefficient matrix, which is then embedded into a newly\nformulated cost function. Second, a structurally sparse regularization is\nintroduced to incorporate prior spatial information, ensuring that the learned\ncoefficient matrix aligns with the inherent road network structure. Finally, an\ninference process with test-time error adjustment is designed to dynamically\nrefine predictions, mitigating the impact of autocorrelated errors in real-time\nforecasting. The effectiveness of the proposed approach is verified on\ndifferent traffic datasets. Results across a wide range of traffic forecasting\nmodels show that our method enhances performance in almost all cases.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u9884\u6d4b\uff0c\u5229\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6355\u6349\u65f6\u7a7a\u6a21\u5f0f\u3002\u867d\u7136\u6d89\u53ca\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4f46\u4e3b\u8981\u96c6\u4e2d\u5728\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\uff0c\u4e0e\u884c\u4eba\u6216\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u5173\u8054\u6027\u7a0d\u5f31\u3002\u8bba\u6587\u672a\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u65f6\u7a7a\u76f8\u5173\u6027\u5efa\u6a21\u65b9\u6cd5\u5728\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002", "keywords": ["traffic forecasting", "spatiotemporal", "deep neural networks", "error adjustment", "autoregressive", "time series prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24161", "pdf": "https://arxiv.org/pdf/2510.24161", "abs": "https://arxiv.org/abs/2510.24161", "authors": ["Wentao Tan", "Bowen Wang", "Heng Zhi", "Chenyu Liu", "Zhe Li", "Jian Liu", "Zengrong Lin", "Yukun Dai", "Yipeng Chen", "Wenjie Yang", "Enci Xie", "Hao Xue", "Baixu Ji", "Chen Xu", "Zhibin Wang", "Tianshi Wang", "Lei Zhu", "Heng Tao Shen"], "title": "BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning", "categories": ["cs.AI", "cs.MM", "cs.RO"], "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced vision-language\nreasoning and are increasingly deployed in embodied agents. However,\nsignificant limitations remain: MLLMs generalize poorly across digital-physical\nspaces and embodiments; vision-language-action models (VLAs) produce low-level\nactions yet lack robust high-level embodied reasoning; and most embodied large\nlanguage models (ELLMs) are constrained to digital-space with poor\ngeneralization to the physical world. Thus, unified models that operate\nseamlessly across digital and physical spaces while generalizing across\nembodiments and tasks remain absent. We introduce the \\textbf{Boundless Large\nModel (BLM$_1$)}, a multimodal spatial foundation model that preserves\ninstruction following and reasoning, incorporates embodied knowledge, and\nsupports robust cross-embodiment control. BLM$_1$ integrates three key\ncapabilities -- \\textit{cross-space transfer, cross-task learning, and\ncross-embodiment generalization} -- via a two-stage training paradigm. Stage I\ninjects embodied knowledge into the MLLM through curated digital corpora while\nmaintaining language competence. Stage II trains a policy module through an\nintent-bridging interface that extracts high-level semantics from the MLLM to\nguide control, without fine-tuning the MLLM backbone. This process is supported\nby a self-collected cross-embodiment demonstration suite spanning four robot\nembodiments and six progressively challenging tasks. Evaluations across digital\nand physical benchmarks show that a single BLM$_1$ instance outperforms four\nmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving\n$\\sim\\!\\textbf{6%}$ gains in digital tasks and $\\sim\\!\\textbf{3%}$ in physical\ntasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on a large language model (BLM1) for embodied agents, which could potentially be used for tasks involving trajectory prediction and control in both digital and physical spaces. While trajectory prediction isn't explicitly mentioned, the model's capabilities related to cross-embodiment control and instruction following suggest a potential application in this area. The strong focus on large models justifies a moderate relevance score.", "keywords": ["large language model", "MLLM", "foundation model", "embodied agents", "cross-embodiment control"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24671", "pdf": "https://arxiv.org/pdf/2510.24671", "abs": "https://arxiv.org/abs/2510.24671", "authors": ["Li Li", "Tobias Brinkmann", "Till Temmen", "Markus Eisenbarth", "Jakob Andert"], "title": "Multi-Agent Scenario Generation in Roundabouts with a Transformer-enhanced Conditional Variational Autoencoder", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "With the increasing integration of intelligent driving functions into\nserial-produced vehicles, ensuring their functionality and robustness poses\ngreater challenges. Compared to traditional road testing, scenario-based\nvirtual testing offers significant advantages in terms of time and cost\nefficiency, reproducibility, and exploration of edge cases. We propose a\nTransformer-enhanced Conditional Variational Autoencoder (CVAE-T) model for\ngenerating multi-agent traffic scenarios in roundabouts, which are\ncharacterized by high vehicle dynamics and complex layouts, yet remain\nrelatively underexplored in current research. The results show that the\nproposed model can accurately reconstruct original scenarios and generate\nrealistic, diverse synthetic scenarios. Besides, two Key-Performance-Indicators\n(KPIs) are employed to evaluate the interactive behavior in the generated\nscenarios. Analysis of the latent space reveals partial disentanglement, with\nseveral latent dimensions exhibiting distinct and interpretable effects on\nscenario attributes such as vehicle entry timing, exit timing, and velocity\nprofiles. The results demonstrate the model's capability to generate scenarios\nfor the validation of intelligent driving functions involving multi-agent\ninteractions, as well as to augment data for their development and iterative\nimprovement.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-agent scenario generation for autonomous driving using a Transformer-enhanced Conditional Variational Autoencoder (CVAE-T). While it utilizes a Transformer, which can be considered related to the broader family of large models, the primary focus is on scenario generation for traffic, implying a connection to trajectory prediction. The paper doesn't explicitly use or discuss large language models, but the use of a Transformer architecture and the application to multi-agent behavior prediction contribute to its relevance.", "keywords": ["multi-agent scenario generation", "Transformer", "Conditional Variational Autoencoder", "traffic scenarios", "autonomous driving", "trajectory prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23685", "pdf": "https://arxiv.org/pdf/2510.23685", "abs": "https://arxiv.org/abs/2510.23685", "authors": ["Junwen Ma", "Mingyu Ge", "Yisen Wang", "Yong Zhang", "Weicheng Fu"], "title": "Parallel BiLSTM-Transformer networks for forecasting chaotic dynamics", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages,7 figures", "summary": "The nonlinear nature of chaotic systems results in extreme sensitivity to\ninitial conditions and highly intricate dynamical behaviors, posing fundamental\nchallenges for accurately predicting their evolution. To overcome the\nlimitation that conventional approaches fail to capture both local features and\nglobal dependencies in chaotic time series simultaneously, this study proposes\na parallel predictive framework integrating Transformer and Bidirectional Long\nShort-Term Memory (BiLSTM) networks. The hybrid model employs a dual-branch\narchitecture, where the Transformer branch mainly captures long-range\ndependencies while the BiLSTM branch focuses on extracting local temporal\nfeatures. The complementary representations from the two branches are fused in\na dedicated feature-fusion layer to enhance predictive accuracy. As\nillustrating examples, the model's performance is systematically evaluated on\ntwo representative tasks in the Lorenz system. The first is autonomous\nevolution prediction, in which the model recursively extrapolates system\ntrajectories from the time-delay embeddings of the state vector to evaluate\nlong-term tracking accuracy and stability. The second is inference of\nunmeasured variable, where the model reconstructs the unobserved states from\nthe time-delay embeddings of partial observations to assess its\nstate-completion capability. The results consistently indicate that the\nproposed hybrid framework outperforms both single-branch architectures across\ntasks, demonstrating its robustness and effectiveness in chaotic system\nprediction.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528 Transformer \u548c BiLSTM \u7684\u6df7\u5408\u6a21\u578b\u6765\u9884\u6d4b\u6df7\u6c8c\u7cfb\u7edf\u7684\u6f14\u5316\uff0c\u672c\u8d28\u4e0a\u662f\u5728\u505a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5176\u4e2d Lorenz \u7cfb\u7edf\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\uff0c\u4f46 Transformer \u7684\u4f7f\u7528\u548c\u5bf9\u65f6\u95f4\u5e8f\u5217\u7684\u9884\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u4f46\u662f\u6ca1\u6709\u6d89\u53ca\u5230\u5927\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["Transformer", "BiLSTM", "time series prediction", "Lorenz system", "dynamical behaviors", "forecasting"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.24397", "pdf": "https://arxiv.org/pdf/2510.24397", "abs": "https://arxiv.org/abs/2510.24397", "authors": ["Jiarui Qin", "Yunjia Xi", "Junjie Huang", "Renting Rui", "Di Yin", "Weiwen Liu", "Yong Yu", "Weinan Zhang", "Xing Sun"], "title": "APTBench: Benchmarking Agentic Potential of Base LLMs During Pre-Training", "categories": ["cs.AI"], "comment": "46 pages", "summary": "With the rapid development of LLM-based agents, there is a growing trend to\nincorporate agent-specific data into the pre-training stage of LLMs, aiming to\nbetter align LLMs with real-world autonomous task execution. However, current\npre-training benchmarks primarily focus on isolated and static skills, e.g.,\ncommon knowledge or mathematical/code reasoning, and fail to reflect model's\nagentic capabilities. On the other hand, agent benchmarks are typically\ndesigned for post-trained models, requiring multi-turn task execution abilities\nthat base models struggle to support. Thus, there is a compelling need for a\nbenchmark that can evaluate agentic potentials during pre-training and guide\nthe model training more effectively. To address this gap, we propose APTBench,\na framework that converts real-world agent tasks and successful trajectories\ninto multiple-choice or text completion questions tailored for base models. It\nfocuses on core agentic abilities, e.g., planning and action, and covers key\nagent scenarios, software engineering and deep research. Compared to existing\ngeneral-purpose benchmarks, APTBench offers a more predictive signal of a\nmodel's downstream performance as an agent, while remaining significantly more\nlightweight and cost-effective than full-scale, end-to-end agent evaluations\nafter post-training.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on benchmarking the agentic potential of base LLMs during pre-training. While it mentions real-world agent tasks and successful trajectories, the core focus is on evaluating LLMs as agents and not directly on trajectory prediction methods. The connection to trajectory prediction is indirect, through the mention of 'successful trajectories'.", "keywords": ["Large Language Models", "LLMs", "agent", "trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.23899", "pdf": "https://arxiv.org/pdf/2510.23899", "abs": "https://arxiv.org/abs/2510.23899", "authors": ["Maria G. Mendoza", "Addison Kalanther", "Daniel Bostwick", "Emma Stephan", "Chinmay Maheshwari", "Shankar Sastry"], "title": "Coordinated Autonomous Drones for Human-Centered Fire Evacuation in Partially Observable Urban Environments", "categories": ["cs.MA", "cs.RO"], "comment": "Accepted to IEEE Global Humanitarian Technology Conference (GHTC\n  2025). 8 pages, 4 figures", "summary": "Autonomous drone technology holds significant promise for enhancing search\nand rescue operations during evacuations by guiding humans toward safety and\nsupporting broader emergency response efforts. However, their application in\ndynamic, real-time evacuation support remains limited. Existing models often\noverlook the psychological and emotional complexity of human behavior under\nextreme stress. In real-world fire scenarios, evacuees frequently deviate from\ndesignated safe routes due to panic and uncertainty. To address these\nchallenges, this paper presents a multi-agent coordination framework in which\nautonomous Unmanned Aerial Vehicles (UAVs) assist human evacuees in real-time\nby locating, intercepting, and guiding them to safety under uncertain\nconditions. We model the problem as a Partially Observable Markov Decision\nProcess (POMDP), where two heterogeneous UAV agents, a high-level rescuer (HLR)\nand a low-level rescuer (LLR), coordinate through shared observations and\ncomplementary capabilities. Human behavior is captured using an agent-based\nmodel grounded in empirical psychology, where panic dynamically affects\ndecision-making and movement in response to environmental stimuli. The\nenvironment features stochastic fire spread, unknown evacuee locations, and\nlimited visibility, requiring UAVs to plan over long horizons to search for\nhumans and adapt in real-time. Our framework employs the Proximal Policy\nOptimization (PPO) algorithm with recurrent policies to enable robust\ndecision-making in partially observable settings. Simulation results\ndemonstrate that the UAV team can rapidly locate and intercept evacuees,\nsignificantly reducing the time required for them to reach safety compared to\nscenarios without UAV assistance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autonomous drone coordination for fire evacuation, which involves trajectory planning and prediction for both drones and humans. While it doesn't directly use or mention large language models, the agent-based modeling of human behavior and the POMDP framework used for decision-making could potentially be enhanced by LLMs in the future. The core is trajectory prediction and path planning in a multi-agent system.", "keywords": ["trajectory prediction", "path planning", "autonomous drones", "multi-agent coordination", "POMDP", "agent-based model", "human behavior"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
