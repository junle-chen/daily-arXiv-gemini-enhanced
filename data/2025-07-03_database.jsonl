{"id": "2507.00094", "pdf": "https://arxiv.org/pdf/2507.00094", "abs": "https://arxiv.org/abs/2507.00094", "authors": ["Jacobo Casas-Ramos", "Sarah Winkler", "Alessandro Gianola", "Marco Montali", "Manuel Mucientes", "Manuel Lama"], "title": "Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)", "categories": ["cs.DB", "cs.AI", "cs.PL"], "comment": "Extended version of the paper of the same title accepted at the 23rd\n  International Conference on Business Process Management (BPM 2025)", "summary": "Despite growing interest in process analysis and mining for data-aware\nspecifications, alignment-based conformance checking for declarative process\nmodels has focused on pure control-flow specifications, or mild data-aware\nextensions limited to numerical data and variable-to-constant comparisons. This\nis not surprising: finding alignments is computationally hard, even more so in\nthe presence of data dependencies. In this paper, we challenge this problem in\nthe case where the reference model is captured using data-aware Declare with\ngeneral data types and data conditions. We show that, unexpectedly, it is\npossible to compute data-aware optimal alignments in this rich setting,\nenjoying at once efficiency and expressiveness. This is achieved by carefully\ncombining the two best-known approaches to deal with control flow and data\ndependencies when computing alignments, namely A* search and SMT solving.\nSpecifically, we introduce a novel algorithmic technique that efficiently\nexplores the search space, generating descendant states through the application\nof repair actions aiming at incrementally resolving constraint violations. We\nprove the correctness of our algorithm and experimentally show its efficiency.\nThe evaluation witnesses that our approach matches or surpasses the performance\nof the state of the art while also supporting significantly more expressive\ndata dependencies, showcasing its potential to support real-world applications."}
{"id": "2507.00188", "pdf": "https://arxiv.org/pdf/2507.00188", "abs": "https://arxiv.org/abs/2507.00188", "authors": ["Qihan Zhang", "Shaolin Xie", "Ibrahim Sabek"], "title": "LIMAO: A Framework for Lifelong Modular Learned Query Optimization", "categories": ["cs.DB"], "comment": "To appear at VLDB 2025 (https://vldb.org/2025/)", "summary": "Query optimizers are crucial for the performance of database systems.\nRecently, many learned query optimizers (LQOs) have demonstrated significant\nperformance improvements over traditional optimizers. However, most of them\noperate under a limited assumption: a static query environment. This limitation\nprevents them from effectively handling complex, dynamic query environments in\nreal-world scenarios. Extensive retraining can lead to the well-known\ncatastrophic forgetting problem, which reduces the LQO generalizability over\ntime. In this paper, we address this limitation and introduce LIMAO (Lifelong\nModular Learned Query Optimizer), a framework for lifelong learning of plan\ncost prediction that can be seamlessly integrated into existing LQOs. LIMAO\nleverages a modular lifelong learning technique, an attention-based neural\nnetwork composition architecture, and an efficient training paradigm designed\nto retain prior knowledge while continuously adapting to new environments. We\nimplement LIMAO in two LQOs, showing that our approach is agnostic to\nunderlying engines. Experimental results show that LIMAO significantly enhances\nthe performance of LQOs, achieving up to a 40% improvement in query execution\ntime and reducing the variance of execution time by up to 60% under dynamic\nworkloads. By leveraging a precise and self-consistent design, LIMAO\neffectively mitigates catastrophic forgetting, ensuring stable and reliable\nplan quality over time. Compared to Postgres, LIMAO achieves up to a 4x speedup\non selected benchmarks, highlighting its practical advantages in real-world\nquery optimization."}
{"id": "2507.00343", "pdf": "https://arxiv.org/pdf/2507.00343", "abs": "https://arxiv.org/abs/2507.00343", "authors": ["Vishal Chakraborty", "Youri Kaminsky", "Sharad Mehrotra", "Felix Naumann", "Faisal Nawab", "Primal Pappachan", "Mohammad Sadoghi", "Nalini Venkatasubramanian"], "title": "Meaningful Data Erasure in the Presence of Dependencies", "categories": ["cs.DB"], "comment": "VLDB 2025 Preprint", "summary": "Data regulations like GDPR require systems to support data erasure but leave\nthe definition of \"erasure\" open to interpretation. This ambiguity makes\ncompliance challenging, especially in databases where data dependencies can\nlead to erased data being inferred from remaining data. We formally define a\nprecise notion of data erasure that ensures any inference about deleted data,\nthrough dependencies, remains bounded to what could have been inferred before\nits insertion. We design erasure mechanisms that enforce this guarantee at\nminimal cost. Additionally, we explore strategies to balance cost and\nthroughput, batch multiple erasures, and proactively compute data retention\ntimes when possible. We demonstrate the practicality and scalability of our\nalgorithms using both real and synthetic datasets."}
{"id": "2507.00379", "pdf": "https://arxiv.org/pdf/2507.00379", "abs": "https://arxiv.org/abs/2507.00379", "authors": ["Zikai Wang", "Qianxi Zhang", "Baotong Lu", "Qi Chen", "Cheng Tan"], "title": "Towards Robustness: A Critique of Current Vector Database Assessments", "categories": ["cs.DB"], "comment": null, "summary": "Vector databases are critical infrastructure in AI systems, and average\nrecall is the dominant metric for their evaluation. Both users and researchers\nrely on it to choose and optimize their systems. We show that relying on\naverage recall is problematic. It hides variability across queries, allowing\nsystems with strong mean performance to underperform significantly on hard\nqueries. These tail cases confuse users and can lead to failure in downstream\napplications such as RAG. We argue that robustness consistently achieving\nacceptable recall across queries is crucial to vector database evaluation. We\npropose Robustness-$\\delta$@K, a new metric that captures the fraction of\nqueries with recall above a threshold $\\delta$. This metric offers a deeper\nview of recall distribution, helps vector index selection regarding application\nneeds, and guides the optimization of tail performance. We integrate\nRobustness-$\\delta$@K into existing benchmarks and evaluate mainstream vector\nindexes, revealing significant robustness differences. More robust vector\nindexes yield better application performance, even with the same average\nrecall. We also identify design factors that influence robustness, providing\nguidance for improving real-world performance."}
{"id": "2507.00427", "pdf": "https://arxiv.org/pdf/2507.00427", "abs": "https://arxiv.org/abs/2507.00427", "authors": ["Hao Wu", "Changzheng Wei", "Yanhao Wang", "Li Lin", "Yilong Leng", "Shiyu He", "Minghao Zhao", "Hanghang Wu", "Ying Yan", "Aoying Zhou"], "title": "Zero-Knowledge Verifiable Graph Query Evaluation via Expansion-Centric Operator Decomposition", "categories": ["cs.DB"], "comment": null, "summary": "This paper investigates the feasibility of achieving zero-knowledge\nverifiability for graph databases, enabling database owners to\ncryptographically prove the query execution correctness without disclosing the\nunderlying data. Although similar capabilities have been explored for\nrelational databases, their implementation for graph databases presents unique\nchallenges. This is mainly attributed to the relatively large complexity of\nqueries in graph databases. When translating graph queries into arithmetic\ncircuits, the circuit scale can be too large to be practically evaluated. To\naddress this issue, we propose to break down graph queries into more\nfine-grained, primitive operators, enabling a step-by-step evaluation through\nsmaller-scale circuits. Accordingly, the verification with ZKP circuits of\ncomplex graph queries can be decomposed into a series of composable\ncryptographic primitives, each designed to verify a fundamental structural\nproperty such as path ordering or edge directionality. Especially, having\nnoticed that the graph expansion (i.e., traversing from nodes to their\nneighbors along edges) operation serves as the backbone of graph query\nevaluation, we design the expansion centric operator decomposition. In addition\nto constructing circuits for the expansion primitives, we also design\nspecialized ZKP circuits for the various attributes that augment this\ntraversal. The circuits are meticulously designed to take advantage of PLONKish\narithmetization. By integrating these optimized circuits, we implement ZKGraph,\na system that provides verifiable query processing while preserving data\nprivacy. Performance evaluation indicates that ZKGraph significantly\noutperforms naive in circuit implementations of graph operators, achieving\nsubstantial improvements in both runtime and memory consumption."}
{"id": "2507.00489", "pdf": "https://arxiv.org/pdf/2507.00489", "abs": "https://arxiv.org/abs/2507.00489", "authors": ["Pengyu Chen", "Zizheng Guo", "Jianwei Yang", "Dongjing Miao"], "title": "Towards Efficient Random-Order Enumeration for Join Queries", "categories": ["cs.DB"], "comment": null, "summary": "In many data analysis pipelines, a basic and time-consuming process is to\nproduce join results and feed them into downstream tasks. Numerous enumeration\nalgorithms have been developed for this purpose. To be a statistically\nmeaningful representation of the whole join result, the result tuples are\nrequired to be enumerated in uniformly random order. However, existing studies\nlack an efficient random-order enumeration algorithm with a worst-case runtime\nguarantee for (cyclic) join queries. In this paper, we study the problem of\nenumerating the results of a join query in random order. We develop an\nefficient random-order enumeration algorithm for join queries with no large\nhidden constants in its complexity, achieving expected\n$O(\\frac{\\mathrm{AGM}(Q)}{|Res(Q)|}\\log^2|Q|)$ delay,\n$O(\\mathrm{AGM}(Q)\\log|Q|)$ total running time after $O(|Q|\\log|Q|)$-time index\nconstruction, where $|Q|$ is the size of input, $\\mathrm{AGM}(Q)$ is the AGM\nbound, and $|Res(Q)|$ is the size of the join result. We prove that our\nalgorithm is near-optimal in the worst case, under the combinatorial $k$-clique\nhypothesis. Our algorithm requires no query-specific preprocessing and can be\nflexibly adapted to many common database indexes with only minor modifications.\nWe also devise two non-trivial techniques to speed up the enumeration, and\nprovide an experimental study on our enumeration algorithm along with the\nspeed-up techniques. The experimental results show that our algorithm, enhanced\nwith the proposed techniques, significantly outperforms existing\nstate-of-the-art methods."}
{"id": "2507.00839", "pdf": "https://arxiv.org/pdf/2507.00839", "abs": "https://arxiv.org/abs/2507.00839", "authors": ["Chiyu Hao", "Jixian Su", "Shixuan Sun", "Hao Zhang", "Sen Gao", "Jianwen Zhao", "Chenyi Zhang", "Jieru Zhao", "Chen Chen", "Minyi Guo"], "title": "RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries", "categories": ["cs.DB"], "comment": "17 pages, 18 figures", "summary": "Dynamic graph storage systems are essential for real-time applications such\nas social networks and recommendation, where graph data continuously evolves.\nHowever, they face significant challenges in efficiently handling concurrent\nread and write operations. We find that existing methods suffer from write\nqueries interfering with read efficiency, substantial time and space overhead\ndue to per-edge versioning, and an inability to balance performance, such as\nslow searches under concurrent workloads. To address these issues, we propose\nRapidStore, a holistic approach for efficient in-memory dynamic graph storage\ndesigned for read-intensive workloads. Our key idea is to exploit the\ncharacteristics of graph queries through a decoupled system design that\nseparates the management of read and write queries and decouples version data\nfrom graph data. Particularly, we design an efficient dynamic graph store to\ncooperate with the graph concurrency control mechanism. Experimental results\ndemonstrate that RapidStore enables fast and scalable concurrent graph queries,\neffectively balancing the performance of inserts, searches, and scans, and\nsignificantly improving efficiency in dynamic graph storage systems."}
{"id": "2507.00938", "pdf": "https://arxiv.org/pdf/2507.00938", "abs": "https://arxiv.org/abs/2507.00938", "authors": ["Zihao Sun", "Meng Fang", "Ling Chen"], "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks", "categories": ["cs.IR", "cs.AI", "cs.DB", "F.2.2; I.2.7"], "comment": "10 pages, 9 figures, 4 tables", "summary": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy."}
