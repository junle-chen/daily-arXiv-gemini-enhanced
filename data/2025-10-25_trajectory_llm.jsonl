{"id": "2510.20275", "pdf": "https://arxiv.org/pdf/2510.20275", "abs": "https://arxiv.org/abs/2510.20275", "authors": ["Yunzhi Liu", "Haokai Tan", "Rushi Kanjaria", "Lihuan Li", "Flora D. Salim"], "title": "Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction", "categories": ["cs.AI"], "comment": "This paper has been accepted by ACM SIGSPATIAL 2025 as a short paper", "summary": "Human mobility forecasting is crucial for disaster relief, city planning, and\npublic health. However, existing models either only model location sequences or\ninclude time information merely as auxiliary input, thereby failing to leverage\nthe rich semantic context provided by points of interest (POIs). To address\nthis, we enrich a BERT-based mobility model with derived temporal descriptors\nand POI embeddings to better capture the semantics underlying human movement.\nWe propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI\nand temporal information at each location to construct a unified, semantically\nenriched representation of mobility. Experimental results show that STaBERT\nsignificantly improves prediction accuracy: for single-city prediction, the\nGEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34\nto 0.56.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528BERT\uff08\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u4eba\u7c7b\u79fb\u52a8\u6027\u9884\u6d4b\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f68\u8ff9\u9884\u6d4b\u7684\u5b50\u9886\u57df\u3002\u8bba\u6587\u5c06BERT\u6a21\u578b\u5e94\u7528\u4e8e\u9884\u6d4b\u4eba\u7c7b\u79fb\u52a8\u8f68\u8ff9\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u90fd\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "human mobility prediction", "BERT", "large language models", "mobility forecasting", "point of interest (POI)"]}}
{"id": "2510.20182", "pdf": "https://arxiv.org/pdf/2510.20182", "abs": "https://arxiv.org/abs/2510.20182", "authors": ["Aaron Appelle", "Jerome P. Lynch"], "title": "Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories", "categories": ["cs.CV"], "comment": "Preprint, under review", "summary": "Large-scale video generation models have demonstrated high visual realism in\ndiverse contexts, spurring interest in their potential as general-purpose world\nsimulators. Existing benchmarks focus on individual subjects rather than scenes\nwith multiple interacting people. However, the plausibility of multi-agent\ndynamics in generated videos remains unverified. We propose a rigorous\nevaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V)\nmodels as implicit simulators of pedestrian dynamics. For I2V, we leverage\nstart frames from established datasets to enable comparison with a ground truth\nvideo dataset. For T2V, we develop a prompt suite to explore diverse pedestrian\ndensities and interactions. A key component is a method to reconstruct 2D\nbird's-eye view trajectories from pixel-space without known camera parameters.\nOur analysis reveals that leading models have learned surprisingly effective\npriors for plausible multi-agent behavior. However, failure modes like merging\nand disappearing people highlight areas for future improvement.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08\u5c5e\u4e8e\u5927\u6a21\u578b\u8303\u7574\uff09\u4f5c\u4e3a\u591a\u4eba\u884c\u4eba\u8f68\u8ff9\u6a21\u62df\u5668\u7684\u80fd\u529b\u3002\u8bba\u6587\u91cd\u70b9\u5728\u4e8e\u8bc4\u4f30\u751f\u6210\u89c6\u9891\u4e2d\u884c\u4eba\u8f68\u8ff9\u7684\u5408\u7406\u6027\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528LLM\uff0c\u4f46\u4f7f\u7528\u4e86\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u884c\u4eba\u8f68\u8ff9\u7684\u6a21\u62df\u548c\u8bc4\u4f30\uff0c\u4f53\u73b0\u4e86\u4e24\u4e2a\u9886\u57df\u7684\u4ea4\u53c9\u3002", "keywords": ["trajectory prediction", "pedestrian trajectories", "video models", "large-scale video generation models", "multi-agent dynamics", "text-to-video", "image-to-video"]}}
{"id": "2510.19981", "pdf": "https://arxiv.org/pdf/2510.19981", "abs": "https://arxiv.org/abs/2510.19981", "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"], "title": "FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework\nthat builds on existing 3D detectors by introducing a transformer-based\nsmoother and a fusion-driven tracker. Inspired by query-based tracking\nframeworks, FutrTrack employs a multimodal two-stage transformer refinement and\ntracking pipeline. Our fusion tracker integrates bounding boxes with multimodal\nbird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without\nthe need for an explicit motion model. The tracker assigns and propagates\nidentities across frames, leveraging both geometric and semantic cues for\nrobust re-identification under occlusion and viewpoint changes. Prior to\ntracking, we refine sequences of bounding boxes with a temporal smoother over a\nmoving window to refine trajectories, reduce jitter, and improve spatial\nconsistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that\nquery-based transformer tracking methods benefit significantly from multimodal\nsensor features compared with previous single-sensor approaches. With an aMOTA\nof 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D\nMOT benchmarks, reducing identity switches while maintaining competitive\naccuracy. Our approach provides an efficient framework for improving\ntransformer-based trackers to compete with other neural-network-based methods\neven with limited data and without pretraining.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce83D\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u5229\u7528Transformer\u8fdb\u884c\u8f68\u8ff9\u5e73\u6ed1\u548c\u878d\u5408\u8ddf\u8e2a\u3002\u867d\u7136\u4f7f\u7528\u4e86Transformer\u67b6\u6784\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u76ee\u6807\u8ddf\u8e2a\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\u3002\u56e0\u6b64\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["multi-object tracking", "3D tracking", "transformer", "trajectory", "fusion", "camera-LiDAR"]}}
{"id": "2510.20008", "pdf": "https://arxiv.org/pdf/2510.20008", "abs": "https://arxiv.org/abs/2510.20008", "authors": ["Swati Dantu", "Robert P\u011bni\u010dka", "Martin Saska"], "title": "Simultaneous learning of state-to-state minimum-time planning and control", "categories": ["cs.RO"], "comment": null, "summary": "This paper tackles the challenge of learning a generalizable minimum-time\nflight policy for UAVs, capable of navigating between arbitrary start and goal\nstates while balancing agile flight and stable hovering. Traditional\napproaches, particularly in autonomous drone racing, achieve impressive speeds\nand agility but are constrained to predefined track layouts, limiting\nreal-world applicability. To address this, we propose a reinforcement\nlearning-based framework that simultaneously learns state-to-state minimum-time\nplanning and control and generalizes to arbitrary state-to-state flights. Our\napproach leverages Point Mass Model (PMM) trajectories as proxy rewards to\napproximate the true optimal flight objective and employs curriculum learning\nto scale the training process efficiently and to achieve generalization. We\nvalidate our method through simulation experiments, comparing it against\nNonlinear Model Predictive Control (NMPC) tracking PMM-generated trajectories\nand conducting ablation studies to assess the impact of curriculum learning.\nFinally, real-world experiments confirm the robustness of our learned policy in\noutdoor environments, demonstrating its ability to generalize and operate on a\nsmall ARM-based single-board computer.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\uff0c\u7279\u522b\u662f\u5b66\u4e60\u4ece\u4e00\u4e2a\u72b6\u6001\u5230\u53e6\u4e00\u4e2a\u72b6\u6001\u7684\u6700\u77ed\u65f6\u95f4\u98de\u884c\u7b56\u7565\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u89c4\u5212\u548c\u63a7\u5236\uff0c\u4f46\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u4f7f\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u65b9\u6cd5\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory planning", "control", "reinforcement learning", "UAV", "flight policy"]}}
{"id": "2510.19889", "pdf": "https://arxiv.org/pdf/2510.19889", "abs": "https://arxiv.org/abs/2510.19889", "authors": ["Mostafa Ameli", "Van Anh Le", "Sulthana Shams", "Alexander Skabardonis"], "title": "From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "The traffic assignment problem is essential for traffic flow analysis,\ntraditionally solved using mathematical programs under the Equilibrium\nprinciple. These methods become computationally prohibitive for large-scale\nnetworks due to non-linear growth in complexity with the number of OD pairs.\nThis study introduces a novel data-driven approach using deep neural networks,\nspecifically leveraging the Transformer architecture, to predict equilibrium\npath flows directly. By focusing on path-level traffic distribution, the\nproposed model captures intricate correlations between OD pairs, offering a\nmore detailed and flexible analysis compared to traditional link-level\napproaches. The Transformer-based model drastically reduces computation time,\nwhile adapting to changes in demand and network structure without the need for\nrecalculation. Numerical experiments are conducted on the Manhattan-like\nsynthetic network, the Sioux Falls network, and the Eastern-Massachusetts\nnetwork. The results demonstrate that the proposed model is orders of magnitude\nfaster than conventional optimization. It efficiently estimates path-level\ntraffic flows in multi-class networks, reducing computational costs and\nimproving prediction accuracy by capturing detailed trip and flow information.\nThe model also adapts flexibly to varying demand and network conditions,\nsupporting traffic management and enabling rapid `what-if' analyses for\nenhanced transportation planning and policy-making.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses a Transformer-based model to predict path flows in traffic networks. While it doesn't directly involve large language models, it does address trajectory prediction (path flow estimation) using a Transformer architecture, which is a key component in many LLMs. Therefore, it has moderate relevance.", "keywords": ["trajectory prediction", "path flow estimation", "Transformer"]}}
{"id": "2510.20161", "pdf": "https://arxiv.org/pdf/2510.20161", "abs": "https://arxiv.org/abs/2510.20161", "authors": ["Ahmed Alanazi", "Duy Ho", "Yugyung Lee"], "title": "PathFormer: A Transformer with 3D Grid Constraints for Digital Twin Robot-Arm Trajectory Generation", "categories": ["cs.RO", "68T07, 68T40", "I.2.9; I.2.10; I.2.11"], "comment": "8 pages, 7 figures, 7 tables", "summary": "Robotic arms require precise, task-aware trajectory planning, yet sequence\nmodels that ignore motion structure often yield invalid or inefficient\nexecutions. We present a Path-based Transformer that encodes robot motion with\na 3-grid (where/what/when) representation and constraint-masked decoding,\nenforcing lattice-adjacent moves and workspace bounds while reasoning over task\ngraphs and action order. Trained on 53,755 trajectories (80% train / 20%\nvalidation), the model aligns closely with ground truth -- 89.44% stepwise\naccuracy, 93.32% precision, 89.44% recall, and 90.40% F1 -- with 99.99% of\npaths legal by construction. Compiled to motor primitives on an xArm Lite 6\nwith a depth-camera digital twin, it attains up to 97.5% reach and 92.5% pick\nsuccess in controlled tests, and 86.7% end-to-end success across 60\nlanguage-specified tasks in cluttered scenes, absorbing slips and occlusions\nvia local re-grounding without global re-planning. These results show that\npath-structured representations enable Transformers to generate accurate,\nreliable, and interpretable robot trajectories, bridging graph-based planning\nand sequence-based learning and providing a practical foundation for\ngeneral-purpose manipulation and sim-to-real transfer.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u624b\u81c2\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u4f7f\u7528\u4e86Transformer\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e863D\u7f51\u683c\u7ea6\u675f\u3002\u867d\u7136\u4f7f\u7528\u4e86Transformer\uff0c\u4f46\u4fa7\u91cd\u4e8e\u8f68\u8ff9\u89c4\u5212\u548c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4e0e\u901a\u7528\u7684\u5927\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory generation", "Transformer", "robot arm", "path planning"]}}
{"id": "2510.20328", "pdf": "https://arxiv.org/pdf/2510.20328", "abs": "https://arxiv.org/abs/2510.20328", "authors": ["Ajay Sridhar", "Jennifer Pan", "Satvik Sharma", "Chelsea Finn"], "title": "MemER: Scaling Up Memory for Robot Control via Experience Retrieval", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Project page: https://jen-pan.github.io/memer/", "summary": "Humans routinely rely on memory to perform tasks, yet most robot policies\nlack this capability; our goal is to endow robot policies with the same\nability. Naively conditioning on long observation histories is computationally\nexpensive and brittle under covariate shift, while indiscriminate subsampling\nof history leads to irrelevant or redundant information. We propose a\nhierarchical policy framework, where the high-level policy is trained to select\nand track previous relevant keyframes from its experience. The high-level\npolicy uses selected keyframes and the most recent frames when generating text\ninstructions for a low-level policy to execute. This design is compatible with\nexisting vision-language-action (VLA) models and enables the system to\nefficiently reason over long-horizon dependencies. In our experiments, we\nfinetune Qwen2.5-VL-7B-Instruct and $\\pi_{0.5}$ as the high-level and low-level\npolicies respectively, using demonstrations supplemented with minimal language\nannotations. Our approach, MemER, outperforms prior methods on three real-world\nlong-horizon robotic manipulation tasks that require minutes of memory. Videos\nand code can be found at https://jen-pan.github.io/memer/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u63a7\u5236\uff0c\u901a\u8fc7\u7ecf\u9a8c\u68c0\u7d22\u6765\u6269\u5c55\u673a\u5668\u4eba\u7684\u8bb0\u5fc6\u80fd\u529b\u3002\u6458\u8981\u63d0\u5230\u4e86vision-language-action (VLA) models \u548c Qwen2.5-VL-7B-Instruct (\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b)\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u673a\u5668\u4eba\u63a7\u5236\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u6d89\u53ca\u5230\u8f68\u8ff9\u89c4\u5212\uff0c\u4f46\u8fd9\u4e0d\u662f\u8bba\u6587\u7684\u6838\u5fc3\u5173\u6ce8\u70b9\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["large language models", "LLMs", "vision-language-action models", "robot control", "Qwen2.5-VL-7B-Instruct"]}}
{"id": "2510.20335", "pdf": "https://arxiv.org/pdf/2510.20335", "abs": "https://arxiv.org/abs/2510.20335", "authors": ["Zixuan Wu", "Hengyuan Zhang", "Ting-Hsuan Chen", "Yuliang Guo", "David Paz", "Xinyu Huang", "Liu Ren"], "title": "Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking", "categories": ["cs.RO", "cs.CV"], "comment": "Code is at\n  https://github.com/ChampagneAndfragrance/Dino_Diffusion_Parking_Official", "summary": "Parking is a critical pillar of driving safety. While recent end-to-end (E2E)\napproaches have achieved promising in-domain results, robustness under domain\nshifts (e.g., weather and lighting changes) remains a key challenge. Rather\nthan relying on additional data, in this paper, we propose Dino-Diffusion\nParking (DDP), a domain-agnostic autonomous parking pipeline that integrates\nvisual foundation models with diffusion-based planning to enable generalized\nperception and robust motion planning under distribution shifts. We train our\npipeline in CARLA at regular setting and transfer it to more adversarial\nsettings in a zero-shot fashion. Our model consistently achieves a parking\nsuccess rate above 90% across all tested out-of-distribution (OOD) scenarios,\nwith ablation studies confirming that both the network architecture and\nalgorithmic design significantly enhance cross-domain performance over existing\nbaselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment\nreconstructed from a real-world parking lot demonstrates promising sim-to-real\ntransfer.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autonomous parking, which involves motion planning and can be considered a form of trajectory prediction. It also uses a visual foundation model (Dino) and diffusion-based planning, which are related to large models and generative models. While not directly using LLMs, the use of foundation models and diffusion models suggests a connection to the broader field of large-scale AI models.", "keywords": ["autonomous parking", "motion planning", "trajectory prediction", "foundation models", "diffusion models"]}}
{"id": "2510.20126", "pdf": "https://arxiv.org/pdf/2510.20126", "abs": "https://arxiv.org/abs/2510.20126", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony S. Maida", "Alan B. Barhorst", "Vijaya Gopu"], "title": "Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects", "categories": ["cs.CV"], "comment": "13 pages, 6 figures", "summary": "While computer vision has advanced considerably for general object detection\nand tracking, the specific problem of fast-moving tiny objects remains\nunderexplored. This paper addresses the significant challenge of detecting and\ntracking rapidly moving small objects using an RGB-D camera. Our novel system\ncombines deep learning-based detection with physics-based tracking to overcome\nthe limitations of existing approaches. Our contributions include: (1) a\ncomprehensive system design for object detection and tracking of fast-moving\nsmall objects in 3D space, (2) an innovative physics-based tracking algorithm\nthat integrates kinematics motion equations to handle outliers and missed\ndetections, and (3) an outlier detection and correction module that\nsignificantly improves tracking performance in challenging scenarios such as\nocclusions and rapid direction changes. We evaluated our proposed system on a\ncustom racquetball dataset. Our evaluation shows our system surpassing kalman\nfilter based trackers with up to 70\\% less Average Displacement Error. Our\nsystem has significant applications for improving robot perception on\nautonomous platforms and demonstrates the effectiveness of combining\nphysics-based models with deep learning approaches for real-time 3D detection\nand tracking of challenging small objects.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 3D tracking of fast-moving objects, which is related to trajectory prediction. While it uses deep learning, it doesn't involve large language models. The physics-based tracking algorithm also touches upon trajectory estimation.", "keywords": ["tracking", "3D tracking", "object detection", "physics-based tracking", "motion equations", "trajectory"]}}
{"id": "2510.20473", "pdf": "https://arxiv.org/pdf/2510.20473", "abs": "https://arxiv.org/abs/2510.20473", "authors": ["Bernhard Rameder", "Hubert Gattringer", "Andreas Mueller", "Ronald Naderer"], "title": "Robot Path and Trajectory Planning Considering a Spatially Fixed TCP", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a method for planning a trajectory in workspace\ncoordinates using a spatially fixed tool center point (TCP), while taking into\naccount the processing path on a part. This approach is beneficial if it is\neasier to move the part rather than moving the tool. Whether a mathematical\ndescription that defines the shape to be processed or single points from a\ndesign program are used, the robot path is finally represented using B-splines.\nThe use of splines enables the path to be continuous with a desired degree,\nwhich finally leads to a smooth robot trajectory. While calculating the robot\ntrajectory through prescribed orientation, additionally a given velocity at the\nTCP has to be considered. The procedure was validated on a real system using an\nindustrial robot moving an arbitrary defined part.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robot trajectory planning, which falls under the broader category of trajectory prediction. However, it doesn't mention or utilize large language models. The relevance score is thus moderate as it is related to trajectory prediction but not LLMs.", "keywords": ["trajectory planning", "trajectory", "robot"]}}
{"id": "2510.20022", "pdf": "https://arxiv.org/pdf/2510.20022", "abs": "https://arxiv.org/abs/2510.20022", "authors": ["Jiazheng Li", "Yawei Wang", "David Yan", "Yijun Tian", "Zhichao Xu", "Huan Song", "Panpan Xu", "Lin Lee Cheong"], "title": "SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nenabling language agents to excel at single-turn tasks. However, their\napplication to complex, multi-step, and long-horizon tasks remains challenging.\nWhile reinforcement learning (RL) offers a promising avenue for addressing\nthese challenges, mainstream approaches typically rely solely on sparse,\noutcome-based rewards, a limitation that becomes especially problematic for\ngroup-based RL algorithms lacking critic models, such as Group Relative Policy\nOptimization (GRPO). In such methods, uniformly rewarding or penalizing all\nactions within a trajectory can lead to training instability and suboptimal\npolicies, because beneficial and detrimental actions are often entangled across\nmulti-step interactions. To address this challenge, we propose SALT, a novel\nand lightweight framework that provides a finer-grained advantage assignment,\nderived solely from outcome rewards. We achieve this by constructing a graph\nfrom trajectories of the same prompt, which allows us to quantify the quality\nof each step and assign advantages accordingly. Crucially, SALT is designed as\na plug-and-play module that seamlessly integrates with existing group-based RL\nalgorithms, requiring no modifications to the rollout procedure and introducing\nnegligible computational overhead. Extensive experiments on the WebShop,\nALFWorld, and AppWorld benchmarks with various model sizes demonstrate that\nSALT consistently improves performance. We also conduct a thorough analysis to\nvalidate the design choices behind SALT and offer actionable insights.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses using Large Language Models (LLMs) for long-horizon tasks and proposes a method to improve their performance using reinforcement learning. While it doesn't directly address trajectory prediction, the concept of \"trajectories\" and \"multi-step interactions\" is central to the approach, and the methods are evaluated on tasks that involve sequential decision-making, which is related to trajectory planning. The connection to trajectory prediction is somewhat indirect, but the use of LLMs in the context of sequential tasks justifies a non-zero relevance score.", "keywords": ["Large Language Models", "LLMs", "reinforcement learning", "trajectories", "long-horizon tasks", "multi-step interactions"]}}
{"id": "2510.20774", "pdf": "https://arxiv.org/pdf/2510.20774", "abs": "https://arxiv.org/abs/2510.20774", "authors": ["Wenhao Wang", "Kehe Ye", "Xinyu Zhou", "Tianxing Chen", "Cao Min", "Qiaoming Zhu", "Xiaokang Yang", "Yongjian Shen", "Yang Yang", "Maoqing Yao", "Yao Mu"], "title": "FieldGen: From Teleoperated Pre-Manipulation Trajectories to Field-Guided Data Generation", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "Webpage: https://fieldgen.github.io/", "summary": "Large-scale and diverse datasets are vital for training robust robotic\nmanipulation policies, yet existing data collection methods struggle to balance\nscale, diversity, and quality. Simulation offers scalability but suffers from\nsim-to-real gaps, while teleoperation yields high-quality demonstrations with\nlimited diversity and high labor cost. We introduce FieldGen, a field-guided\ndata generation framework that enables scalable, diverse, and high-quality\nreal-world data collection with minimal human supervision. FieldGen decomposes\nmanipulation into two stages: a pre-manipulation phase, allowing trajectory\ndiversity, and a fine manipulation phase requiring expert precision. Human\ndemonstrations capture key contact and pose information, after which an\nattraction field automatically generates diverse trajectories converging to\nsuccessful configurations. This decoupled design combines scalable trajectory\ndiversity with precise supervision. Moreover, FieldGen-Reward augments\ngenerated data with reward annotations to further enhance policy learning.\nExperiments demonstrate that policies trained with FieldGen achieve higher\nsuccess rates and improved stability compared to teleoperation-based baselines,\nwhile significantly reducing human effort in long-term real-world data\ncollection. Webpage is available at https://fieldgen.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u901a\u8fc7\u5438\u5f15\u573a\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u8f68\u8ff9\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u751f\u6210/\u89c4\u5212\u7684\u8303\u7574\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cLarge-scale\u201d datasets\uff0c\u4e0e\u5927\u6a21\u578b\u7684\u6570\u636e\u9700\u6c42\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u4f46\u8bba\u6587\u672c\u8eab\u5e76\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory generation", "manipulation", "attraction field", "large-scale datasets"]}}
{"id": "2510.20818", "pdf": "https://arxiv.org/pdf/2510.20818", "abs": "https://arxiv.org/abs/2510.20818", "authors": ["Mateo Guaman Castro", "Sidharth Rajagopal", "Daniel Gorbatov", "Matt Schmittle", "Rohan Baijal", "Octi Zhang", "Rosario Scalise", "Sidharth Talia", "Emma Romig", "Celso de Melo", "Byron Boots", "Abhishek Gupta"], "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "A fundamental challenge in robot navigation lies in learning policies that\ngeneralize across diverse environments while conforming to the unique physical\nconstraints and capabilities of a specific embodiment (e.g., quadrupeds can\nwalk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that\ndecouples semantic planning from embodiment grounding: a generalist planner\nlearns from diverse, open-world data, while a specialist affordance model\nlearns the robot's physical constraints and capabilities in safe, low-cost\nsimulation. We enabled this separation by carefully designing an interface that\nlets a high-level planner propose candidate paths directly in image space that\nthe affordance model then evaluates and re-ranks. Our real-world experiments\nshow that VAMOS achieves higher success rates in both indoor and complex\noutdoor navigation than state-of-the-art model-based and end-to-end learning\nmethods. We also show that our hierarchical design enables cross-embodied\nnavigation across legged and wheeled robots and is easily steerable using\nnatural language. Real-world ablations confirm that the specialist model is key\nto embodiment grounding, enabling a single high-level planner to be deployed\nacross physically distinct wheeled and legged robots. Finally, this model\nsignificantly enhances single-robot reliability, achieving 3X higher success\nrates by rejecting physically infeasible plans. Website:\nhttps://vamos-vla.github.io/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u5176\u4e2d\u8def\u5f84\u89c4\u5212\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86Vision-Language-Action Model (VLA)\uff0c\u8fd9\u4e0e\u5927\u6a21\u578b\u76f8\u5173\uff0c\u7279\u522b\u662f\u5f53\u5b83\u4eec\u7528\u4e8e\u7406\u89e3\u548c\u751f\u6210\u5bfc\u822a\u6307\u4ee4\u65f6\u3002\u7136\u800c\uff0c\u6458\u8981\u5e76\u6ca1\u6709\u660e\u786e\u8bf4\u660e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u800c\u662f\u66f4\u4fa7\u91cd\u4e8e\u673a\u5668\u4eba\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027\u3002", "keywords": ["navigation", "path planning", "Vision-Language-Action Model", "robot navigation"]}}
{"id": "2510.20437", "pdf": "https://arxiv.org/pdf/2510.20437", "abs": "https://arxiv.org/abs/2510.20437", "authors": ["Alvaro Carrizosa-Rendon", "Jian Zhou", "Erik Frisk", "Vicenc Puig", "Fatiha Nejjari"], "title": "Behavior-Aware Online Prediction of Obstacle Occupancy using Zonotopes", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "64th IEEE Conference on Decision and Control", "summary": "Predicting the motion of surrounding vehicles is key to safe autonomous\ndriving, especially in unstructured environments without prior information.\nThis paper proposes a novel online method to accurately predict the occupancy\nsets of surrounding vehicles based solely on motion observations. The approach\nis divided into two stages: first, an Extended Kalman Filter and a Linear\nProgramming (LP) problem are used to estimate a compact zonotopic set of\ncontrol actions; then, a reachability analysis propagates this set to predict\nfuture occupancy. The effectiveness of the method has been validated through\nsimulations in an urban environment, showing accurate and compact predictions\nwithout relying on prior assumptions or prior training data.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on predicting the occupancy of surrounding vehicles for autonomous driving, which falls under the umbrella of trajectory prediction. However, it does not mention or utilize large language models. The techniques employed are based on Kalman Filters and reachability analysis.", "keywords": ["trajectory prediction", "motion prediction", "autonomous driving", "occupancy prediction"]}}
{"id": "2510.20578", "pdf": "https://arxiv.org/pdf/2510.20578", "abs": "https://arxiv.org/abs/2510.20578", "authors": ["Ding Zou", "Feifan Wang", "Mengyu Ge", "Siyuan Fan", "Zongbing Zhang", "Wei Chen", "Lingfeng Wang", "Zhongyou Hu", "Wenrui Yan", "Zhengwei Gao", "Hao Wang", "Weizhao Jin", "Yu Zhang", "Hainan Zhao", "Mingliang Zhang", "Xianxian Xi", "Yaru Zhang", "Wenyuan Li", "Zhengguang Gao", "Yurui Zhu"], "title": "EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The realization of Artificial General Intelligence (AGI) necessitates\nEmbodied AI agents capable of robust spatial perception, effective task\nplanning, and adaptive execution in physical environments. However, current\nlarge language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks\nsuffer from key limitations, including a significant gap between model design\nand agent requirements, an unavoidable trade-off between real-time latency and\nperformance, and the use of unauthentic, offline evaluation metrics. To address\nthese challenges, we propose EmbodiedBrain, a novel vision-language foundation\nmodel available in both 7B and 32B parameter sizes. Our framework features an\nagent-aligned data structure and employs a powerful training methodology that\nintegrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group\nRelative Policy Optimization (Step-GRPO), which boosts long-horizon task\nsuccess by integrating preceding steps as Guided Precursors. Furthermore, we\nincorporate a comprehensive reward system, including a Generative Reward Model\n(GRM) accelerated at the infrastructure level, to improve training efficiency.\nFor enable thorough validation, we establish a three-part evaluation system\nencompassing General, Planning, and End-to-End Simulation Benchmarks,\nhighlighted by the proposal and open-sourcing of a novel, challenging\nsimulation environment. Experimental results demonstrate that EmbodiedBrain\nachieves superior performance across all metrics, establishing a new\nstate-of-the-art for embodied foundation models. Towards paving the way for the\nnext generation of generalist embodied agents, we open-source all of our data,\nmodel weight, and evaluating methods, which are available at\nhttps://zterobot.github.io/EmbodiedBrain.github.io.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aEmbodiedBrain\u7684\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4efb\u52a1\u89c4\u5212\u901a\u5e38\u6d89\u53ca\u5bf9\u672a\u6765\u72b6\u6001\u7684\u9884\u6d4b\uff0c\u5e76\u4e14\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001LLMs\uff08MLLMs\uff09\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["large language models", "LLMs", "foundation model", "embodied AI", "task planning"]}}
