{"id": "2510.26909", "pdf": "https://arxiv.org/pdf/2510.26909", "abs": "https://arxiv.org/abs/2510.26909", "authors": ["Tim Windecker", "Manthan Patel", "Moritz Reuss", "Richard Schwarzkopf", "Cesar Cadena", "Rudolf Lioutikov", "Marco Hutter", "Jonas Frey"], "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models", "categories": ["cs.RO"], "comment": "9 pages, 6 figures, under review at IEEE conference", "summary": "Vision-language models demonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models into robotic navigation systems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduce NaviTrace, a high-quality\nVisual Question Answering benchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introduced semantic-aware trace score. This metric combines Dynamic Time\nWarping distance, goal endpoint error, and embodiment-conditioned penalties\nderived from per-pixel semantics and correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization. NaviTrace establishes a scalable and\nreproducible benchmark for real-world robotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86Vision-Language Models (VLMs)\uff0c\u5c5e\u4e8e\u5927\u6a21\u578b\u8303\u7574\u3002\u6458\u8981\u63cf\u8ff0\u4e86\u6a21\u578b\u8f93\u51fa2D\u5bfc\u822a\u8f68\u8ff9\uff0c\u5e76\u4e14\u63d0\u5230\u4e86\u5bfc\u822atrace\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u91cd\u70b9\u662f\u8bc4\u4f30VLMs\u5728\u5bfc\u822a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4f46\u6d89\u53ca\u5230\u4e86\u8f68\u8ff9\u7684\u751f\u6210\u548c\u8bc4\u4f30\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language Models", "VLMs", "navigation trace", "robotic navigation", "foundation models", "2D navigation"]}}
{"id": "2510.27383", "pdf": "https://arxiv.org/pdf/2510.27383", "abs": "https://arxiv.org/abs/2510.27383", "authors": ["Yueyang Wang", "Mehmet Dogar", "Gustav Markkula"], "title": "Realistic pedestrian-driver interaction modelling using multi-agent RL with human perceptual-motor constraints", "categories": ["cs.AI"], "comment": null, "summary": "Modelling pedestrian-driver interactions is critical for understanding human\nroad user behaviour and developing safe autonomous vehicle systems. Existing\napproaches often rely on rule-based logic, game-theoretic models, or\n'black-box' machine learning methods. However, these models typically lack\nflexibility or overlook the underlying mechanisms, such as sensory and motor\nconstraints, which shape how pedestrians and drivers perceive and act in\ninteractive scenarios. In this study, we propose a multi-agent reinforcement\nlearning (RL) framework that integrates both visual and motor constraints of\npedestrian and driver agents. Using a real-world dataset from an unsignalised\npedestrian crossing, we evaluate four model variants, one without constraints,\ntwo with either motor or visual constraints, and one with both, across\nbehavioural metrics of interaction realism. Results show that the combined\nmodel with both visual and motor constraints performs best. Motor constraints\nlead to smoother movements that resemble human speed adjustments during\ncrossing interactions. The addition of visual constraints introduces perceptual\nuncertainty and field-of-view limitations, leading the agents to exhibit more\ncautious and variable behaviour, such as less abrupt deceleration. In this\ndata-limited setting, our model outperforms a supervised behavioural cloning\nmodel, demonstrating that our approach can be effective without large training\ndatasets. Finally, our framework accounts for individual differences by\nmodelling parameters controlling the human constraints as population-level\ndistributions, a perspective that has not been explored in previous work on\npedestrian-vehicle interaction modelling. Overall, our work demonstrates that\nmulti-agent RL with human constraints is a promising modelling approach for\nsimulating realistic road user interactions.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on modeling pedestrian-driver interactions using multi-agent reinforcement learning (RL) and human perceptual-motor constraints. While it doesn't directly involve Large Language Models, it is highly relevant to trajectory prediction, specifically pedestrian and vehicle trajectory prediction in interactive scenarios. The keywords extracted reflect this focus.", "keywords": ["pedestrian trajectory prediction", "vehicle trajectory prediction", "multi-agent reinforcement learning", "human-robot interaction", "autonomous vehicles", "interaction modelling"]}}
{"id": "2510.26915", "pdf": "https://arxiv.org/pdf/2510.26915", "abs": "https://arxiv.org/abs/2510.26915", "authors": ["Zachary Ravichandran", "Fernando Cladera", "Ankit Prabhu", "Jason Hughes", "Varun Murali", "Camillo Taylor", "George J. Pappas", "Vijay Kumar"], "title": "Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Heterogeneous robot teams operating in realistic settings often must\naccomplish complex missions requiring collaboration and adaptation to\ninformation acquired online. Because robot teams frequently operate in\nunstructured environments -- uncertain, open-world settings without prior maps\n-- subtasks must be grounded in robot capabilities and the physical world.\nWhile heterogeneous teams have typically been designed for fixed\nspecifications, generative intelligence opens the possibility of teams that can\naccomplish a wide range of missions described in natural language. However,\ncurrent large language model (LLM)-enabled teaming methods typically assume\nwell-structured and known environments, limiting deployment in unstructured\nenvironments. We present SPINE-HT, a framework that addresses these limitations\nby grounding the reasoning abilities of LLMs in the context of a heterogeneous\nrobot team through a three-stage process. Given language specifications\ndescribing mission goals and team capabilities, an LLM generates grounded\nsubtasks which are validated for feasibility. Subtasks are then assigned to\nrobots based on capabilities such as traversability or perception and refined\ngiven feedback collected during online operation. In simulation experiments\nwith closed-loop perception and control, our framework achieves nearly twice\nthe success rate compared to prior LLM-enabled heterogeneous teaming\napproaches. In real-world experiments with a Clearpath Jackal, a Clearpath\nHusky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an\n87\\% success rate in missions requiring reasoning about robot capabilities and\nrefining subtasks with online feedback. More information is provided at\nhttps://zacravichandran.github.io/SPINE-HT.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on heterogeneous robot collaboration using Large Language Models (LLMs) for task planning and execution in unstructured environments. While it doesn't directly address trajectory prediction as its primary focus, the robot navigation and task execution implicitly involve trajectory planning and adaptation. The grounding of LLM reasoning in robot capabilities and online feedback suggests a connection to adapting trajectories based on real-time information, although this is not explicitly stated. The use of LLMs is a strong indicator of relevance.", "keywords": ["Large Language Models", "LLMs", "robot navigation", "task planning", "unstructured environments"]}}
{"id": "2510.26940", "pdf": "https://arxiv.org/pdf/2510.26940", "abs": "https://arxiv.org/abs/2510.26940", "authors": ["Ashwin Kumar", "Hanyu Zhang", "David A. Schweidel", "William Yeoh"], "title": "Mind the Gaps: Auditing and Reducing Group Inequity in Large-Scale Mobility Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Next location prediction underpins a growing number of mobility, retail, and\npublic-health applications, yet its societal impacts remain largely unexplored.\nIn this paper, we audit state-of-the-art mobility prediction models trained on\na large-scale dataset, highlighting hidden disparities based on user\ndemographics. Drawing from aggregate census data, we compute the difference in\npredictive performance on racial and ethnic user groups and show a systematic\ndisparity resulting from the underlying dataset, resulting in large differences\nin accuracy based on location and user groups. To address this, we propose\nFairness-Guided Incremental Sampling (FGIS), a group-aware sampling strategy\ndesigned for incremental data collection settings. Because individual-level\ndemographic labels are unavailable, we introduce Size-Aware K-Means (SAKM), a\nclustering method that partitions users in latent mobility space while\nenforcing census-derived group proportions. This yields proxy racial labels for\nthe four largest groups in the state: Asian, Black, Hispanic, and White. Built\non these labels, our sampling algorithm prioritizes users based on expected\nperformance gains and current group representation. This method incrementally\nconstructs training datasets that reduce demographic performance gaps while\npreserving overall accuracy. Our method reduces total disparity between groups\nby up to 40\\% with minimal accuracy trade-offs, as evaluated on a state-of-art\nMetaPath2Vec model and a transformer-encoder model. Improvements are most\nsignificant in early sampling stages, highlighting the potential for\nfairness-aware strategies to deliver meaningful gains even in low-resource\nsettings. Our findings expose structural inequities in mobility prediction\npipelines and demonstrate how lightweight, data-centric interventions can\nimprove fairness with little added complexity, especially for low-data\napplications.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u89c4\u6a21\u79fb\u52a8\u9884\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u4f7f\u7528\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u63d0\u5230\u4e86MetaPath2Vec\u6a21\u578b\u548ctransformer-encoder\u6a21\u578b\u3002\u867d\u7136\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u5bf9\u8c61\u662f\u5927\u89c4\u6a21\u7684\u79fb\u52a8\u9884\u6d4b\uff0c\u4e14\u4f7f\u7528\u4e86transformer-encoder\u6a21\u578b\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["mobility prediction", "large-scale dataset", "transformer-encoder model", "MetaPath2Vec"]}}
{"id": "2510.27210", "pdf": "https://arxiv.org/pdf/2510.27210", "abs": "https://arxiv.org/abs/2510.27210", "authors": ["Tao Liu", "Chongyu Wang", "Rongjie Li", "Yingchen Yu", "Xuming He", "Bai Song"], "title": "GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation", "categories": ["cs.AI", "cs.CV"], "comment": "Published in NeurIPS 2025", "summary": "While Multimodal Large Language Models (MLLMs) have advanced GUI navigation\nagents, current approaches face limitations in cross-domain generalization and\neffective history utilization. We present a reasoning-enhanced framework that\nsystematically integrates structured reasoning, action prediction, and history\nsummarization. The structured reasoning component generates coherent\nChain-of-Thought analyses combining progress estimation and decision reasoning,\nwhich inform both immediate action predictions and compact history summaries\nfor future steps. Based on this framework, we train a GUI agent,\n\\textbf{GUI-Rise}, through supervised fine-tuning on pseudo-labeled\ntrajectories and reinforcement learning with Group Relative Policy Optimization\n(GRPO). This framework employs specialized rewards, including a history-aware\nobjective, directly linking summary quality to subsequent action performance.\nComprehensive evaluations on standard benchmarks demonstrate state-of-the-art\nresults under identical training data conditions, with particularly strong\nperformance in out-of-domain scenarios. These findings validate our framework's\nability to maintain robust reasoning and generalization across diverse GUI\nnavigation tasks. Code is available at https://leon022.github.io/GUI-Rise.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on GUI navigation agents using Multimodal Large Language Models (MLLMs). While it doesn't directly address trajectory prediction in the traditional sense (e.g., pedestrian or vehicle trajectory), it does involve predicting a sequence of actions (a 'trajectory') within a GUI environment. The use of MLLMs firmly places it within the scope of Large Language Models. The 'pseudo-labeled trajectories' also hints at a connection to trajectory-like data.", "keywords": ["Multimodal Large Language Models (MLLMs)", "action prediction", "trajectories", "reasoning", "GUI navigation"]}}
{"id": "2510.27333", "pdf": "https://arxiv.org/pdf/2510.27333", "abs": "https://arxiv.org/abs/2510.27333", "authors": ["Hao Cheng", "Yanbo Jiang", "Qingyuan Shi", "Qingwen Meng", "Keyu Chen", "Wenhao Yu", "Jianqiang Wang", "Sifa Zheng"], "title": "Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict", "categories": ["cs.RO"], "comment": null, "summary": "Effective, reliable, and efficient evaluation of autonomous driving safety is\nessential to demonstrate its trustworthiness. Criticality metrics provide an\nobjective means of assessing safety. However, as existing metrics primarily\ntarget longitudinal conflicts, accurately quantifying the risks of lateral\nconflicts - prevalent in urban settings - remains challenging. This paper\nproposes the Modified-Emergency Index (MEI), a metric designed to quantify\nevasive effort in lateral conflicts. Compared to the original Emergency Index\n(EI), MEI refines the estimation of the time available for evasive maneuvers,\nenabling more precise risk quantification. We validate MEI on a public lateral\nconflict dataset based on Argoverse-2, from which we extract over 1,500\nhigh-quality AV conflict cases, including more than 500 critical events. MEI is\nthen compared with the well-established ACT and the widely used PET metrics.\nResults show that MEI consistently outperforms them in accurately quantifying\ncriticality and capturing risk evolution. Overall, these findings highlight MEI\nas a promising metric for evaluating urban conflicts and enhancing the safety\nassessment framework for autonomous driving. The open-source implementation is\navailable at https://github.com/AutoChengh/MEI.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on evaluating autonomous driving safety, specifically in lateral conflict scenarios. While it doesn't directly involve Large Language Models, it utilizes trajectory data from the Argoverse-2 dataset and deals with predicting and evaluating the criticality of vehicle trajectories. Therefore, it has a moderate relevance to trajectory prediction.", "keywords": ["trajectory prediction", "autonomous driving", "lateral conflict", "Argoverse-2", "criticality metrics"]}}
{"id": "2510.27558", "pdf": "https://arxiv.org/pdf/2510.27558", "abs": "https://arxiv.org/abs/2510.27558", "authors": ["Sushil Samuel Dinesh", "Shinkyu Park"], "title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "This paper presents a framework that leverages pre-trained foundation models\nfor robotic manipulation without domain-specific training. The framework\nintegrates off-the-shelf models, combining multimodal perception from\nfoundation models with a general-purpose reasoning model capable of robust task\nsequencing. Scene graphs, dynamically maintained within the framework, provide\nspatial awareness and enable consistent reasoning about the environment. The\nframework is evaluated through a series of tabletop robotic manipulation\nexperiments, and the results highlight its potential for building robotic\nmanipulation systems directly on top of off-the-shelf foundation models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes foundation models (a type of large model) for robotic manipulation, which implicitly involves predicting the robot's trajectory to achieve the desired task. While not explicitly focused on trajectory prediction in the traditional sense, the language-to-action aspect and the need for accurate long-horizon manipulation suggest a connection to trajectory planning and execution. The scene graph provides spatial awareness which is crucial for trajectory prediction.", "keywords": ["foundation models", "large language models", "robotic manipulation", "scene graphs", "language-to-action"]}}
{"id": "2510.27623", "pdf": "https://arxiv.org/pdf/2510.27623", "abs": "https://arxiv.org/abs/2510.27623", "authors": ["Qiusi Zhan", "Hyeonjeong Ha", "Rui Yang", "Sirui Xu", "Hanyang Chen", "Liang-Yan Gui", "Yu-Xiong Wang", "Huan Zhang", "Heng Ji", "Daniel Kang"], "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive Trigger Learning", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5177\u8eab\u667a\u80fd\u4f53\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u89c6\u89c9\u540e\u95e8\u653b\u51fb\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5177\u8eab\u667a\u80fd\u4f53\u901a\u5e38\u9700\u8981\u8fdb\u884c\u8def\u5f84\u89c4\u5212\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u540c\u65f6\uff0c\u8bba\u6587\u660e\u786e\u63d0\u53ca\u4e86MLLM\uff0c\u4e0e\u5927\u6a21\u578b\u4e3b\u9898\u76f8\u5173\u3002", "keywords": ["Multimodal large language models", "MLLMs", "embodied agents", "visual inputs", "planning"]}}
{"id": "2510.27630", "pdf": "https://arxiv.org/pdf/2510.27630", "abs": "https://arxiv.org/abs/2510.27630", "authors": ["Dayuan Fu", "Yunze Wu", "Xiaojie Cai", "Lyumanshan Ye", "Shijie Xia", "Zhen Huang", "Weiye Si", "Tianze Xu", "Jie Sun", "Keyu Li", "Mohan Jiang", "Junfei Wang", "Qishuo Hua", "Pengrui Lu", "Yang Xiao", "Pengfei Liu"], "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout for Long-Horizon Task Training", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528LLM agent\u8fdb\u884c\u957f\u7a0b\u4efb\u52a1\u8bad\u7ec3\uff0c\u5e76\u5229\u7528\u4eba\u673a\u4ea4\u4e92\u63d0\u5347LLM\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662fagent\u7684\u884c\u4e3a\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\uff0c\u4e14\u8bba\u6587\u91cd\u70b9\u5728\u4e8e\u63d0\u5347agent\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u884c\u52a8\u80fd\u529b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002 \u8bba\u6587\u4e2d\u63d0\u5230\u4e86\u957fhorizon\u7684\u4efb\u52a1\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u957f\u671f\u7684\u8f68\u8ff9\u89c4\u5212\uff0c\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002", "keywords": ["Large Language Model", "LLM agents", "long-horizon tasks", "human-agent interaction", "GLM-4.5"]}}
{"id": "2510.27255", "pdf": "https://arxiv.org/pdf/2510.27255", "abs": "https://arxiv.org/abs/2510.27255", "authors": ["Yehna Kim andYoung-Eun Kim", "Seong-Whan Lee"], "title": "Enhancing Spatio-Temporal Zero-shot Action Recognition with Language-driven Description Attributes", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated impressive capabilities in\nzero-shot action recognition by learning to associate video embeddings with\nclass embeddings. However, a significant challenge arises when relying solely\non action classes to provide semantic context, particularly due to the presence\nof multi-semantic words, which can introduce ambiguity in understanding the\nintended concepts of actions. To address this issue, we propose an innovative\napproach that harnesses web-crawled descriptions, leveraging a large-language\nmodel to extract relevant keywords. This method reduces the need for human\nannotators and eliminates the laborious manual process of attribute data\ncreation. Additionally, we introduce a spatio-temporal interaction module\ndesigned to focus on objects and action units, facilitating alignment between\ndescription attributes and video content. In our zero-shot experiments, our\nmodel achieves impressive results, attaining accuracies of 81.0%, 53.1%, and\n68.9% on UCF-101, HMDB-51, and Kinetics-600, respectively, underscoring the\nmodel's adaptability and effectiveness across various downstream tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on action recognition using vision-language models (VLMs) and large language models (LLMs) to enhance spatio-temporal understanding. While not directly addressing trajectory prediction, the spatio-temporal aspect and the use of LLMs connect to the broader themes of understanding movement and employing large models for vision-related tasks. The action recognition task inherently involves understanding the temporal evolution of actions, which is related to trajectory prediction. The use of LLMs is a direct connection to the second major theme.", "keywords": ["large-language model", "spatio-temporal", "action recognition", "Vision-Language Models"]}}
