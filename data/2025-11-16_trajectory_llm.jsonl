{"id": "2511.10203", "pdf": "https://arxiv.org/pdf/2511.10203", "abs": "https://arxiv.org/abs/2511.10203", "authors": ["Stephane Da Silva Martins", "Emanuel Aldea", "Sylvie Le H\u00e9garat-Mascle"], "title": "VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Paper accepted at WACV 2026", "summary": "Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u793e\u4ea4\u4e92\u52a8\u5efa\u6a21\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u751f\u6210\u66f4\u903c\u771f\u548c\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u7684\u8f68\u8ff9\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u7684\u7814\u7a76\u76ee\u6807\u4e00\u81f4\u3002\u8bba\u6587\u4e2d\u4f7f\u7528\u7684Transformer\u67b6\u6784\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5e38\u7528\u7684\u6280\u672f\u76f8\u5173\uff0c\u4f46\u6b64\u5904\u4e3b\u8981\u5e94\u7528\u4e8e\u89c6\u89c9\u548c\u610f\u56fe\u7406\u89e3\u3002", "keywords": ["trajectory prediction", "multi-agent", "social attention", "transformer", "intent-aware", "goal-conditioned"]}}
{"id": "2511.10418", "pdf": "https://arxiv.org/pdf/2511.10418", "abs": "https://arxiv.org/abs/2511.10418", "authors": ["Yaqiao Zhu", "Hongkai Wen", "Mark Birkin", "Man Luo"], "title": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models", "categories": ["cs.DB"], "comment": null, "summary": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u53ca\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u57ce\u5e02\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4e14\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cPrediction\u201d\u4efb\u52a1\uff0c\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5c5e\u4e8e\u9884\u6d4b\u5206\u6790\u7684\u8303\u7574\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "urban computing", "prediction"]}}
{"id": "2511.09735", "pdf": "https://arxiv.org/pdf/2511.09735", "abs": "https://arxiv.org/abs/2511.09735", "authors": ["Ahmed Alia", "Mohcine Chraibi", "Armin Seyfried"], "title": "Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 9 figures, 4 tables", "summary": "In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86Social LSTM\u7b49\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "pedestrian trajectory prediction", "Social LSTM", "deep learning", "collision avoidance", "dynamic occupancy modeling"]}}
{"id": "2511.10411", "pdf": "https://arxiv.org/pdf/2511.10411", "abs": "https://arxiv.org/abs/2511.10411", "authors": ["Benjamin Stoler", "Jonathan Francis", "Jean Oh"], "title": "LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction", "categories": ["cs.RO"], "comment": "8 pages, 3 figures", "summary": "Methods for trajectory prediction in Autonomous Driving must contend with rare, safety-critical scenarios that make reliance on real-world data collection alone infeasible. To assess robustness under such conditions, we propose new long-tail evaluation settings that repartition datasets to create challenging out-of-distribution (OOD) test sets. We first introduce a safety-informed scenario factorization framework, which disentangles scenarios into discrete ego and social contexts. Building on analogies to compositional zero-shot image-labeling in Computer Vision, we then hold out novel context combinations to construct challenging closed-world and open-world settings. This process induces OOD performance gaps in future motion prediction of 5.0% and 14.7% in closed-world and open-world settings, respectively, relative to in-distribution performance for a state-of-the-art baseline. To improve generalization, we extend task-modular gating networks to operate within trajectory prediction models, and develop an auxiliary, difficulty-prediction head to refine internal representations. Our strategies jointly reduce the OOD performance gaps to 2.8% and 11.5% in the two settings, respectively, while still improving in-distribution performance.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u63d0\u5230\u4e86zero-shot generalization\uff0c\u8fd9\u4e0e\u5927\u6a21\u578b\u7684\u4e00\u4e9b\u7814\u7a76\u65b9\u5411\u6709\u6f5c\u5728\u8054\u7cfb\u3002", "keywords": ["trajectory prediction", "Autonomous Driving", "zero-shot generalization", "out-of-distribution", "motion prediction"]}}
{"id": "2511.10110", "pdf": "https://arxiv.org/pdf/2511.10110", "abs": "https://arxiv.org/abs/2511.10110", "authors": ["Kamil Dreczkowski", "Pietro Vitiello", "Vitalis Vosylius", "Edward Johns"], "title": "Learning a Thousand Tasks in a Day", "categories": ["cs.RO"], "comment": "This is the author's version of the work. It is posted here by permission of the AAAS for personal use, not for redistribution. The definitive version was published in Science Robotics on 12 November 2025, DOI: https://www.science.org/doi/10.1126/scirobotics.adv7594. Link to project website: https://www.robot-learning.uk/learning-1000-tasks", "summary": "Humans are remarkably efficient at learning tasks from demonstrations, but today's imitation learning methods for robot manipulation often require hundreds or thousands of demonstrations per task. We investigate two fundamental priors for improving learning efficiency: decomposing manipulation trajectories into sequential alignment and interaction phases, and retrieval-based generalisation. Through 3,450 real-world rollouts, we systematically study this decomposition. We compare different design choices for the alignment and interaction phases, and examine generalisation and scaling trends relative to today's dominant paradigm of behavioural cloning with a single-phase monolithic policy. In the few-demonstrations-per-task regime (<10 demonstrations), decomposition achieves an order of magnitude improvement in data efficiency over single-phase learning, with retrieval consistently outperforming behavioural cloning for both alignment and interaction. Building on these insights, we develop Multi-Task Trajectory Transfer (MT3), an imitation learning method based on decomposition and retrieval. MT3 learns everyday manipulation tasks from as little as a single demonstration each, whilst also generalising to novel object instances. This efficiency enables us to teach a robot 1,000 distinct everyday tasks in under 24 hours of human demonstrator time. Through 2,200 additional real-world rollouts, we reveal MT3's capabilities and limitations across different task families. Videos of our experiments can be found on at https://www.robot-learning.uk/learning-1000-tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6a21\u4eff\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u5206\u89e3\u8f68\u8ff9\u548c\u57fa\u4e8e\u68c0\u7d22\u7684\u6cdb\u5316\u6765\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002\u867d\u7136\u8bba\u6587\u6d89\u53ca\u8f68\u8ff9\u7684\u5b66\u4e60\u548c\u8f6c\u79fb\uff08trajectory transfer\uff09\uff0c\u4f46\u5e76\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u9002\u4e2d\u3002", "keywords": ["trajectory transfer", "imitation learning", "manipulation trajectories", "robot manipulation"]}}
{"id": "2511.10403", "pdf": "https://arxiv.org/pdf/2511.10403", "abs": "https://arxiv.org/abs/2511.10403", "authors": ["Mingxing Peng", "Ruoyu Yao", "Xusen Guo", "Jun Ma"], "title": "nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 3 figures", "summary": "Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u95ed\u73af\u89c4\u5212\u57fa\u51c6\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u5b66\u4e60\u7684\u53cd\u5e94\u5f0f\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6765\u63d0\u9ad8\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u3002\u867d\u7136\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u548c\u8f68\u8ff9\u9884\u6d4b\uff08\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff09\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u53cd\u5e94\u5f0f\u667a\u80fd\u4f53\u6a21\u578b\u53ef\u80fd\u4f7f\u7528\u4e86\u6df1\u5ea6\u5b66\u4e60\uff0c\u4f46\u6458\u8981\u4e2d\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["closed-loop planning", "autonomous driving", "multi-agent simulation", "reactive agents", "trajectory prediction"]}}
{"id": "2511.09820", "pdf": "https://arxiv.org/pdf/2511.09820", "abs": "https://arxiv.org/abs/2511.09820", "authors": ["Jeongho Min", "Dongyoung Kim", "Jaehyup Lee"], "title": "From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to WACV 2026, 10pages, 4 figures", "summary": "Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a Large Language Model (LLM) for location inference in a cross-view image retrieval task, which is relevant to autonomous navigation. Autonomous navigation is closely related to trajectory prediction, although this paper doesn't directly address trajectory prediction itself. The LLM component contributes to the relevance.", "keywords": ["Large Language Models", "LLM", "autonomous navigation", "location inference"]}}
{"id": "2511.09737", "pdf": "https://arxiv.org/pdf/2511.09737", "abs": "https://arxiv.org/abs/2511.09737", "authors": ["Bram Grooten", "Patrick MacAlpine", "Kaushik Subramanian", "Peter Stone", "Peter R. Wurman"], "title": "Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted as an oral at AAAI 2026. For code and videos, please see https://github.com/bramgrooten/sparc", "summary": "Generalization to unseen environments is a significant challenge in the field of robotics and control. In this work, we focus on contextual reinforcement learning, where agents act within environments with varying contexts, such as self-driving cars or quadrupedal robots that need to operate in different terrains or weather conditions than they were trained for. We tackle the critical task of generalizing to out-of-distribution (OOD) settings, without access to explicit context information at test time. Recent work has addressed this problem by training a context encoder and a history adaptation module in separate stages. While promising, this two-phase approach is cumbersome to implement and train. We simplify the methodology and introduce SPARC: single-phase adaptation for robust control. We test SPARC on varying contexts within the high-fidelity racing simulator Gran Turismo 7 and wind-perturbed MuJoCo environments, and find that it achieves reliable and robust OOD generalization.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on out-of-distribution generalization in robotics and control, specifically self-driving cars. While it doesn't explicitly mention trajectory prediction or large language models, the context of self-driving cars inherently involves trajectory prediction. The use of reinforcement learning for control also suggests a potential link to predicting future states, which is related to trajectory prediction. However, the absence of any mention of LLMs lowers the relevance score.", "keywords": ["self-driving cars", "reinforcement learning", "out-of-distribution generalization", "robotics", "control"]}}
{"id": "2511.09883", "pdf": "https://arxiv.org/pdf/2511.09883", "abs": "https://arxiv.org/abs/2511.09883", "authors": ["Liheng Zhang", "Jin Wang", "Hui Li", "Bingfeng Zhang", "Weifeng Liu"], "title": "HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u57283D\u7406\u89e3\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5982\u4f55\u538b\u7f293D tokens\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u5904\u7406\uff0c\u5e76\u4e14\u4e0e3D\u6570\u636e\u76f8\u5173\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5904\u74063D\u7a7a\u95f4\u6570\u636e\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u8bc4\u5206\u4e3a0.6\u3002", "keywords": ["Vision-Language Models", "VLMs", "Large Language Model", "LLM", "3D", "point cloud"]}}
{"id": "2511.10586", "pdf": "https://arxiv.org/pdf/2511.10586", "abs": "https://arxiv.org/abs/2511.10586", "authors": ["Omid Mirzaeedodangeh", "Eliot Shekhtman", "Nikolai Matni", "Lars Lindemann"], "title": "Safe Planning in Interactive Environments via Iterative Policy Updates and Adversarially Robust Conformal Prediction", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "Safe planning of an autonomous agent in interactive environments -- such as the control of a self-driving vehicle among pedestrians and human-controlled vehicles -- poses a major challenge as the behavior of the environment is unknown and reactive to the behavior of the autonomous agent. This coupling gives rise to interaction-driven distribution shifts where the autonomous agent's control policy may change the environment's behavior, thereby invalidating safety guarantees in existing work. Indeed, recent works have used conformal prediction (CP) to generate distribution-free safety guarantees using observed data of the environment. However, CP's assumption on data exchangeability is violated in interactive settings due to a circular dependency where a control policy update changes the environment's behavior, and vice versa. To address this gap, we propose an iterative framework that robustly maintains safety guarantees across policy updates by quantifying the potential impact of a planned policy update on the environment's behavior. We realize this via adversarially robust CP where we perform a regular CP step in each episode using observed data under the current policy, but then transfer safety guarantees across policy updates by analytically adjusting the CP result to account for distribution shifts. This adjustment is performed based on a policy-to-trajectory sensitivity analysis, resulting in a safe, episodic open-loop planner. We further conduct a contraction analysis of the system providing conditions under which both the CP results and the policy updates are guaranteed to converge. We empirically demonstrate these safety and convergence guarantees on a two-dimensional car-pedestrian case study. To the best of our knowledge, these are the first results that provide valid safety guarantees in such interactive settings.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5b89\u5168\u89c4\u5212\uff0c\u6d89\u53ca\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u884c\u4eba\u7b49\u4ea4\u4e92\uff0c\u4f7f\u7528\u4e86Conformal Prediction\u65b9\u6cd5\u6765\u4fdd\u8bc1\u5b89\u5168\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u9884\u6d4b\u73af\u5883\u884c\u4e3a\u5e76\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u4e0e\u5927\u6a21\u578b\u7684\u76f8\u5173\u6027\u8f83\u4f4e\u3002", "keywords": ["trajectory prediction", "safe planning", "autonomous agent", "interactive environments", "pedestrian", "self-driving vehicle", "conformal prediction"]}}
{"id": "2511.10218", "pdf": "https://arxiv.org/pdf/2511.10218", "abs": "https://arxiv.org/abs/2511.10218", "authors": ["Haolong Xiang", "Peisi Wang", "Xiaolong Xu", "Kun Yi", "Xuyun Zhang", "Quanzheng Sheng", "Amin Beheshti", "Wei Fan"], "title": "MTP: Exploring Multimodal Urban Traffic Profiling with Modality Augmentation and Spectrum Fusion", "categories": ["cs.AI"], "comment": null, "summary": "With rapid urbanization in the modern era, traffic signals from various sensors have been playing a significant role in monitoring the states of cities, which provides a strong foundation in ensuring safe travel, reducing traffic congestion and optimizing urban mobility. Most existing methods for traffic signal modeling often rely on the original data modality, i.e., numerical direct readings from the sensors in cities. However, this unimodal approach overlooks the semantic information existing in multimodal heterogeneous urban data in different perspectives, which hinders a comprehensive understanding of traffic signals and limits the accurate prediction of complex traffic dynamics. To address this problem, we propose a novel \\textit{M}ultimodal framework, \\textit{MTP}, for urban \\textit{T}raffic \\textit{P}rofiling, which learns multimodal features through numeric, visual, and textual perspectives. The three branches drive for a multimodal perspective of urban traffic signal learning in the frequency domain, while the frequency learning strategies delicately refine the information for extraction. Specifically, we first conduct the visual augmentation for the traffic signals, which transforms the original modality into frequency images and periodicity images for visual learning. Also, we augment descriptive texts for the traffic signals based on the specific topic, background information and item description for textual learning. To complement the numeric information, we utilize frequency multilayer perceptrons for learning on the original modality. We design a hierarchical contrastive learning on the three branches to fuse the spectrum of three modalities. Finally, extensive experiments on six real-world datasets demonstrate superior performance compared with the state-of-the-art approaches.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u57ce\u5e02\u4ea4\u901a\u4fe1\u53f7\u7684\u5efa\u6a21\u548c\u5206\u6790\uff0c\u901a\u8fc7\u878d\u5408\u6570\u503c\u3001\u89c6\u89c9\u548c\u6587\u672c\u7b49\u591a\u6a21\u6001\u4fe1\u606f\u6765\u63d0\u9ad8\u4ea4\u901a\u52a8\u529b\u5b66\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u867d\u7136\u6d89\u53ca\u5230\u4e86\u4ea4\u901a\u52a8\u529b\u5b66\u9884\u6d4b\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u5e7f\u4e49\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5e76\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["traffic prediction", "multimodal", "traffic dynamics"]}}
{"id": "2511.09955", "pdf": "https://arxiv.org/pdf/2511.09955", "abs": "https://arxiv.org/abs/2511.09955", "authors": ["Uday Bhaskar", "Rishabh Bhattacharya", "Avinash Patel", "Sarthak Khoche", "Praveen Anil Kulkarni", "Naresh Manwani"], "title": "Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peers' per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant mAP@0.5 boost ($31.12\\%$ to $46.61\\%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10\\%$) leads to further performance gains, reaching $57.97\\%$ mAP@0.5 on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on object detection for autonomous driving using Vision-Language Models (VLMs) to generate pseudo-labels. While it doesn't directly address trajectory prediction, autonomous driving is a common application area for trajectory prediction. The use of VLMs connects it to the large model aspect. The mention of autonomous driving also indicates a potential downstream application in trajectory prediction.", "keywords": ["vision-language models", "VLMs", "autonomous driving", "object detection", "foundation models"]}}
{"id": "2511.10017", "pdf": "https://arxiv.org/pdf/2511.10017", "abs": "https://arxiv.org/abs/2511.10017", "authors": ["Xinyi Wang", "Xun Yang", "Yanlong Xu", "Yuchen Wu", "Zhen Li", "Na Zhao"], "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "NeurIPS 2025", "summary": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce83D\u5177\u8eab\u63a8\u7406\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b3D\u573a\u666f\u4e2d\u53ef\u4f9b\u64cd\u4f5c\u7684\u5143\u7d20\u7684\u7a7a\u95f4\u4f4d\u7f6e\u3001\u8fd0\u52a8\u7c7b\u578b\u548c\u8fd0\u52a8\u8f74\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u8fd0\u52a8\u7c7b\u578b\u548c\u8fd0\u52a8\u8f74\u7684\u9884\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "MLLMs", "motion type", "motion axis", "3D Embodied Reasoning"]}}
{"id": "2511.10020", "pdf": "https://arxiv.org/pdf/2511.10020", "abs": "https://arxiv.org/abs/2511.10020", "authors": ["Yuxin Jiang", "Wei Luo", "Hui Zhang", "Qiyu Chen", "Haiming Yao", "Weiming Shen", "Yunkang Cao"], "title": "Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose Anomagic, a zero-shot anomaly generation method that produces semantically coherent anomalies without requiring any exemplar anomalies. By unifying both visual and textual cues through a crossmodal prompt encoding scheme, Anomagic leverages rich contextual information to steer an inpainting-based generation pipeline. A subsequent contrastive refinement strategy enforces precise alignment between synthesized anomalies and their masks, thereby bolstering downstream anomaly detection accuracy. To facilitate training, we introduce AnomVerse, a collection of 12,987 anomaly-mask-caption triplets assembled from 13 publicly available datasets, where captions are automatically generated by multimodal large language models using structured visual prompts and template-based textual hints. Extensive experiments demonstrate that Anomagic trained on AnomVerse can synthesize more realistic and varied anomalies than prior methods, yielding superior improvements in downstream anomaly detection. Furthermore, Anomagic can generate anomalies for any normal-category image using user-defined prompts, establishing a versatile foundation model for anomaly generation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on anomaly generation using crossmodal prompts and large language models. While it doesn't directly address trajectory prediction, it utilizes multimodal large language models for caption generation and introduces a foundation model for anomaly generation, linking it to the broader theme of large models. The connection to trajectory prediction is weak but present through the potential applicability of anomaly detection in trajectory analysis.", "keywords": ["large language models", "foundation models", "crossmodal prompt", "anomaly generation"]}}
{"id": "2511.10091", "pdf": "https://arxiv.org/pdf/2511.10091", "abs": "https://arxiv.org/abs/2511.10091", "authors": ["Qilang Ye", "Yu Zhou", "Lian He", "Jie Zhang", "Xuanming Guo", "Jiayu Zhang", "Mingkui Tan", "Weicheng Xie", "Yue Sun", "Tao Tan", "Xiaochen Yuan", "Ghada Khoriba", "Zitong Yu"], "title": "SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2026 Main Track", "summary": "Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses using Large Language Models (LLMs) for action recognition based on human skeleton data. While it doesn't directly address trajectory prediction, the use of motion information and temporal modeling of skeleton signals has some relevance. The core focus is on action recognition using LLMs, which connects to the large language model aspect of the prompt.", "keywords": ["Large Language Models", "LLMs", "skeleton", "action recognition", "motion information"]}}
