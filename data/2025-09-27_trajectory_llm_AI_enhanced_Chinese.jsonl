{"id": "2509.20499", "pdf": "https://arxiv.org/pdf/2509.20499", "abs": "https://arxiv.org/abs/2509.20499", "authors": ["Boqi Li", "Siyuan Li", "Weiyi Wang", "Anran Li", "Zhong Cao", "Henry X. Liu"], "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "With the rapid progress of foundation models and robotics, vision-language\nnavigation (VLN) has emerged as a key task for embodied agents with broad\npractical applications. We address VLN in continuous environments, a\nparticularly challenging setting where an agent must jointly interpret natural\nlanguage instructions, perceive its surroundings, and plan low-level actions.\nWe propose a zero-shot framework that integrates a simplified yet effective\nwaypoint predictor with a multimodal large language model (MLLM). The predictor\noperates on an abstract obstacle map, producing linearly reachable waypoints,\nwhich are incorporated into a dynamically updated topological graph with\nexplicit visitation records. The graph and visitation information are encoded\ninto the prompt, enabling reasoning over both spatial structure and exploration\nhistory to encourage exploration and equip MLLM with local path planning for\nerror correction. Extensive experiments on R2R-CE and RxR-CE show that our\nmethod achieves state-of-the-art zero-shot performance, with success rates of\n41% and 36%, respectively, outperforming prior state-of-the-art methods.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a(VLN)\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u8def\u5f84\u89c4\u5212\u548c\u52a8\u4f5c\u9884\u6d4b\uff08\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff09\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4f7f\u7528\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\uff0c\u5e76\u5c06\u8f68\u8ff9\u9884\u6d4b\u5668\u4e0eMLLM\u7ed3\u5408\uff0c\u5b9e\u73b0zero-shot VLN\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u90fd\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "large language model", "MLLM", "vision-language navigation", "waypoint prediction", "path planning", "action prediction"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u822a\u70b9\u9884\u6d4b\u5668\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8fde\u7eed\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\uff0c\u5e76\u5728R2R-CE\u548cRxR-CE\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u662f\u5177\u8eab\u667a\u80fd\u4f53\u7684\u5173\u952e\u4efb\u52a1\uff0c\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u9762\u4e34\u7740\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u73af\u5883\u611f\u77e5\u548c\u4f4e\u7ea7\u52a8\u4f5c\u89c4\u5212\u7684\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u822a\u70b9\u9884\u6d4b\u5668\u548c\u4e00\u4e2a\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\uff0c\u822a\u70b9\u9884\u6d4b\u5668\u5728\u62bd\u8c61\u969c\u788d\u7269\u5730\u56fe\u4e0a\u751f\u6210\u7ebf\u6027\u53ef\u8fbe\u7684\u822a\u70b9\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u52a8\u6001\u66f4\u65b0\u7684\u62d3\u6251\u56fe\u4e2d\uff0c\u901a\u8fc7\u7f16\u7801\u7a7a\u95f4\u7ed3\u6784\u548c\u63a2\u7d22\u5386\u53f2\u6765\u4fc3\u8fdb\u63a2\u7d22\u548c\u7ea0\u9519\u3002", "result": "\u5728R2R-CE\u548cRxR-CE\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u6210\u529f\u7387\u5206\u522b\u4e3a41%\u548c36%\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u96f6\u6837\u672c\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8fde\u7eed\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "summary_zh": "\u968f\u7740\u57fa\u7840\u6a21\u578b\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u98de\u901f\u53d1\u5c55\uff0c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u5df2\u6210\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u7684\u5173\u952e\u4efb\u52a1\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u7814\u7a76\u4e86\u8fde\u7eed\u73af\u5883\u4e0b\u7684VLN\uff0c\u8fd9\u662f\u4e00\u4e2a\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\uff0c\u667a\u80fd\u4f53\u5fc5\u987b\u8054\u5408\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u611f\u77e5\u5176\u5468\u56f4\u73af\u5883\uff0c\u5e76\u89c4\u5212\u4f4e\u7ea7\u52a8\u4f5c\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u96f6\u6837\u672c\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u7b80\u5316\u7684\u4f46\u6709\u6548\u7684\u822a\u70b9\u9884\u6d4b\u5668\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u3002\u8be5\u9884\u6d4b\u5668\u5728\u62bd\u8c61\u969c\u788d\u7269\u5730\u56fe\u4e0a\u8fd0\u884c\uff0c\u751f\u6210\u7ebf\u6027\u53ef\u8fbe\u7684\u822a\u70b9\uff0c\u8fd9\u4e9b\u822a\u70b9\u88ab\u5408\u5e76\u5230\u5177\u6709\u663e\u5f0f\u8bbf\u95ee\u8bb0\u5f55\u7684\u52a8\u6001\u66f4\u65b0\u7684\u62d3\u6251\u56fe\u4e2d\u3002\u56fe\u548c\u8bbf\u95ee\u4fe1\u606f\u88ab\u7f16\u7801\u5230\u63d0\u793a\u4e2d\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u7a7a\u95f4\u7ed3\u6784\u548c\u63a2\u7d22\u5386\u53f2\u8fdb\u884c\u63a8\u7406\uff0c\u4ee5\u9f13\u52b1\u63a2\u7d22\u5e76\u4f7fMLLM\u5177\u5907\u7528\u4e8e\u7ea0\u9519\u7684\u672c\u5730\u8def\u5f84\u89c4\u5212\u3002\u5728R2R-CE\u548cRxR-CE\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u6210\u529f\u7387\u5206\u522b\u4e3a41\uff05\u548c36\uff05\uff0c\u4f18\u4e8e\u5148\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2509.20709", "pdf": "https://arxiv.org/pdf/2509.20709", "abs": "https://arxiv.org/abs/2509.20709", "authors": ["Mani Amani", "Reza Akhavian"], "title": "Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor", "categories": ["cs.RO"], "comment": null, "summary": "Integrating natural language (NL) prompts into robotic mission planning has\nattracted significant interest in recent years. In the construction domain,\nBuilding Information Models (BIM) encapsulate rich NL descriptions of the\nenvironment. We present a novel framework that fuses NL directives with\nBIM-derived semantic maps via a Beta-Bernoulli Bayesian fusion by interpreting\nthe LLM as a sensor: each obstacle's design-time repulsive coefficient is\ntreated as a Beta(alpha, beta) random variable and LLM-returned danger scores\nare incorporated as pseudo-counts to update alpha and beta. The resulting\nposterior mean yields a continuous, context-aware repulsive gain that augments\na Euclidean-distance-based potential field for cost heuristics. By adjusting\ngains based on sentiment and context inferred from user prompts, our method\nguides robots along safer, more context-aware paths. This provides a\nnumerically stable method that can chain multiple natural commands and prompts\nfrom construction workers and foreman to enable planning while giving\nflexibility to be integrated in any learned or classical AI framework.\nSimulation results demonstrate that this Beta-Bernoulli fusion yields both\nqualitative and quantitative improvements in path robustness and validity.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on robot path planning, which is related to trajectory prediction. It also explicitly uses a Large Language Model (LLM) as a sensor to incorporate natural language directives into the planning process. The combination of path planning and LLMs makes it highly relevant.", "keywords": ["robot path planning", "trajectory prediction", "large language model", "LLM", "natural language", "BIM", "Beta-Bernoulli fusion"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7 Beta-Bernoulli \u8d1d\u53f6\u65af\u878d\u5408\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4e0e BIM \u5bfc\u51fa\u7684\u8bed\u4e49\u5730\u56fe\u878d\u5408\uff0c\u4ece\u800c\u5f15\u5bfc\u673a\u5668\u4eba\u5728\u5efa\u7b51\u73af\u5883\u4e2d\u6cbf\u66f4\u5b89\u5168\u3001\u66f4\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u8def\u5f84\u79fb\u52a8\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u5c06\u81ea\u7136\u8bed\u8a00 (NL) \u63d0\u793a\u96c6\u6210\u5230\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u6781\u5927\u5174\u8da3\u3002\u5728\u5efa\u7b51\u9886\u57df\uff0c\u5efa\u7b51\u4fe1\u606f\u6a21\u578b (BIM) \u5c01\u88c5\u4e86\u4e30\u5bcc\u7684\u73af\u5883 NL \u63cf\u8ff0\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06 LLM \u89e3\u91ca\u4e3a\u4f20\u611f\u5668\uff0c\u5c06\u6bcf\u4e2a\u969c\u788d\u7269\u7684\u8bbe\u8ba1\u65f6\u6392\u65a5\u7cfb\u6570\u89c6\u4e3a Beta(alpha, beta) \u968f\u673a\u53d8\u91cf\uff0c\u5e76\u5c06 LLM \u8fd4\u56de\u7684\u5371\u9669\u5206\u6570\u4f5c\u4e3a\u4f2a\u8ba1\u6570\u6765\u66f4\u65b0 alpha \u548c beta\uff0c\u4ece\u800c\u901a\u8fc7 Beta-Bernoulli \u8d1d\u53f6\u65af\u878d\u5408\u6765\u878d\u5408 NL \u6307\u4ee4\u548c BIM \u5bfc\u51fa\u7684\u8bed\u4e49\u5730\u56fe\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd Beta-Bernoulli \u878d\u5408\u5728\u8def\u5f84\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u4ea7\u751f\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7684\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u6839\u636e\u4ece\u7528\u6237\u63d0\u793a\u63a8\u65ad\u7684\u60c5\u611f\u548c\u4e0a\u4e0b\u6587\u8c03\u6574\u589e\u76ca\uff0c\u8be5\u65b9\u6cd5\u5f15\u5bfc\u673a\u5668\u4eba\u5728\u66f4\u5b89\u5168\u3001\u66f4\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u8def\u5f84\u4e0a\u79fb\u52a8\u3002\u8fd9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u503c\u7a33\u5b9a\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u94fe\u63a5\u6765\u81ea\u5efa\u7b51\u5de5\u4eba\u548c\u5de5\u5934\u7684\u591a\u4e2a\u81ea\u7136\u547d\u4ee4\u548c\u63d0\u793a\uff0c\u4ece\u800c\u5728\u89c4\u5212\u7684\u540c\u65f6\u63d0\u4f9b\u7075\u6d3b\u6027\uff0c\u4ee5\u4fbf\u96c6\u6210\u5230\u4efb\u4f55\u5b66\u4e60\u6216\u7ecf\u5178 AI \u6846\u67b6\u4e2d\u3002", "summary_zh": "\u8fd1\u5e74\u6765\uff0c\u5c06\u81ea\u7136\u8bed\u8a00 (NL) \u63d0\u793a\u96c6\u6210\u5230\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u6781\u5927\u5174\u8da3\u3002\u5728\u5efa\u7b51\u9886\u57df\uff0c\u5efa\u7b51\u4fe1\u606f\u6a21\u578b (BIM) \u5c01\u88c5\u4e86\u4e30\u5bcc\u7684\u73af\u5883 NL \u63cf\u8ff0\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7 Beta-Bernoulli \u8d1d\u53f6\u65af\u878d\u5408\u5c06 NL \u6307\u4ee4\u4e0e BIM \u5bfc\u51fa\u7684\u8bed\u4e49\u5730\u56fe\u878d\u5408\uff0c\u65b9\u6cd5\u662f\u5c06 LLM \u89e3\u91ca\u4e3a\u4f20\u611f\u5668\uff1a\u6bcf\u4e2a\u969c\u788d\u7269\u7684\u8bbe\u8ba1\u65f6\u6392\u65a5\u7cfb\u6570\u88ab\u89c6\u4e3a Beta(alpha, beta) \u968f\u673a\u53d8\u91cf\uff0c\u5e76\u4e14 LLM \u8fd4\u56de\u7684\u5371\u9669\u5206\u6570\u88ab\u5408\u5e76\u4e3a\u4f2a\u8ba1\u6570\u4ee5\u66f4\u65b0 alpha \u548c beta\u3002\u7531\u6b64\u4ea7\u751f\u7684\u540e\u9a8c\u5747\u503c\u4ea7\u751f\u4e00\u4e2a\u8fde\u7eed\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6392\u65a5\u589e\u76ca\uff0c\u8be5\u589e\u76ca\u589e\u5f3a\u4e86\u57fa\u4e8e\u6b27\u51e0\u91cc\u5fb7\u8ddd\u79bb\u7684\u52bf\u573a\u4ee5\u7528\u4e8e\u6210\u672c\u542f\u53d1\u5f0f\u3002\u901a\u8fc7\u6839\u636e\u4ece\u7528\u6237\u63d0\u793a\u63a8\u65ad\u7684\u60c5\u611f\u548c\u4e0a\u4e0b\u6587\u8c03\u6574\u589e\u76ca\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5f15\u5bfc\u673a\u5668\u4eba\u5728\u66f4\u5b89\u5168\u3001\u66f4\u7b26\u5408\u4e0a\u4e0b\u6587\u7684\u8def\u5f84\u4e0a\u79fb\u52a8\u3002\u8fd9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u503c\u7a33\u5b9a\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u94fe\u63a5\u6765\u81ea\u5efa\u7b51\u5de5\u4eba\u548c\u5de5\u5934\u7684\u591a\u4e2a\u81ea\u7136\u547d\u4ee4\u548c\u63d0\u793a\uff0c\u4ece\u800c\u5728\u89c4\u5212\u7684\u540c\u65f6\u63d0\u4f9b\u7075\u6d3b\u6027\uff0c\u4ee5\u4fbf\u96c6\u6210\u5230\u4efb\u4f55\u5b66\u4e60\u6216\u7ecf\u5178 AI \u6846\u67b6\u4e2d\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd Beta-Bernoulli \u878d\u5408\u5728\u8def\u5f84\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u4ea7\u751f\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7684\u6539\u8fdb\u3002"}}
{"id": "2509.20501", "pdf": "https://arxiv.org/pdf/2509.20501", "abs": "https://arxiv.org/abs/2509.20501", "authors": ["Kishor Datta Gupta", "Mohd Ariful Haque", "Marufa Kamal", "Ahmed Rafi Hasan", "Md. Mahfuzur Rahman", "Roy George"], "title": "Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 9 figures", "summary": "Traditional clustering techniques often rely solely on similarity in the\ninput data, limiting their ability to capture structural or semantic\nconstraints that are critical in many domains. We introduce the Domain Aware\nRule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal\nclustering framework that incorporates domain specific constraints directly\ninto the representation learning process. DARTVAE extends the VAE architecture\nby embedding explicit rules, semantic representations, and data driven features\ninto a unified latent space, while enforcing constraint compliance through rule\nconsistency and violation penalties in the loss function. Unlike conventional\nclustering methods that rely only on visual similarity or apply rules as post\nhoc filters, DARTVAE treats rules as first class learning signals. The rules\nare generated by LLMs, structured into knowledge graphs, and enforced through a\nloss function combining reconstruction, KL divergence, consistency, and\nviolation penalties. Experiments on aircraft and automotive datasets\ndemonstrate that rule guided clustering produces more operationally meaningful\nand interpretable clusters for example, isolating UAVs, unifying stealth\naircraft, or separating SUVs from sedans while improving traditional clustering\nmetrics. However, the framework faces challenges: LLM generated rules may\nhallucinate or conflict, excessive rules risk overfitting, and scaling to\ncomplex domains increases computational and consistency difficulties. By\ncombining rule encodings with learned representations, DARTVAE achieves more\nmeaningful and consistent clustering outcomes than purely data driven models,\nhighlighting the utility of constraint guided multimodal clustering for\ncomplex, knowledge intensive settings.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses LLMs to generate rules for multimodal clustering, specifically applied to aircraft and automotive datasets. While not directly trajectory prediction, the automotive dataset and the mention of UAVs (Unmanned Aerial Vehicles) suggest a potential connection to moving object analysis, and the LLM usage is explicit. Therefore, it has moderate relevance.", "keywords": ["LLMs", "Large Language Models", "clustering", "automotive datasets", "UAVs"]}, "AI": {"tldr": "DARTVAE \u901a\u8fc7\u5c06\u9886\u57df\u77e5\u8bc6\u89c4\u5219\u878d\u5165\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u8bed\u4e49\u548c\u53ef\u89e3\u91ca\u6027\u7684\u805a\u7c7b\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u6280\u672f\u4ec5\u4f9d\u8d56\u8f93\u5165\u6570\u636e\u7684\u76f8\u4f3c\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u9886\u57df\u76f8\u5173\u7684\u7ed3\u6784\u6216\u8bed\u4e49\u7ea6\u675f\u3002", "method": "DARTVAE \u5c06\u663e\u5f0f\u89c4\u5219\u3001\u8bed\u4e49\u8868\u793a\u548c\u6570\u636e\u9a71\u52a8\u7279\u5f81\u5d4c\u5165\u5230\u7edf\u4e00\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u5e76\u901a\u8fc7\u635f\u5931\u51fd\u6570\u4e2d\u7684\u89c4\u5219\u4e00\u81f4\u6027\u548c\u8fdd\u53cd\u60e9\u7f5a\u6765\u5f3a\u5236\u6267\u884c\u7ea6\u675f\u3002", "result": "\u5728\u98de\u673a\u548c\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u89c4\u5219\u5f15\u5bfc\u7684\u805a\u7c7b\u4ea7\u751f\u4e86\u66f4\u5177\u64cd\u4f5c\u610f\u4e49\u548c\u53ef\u89e3\u91ca\u6027\u7684\u805a\u7c7b\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u4f20\u7edf\u805a\u7c7b\u6307\u6807\u3002", "conclusion": "DARTVAE \u901a\u8fc7\u7ed3\u5408\u89c4\u5219\u7f16\u7801\u548c\u5b66\u4e60\u5230\u7684\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u6bd4\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u66f4\u6709\u610f\u4e49\u548c\u4e00\u81f4\u7684\u805a\u7c7b\u7ed3\u679c\uff0c\u7a81\u51fa\u4e86\u7ea6\u675f\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u805a\u7c7b\u5728\u590d\u6742\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u73af\u5883\u4e2d\u7684\u6548\u7528\u3002", "summary_zh": "\u4f20\u7edf\u7684\u805a\u7c7b\u6280\u672f\u901a\u5e38\u53ea\u4f9d\u8d56\u4e8e\u8f93\u5165\u6570\u636e\u7684\u76f8\u4f3c\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u8bb8\u591a\u9886\u57df\u4e2d\u6355\u6349\u7ed3\u6784\u6216\u8bed\u4e49\u7ea6\u675f\u7684\u80fd\u529b\u3002\u6211\u4eec\u5f15\u5165\u4e86\u9886\u57df\u611f\u77e5\u89c4\u5219\u89e6\u53d1\u53d8\u5206\u81ea\u7f16\u7801\u5668 (DARTVAE)\uff0c\u8fd9\u662f\u4e00\u4e2a\u89c4\u5219\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u805a\u7c7b\u6846\u67b6\uff0c\u5b83\u5c06\u7279\u5b9a\u9886\u57df\u7684\u7ea6\u675f\u76f4\u63a5\u6574\u5408\u5230\u8868\u793a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u3002DARTVAE \u901a\u8fc7\u5c06\u663e\u5f0f\u89c4\u5219\u3001\u8bed\u4e49\u8868\u793a\u548c\u6570\u636e\u9a71\u52a8\u7684\u7279\u5f81\u5d4c\u5165\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u540c\u65f6\u901a\u8fc7\u635f\u5931\u51fd\u6570\u4e2d\u7684\u89c4\u5219\u4e00\u81f4\u6027\u548c\u8fdd\u53cd\u60e9\u7f5a\u6765\u5f3a\u5236\u6267\u884c\u7ea6\u675f\uff0c\u4ece\u800c\u6269\u5c55\u4e86 VAE \u67b6\u6784\u3002\u4e0e\u4ec5\u4f9d\u8d56\u89c6\u89c9\u76f8\u4f3c\u6027\u6216\u5c06\u89c4\u5219\u5e94\u7528\u4e3a\u4e8b\u540e\u8fc7\u6ee4\u5668\u7684\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u4e0d\u540c\uff0cDARTVAE \u5c06\u89c4\u5219\u89c6\u4e3a\u7b2c\u4e00\u7c7b\u5b66\u4e60\u4fe1\u53f7\u3002\u8fd9\u4e9b\u89c4\u5219\u7531 LLM \u751f\u6210\uff0c\u6784\u5efa\u6210\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u4e86\u91cd\u6784\u3001KL \u6563\u5ea6\u3001\u4e00\u81f4\u6027\u548c\u8fdd\u53cd\u60e9\u7f5a\u7684\u635f\u5931\u51fd\u6570\u6765\u5f3a\u5236\u6267\u884c\u3002\u5728\u98de\u673a\u548c\u6c7d\u8f66\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u89c4\u5219\u5f15\u5bfc\u7684\u805a\u7c7b\u4ea7\u751f\u4e86\u66f4\u5177\u64cd\u4f5c\u610f\u4e49\u548c\u53ef\u89e3\u91ca\u6027\u7684\u805a\u7c7b\uff0c\u4f8b\u5982\uff0c\u9694\u79bb\u65e0\u4eba\u673a\u3001\u7edf\u4e00\u9690\u5f62\u98de\u673a\u6216\u5c06 SUV \u4e0e\u8f7f\u8f66\u5206\u79bb\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u4f20\u7edf\u805a\u7c7b\u6307\u6807\u3002\u7136\u800c\uff0c\u8be5\u6846\u67b6\u9762\u4e34\u7740\u4e00\u4e9b\u6311\u6218\uff1aLLM \u751f\u6210\u7684\u89c4\u5219\u53ef\u80fd\u4f1a\u4ea7\u751f\u5e7b\u89c9\u6216\u51b2\u7a81\uff0c\u8fc7\u591a\u7684\u89c4\u5219\u6709\u8fc7\u5ea6\u62df\u5408\u7684\u98ce\u9669\uff0c\u5e76\u4e14\u6269\u5c55\u5230\u590d\u6742\u9886\u57df\u4f1a\u589e\u52a0\u8ba1\u7b97\u548c\u4e00\u81f4\u6027\u65b9\u9762\u7684\u56f0\u96be\u3002\u901a\u8fc7\u5c06\u89c4\u5219\u7f16\u7801\u4e0e\u5b66\u4e60\u5230\u7684\u8868\u793a\u76f8\u7ed3\u5408\uff0cDARTVAE \u5b9e\u73b0\u4e86\u6bd4\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u66f4\u6709\u610f\u4e49\u548c\u4e00\u81f4\u7684\u805a\u7c7b\u7ed3\u679c\uff0c\u7a81\u51fa\u4e86\u7ea6\u675f\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u805a\u7c7b\u5728\u590d\u6742\u3001\u77e5\u8bc6\u5bc6\u96c6\u578b\u73af\u5883\u4e2d\u7684\u6548\u7528\u3002"}}
{"id": "2509.20623", "pdf": "https://arxiv.org/pdf/2509.20623", "abs": "https://arxiv.org/abs/2509.20623", "authors": ["Satyajeet Das", "Darren Chiu", "Zhehui Huang", "Lars Lindemann", "Gaurav S. Sukhatme"], "title": "Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning has enabled significant progress in complex domains\nsuch as coordinating and navigating multiple quadrotors. However, even\nwell-trained policies remain vulnerable to collisions in obstacle-rich\nenvironments. Addressing these infrequent but critical safety failures through\nretraining or fine-tuning is costly and risks degrading previously learned\nskills. Inspired by activation steering in large language models and latent\nediting in computer vision, we introduce a framework for inference-time Latent\nActivation Editing (LAE) that refines the behavior of pre-trained policies\nwithout modifying their weights or architecture. The framework operates in two\nstages: (i) an online classifier monitors intermediate activations to detect\nstates associated with undesired behaviors, and (ii) an activation editing\nmodule that selectively modifies flagged activations to shift the policy\ntowards safer regimes. In this work, we focus on improving safety in\nmulti-quadrotor navigation. We hypothesize that amplifying a policy's internal\nperception of risk can induce safer behaviors. We instantiate this idea through\na latent collision world model trained to predict future pre-collision\nactivations, thereby prompting earlier and more cautious avoidance responses.\nExtensive simulations and real-world Crazyflie experiments demonstrate that LAE\nachieves statistically significant reduction in collisions (nearly 90% fewer\ncumulative collisions compared to the unedited baseline) and substantially\nincreases the fraction of collision-free trajectories, while preserving task\ncompletion. More broadly, our results establish LAE as a lightweight paradigm,\nfeasible on resource-constrained hardware, for post-deployment refinement of\nlearned robot policies.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on improving the safety of multi-robot navigation policies learned through reinforcement learning. While it mentions \"activation steering in large language models\" as inspiration, the core contribution is in refining learned policies at inference time for safer navigation. It does not directly involve trajectory prediction in the sense of forecasting future trajectories but focuses on collision avoidance, which is related. The connection to LLMs is primarily through inspiration, not direct application. Therefore the relevance score is moderate.", "keywords": ["multi-robot navigation", "collision avoidance", "reinforcement learning", "activation steering", "learned policies"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6f5c\u5728\u6fc0\u6d3b\u7f16\u8f91\uff08LAE\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u4fee\u6539\u9884\u8bad\u7ec3\u7b56\u7565\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u63a8\u7406\u65f6\u6539\u8fdb\u591a\u65cb\u7ffc\u98de\u884c\u5668\u7684\u5bfc\u822a\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u590d\u6742\u73af\u5883\u4e2d\u6613\u53d1\u751f\u78b0\u649e\uff0c\u91cd\u65b0\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u53ef\u80fd\u964d\u4f4e\u5df2\u5b66\u6280\u80fd\u3002", "method": "\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u7ebf\u5206\u7c7b\u5668\u68c0\u6d4b\u4e0d\u826f\u884c\u4e3a\u72b6\u6001\uff0c\u5e76\u4f7f\u7528\u6fc0\u6d3b\u7f16\u8f91\u6a21\u5757\u9009\u62e9\u6027\u5730\u4fee\u6539\u6fc0\u6d3b\uff0c\u4ece\u800c\u5f15\u5bfc\u7b56\u7565\u8f6c\u5411\u66f4\u5b89\u5168\u7684\u6a21\u5f0f\u3002\u901a\u8fc7\u8bad\u7ec3\u6f5c\u5728\u78b0\u649e\u4e16\u754c\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u7684\u9884\u78b0\u649e\u6fc0\u6d3b\uff0c\u4ece\u800c\u4fc3\u4f7f\u66f4\u65e9\u548c\u66f4\u8c28\u614e\u7684\u907f\u969c\u53cd\u5e94\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9eCrazyflie\u5b9e\u9a8c\u8868\u660e\uff0cLAE\u663e\u8457\u51cf\u5c11\u4e86\u78b0\u649e\uff08\u4e0e\u672a\u7f16\u8f91\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7d2f\u79ef\u78b0\u649e\u51cf\u5c11\u4e86\u8fd190%\uff09\uff0c\u5e76\u5927\u5927\u589e\u52a0\u4e86\u65e0\u78b0\u649e\u8f68\u8ff9\u7684\u6bd4\u4f8b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u5b8c\u6210\u3002", "conclusion": "LAE\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8303\u4f8b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\uff0c\u53ef\u7528\u4e8e\u5df2\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u90e8\u7f72\u540e\u6539\u8fdb\u3002", "summary_zh": "\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u534f\u8c03\u548c\u5bfc\u822a\u591a\u4e2a\u56db\u65cb\u7ffc\u98de\u884c\u5668\uff09\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\u3002\u7136\u800c\uff0c\u5373\u4f7f\u662f\u7ecf\u8fc7\u826f\u597d\u8bad\u7ec3\u7684\u7b56\u7565\uff0c\u5728\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u4ecd\u7136\u5bb9\u6613\u53d1\u751f\u78b0\u649e\u3002\u901a\u8fc7\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u6765\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u5e38\u53d1\u751f\u4f46\u81f3\u5173\u91cd\u8981\u7684\u5b89\u5168\u95ee\u9898\uff0c\u6210\u672c\u9ad8\u6602\uff0c\u5e76\u4e14\u6709\u964d\u4f4e\u5148\u524d\u5b66\u4e60\u6280\u80fd\u7684\u98ce\u9669\u3002\u53d7\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u5f15\u5bfc\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u6f5c\u5728\u7f16\u8f91\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u63a8\u7406\u65f6\u6f5c\u5728\u6fc0\u6d3b\u7f16\u8f91\uff08LAE\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u5728\u4e0d\u4fee\u6539\u5176\u6743\u91cd\u6216\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u6539\u8fdb\u9884\u8bad\u7ec3\u7b56\u7565\u7684\u884c\u4e3a\u3002\u8be5\u6846\u67b6\u5206\u4e24\u4e2a\u9636\u6bb5\u8fd0\u884c\uff1a\uff08i\uff09\u5728\u7ebf\u5206\u7c7b\u5668\u76d1\u63a7\u4e2d\u95f4\u6fc0\u6d3b\uff0c\u4ee5\u68c0\u6d4b\u4e0e\u4e0d\u826f\u884c\u4e3a\u76f8\u5173\u7684\u72b6\u6001\uff0c\u4ee5\u53ca\uff08ii\uff09\u6fc0\u6d3b\u7f16\u8f91\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u9009\u62e9\u6027\u5730\u4fee\u6539\u6807\u8bb0\u7684\u6fc0\u6d3b\uff0c\u4ee5\u5c06\u7b56\u7565\u8f6c\u79fb\u5230\u66f4\u5b89\u5168\u7684\u6a21\u5f0f\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u4e13\u6ce8\u4e8e\u63d0\u9ad8\u591a\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5bfc\u822a\u7684\u5b89\u5168\u6027\u3002\u6211\u4eec\u5047\u8bbe\u653e\u5927\u7b56\u7565\u5bf9\u98ce\u9669\u7684\u5185\u90e8\u611f\u77e5\u53ef\u4ee5\u8bf1\u5bfc\u66f4\u5b89\u5168\u7684\u884c\u4e3a\u3002\u6211\u4eec\u901a\u8fc7\u8bad\u7ec3\u6f5c\u5728\u78b0\u649e\u4e16\u754c\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u7684\u9884\u78b0\u649e\u6fc0\u6d3b\u6765\u5b9e\u73b0\u8fd9\u4e00\u60f3\u6cd5\uff0c\u4ece\u800c\u4fc3\u4f7f\u66f4\u65e9\u548c\u66f4\u8c28\u614e\u7684\u907f\u969c\u53cd\u5e94\u3002\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u771f\u5b9e\u7684Crazyflie\u5b9e\u9a8c\u8868\u660e\uff0cLAE\u5728\u7edf\u8ba1\u4e0a\u663e\u7740\u51cf\u5c11\u4e86\u78b0\u649e\uff08\u4e0e\u672a\u7f16\u8f91\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u7d2f\u79ef\u78b0\u649e\u51cf\u5c11\u4e86\u8fd190\uff05\uff09\uff0c\u5e76\u5927\u5927\u589e\u52a0\u4e86\u65e0\u78b0\u649e\u8f68\u8ff9\u7684\u6bd4\u4f8b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u5b8c\u6210\u3002\u66f4\u5e7f\u6cdb\u5730\u8bf4\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cLAE\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8303\u4f8b\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u786c\u4ef6\u4e0a\u53ef\u884c\uff0c\u53ef\u7528\u4e8e\u5df2\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\u7684\u90e8\u7f72\u540e\u6539\u8fdb\u3002"}}
{"id": "2509.20754", "pdf": "https://arxiv.org/pdf/2509.20754", "abs": "https://arxiv.org/abs/2509.20754", "authors": ["Yufan Mao", "Hanjing Ye", "Wenlong Dong", "Chengjie Zhang", "Hong Zhang"], "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Navigating complex environments requires robots to effectively store\nobservations as memories and leverage them to answer human queries about\nspatial locations, which is a critical yet underexplored research challenge.\nWhile prior work has made progress in constructing robotic memory, few have\naddressed the principled mechanisms needed for efficient memory retrieval and\nintegration. To bridge this gap, we propose Meta-Memory, a large language model\n(LLM)-driven agent that constructs a high-density memory representation of the\nenvironment. The key innovation of Meta-Memory lies in its capacity to retrieve\nand integrate relevant memories through joint reasoning over semantic and\nspatial modalities in response to natural language location queries, thereby\nempowering robots with robust and accurate spatial reasoning capabilities. To\nevaluate its performance, we introduce SpaceLocQA, a large-scale dataset\nencompassing diverse real-world spatial question-answering scenarios.\nExperimental results show that Meta-Memory significantly outperforms\nstate-of-the-art methods on both the SpaceLocQA and the public NaVQA\nbenchmarks. Furthermore, we successfully deployed Meta-Memory on real-world\nrobotic platforms, demonstrating its practical utility in complex environments.\nProject page: https://itsbaymax.github.io/meta-memory.github.io/ .", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses a large language model (LLM) to build a memory representation for robots navigating complex environments. While it focuses on spatial reasoning and question answering based on semantic and spatial information, the underlying task of robot navigation and understanding of the environment has connections to trajectory planning and prediction, albeit indirectly. The paper directly mentions LLMs and spatial reasoning, making it somewhat relevant.", "keywords": ["large language model", "LLM", "spatial reasoning", "robot navigation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20681", "pdf": "https://arxiv.org/pdf/2509.20681", "abs": "https://arxiv.org/abs/2509.20681", "authors": ["Wei-Teng Chu", "Tianyi Zhang", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Efficient Construction of Implicit Surface Models From a Single Image for Motion Generation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Implicit representations have been widely applied in robotics for obstacle\navoidance and path planning. In this paper, we explore the problem of\nconstructing an implicit distance representation from a single image. Past\nmethods for implicit surface reconstruction, such as \\emph{NeuS} and its\nvariants generally require a large set of multi-view images as input, and\nrequire long training times. In this work, we propose Fast Image-to-Neural\nSurface (FINS), a lightweight framework that can reconstruct high-fidelity\nsurfaces and SDF fields based on a single or a small set of images. FINS\nintegrates a multi-resolution hash grid encoder with lightweight geometry and\ncolor heads, making the training via an approximate second-order optimizer\nhighly efficient and capable of converging within a few seconds. Additionally,\nwe achieve the construction of a neural surface requiring only a single RGB\nimage, by leveraging pre-trained foundation models to estimate the geometry\ninherent in the image. Our experiments demonstrate that under the same\nconditions, our method outperforms state-of-the-art baselines in both\nconvergence speed and accuracy on surface reconstruction and SDF field\nestimation. Moreover, we demonstrate the applicability of FINS for robot\nsurface following tasks and show its scalability to a variety of benchmark\ndatasets.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on constructing implicit surface models from a single image for motion generation, which relates to path planning and obstacle avoidance. It also leverages pre-trained foundation models for geometry estimation. While not directly addressing trajectory prediction, the motion generation aspect and the use of foundation models contribute to its relevance. The connection to large models is present but not central.", "keywords": ["motion generation", "path planning", "foundation models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20715", "pdf": "https://arxiv.org/pdf/2509.20715", "abs": "https://arxiv.org/abs/2509.20715", "authors": ["Ruixu Zhang", "Yuran Wang", "Xinyi Hu", "Chaoyu Mai", "Wenxuan Liu", "Danni Xu", "Xian Zhong", "Zheng Wang"], "title": "Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Intention recognition has traditionally focused on individual intentions,\noverlooking the complexities of collective intentions in group settings. To\naddress this limitation, we introduce the concept of group intention, which\nrepresents shared goals emerging through the actions of multiple individuals,\nand Group Intention Forecasting (GIF), a novel task that forecasts when group\nintentions will occur by analyzing individual actions and interactions before\nthe collective goal becomes apparent. To investigate GIF in a specific\nscenario, we propose SHOT, the first large-scale dataset for GIF, consisting of\n1,979 basketball video clips captured from 5 camera views and annotated with 6\ntypes of individual attributes. SHOT is designed with 3 key characteristics:\nmulti-individual information, multi-view adaptability, and multi-level\nintention, making it well-suited for studying emerging group intentions.\nFurthermore, we introduce GIFT (Group Intention ForecasTer), a framework that\nextracts fine-grained individual features and models evolving group dynamics to\nforecast intention emergence. Experimental results confirm the effectiveness of\nSHOT and GIFT, establishing a strong foundation for future research in group\nintention forecasting. The dataset is available at\nhttps://xinyi-hu.github.io/SHOT_DATASET.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on forecasting group intentions based on individual actions and interactions, which is related to trajectory prediction (specifically action prediction). However, it doesn't explicitly mention or utilize large language models. The 'forecasting' aspect links it to trajectory prediction, but the lack of LLM involvement reduces the relevance.", "keywords": ["intention forecasting", "action prediction", "group intention", "forecasting"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20705", "pdf": "https://arxiv.org/pdf/2509.20705", "abs": "https://arxiv.org/abs/2509.20705", "authors": ["Reza Akhavian", "Mani Amani", "Johannes Mootz", "Robert Ashe", "Behrad Beheshti"], "title": "Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework", "categories": ["cs.RO"], "comment": null, "summary": "The adoption of cyber-physical systems and jobsite intelligence that connects\ndesign models, real-time site sensing, and autonomous field operations can\ndramatically enhance digital management in the construction industry. This\npaper introduces BIM2RDT (Building Information Models to Robot-Ready Site\nDigital Twins), an agentic artificial intelligence (AI) framework designed to\ntransform static Building Information Modeling (BIM) into dynamic, robot-ready\ndigital twins (DTs) that prioritize safety during execution. The framework\nbridges the gap between pre-existing BIM data and real-time site conditions by\nintegrating three key data streams: geometric and semantic information from BIM\nmodels, activity data from IoT sensor networks, and visual-spatial data\ncollected by robots during site traversal. The methodology introduces\nSemantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that\nleverages large language model (LLM) reasoning. Unlike traditional methods,\nSG-ICP utilizes an LLM to infer object-specific, plausible orientation priors\nbased on BIM semantics, improving alignment accuracy by avoiding convergence on\nlocal minima. This creates a feedback loop where robot-collected data updates\nthe DT, which in turn optimizes paths for missions. The framework employs YOLOE\nobject detection and Shi-Tomasi corner detection to identify and track\nconstruction elements while using BIM geometry as a priori maps. The framework\nalso integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping\nsensor-detected safety events to the digital twin using IFC standards for\nintervention. Experiments demonstrate SG-ICP's superiority over standard ICP,\nachieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with\noccluded features, ensuring plausible orientations. HAV integration triggers\nwarnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes Large Language Models (LLMs) for reasoning within a framework that involves robot path optimization and data integration from real-world environments. While the primary focus isn't trajectory prediction in the traditional sense, the integration of robot data and optimization of paths touches upon related concepts. The use of LLMs directly contributes to the relevance.", "keywords": ["Large Language Model (LLM)", "robot", "path optimization", "digital twin"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20739", "pdf": "https://arxiv.org/pdf/2509.20739", "abs": "https://arxiv.org/abs/2509.20739", "authors": ["Guoyang Zhao", "Yudong Li", "Weiqing Qi", "Kai Zhang", "Bonan Liu", "Kai Chen", "Haoang Li", "Jun Ma"], "title": "SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Conventional SLAM pipelines for legged robot navigation are fragile under\nrapid motion, calibration demands, and sensor drift, while offering limited\nsemantic reasoning for task-driven exploration. To deal with these issues, we\npropose a vision-only, SLAM-free navigation framework that replaces dense\ngeometry with semantic reasoning and lightweight topological representations. A\nhierarchical vision-language perception module fuses scene-level context with\nobject-level cues for robust semantic inference. And a semantic-probabilistic\ntopological map supports coarse-to-fine planning: LLM-based global reasoning\nfor subgoal selection and vision-based local planning for obstacle avoidance.\nIntegrated with reinforcement-learning locomotion controllers, the framework is\ndeployable across diverse legged robot platforms. Experiments in simulation and\nreal-world settings demonstrate consistent improvements in semantic accuracy,\nplanning quality, and navigation success, while ablation studies further\nshowcase the necessity of both hierarchical perception and fine local planning.\nThis work introduces a new paradigm for SLAM-free, vision-language-driven\nnavigation, shifting robotic exploration from geometry-centric mapping to\nsemantics-driven decision making.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper combines vision-language models with planning for robot navigation. While it doesn't directly focus on trajectory prediction in the typical sense (e.g., predicting future trajectories of agents), the LLM-based global reasoning for subgoal selection and the vision-based local planning components have aspects related to path planning, which is tangentially related to trajectory generation. It also involves large language models.", "keywords": ["Large Language Models", "LLM", "vision-language perception", "planning", "navigation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20843", "pdf": "https://arxiv.org/pdf/2509.20843", "abs": "https://arxiv.org/abs/2509.20843", "authors": ["Ziang Luo", "Kangan Qian", "Jiahua Wang", "Yuechen Luo", "Jinyu Miao", "Zheng Fu", "Yunlong Wang", "Sicong Jiang", "Zilin Huang", "Yifei Hu", "Yuhao Yang", "Hao Ye", "Mengmeng Yang", "Xiaojian Dong", "Kun Jiang", "Diange Yang"], "title": "MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Vision-Language Models(VLMs) have demonstrated significant potential for\nend-to-end autonomous driving, yet a substantial gap remains between their\ncurrent capabilities and the reliability necessary for real-world deployment. A\ncritical challenge is their fragility, characterized by hallucinations and poor\ngeneralization in out-of-distribution (OOD) scenarios. To bridge this gap, we\nintroduce MTRDrive, a novel framework that integrates procedural driving\nexperiences with a dynamic toolkit to enhance generalization and proactive\ndecision-making.\n  MTRDrive addresses these limitations through a closed-loop system that\ncombines a memory-based experience retrieval mechanism with dynamic toolkits.\nThis synergy enables the model to interact more effectively with its\nenvironment, improving both reasoning and decision-making capabilities with the\nhelp of our memory-tool synergistic reasoning. Additionally, we introduce a new\nbenchmark based on complex Roadwork construction scenarios to rigorously\nevaluate zero-shot generalization.\n  Extensive experiments demonstrate the superior effectiveness of our approach.\nOn the public NAVSIM benchmark, our 3B-parameter MTRDrive model achieves an\nexceptional PDMS of 88.3 without chain-of-thought and sets a state-of-the-art\nperformance bar on high-level planning, with a driving metric score of 79.8\\%\nand a planning accuracy of 82.6\\%. Rigorous zero-shot evaluation on the new\nRoadwork-VLM benchmark shows a strong ability to reason robustly in unseen\nscenarios, achieving a driving metric score of 80.2\\%. These results highlight\nMTRDrive's potential to advance autonomous driving toward safer and more\nreliable systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u89e3\u51b3corner case\u95ee\u9898\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u81ea\u52a8\u9a7e\u9a76\u672c\u8eab\u5305\u542b\u8f68\u8ff9\u9884\u6d4b\u7684\u5b50\u4efb\u52a1\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u5927\u6a21\u578b\uff08VLMs\uff09\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["autonomous driving", "Vision-Language Models", "VLMs", "large language models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21134", "pdf": "https://arxiv.org/pdf/2509.21134", "abs": "https://arxiv.org/abs/2509.21134", "authors": ["Yiwen Zhang", "Ziang Chen", "Fanqi Kong", "Yizhe Huang", "Xue Feng"], "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective", "categories": ["cs.AI", "cs.MA"], "comment": "22 pages, 14 figures", "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u51b3\u7b56\u5236\u5b9a\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86ToMPO\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\u7684\u7b56\u7565\u51b3\u7b56\u80fd\u529b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u51b3\u7b56\u5236\u5b9a\u5728\u79fb\u52a8\u7269\u4f53\u8def\u5f84\u89c4\u5212\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u8bba\u6587\u91cd\u70b9\u5728\u4e8eLLM\u7684\u7b56\u7565\u4f18\u5316\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u5173\u8054\u8f83\u4e3a\u95f4\u63a5\u3002", "keywords": ["Large Language Models", "LLMs", "strategic decision-making", "policy optimization"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20938", "pdf": "https://arxiv.org/pdf/2509.20938", "abs": "https://arxiv.org/abs/2509.20938", "authors": ["Jianbo Zhao", "Taiyu Ban", "Xiangjie Li", "Xingtai Gui", "Hangning Zhou", "Lei Liu", "Hongwei Zhao", "Bin Li"], "title": "Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The inherent sequential modeling capabilities of autoregressive models make\nthem a formidable baseline for end-to-end planning in autonomous driving.\nNevertheless, their performance is constrained by a spatio-temporal\nmisalignment, as the planner must condition future actions on past sensory\ndata. This creates an inconsistent worldview, limiting the upper bound of\nperformance for an otherwise powerful approach. To address this, we propose a\nTime-Invariant Spatial Alignment (TISA) module that learns to project initial\nenvironmental features into a consistent ego-centric frame for each future time\nstep, effectively correcting the agent's worldview without explicit future\nscene prediction. In addition, we employ a kinematic action prediction head\n(i.e., acceleration and yaw rate) to ensure physically feasible trajectories.\nFinally, we introduce a multi-objective post-training stage using Direct\nPreference Optimization (DPO) to move beyond pure imitation. Our approach\nprovides targeted feedback on specific driving behaviors, offering a more\nfine-grained learning signal than the single, overall objective used in\nstandard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM\ndataset among autoregressive models. The video document is available at\nhttps://tisa-dpo-e2e.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on end-to-end planning for autonomous driving using an autoregressive model. While it doesn't explicitly use a Large Language Model, it uses techniques like Direct Preference Optimization (DPO), which are also used in training LLMs. The core problem addressed is planning trajectories, which is related to trajectory prediction. The mention of kinematic action prediction and NAVSIM dataset further strengthens the relation to trajectory planning/prediction.", "keywords": ["autoregressive models", "end-to-end planning", "autonomous driving", "trajectory", "Direct Preference Optimization (DPO)", "kinematic action prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21027", "pdf": "https://arxiv.org/pdf/2509.21027", "abs": "https://arxiv.org/abs/2509.21027", "authors": ["Sibo Li", "Qianyue Hao", "Yu Shang", "Yong Li"], "title": "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic world models are a promising paradigm for forecasting future\nenvironment states, yet their inference speed and the physical plausibility of\ngenerated trajectories remain critical bottlenecks, limiting their real-world\napplications. This stems from the redundancy of the prevailing frame-to-frame\ngeneration approach, where the model conducts costly computation on similar\nframes, as well as neglecting the semantic importance of key transitions. To\naddress this inefficiency, we propose KeyWorld, a framework that improves\ntext-conditioned robotic world models by concentrating transformers computation\non a few semantic key frames while employing a lightweight convolutional model\nto fill the intermediate frames. Specifically, KeyWorld first identifies\nsignificant transitions by iteratively simplifying the robot's motion\ntrajectories, obtaining the ground truth key frames. Then, a DiT model is\ntrained to reason and generate these physically meaningful key frames from\ntextual task descriptions. Finally, a lightweight interpolator efficiently\nreconstructs the full video by inpainting all intermediate frames. Evaluations\non the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\\times$\nacceleration compared to the frame-to-frame generation baseline, and focusing\non the motion-aware key frames further contributes to the physical validity of\nthe generated videos, especially on complex tasks. Our approach highlights a\npractical path toward deploying world models in real-time robotic control and\nother domains requiring both efficient and effective world models. Code is\nreleased at https://anonymous.4open.science/r/Keyworld-E43D.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53caworld models\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u5e7f\u4e49\u4e0a\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u56e0\u4e3aworld model\u9884\u6d4b\u73af\u5883\u672a\u6765\u7684\u72b6\u6001\u3002\u8bba\u6587\u4f7f\u7528\u4e86DiT\u6a21\u578b\uff08diffusion transformer\uff09\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u662fLarge Model\u7684\u4e00\u79cd\uff0c\u4f46\u6ca1\u6709\u660e\u786e\u6307\u51fa\u662fLarge Language Model\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["world models", "trajectory", "DiT model"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20712", "pdf": "https://arxiv.org/pdf/2509.20712", "abs": "https://arxiv.org/abs/2509.20712", "authors": ["Zhenpeng Su", "Leiyu Pan", "Minxuan Lv", "Yuntao Li", "Wenping Hu", "Fuzheng Zhang", "Kun Gai", "Guorui Zhou"], "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}ontrolling \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on reinforcement learning for optimizing large language models, specifically addressing the challenge of managing policy entropy. While it doesn't directly involve trajectory prediction, it is relevant due to its application to LLMs.", "keywords": ["large language models", "LLMs", "reinforcement learning", "policy optimization", "entropy"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21107", "pdf": "https://arxiv.org/pdf/2509.21107", "abs": "https://arxiv.org/abs/2509.21107", "authors": ["William Barron", "Xiaoxiang Dong", "Matthew Johnson-Roberson", "Weiming Zhi"], "title": "Cross-Modal Instructions for Robot Motion Generation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Teaching robots novel behaviors typically requires motion demonstrations via\nteleoperation or kinaesthetic teaching, that is, physically guiding the robot.\nWhile recent work has explored using human sketches to specify desired\nbehaviors, data collection remains cumbersome, and demonstration datasets are\ndifficult to scale. In this paper, we introduce an alternative paradigm,\nLearning from Cross-Modal Instructions, where robots are shaped by\ndemonstrations in the form of rough annotations, which can contain free-form\ntext labels, and are used in lieu of physical motion. We introduce the\nCrossInstruct framework, which integrates cross-modal instructions as examples\ninto the context input to a foundational vision-language model (VLM). The VLM\nthen iteratively queries a smaller, fine-tuned model, and synthesizes the\ndesired motion over multiple 2D views. These are then subsequently fused into a\ncoherent distribution over 3D motion trajectories in the robot's workspace. By\nincorporating the reasoning of the large VLM with a fine-grained pointing\nmodel, CrossInstruct produces executable robot behaviors that generalize beyond\nthe environment of in the limited set of instruction examples. We then\nintroduce a downstream reinforcement learning pipeline that leverages\nCrossInstruct outputs to efficiently learn policies to complete fine-grained\ntasks. We rigorously evaluate CrossInstruct on benchmark simulation tasks and\nreal hardware, demonstrating effectiveness without additional fine-tuning and\nproviding a strong initialization for policies subsequently refined via\nreinforcement learning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper describes a method for generating robot motion using cross-modal instructions, leveraging a vision-language model (VLM). While it doesn't explicitly focus on trajectory prediction as its primary goal, the generation of robot motion inherently involves predicting future states or trajectories. The use of a foundational vision-language model (VLM) makes it relevant to the large language model aspect. The connection to trajectory prediction is weaker but present.", "keywords": ["robot motion generation", "vision-language model", "VLM", "motion trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.21189", "pdf": "https://arxiv.org/pdf/2509.21189", "abs": "https://arxiv.org/abs/2509.21189", "authors": ["Bhargav Chandaka", "Gloria X. Wang", "Haozhe Chen", "Henry Che", "Albert J. Zhai", "Shenlong Wang"], "title": "Human-like Navigation in a World Built for Humans", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "CoRL 2025. Project website: https://reasonnav.github.io/", "summary": "When navigating in a man-made environment they haven't visited before--like\nan office building--humans employ behaviors such as reading signs and asking\nothers for directions. These behaviors help humans reach their destinations\nefficiently by reducing the need to search through large areas. Existing robot\nnavigation systems lack the ability to execute such behaviors and are thus\nhighly inefficient at navigating within large environments. We present\nReasonNav, a modular navigation system which integrates these human-like\nnavigation skills by leveraging the reasoning capabilities of a vision-language\nmodel (VLM). We design compact input and output abstractions based on\nnavigation landmarks, allowing the VLM to focus on language understanding and\nreasoning. We evaluate ReasonNav on real and simulated navigation tasks and\nshow that the agent successfully employs higher-order reasoning to navigate\nefficiently in large, complex buildings.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a Vision-Language Model (VLM) for robot navigation, which involves planning and executing trajectories in an environment. While it doesn't explicitly focus on trajectory prediction, the navigation aspect and the use of a large model contribute to the relevance. The connection to large models is strong, while the connection to trajectory prediction is implicit through navigation.", "keywords": ["navigation", "vision-language model", "VLM", "reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20823", "pdf": "https://arxiv.org/pdf/2509.20823", "abs": "https://arxiv.org/abs/2509.20823", "authors": ["Luca Zhou", "Pratham Yashwante", "Marshall Fisher", "Alessio Sampieri", "Zihao Zhou", "Fabio Galasso", "Rose Yu"], "title": "CaTS-Bench: Can Language Models Describe Numeric Time Series?", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "9 pages, 4 images, 4 tables in the main paper. Many more in the\n  appendix", "summary": "Time series captioning, the task of describing numeric time series in natural\nlanguage, requires numerical reasoning, trend interpretation, and contextual\nunderstanding. Existing benchmarks, however, often rely on synthetic data or\noverly simplistic captions, and typically neglect metadata and visual\nrepresentations. To close this gap, we introduce CaTS-Bench, the first\nlarge-scale, real-world benchmark for Context-aware Time Series captioning.\nCaTS-Bench is derived from 11 diverse datasets reframed as captioning and Q&A\ntasks, comprising roughly 465k training and 105k test timestamps. Each sample\nincludes a numeric series segment, contextual metadata, a line-chart image, and\na caption. A key contribution of this work is the scalable pipeline used to\ngenerate reference captions: while most references are produced by an oracle\nLLM and verified through factual checks, human indistinguishability studies,\nand diversity analyses, we also provide a human-revisited subset of 579 test\ncaptions, refined from LLM outputs to ensure accuracy and human-like style.\nBeyond captioning, CaTS-Bench offers 460 multiple-choice questions targeting\ndeeper aspects of time series reasoning. We further propose new tailored\nevaluation metrics and benchmark leading VLMs, highlighting both their\nstrengths and persistent limitations. Together, these contributions establish\nCaTS-Bench and its captioning pipeline as a reliable and extensible foundation\nfor future research at the intersection of time series analysis and foundation\nmodels.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on time series captioning using large language models. While not directly about trajectory prediction, time series analysis is a relevant field, and the use of LLMs is central to the work. The connection is not strong, hence the score is not very high.", "keywords": ["Large Language Models", "LLMs", "time series", "foundation models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.20927", "pdf": "https://arxiv.org/pdf/2509.20927", "abs": "https://arxiv.org/abs/2509.20927", "authors": ["Akihisa Watanabe", "Jiawei Ren", "Li Siyao", "Yichen Peng", "Erwin Wu", "Edgar Simo-Serra"], "title": "SimDiff: Simulator-constrained Diffusion Model for Physically Plausible Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating physically plausible human motion is crucial for applications such\nas character animation and virtual reality. Existing approaches often\nincorporate a simulator-based motion projection layer to the diffusion process\nto enforce physical plausibility. However, such methods are computationally\nexpensive due to the sequential nature of the simulator, which prevents\nparallelization. We show that simulator-based motion projection can be\ninterpreted as a form of guidance, either classifier-based or classifier-free,\nwithin the diffusion process. Building on this insight, we propose SimDiff, a\nSimulator-constrained Diffusion Model that integrates environment parameters\n(e.g., gravity, wind) directly into the denoising process. By conditioning on\nthese parameters, SimDiff generates physically plausible motions efficiently,\nwithout repeated simulator calls at inference, and also provides fine-grained\ncontrol over different physical coefficients. Moreover, SimDiff successfully\ngeneralizes to unseen combinations of environmental parameters, demonstrating\ncompositional generalization.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on motion generation using a diffusion model, aiming for physically plausible human motion. While it doesn't directly involve trajectory prediction in the traditional sense (e.g., predicting future locations), the generated motion can be seen as a short-term trajectory. It also doesn't explicitly use or discuss large language models. However, diffusion models are a type of generative model that have connections to the broader field of large models. The connection to trajectory prediction is weaker but present.", "keywords": ["motion generation", "diffusion model", "physical plausibility"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
