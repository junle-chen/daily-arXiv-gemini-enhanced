{"id": "2510.15446", "pdf": "https://arxiv.org/pdf/2510.15446", "abs": "https://arxiv.org/abs/2510.15446", "authors": ["Ziang Guo", "Zufeng Zhang"], "title": "VDRive: Leveraging Reinforced VLA and Diffusion Policy for End-to-end Autonomous Driving", "categories": ["cs.RO"], "comment": "1st version", "summary": "In autonomous driving, dynamic environment and corner cases pose significant\nchallenges to the robustness of ego vehicle's state understanding and decision\nmaking. We introduce VDRive, a novel pipeline for end-to-end autonomous driving\nthat explicitly models state-action mapping to address these challenges,\nenabling interpretable and robust decision making. By leveraging the\nadvancement of the state understanding of the Vision Language Action Model\n(VLA) with generative diffusion policy-based action head, our VDRive guides the\ndriving contextually and geometrically. Contextually, VLA predicts future\nobservations through token generation pre-training, where the observations are\nrepresented as discrete codes by a Conditional Vector Quantized Variational\nAutoencoder (CVQ-VAE). Geometrically, we perform reinforcement learning\nfine-tuning of the VLA to predict future trajectories and actions based on\ncurrent driving conditions. VLA supplies the current state tokens and predicted\nstate tokens for the action policy head to generate hierarchical actions and\ntrajectories. During policy training, a learned critic evaluates the actions\ngenerated by the policy and provides gradient-based feedback, forming an\nactor-critic framework that enables a reinforcement-based policy learning\npipeline. Experiments show that our VDRive achieves state-of-the-art\nperformance in the Bench2Drive closed-loop benchmark and nuScenes open-loop\nplanning.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant as it combines elements of both trajectory prediction (future trajectories and actions based on current driving conditions) and large models (Vision Language Action Model - VLA). It uses a VLA to predict future observations and trajectories, and a diffusion policy for action generation.", "keywords": ["trajectory prediction", "autonomous driving", "large language models", "VLA", "diffusion policy", "action prediction", "reinforcement learning"]}}
{"id": "2510.15104", "pdf": "https://arxiv.org/pdf/2510.15104", "abs": "https://arxiv.org/abs/2510.15104", "authors": ["Guofeng Zhang", "Angtian Wang", "Jacob Zhiyuan Fang", "Liming Jiang", "Haotian Yang", "Bo Liu", "Yiding Yang", "Guang Chen", "Longyin Wen", "Alan Yuille", "Chongyang Ma"], "title": "TGT: Text-Grounded Trajectories for Locally Controlled Video Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video generation has advanced rapidly in visual fidelity, whereas\nstandard methods still have limited ability to control the subject composition\nof generated scenes. Prior work shows that adding localized text control\nsignals, such as bounding boxes or segmentation masks, can help. However, these\nmethods struggle in complex scenarios and degrade in multi-object settings,\noffering limited precision and lacking a clear correspondence between\nindividual trajectories and visual entities as the number of controllable\nobjects increases. We introduce Text-Grounded Trajectories (TGT), a framework\nthat conditions video generation on trajectories paired with localized text\ndescriptions. We propose Location-Aware Cross-Attention (LACA) to integrate\nthese signals and adopt a dual-CFG scheme to separately modulate local and\nglobal text guidance. In addition, we develop a data processing pipeline that\nproduces trajectories with localized descriptions of tracked entities, and we\nannotate two million high quality video clips to train TGT. Together, these\ncomponents enable TGT to use point trajectories as intuitive motion handles,\npairing each trajectory with text to control both appearance and motion.\nExtensive experiments show that TGT achieves higher visual quality, more\naccurate text alignment, and improved motion controllability compared with\nprior approaches. Website: https://textgroundedtraj.github.io.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\"Trajectories\"\uff08\u8f68\u8ff9\uff09\uff0c\u5e76\u4e14\u6458\u8981\u4e2d\u63cf\u8ff0\u4e86\u4f7f\u7528\u8f68\u8ff9\u6765\u63a7\u5236\u89c6\u9891\u751f\u6210\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u6587\u672c\u63cf\u8ff0\u6765\u5f15\u5bfc\u8f68\u8ff9\uff0c\u6697\u793a\u4e86\u6587\u672c\u5904\u7406\u80fd\u529b\uff0c\u5e76\u4e14\u751f\u6210\u89c6\u9891\u4e5f\u53ef\u80fd\u6d89\u53ca\u5927\u578b\u89c6\u89c9\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectories", "video generation", "text grounding", "motion controllability"]}}
{"id": "2510.15018", "pdf": "https://arxiv.org/pdf/2510.15018", "abs": "https://arxiv.org/abs/2510.15018", "authors": ["Mingxuan Liu", "Honglin He", "Elisa Ricci", "Wayne Wu", "Bolei Zhou"], "title": "UrbanVerse: Scaling Urban Simulation by Watching City-Tour Videos", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Technical report. Project page: https://urbanverseproject.github.io/", "summary": "Urban embodied AI agents, ranging from delivery robots to quadrupeds, are\nincreasingly populating our cities, navigating chaotic streets to provide\nlast-mile connectivity. Training such agents requires diverse, high-fidelity\nurban environments to scale, yet existing human-crafted or procedurally\ngenerated simulation scenes either lack scalability or fail to capture\nreal-world complexity. We introduce UrbanVerse, a data-driven real-to-sim\nsystem that converts crowd-sourced city-tour videos into physics-aware,\ninteractive simulation scenes. UrbanVerse consists of: (i) UrbanVerse-100K, a\nrepository of 100k+ annotated urban 3D assets with semantic and physical\nattributes, and (ii) UrbanVerse-Gen, an automatic pipeline that extracts scene\nlayouts from video and instantiates metric-scale 3D simulations using retrieved\nassets. Running in IsaacSim, UrbanVerse offers 160 high-quality constructed\nscenes from 24 countries, along with a curated benchmark of 10 artist-designed\ntest scenes. Experiments show that UrbanVerse scenes preserve real-world\nsemantics and layouts, achieving human-evaluated realism comparable to manually\ncrafted scenes. In urban navigation, policies trained in UrbanVerse exhibit\nscaling power laws and strong generalization, improving success by +6.3% in\nsimulation and +30.1% in zero-shot sim-to-real transfer comparing to prior\nmethods, accomplishing a 300 m real-world mission with only two interventions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u57ce\u5e02\u73af\u5883\u7684\u6a21\u62df\uff0c\u4ee5\u8bad\u7ec3\u5177\u8eab\u667a\u80fd\u4f53\uff08embodied AI agents\uff09\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u4e3a\u57ce\u5e02\u5bfc\u822a\u7b49\u4efb\u52a1\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u6a21\u62df\u73af\u5883\uff0c\u800c\u57ce\u5e02\u5bfc\u822a\u5fc5\u7136\u6d89\u53ca\u5230\u667a\u80fd\u4f53\u7684\u8f68\u8ff9\u89c4\u5212\u548c\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u4f46\u4e0e\u5927\u6a21\u578b\u6ca1\u6709\u660e\u663e\u5173\u8054\u3002", "keywords": ["urban navigation", "simulation", "embodied AI agents"]}}
{"id": "2510.15047", "pdf": "https://arxiv.org/pdf/2510.15047", "abs": "https://arxiv.org/abs/2510.15047", "authors": ["Shiqi Chen", "Tongyao Zhu", "Zian Wang", "Jinghan Zhang", "Kangrui Wang", "Siyang Gao", "Teng Xiao", "Yee Whye Teh", "Junxian He", "Manling Li"], "title": "Internalizing World Models via Self-Play Finetuning for Agentic RL", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) as agents often struggle in out-of-distribution\n(OOD) scenarios. Real-world environments are complex and dynamic, governed by\ntask-specific rules and stochasticity, which makes it difficult for LLMs to\nground their internal knowledge in those dynamics. Under such OOD conditions,\nvanilla RL training often fails to scale; we observe Pass@k--the probability\nthat at least one of (k) sampled trajectories succeeds--drops markedly across\ntraining steps, indicating brittle exploration and limited generalization.\nInspired by model-based reinforcement learning, we hypothesize that equipping\nLLM agents with an internal world model can better align reasoning with\nenvironmental dynamics and improve decision-making. We show how to encode this\nworld model by decomposing it into two components: state representation and\ntransition modeling. Building on this, we introduce SPA, a simple reinforcement\nlearning framework that cold-starts the policy via a Self-Play supervised\nfinetuning (SFT) stage to learn the world model by interacting with the\nenvironment, then uses it to simulate future states prior to policy\noptimization. This simple initialization outperforms the online world-modeling\nbaseline and greatly boosts the RL-based agent training performance.\nExperiments across diverse environments like Sokoban, FrozenLake, and Sudoku\nshow that our approach significantly improves performance. For example, SPA\nboosts the Sokoban success rate from 25.6% to 59.8% and raises the FrozenLake\nscore from 22.1% to 70.9% for the Qwen2.5-1.5B-Instruct model.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u901a\u8fc7\u81ea\u535a\u5f08\u5fae\u8c03\u6765\u63d0\u5347LLM\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8868\u73b0\uff0c\u4f7f\u5176\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u73af\u5883\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u63d0\u5347agent\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u66f4\u597d\u5730\u89c4\u5212\u672a\u6765\u72b6\u6001\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u540c\u65f6\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86Large Language Models (LLMs)\u3002", "keywords": ["Large Language Models", "LLMs", "reinforcement learning", "agent", "world model", "decision-making"]}}
{"id": "2510.15505", "pdf": "https://arxiv.org/pdf/2510.15505", "abs": "https://arxiv.org/abs/2510.15505", "authors": ["Aron Distelzweig", "Faris Janjo\u0161", "Oliver Scheel", "Sirish Reddy Varra", "Raghu Rajan", "Joschka Boedecker"], "title": "Perfect Prediction or Plenty of Proposals? What Matters Most in Planning for Autonomous Driving", "categories": ["cs.RO"], "comment": "8 pages, 5 figures", "summary": "Traditionally, prediction and planning in autonomous driving (AD) have been\ntreated as separate, sequential modules. Recently, there has been a growing\nshift towards tighter integration of these components, known as Integrated\nPrediction and Planning (IPP), with the aim of enabling more informed and\nadaptive decision-making. However, it remains unclear to what extent this\nintegration actually improves planning performance. In this work, we\ninvestigate the role of prediction in IPP approaches, drawing on the widely\nadopted Val14 benchmark, which encompasses more common driving scenarios with\nrelatively low interaction complexity, and the interPlan benchmark, which\nincludes highly interactive and out-of-distribution driving situations. Our\nanalysis reveals that even access to perfect future predictions does not lead\nto better planning outcomes, indicating that current IPP methods often fail to\nfully exploit future behavior information. Instead, we focus on high-quality\nproposal generation, while using predictions primarily for collision checks. We\nfind that many imitation learning-based planners struggle to generate realistic\nand plausible proposals, performing worse than PDM - a simple lane-following\napproach. Motivated by this observation, we build on PDM with an enhanced\nproposal generation method, shifting the emphasis towards producing diverse but\nrealistic and high-quality proposals. This proposal-centric approach\nsignificantly outperforms existing methods, especially in out-of-distribution\nand highly interactive settings, where it sets new state-of-the-art results.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u9884\u6d4b\u548c\u89c4\u5212\uff0c\u7279\u522b\u662fintegrated prediction and planning (IPP)\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u6a21\u578b\uff0c\u4f46\u8ba8\u8bba\u4e86\u9884\u6d4b\u5728\u89c4\u5212\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u4e14\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff08\u672a\u6765\u884c\u4e3a\u9884\u6d4b\uff09\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u5173\u952e\u8bcd\u5305\u542bprediction\u548cplanning\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002", "keywords": ["prediction", "planning", "autonomous driving", "trajectory prediction", "IPP"]}}
{"id": "2510.15264", "pdf": "https://arxiv.org/pdf/2510.15264", "abs": "https://arxiv.org/abs/2510.15264", "authors": ["Weijie Wang", "Jiagang Zhu", "Zeyu Zhang", "Xiaofeng Wang", "Zheng Zhu", "Guosheng Zhao", "Chaojun Ni", "Haoxiao Wang", "Guan Huang", "Xinze Chen", "Yukun Zhou", "Wenkang Qin", "Duochao Shi", "Haoyun Li", "Guanghong Jia", "Jiwen Lu"], "title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion", "categories": ["cs.CV"], "comment": "Accepted by NeurIPS Workshop on Next Practices in Video Generation\n  and Evaluation (Short Paper Track)", "summary": "We present DriveGen3D, a novel framework for generating high-quality and\nhighly controllable dynamic 3D driving scenes that addresses critical\nlimitations in existing methodologies. Current approaches to driving scene\nsynthesis either suffer from prohibitive computational demands for extended\ntemporal generation, focus exclusively on prolonged video synthesis without 3D\nrepresentation, or restrict themselves to static single-scene reconstruction.\nOur work bridges this methodological gap by integrating accelerated long-term\nvideo generation with large-scale dynamic scene reconstruction through\nmultimodal conditional control. DriveGen3D introduces a unified pipeline\nconsisting of two specialized components: FastDrive-DiT, an efficient video\ndiffusion transformer for high-resolution, temporally coherent video synthesis\nunder text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a\nfeed-forward reconstruction module that rapidly builds 3D Gaussian\nrepresentations across time, ensuring spatial-temporal consistency. Together,\nthese components enable real-time generation of extended driving videos (up to\n$424\\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM\nof 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining\nparameter efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating dynamic 3D driving scenes using a video diffusion transformer (FastDrive-DiT). While it doesn't directly address trajectory prediction, the generation of driving scenes inherently involves reasoning about the future movement and behavior of agents within the scene. The use of a diffusion transformer, a type of large model, also contributes to the relevance, although the primary focus isn't on the language aspect of large language models. The generated scenes could potentially be used for training or evaluating trajectory prediction models.", "keywords": ["video diffusion transformer", "driving scene generation", "3D scene reconstruction", "dynamic scenes", "Bird's-Eye-View"]}}
{"id": "2510.15686", "pdf": "https://arxiv.org/pdf/2510.15686", "abs": "https://arxiv.org/abs/2510.15686", "authors": ["Taehyeon Kim", "Vishnunandan L. N. Venkatesh", "Byung-Cheol Min"], "title": "Few-Shot Demonstration-Driven Task Coordination and Trajectory Execution for Multi-Robot Systems", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we propose a novel few-shot learning framework for multi-robot\nsystems that integrate both spatial and temporal elements: Few-Shot\nDemonstration-Driven Task Coordination and Trajectory Execution (DDACE). Our\napproach leverages temporal graph networks for learning task-agnostic temporal\nsequencing and Gaussian Processes for spatial trajectory modeling, ensuring\nmodularity and generalization across various tasks. By decoupling temporal and\nspatial aspects, DDACE requires only a small number of demonstrations,\nsignificantly reducing data requirements compared to traditional learning from\ndemonstration approaches. To validate our proposed framework, we conducted\nextensive experiments in task environments designed to assess various aspects\nof multi-robot coordination-such as multi-sequence execution, multi-action\ndynamics, complex trajectory generation, and heterogeneous configurations. The\nexperimental results demonstrate that our approach successfully achieves task\nexecution under few-shot learning conditions and generalizes effectively across\ndynamic and diverse settings. This work underscores the potential of modular\narchitectures in enhancing the practicality and scalability of multi-robot\nsystems in real-world applications. Additional materials are available at\nhttps://sites.google.com/view/ddace.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory execution for multi-robot systems using few-shot learning. While it involves trajectory generation and execution, it doesn't explicitly mention or utilize large language models. The connection to trajectory prediction is present through trajectory execution and modeling, but the absence of LLMs lowers the relevance score.", "keywords": ["trajectory execution", "multi-robot systems", "trajectory modeling"]}}
{"id": "2510.15254", "pdf": "https://arxiv.org/pdf/2510.15254", "abs": "https://arxiv.org/abs/2510.15254", "authors": ["Dingya Feng", "Dingyuan Xue"], "title": "Spatiotemporal Transformers for Predicting Avian Disease Risk from Migration Trajectories", "categories": ["cs.LG"], "comment": null, "summary": "Accurate forecasting of avian disease outbreaks is critical for wildlife\nconservation and public health. This study presents a Transformer-based\nframework for predicting the disease risk at the terminal locations of\nmigratory bird trajectories. We integrate multi-source datasets, including GPS\ntracking data from Movebank, outbreak records from the World Organisation for\nAnimal Health (WOAH), and geospatial context from GADM and Natural Earth. The\nraw coordinates are processed using H3 hierarchical geospatial encoding to\ncapture spatial patterns. The model learns spatiotemporal dependencies from\nbird movement sequences to estimate endpoint disease risk. Evaluation on a\nheld-out test set demonstrates strong predictive performance, achieving an\naccuracy of 0.9821, area under the ROC curve (AUC) of 0.9803, average precision\n(AP) of 0.9299, and an F1-score of 0.8836 at the optimal threshold. These\nresults highlight the potential of Transformer architectures to support\nearly-warning systems for avian disease surveillance, enabling timely\nintervention and prevention strategies.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on predicting disease risk based on bird migration trajectories using a Transformer-based architecture. While it deals with trajectory prediction (bird migration), it doesn't explicitly involve Large Language Models. The Transformer architecture itself can be considered related to the broader field of large models, but not specifically LLMs.", "keywords": ["trajectory prediction", "migration trajectories", "Transformer"]}}
