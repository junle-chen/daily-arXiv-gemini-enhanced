{"id": "2510.14631", "pdf": "https://arxiv.org/pdf/2510.14631", "abs": "https://arxiv.org/abs/2510.14631", "authors": ["U\u00e9lison Jean Lopes dos Santos", "Alessandro Ferri", "Szilard Nistor", "Riccardo Tommasini", "Carsten Binnig", "Manisha Luthra"], "title": "Towards a Multimodal Stream Processing System", "categories": ["cs.DB"], "comment": null, "summary": "In this paper, we present a vision for a new generation of multimodal\nstreaming systems that embed MLLMs as first-class operators, enabling real-time\nquery processing across multiple modalities. Achieving this is non-trivial:\nwhile recent work has integrated MLLMs into databases for multimodal queries,\nstreaming systems require fundamentally different approaches due to their\nstrict latency and throughput requirements. Our approach proposes novel\noptimizations at all levels, including logical, physical, and semantic query\ntransformations that reduce model load to improve throughput while preserving\naccuracy. We demonstrate this with \\system{}, a prototype leveraging such\noptimizations to improve performance by more than an order of magnitude.\nMoreover, we discuss a research roadmap that outlines open research challenges\nfor building a scalable and efficient multimodal stream processing systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u6d41\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5c06\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5d4c\u5165\u4e3a\u4e00\u7b49\u7b97\u5b50\uff0c\u4ee5\u5b9e\u73b0\u8de8\u591a\u79cd\u6a21\u6001\u7684\u5b9e\u65f6\u67e5\u8be2\u5904\u7406\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u67e5\u8be2\u6570\u636e\u5e93\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u6d41\u7cfb\u7edf\u7684\u4e25\u683c\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u8981\u6c42\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u903b\u8f91\u3001\u7269\u7406\u548c\u8bed\u4e49\u67e5\u8be2\u8f6c\u6362\u7b49\u591a\u4e2a\u5c42\u9762\u7684\u4f18\u5316\uff0c\u4ee5\u51cf\u5c11\u6a21\u578b\u8d1f\u8f7d\uff0c\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "result": "\u901a\u8fc7\u539f\u578b\u7cfb\u7edf\\[\u7cfb\u7edf\u540d]\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u4f18\u5316\u65b9\u6cd5\u53ef\u4ee5\u5c06\u6027\u80fd\u63d0\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002", "conclusion": "\u672c\u6587\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6d41\u5904\u7406\u7cfb\u7edf\u63d0\u51fa\u4e86\u4e00\u4e2a\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u6982\u8ff0\u4e86\u5f00\u653e\u7684\u7814\u7a76\u6311\u6218\u3002", "summary_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u591a\u6a21\u6001\u6d41\u7cfb\u7edf\u613f\u666f\uff0c\u8be5\u7cfb\u7edf\u5c06\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5d4c\u5165\u4e3a\u4e00\u7b49\u7b97\u5b50\uff0c\u4ece\u800c\u80fd\u591f\u8de8\u591a\u79cd\u6a21\u6001\u8fdb\u884c\u5b9e\u65f6\u67e5\u8be2\u5904\u7406\u3002\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u5e76\u975e\u6613\u4e8b\uff1a\u867d\u7136\u6700\u8fd1\u7684\u7814\u7a76\u5de5\u4f5c\u5df2\u5c06MLLM\u96c6\u6210\u5230\u6570\u636e\u5e93\u4e2d\u4ee5\u8fdb\u884c\u591a\u6a21\u6001\u67e5\u8be2\uff0c\u4f46\u6d41\u7cfb\u7edf\u7531\u4e8e\u5176\u4e25\u683c\u7684\u5ef6\u8fdf\u548c\u541e\u5410\u91cf\u8981\u6c42\uff0c\u9700\u8981\u4ece\u6839\u672c\u4e0a\u4e0d\u540c\u7684\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u51fa\u4e86\u5728\u903b\u8f91\u3001\u7269\u7406\u548c\u8bed\u4e49\u67e5\u8be2\u8f6c\u6362\u7b49\u5404\u4e2a\u5c42\u9762\u7684\u65b0\u578b\u4f18\u5316\uff0c\u4ece\u800c\u51cf\u5c11\u6a21\u578b\u8d1f\u8f7d\uff0c\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002\u6211\u4eec\u901a\u8fc7\u539f\u578b\u7cfb\u7edf\\[\u7cfb\u7edf\u540d]\u5c55\u793a\u4e86\u8fd9\u4e00\u70b9\uff0c\u8be5\u539f\u578b\u7cfb\u7edf\u5229\u7528\u8fd9\u4e9b\u4f18\u5316\u5c06\u6027\u80fd\u63d0\u9ad8\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8ba8\u8bba\u4e86\u4e00\u4e2a\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u6982\u8ff0\u4e86\u6784\u5efa\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6d41\u5904\u7406\u7cfb\u7edf\u7684\u5f00\u653e\u7814\u7a76\u6311\u6218\u3002"}}
{"id": "2510.13853", "pdf": "https://arxiv.org/pdf/2510.13853", "abs": "https://arxiv.org/abs/2510.13853", "authors": ["Fabian Wenz", "Omar Bouattour", "Devin Yang", "Justin Choi", "Cecil Gregg", "Nesime Tatbul", "\u00c7a\u011fatay Demiralp"], "title": "BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "comment": "CIDR'26", "summary": "Large language models (LLMs) have been successfully applied to many tasks,\nincluding text-to-SQL generation. However, much of this work has focused on\npublicly available datasets, such as Fiben, Spider, and Bird. Our earlier work\nshowed that LLMs are much less effective in querying large private enterprise\ndata warehouses and released Beaver, the first private enterprise text-to-SQL\nbenchmark. To create Beaver, we leveraged SQL logs, which are often readily\navailable. However, manually annotating these logs to identify which natural\nlanguage questions they answer is a daunting task. Asking database\nadministrators, who are highly trained experts, to take on additional work to\nconstruct and validate corresponding natural language utterances is not only\nchallenging but also quite costly. To address this challenge, we introduce\nBenchPress, a human-in-the-loop system designed to accelerate the creation of\ndomain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses\nretrieval-augmented generation (RAG) and LLMs to propose multiple natural\nlanguage descriptions. Human experts then select, rank, or edit these drafts to\nensure accuracy and domain alignment. We evaluated BenchPress on annotated\nenterprise SQL logs, demonstrating that LLM-assisted annotation drastically\nreduces the time and effort required to create high-quality benchmarks. Our\nresults show that combining human verification with LLM-generated suggestions\nenhances annotation accuracy, benchmark reliability, and model evaluation\nrobustness. By streamlining the creation of custom benchmarks, BenchPress\noffers researchers and practitioners a mechanism for assessing text-to-SQL\nmodels on a given domain-specific workload. BenchPress is freely available via\nour public GitHub repository at\nhttps://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our\nwebsite at http://dsg-mcgraw.csail.mit.edu:5000.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
