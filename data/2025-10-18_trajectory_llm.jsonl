{"id": "2510.14702", "pdf": "https://arxiv.org/pdf/2510.14702", "abs": "https://arxiv.org/abs/2510.14702", "authors": ["Penglong Zhai", "Jie Li", "Fanyi Di", "Yue Liu", "Yifang Yuan", "Jie Huang", "Peng Wu", "Sicong Wang", "Mingyang Yin", "Tingting Hu", "Yao Xu", "Xin Li"], "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction", "categories": ["cs.AI"], "comment": "12 pages, 5 figures", "summary": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "This paper is highly relevant as it directly combines large language models (LLMs) with next point-of-interest (POI) prediction, which is a form of trajectory prediction. The abstract explicitly mentions using LLMs for predicting user's next destinations and addresses the limitations of LLMs in understanding geographical entities and mobility patterns. The proposed framework, CoAST, further strengthens the relevance by incorporating spatio-temporal trajectory data and cognitive alignment.", "keywords": ["large language models", "LLMs", "trajectory prediction", "next point-of-interest prediction", "POI prediction", "spatio-temporal trajectory", "recommendation system", "mobility patterns"]}}
{"id": "2510.14615", "pdf": "https://arxiv.org/pdf/2510.14615", "abs": "https://arxiv.org/abs/2510.14615", "authors": ["Edward Sandra", "Lander Vanroye", "Dries Dirckx", "Ruben Cartuyvels", "Jan Swevers", "Wilm Decr\u00e9"], "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models", "categories": ["cs.RO"], "comment": "This paper has been submitted and has not yet been peer reviewed or\n  accepted for publication", "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u8054\u7cfb\u3002\u8bba\u6587\u91cd\u70b9\u662f\u8fd0\u52a8\u89c4\u5212\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["motion planning", "diffusion models", "trajectory generation", "multi-modal trajectories"]}}
{"id": "2510.14627", "pdf": "https://arxiv.org/pdf/2510.14627", "abs": "https://arxiv.org/abs/2510.14627", "authors": ["Yao Zhong", "Hanzhi Chen", "Simon Schaefer", "Anran Zhang", "Stefan Leutenegger"], "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u7269\u4f53\u653e\u7f6e\u4efb\u52a1\u3002 \u867d\u7136\u5b83\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u4f7f\u7528 LLM \u6765\u7406\u89e3\u6307\u4ee4\u5e76\u751f\u6210\u8ba1\u5212\uff0c\u8fd9\u4e9b\u8ba1\u5212\u95f4\u63a5\u5f71\u54cd\u7269\u4f53\u7684\u653e\u7f6e\u548c\u53ef\u80fd\u7684\u79fb\u52a8\u8def\u5f84\u3002\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4f7f\u7528\u3002", "keywords": ["large language model", "LLM", "object placement", "robotic placement", "diffusion-based planner"]}}
{"id": "2510.14827", "pdf": "https://arxiv.org/pdf/2510.14827", "abs": "https://arxiv.org/abs/2510.14827", "authors": ["Yufei Zhu", "Shih-Min Yang", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping", "categories": ["cs.RO"], "comment": null, "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u7a7a\u8fd0\u52a8\u6a21\u5f0f\u7684\u5efa\u6a21\uff0c\u4f7f\u7528\u4e86\u795e\u7ecf\u9690\u51fd\u6570\u6765\u8868\u793a\u52a8\u6001\u5730\u56fe\uff0c\u80fd\u591f\u5bf9\u4eba\u7c7b\u8fd0\u52a8\u8f68\u8ff9\u8fdb\u884c\u5efa\u6a21\u548c\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4f7f\u7528\u4e86\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "motion mapping", "spatio-temporal motion", "motion patterns", "robot operation", "human motion", "neural implicit functions"]}}
{"id": "2510.14893", "pdf": "https://arxiv.org/pdf/2510.14893", "abs": "https://arxiv.org/abs/2510.14893", "authors": ["Helene J. Levy", "Brett T. Lopez"], "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u89c4\u5212\uff0c\u7279\u522b\u662f\u9488\u5bf9\u65e0\u4eba\u673a\u7b49\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u8f68\u8ff9\u751f\u6210\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u8f68\u8ff9\u9884\u6d4b\u548c\u8def\u5f84\u89c4\u5212\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory planning", "real-time", "motion primitive search", "trajectory segments", "collision-free", "autonomous navigation"]}}
{"id": "2510.14000", "pdf": "https://arxiv.org/pdf/2510.14000", "abs": "https://arxiv.org/abs/2510.14000", "authors": ["Mingyang Jiang", "Yueyuan Li", "Jiaru Zhang", "Songan Zhang", "Ming Yang"], "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking", "categories": ["cs.RO"], "comment": null, "summary": "The growing demand for parking has increased the need for automated parking\nplanning methods that can operate reliably in confined spaces. In restricted\nand complex environments, high-precision maneuvers are required to achieve a\nhigh success rate in planning, yet existing approaches often rely on explicit\naction modeling, which faces challenges when accurately modeling the optimal\naction distribution. In this paper, we propose DRIP, a diffusion-refined\nplanner anchored in reinforcement learning (RL) prior action distribution, in\nwhich an RL-pretrained policy provides prior action distributions to regularize\nthe diffusion training process. During the inference phase the denoising\nprocess refines these coarse priors into more precise action distributions. By\nsteering the denoising trajectory through the reinforcement learning prior\ndistribution during training, the diffusion model inherits a well-informed\ninitialization, resulting in more accurate action modeling, a higher planning\nsuccess rate, and reduced inference steps. We evaluate our approach across\nparking scenarios with varying degrees of spatial constraints. Experimental\nresults demonstrate that our method significantly improves planning performance\nin confined-space parking environments while maintaining strong generalization\nin common scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses automated parking planning in confined spaces, which relates to trajectory prediction and path planning for vehicles. While it doesn't explicitly mention large language models, it uses reinforcement learning and diffusion models for action modeling, which are related to broader AI models used in trajectory prediction. The connection to trajectory prediction is stronger than the connection to large language models.", "keywords": ["trajectory prediction", "path planning", "diffusion model", "reinforcement learning", "action modeling", "automated parking"]}}
{"id": "2510.14063", "pdf": "https://arxiv.org/pdf/2510.14063", "abs": "https://arxiv.org/abs/2510.14063", "authors": ["Nan Li", "Jiming Ren", "Haris Miller", "Samuel Coogan", "Karen M. Feigh", "Ye Zhao"], "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming", "categories": ["cs.RO"], "comment": "16 pages, 11 figures, 4 tables", "summary": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing\nattention but remains challenging in terms of scalability, spatial reasoning,\nand adaptability in obstacle-rich environments. To address these challenges, we\npropose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for\nHeterogeneous Robot Teaming, which advances MATP by introducing a novel\nobstacle-aware strategy for task assignment. First, we develop an adaptive\nHalton sequence map, the first known application of Halton sampling with\nobstacle-aware adaptation in MATP, which adjusts sampling density based on\nobstacle distribution. Second, we propose a cluster-auction-selection framework\nthat integrates obstacle-aware clustering with weighted auctions and\nintra-cluster task selection. These mechanisms jointly enable effective\ncoordination among heterogeneous robots while maintaining scalability and\nnear-optimal allocation performance. In addition, our framework leverages an\nLLM to interpret human instructions and directly guide the planner in real\ntime. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in\ntask assignment quality, scalability, adaptability to dynamic changes, and\noverall execution performance compared to state-of-the-art MATP baselines. A\nproject website is available at https://llm-oath.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-agent task assignment and planning in robotics, which is related to trajectory planning. It also mentions leveraging an LLM to interpret human instructions and guide the planner, connecting it to large language models. However, the primary focus is not trajectory prediction itself, but rather task assignment and planning with obstacle avoidance, using LLMs for instruction understanding.", "keywords": ["LLM", "task assignment", "planning", "obstacle-aware", "robot", "multi-agent"]}}
{"id": "2510.14293", "pdf": "https://arxiv.org/pdf/2510.14293", "abs": "https://arxiv.org/abs/2510.14293", "authors": ["Yushi Du", "Yixuan Li", "Baoxiong Jia", "Yutang Lin", "Pei Zhou", "Wei Liang", "Yanchao Yang", "Siyuan Huang"], "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4eba-\u4eba\u5f62\u673a\u5668\u4eba\u534f\u4f5c\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u8f68\u8ff9\u89c4\u5212\uff0c\u4ee5\u5b9e\u73b0\u534f\u540c\u642c\u8fd0\u3002\u867d\u7136\u6d89\u53ca\u5230\u8f68\u8ff9\u89c4\u5212\u548c\u52a8\u4f5c\u9884\u6d4b\uff08\u901a\u8fc7\u9884\u6d4b\u7269\u4f53\u8fd0\u52a8\u6a21\u5f0f\u548c\u4eba\u7c7b\u610f\u56fe\uff09\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory planning", "motion patterns", "intention prediction"]}}
{"id": "2510.14300", "pdf": "https://arxiv.org/pdf/2510.14300", "abs": "https://arxiv.org/abs/2510.14300", "authors": ["Weijie Shen", "Yitian Liu", "Yuhao Wu", "Zhixuan Liang", "Sijia Gu", "Dehui Wang", "Tian Nian", "Lei Xu", "Yusen Qin", "Jiangmiao Pang", "Xinping Guan", "Xiaokang Yang", "Yao Mu"], "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action (VLA) models for robotic manipulation. While it doesn't directly address trajectory prediction, the 'action' component and robotic manipulation tasks are related, suggesting a potential indirect connection. The use of Mixture of Experts (MoE) can be seen as a form of large model architecture. Therefore, there's moderate relevance.", "keywords": ["Vision-Language-Action models", "VLA models", "Mixture-of-Experts", "MoE", "robotic manipulation"]}}
{"id": "2510.14319", "pdf": "https://arxiv.org/pdf/2510.14319", "abs": "https://arxiv.org/abs/2510.14319", "authors": ["Xu Shen", "Qi Zhang", "Song Wang", "Zhen Tan", "Xinyu Zhao", "Laura Yao", "Vaishnav Tadiparthi", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi Pari", "Kwonjoon Lee", "Tianlong Chen"], "title": "Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model based multi-agent systems (MAS) excel at collaborative\nproblem solving but remain brittle to cascading errors: a single faulty step\ncan propagate across agents and disrupt the trajectory. In this paper, we\npresent MASC, a metacognitive framework that endows MAS with real-time,\nunsupervised, step-level error detection and self-correction. MASC rethinks\ndetection as history-conditioned anomaly scoring via two complementary designs:\n(1) Next-Execution Reconstruction, which predicts the embedding of the next\nstep from the query and interaction history to capture causal consistency, and\n(2) Prototype-Guided Enhancement, which learns a prototype prior over\nnormal-step embeddings and uses it to stabilize reconstruction and anomaly\nscoring under sparse context (e.g., early steps). When an anomaly step is\nflagged, MASC triggers a correction agent to revise the acting agent's output\nbefore information flows downstream. On the Who&When benchmark, MASC\nconsistently outperforms all baselines, improving step-level error detection by\nup to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers\nconsistent end-to-end gains across architectures, confirming that our\nmetacognitive monitoring and targeted correction can mitigate error propagation\nwith minimal overhead.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses multi-agent systems and uses large language models. While it doesn't directly focus on trajectory prediction in the traditional sense (e.g., predicting the path of a vehicle or pedestrian), the concept of 'trajectory' is used metaphorically to represent the sequence of actions or steps taken by the agents in the system. The 'Next-Execution Reconstruction' component could be loosely related to predicting the next state or action, similar to trajectory prediction.", "keywords": ["Large Language Model", "multi-agent systems", "error detection", "self-correction", "Next-Execution Reconstruction"]}}
{"id": "2510.14677", "pdf": "https://arxiv.org/pdf/2510.14677", "abs": "https://arxiv.org/abs/2510.14677", "authors": ["Steffen Hagedorn", "Luka Donkov", "Aron Distelzweig", "Alexandru P. Condurache"], "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u89c4\u5212\u548c\u667a\u80fd\u4f53\uff0c\u5e76\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u4ea4\u901a\u667a\u80fd\u4f53\u6a21\u578b\uff08SMART\uff09\u6765\u8bc4\u4f30\u89c4\u5212\u5668\u5728nuPlan\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u548c\u89c4\u5212\u7684\u8303\u7574\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5b66\u4e60\u5230\u7684\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["traffic agents", "planner evaluation", "closed-loop simulation", "nuPlan", "learned traffic agent model", "SMART", "planning performance"]}}
{"id": "2510.14190", "pdf": "https://arxiv.org/pdf/2510.14190", "abs": "https://arxiv.org/abs/2510.14190", "authors": ["Ruchi Sandilya", "Sumaira Perez", "Charles Lynch", "Lindsay Victoria", "Benjamin Zebley", "Derrick Matthew Buchanan", "Mahendra T. Bhati", "Nolan Williams", "Timothy J. Spellman", "Faith M. Gunning", "Conor Liston", "Logan Grosenick"], "title": "Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion models excel at generation, but their latent spaces are not\nexplicitly organized for interpretable control. We introduce ConDA (Contrastive\nDiffusion Alignment), a framework that applies contrastive learning within\ndiffusion embeddings to align latent geometry with system dynamics. Motivated\nby recent advances showing that contrastive objectives can recover more\ndisentangled and structured representations, ConDA organizes diffusion latents\nsuch that traversal directions reflect underlying dynamical factors. Within\nthis contrastively structured space, ConDA enables nonlinear trajectory\ntraversal that supports faithful interpolation, extrapolation, and controllable\ngeneration. Across benchmarks in fluid dynamics, neural calcium imaging,\ntherapeutic neurostimulation, and facial expression, ConDA produces\ninterpretable latent representations with improved controllability compared to\nlinear traversals and conditioning-based baselines. These results suggest that\ndiffusion latents encode dynamics-relevant structure, but exploiting this\nstructure requires latent organization and traversal along the latent manifold.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on improving the latent space of diffusion models for controllable generation by aligning it with system dynamics. While it doesn't explicitly mention trajectory prediction or large language models, the concepts of \"system dynamics\", \"trajectory traversal\", \"interpolation\", and \"extrapolation\" are related to trajectory prediction. The use of diffusion models, which can be considered a type of large model, also contributes to the relevance. However, the absence of direct mention of trajectory prediction or LLMs lowers the score.", "keywords": ["diffusion models", "latent space", "system dynamics", "trajectory traversal", "controllable generation", "interpolation", "extrapolation"]}}
{"id": "2510.14952", "pdf": "https://arxiv.org/pdf/2510.14952", "abs": "https://arxiv.org/abs/2510.14952", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Yibo Peng", "Tao Huang", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang", "Chang Xu"], "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u8bed\u8a00\u6a21\u578b\uff08Transformer\uff09\u6765\u6307\u5bfc\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u6d89\u53ca\u8bed\u8a00\u5230\u52a8\u4f5c\u7684\u8f6c\u6362\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u751f\u6210\u53ef\u6267\u884c\u7684\u52a8\u4f5c\uff0c\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u76f8\u5173\u9886\u57df\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u4e86\u4f7f\u7528language-grounded motion latents\uff0c\u8868\u660e\u8bed\u8a00\u6a21\u578b\u53c2\u4e0e\u4e86\u8fd0\u52a8\u751f\u6210\u8fc7\u7a0b\u3002", "keywords": ["language model", "humanoid locomotion", "transformer", "motion generation", "language-grounded motion latents"]}}
{"id": "2509.26255", "pdf": "https://arxiv.org/pdf/2509.26255", "abs": "https://arxiv.org/abs/2509.26255", "authors": ["Yichao Liang", "Dat Nguyen", "Cambridge Yang", "Tianyang Li", "Joshua B. Tenenbaum", "Carl Edward Rasmussen", "Adrian Weller", "Zenna Tavares", "Tom Silver", "Kevin Ellis"], "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "41 pages. The last two authors contributed equally in co-advising", "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on learning abstract models of dynamic worlds for robot planning, which can be related to trajectory prediction in dynamic environments. It also uses Large Language Models (LLMs) for generating proposals during the learning process. While not directly focusing on trajectory prediction, the robot planning aspect and LLM usage contribute to a moderate relevance.", "keywords": ["robot planning", "dynamic worlds", "LLM", "causal processes"]}}
{"id": "2510.14828", "pdf": "https://arxiv.org/pdf/2510.14828", "abs": "https://arxiv.org/abs/2510.14828", "authors": ["Jinrui Liu", "Bingyan Nie", "Boyu Li", "Yaran Chen", "Yuze Wang", "Shunsen He", "Haoran Li"], "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses robot planning using large language models and reinforcement learning. While not directly focused on trajectory prediction, the 'planning' aspect can involve generating trajectories. The use of Qwen2.5-VL-3B and GPT-4o-mini clearly relates to large language models.", "keywords": ["large language models", "LLMs", "robot planning", "reinforcement learning", "Qwen2.5-VL-3B", "GPT-4o-mini"]}}
