{"id": "2511.21160", "pdf": "https://arxiv.org/pdf/2511.21160", "abs": "https://arxiv.org/abs/2511.21160", "authors": ["Wu Sai", "Xia Ruichen", "Yang Dingyu", "Wang Rui", "Lai Huihang", "Guan Jiarui", "Bai Jiameng", "Zhang Dongxiang", "Tang Xiu", "Xie Zhongle", "Lu Peng", "Chen Gang"], "title": "MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference", "categories": ["cs.DB"], "comment": null, "summary": "The increasing demand for deep neural inference within database environments has driven the emergence of AI-native DBMSs. However, existing solutions either rely on model-centric designs requiring developers to manually select, configure, and maintain models, resulting in high development overhead, or adopt task-centric AutoML approaches with high computational costs and poor DBMS integration. We present MorphingDB, a task-centric AI-native DBMS that automates model storage, selection, and inference within PostgreSQL. To enable flexible, I/O-efficient storage of deep learning models, we first introduce specialized schemas and multi-dimensional tensor data types to support BLOB-based all-in-one and decoupled model storage. Then we design a transfer learning framework for model selection in two phases, which builds a transferability subspace via offline embedding of historical tasks and employs online projection through feature-aware mapping for real-time tasks. To further optimize inference throughput, we propose pre-embedding with vectoring sharing to eliminate redundant computations and DAG-based batch pipelines with cost-aware scheduling to minimize the inference time. Implemented as a PostgreSQL extension with LibTorch, MorphingDB outperforms AI-native DBMSs (EvaDB, Madlib, GaussML) and AutoML platforms (AutoGluon, AutoKeras, AutoSklearn) across nine public datasets, encompassing series, NLP, and image tasks. Our evaluation demonstrates a robust balance among accuracy, resource consumption, and time cost in model selection and significant gains in throughput and resource efficiency.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21307", "pdf": "https://arxiv.org/pdf/2511.21307", "abs": "https://arxiv.org/abs/2511.21307", "authors": ["Xinyi Zhang", "Liang Liang", "Anastasia Ailamaki", "Jianliang Xu"], "title": "HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads", "categories": ["cs.DB"], "comment": "Accepted to SIGMOD 2026. This is the extended technical report", "summary": "Indexes are critical for efficient data retrieval and updates in modern databases. Recent advances in machine learning have led to the development of learned indexes, which model the cumulative distribution function of data to predict search positions and accelerate query processing. While learned indexes substantially outperform traditional structures for point lookups, they often suffer from high tail latency, suboptimal range query performance, and inconsistent effectiveness across diverse workloads. To address these challenges, this paper proposes HIRE, a hybrid in-memory index structure designed to deliver efficient performance consistently. HIRE combines the structural and performance robustness of traditional indexes with the predictive power of model-based prediction to reduce search overhead while maintaining worst-case stability. Specifically, it employs (1) hybrid leaf nodes adaptive to varying data distributions and workloads, (2) model-accelerated internal nodes augmented by log-based updates for efficient updates, (3) a nonblocking, cost-driven recalibration mechanism for dynamic data, and (4) an inter-level optimized bulk-loading algorithm accounting for leaf and internal-node errors. Experimental results on multiple real-world datasets demonstrate that HIRE outperforms both state-of-the-art learned indexes and traditional structures in range-query throughput, tail latency, and overall stability. Compared to state-of-the-art learned indexes and traditional indexes, HIRE achieves up to 41.7$\\times$ higher throughput under mixed workloads, reduces tail latency by up to 98% across varying scenarios.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21607", "pdf": "https://arxiv.org/pdf/2511.21607", "abs": "https://arxiv.org/abs/2511.21607", "authors": ["Zarin Tahia Hossain", "Mostafa Milani"], "title": "Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation", "categories": ["cs.DB", "cs.LG"], "comment": "To appear in conference proceedings", "summary": "Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.20677", "pdf": "https://arxiv.org/pdf/2511.20677", "abs": "https://arxiv.org/abs/2511.20677", "authors": ["Saleh Almohaimeed", "May Alsofyani", "Saad Almohaimeed", "Mansour Al Ghanim", "Liqiang Wang"], "title": "Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic", "categories": ["cs.CL", "cs.DB"], "comment": "Accepted at IJCNN 2025 (to appear in IEEE/IJCNN proceedings). This arXiv submission corresponds to the camera-ready version", "summary": "In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u963f\u62c9\u4f2f\u8bed\u8de8\u9886\u57df\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6587\u672c\u5230 SQL \u6570\u636e\u96c6 Ar-SParC\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86 GAT corrector \u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u963f\u62c9\u4f2f\u8bed\u7684\u8de8\u9886\u57df\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6587\u672c\u5230 SQL \u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u8be5\u4efb\u52a1\u5728\u963f\u62c9\u4f2f\u8bed\u4e0a\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b 3,450 \u4e2a\u95ee\u9898\u5e8f\u5217\u7684 Ar-SParC \u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u4e86 GAT corrector \u65b9\u6cd5\uff0c\u7ed3\u5408\u4e0d\u540c\u7684 prompt \u5de5\u7a0b\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "GAT corrector \u5728\u96f6\u6837\u672c\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bbe\u7f6e\u4e0b\uff0c\u5206\u522b\u5e73\u5747\u63d0\u5347\u4e86 1.9% \u7684\u6267\u884c\u51c6\u786e\u7387 (EX) \u548c\u4ea4\u4e92\u51c6\u786e\u7387 (IX)\uff0c\u4ee5\u53ca 1.72% EX \u548c 0.92% IX\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684 Ar-SParC \u6570\u636e\u96c6\u548c GAT corrector \u65b9\u6cd5\u4e3a\u963f\u62c9\u4f2f\u8bed\u6587\u672c\u5230 SQL \u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\u548c\u89e3\u51b3\u65b9\u6848\u3002", "summary_zh": "\u8fd1\u5e74\u6765\uff0c\u8de8\u9886\u57df\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6587\u672c\u5230 SQL \u4efb\u52a1\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\uff0c\u5b83\u4f7f\u7528\u6237\u65e0\u9700\u5177\u5907 SQL \u77e5\u8bc6\u5373\u53ef\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u6570\u636e\u5e93\u8fdb\u884c\u5bf9\u8bdd\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u53ef\u7528\u7684\u6570\u636e\u96c6\u548c\u7814\u7a76\u90fd\u96c6\u4e2d\u5728\u82f1\u8bed\u4e0a\uff0c\u4e5f\u6709\u4e00\u4e9b\u5de5\u4f5c\u6d89\u53ca\u4e2d\u6587\u3002\u4f46\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u8fd8\u6ca1\u6709\u4eba\u81f4\u529b\u4e8e\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u4e2d\u7684\u8fd9\u9879\u4efb\u52a1\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86 Ar-SParC\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u8de8\u9886\u57df\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6587\u672c\u5230 SQL \u6570\u636e\u96c6\u3002\u8be5\u6570\u636e\u96c6\u7531 3,450 \u4e2a\u76f8\u4e92\u5173\u8054\u7684\u95ee\u9898\u5e8f\u5217\u7ec4\u6210\uff0c\u6bcf\u4e2a\u5e8f\u5217\u5e73\u5747\u5305\u542b\u5927\u7ea6\u4e09\u4e2a\u95ee\u9898\uff0c\u603b\u5171\u5305\u542b 10225 \u4e2a\u95ee\u9898\u53ca\u5176\u5bf9\u5e94\u7684 SQL \u67e5\u8be2\u3002\u6211\u4eec\u4f7f\u7528\u4e24\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b GPT-3.5-turbo \u548c GPT-4.5-turbo\uff0c\u5728 Ar-SParC \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86 40 \u9879\u5b9e\u9a8c\uff0c\u5e94\u7528\u4e86 10 \u79cd\u4e0d\u540c\u7684 prompt \u5de5\u7a0b\u6280\u672f\uff0c\u5305\u62ec\u56db\u79cd\u95ee\u9898\u8868\u793a\u65b9\u6cd5\u548c\u516d\u79cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u6280\u672f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3a GAT corrector \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u589e\u5f3a\u4e86\u6240\u6709 40 \u9879\u5b9e\u9a8c\u7684\u6027\u80fd\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u6267\u884c\u51c6\u786e\u7387 (EX) \u5e73\u5747\u63d0\u9ad8\u4e86 1.9%\uff0c\u4ea4\u4e92\u51c6\u786e\u7387 (IX) \u5e73\u5747\u63d0\u9ad8\u4e86 1.9%\uff0c\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bbe\u7f6e\u4e0b\uff0c\u6267\u884c\u51c6\u786e\u7387 (EX) \u5e73\u5747\u63d0\u9ad8\u4e86 1.72%\uff0c\u4ea4\u4e92\u51c6\u786e\u7387 (IX) \u5e73\u5747\u63d0\u9ad8\u4e86 0.92%\u3002\u6700\u540e\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u4e24\u6b21\u989d\u5916\u7684\u6d88\u878d\u7814\u7a76\uff0c\u4ee5\u89e3\u91ca\u4e3a\u4ec0\u4e48 GAT corrector \u4f18\u4e8e\u4e4b\u524d\u7684 GAT verifier \u6280\u672f\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u963f\u62c9\u4f2f\u8bed\u3002"}}
{"id": "2511.20691", "pdf": "https://arxiv.org/pdf/2511.20691", "abs": "https://arxiv.org/abs/2511.20691", "authors": ["Lijun Shang", "Yadong Yu", "Wenqiang Kang", "Jian Zhou", "Dongyue Gao", "Pan Xiang", "Zhe Liu", "Mengyan Dai", "Zhonglu Guo", "Zhimei Sun"], "title": "LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data", "categories": ["cs.CL", "cond-mat.mtrl-sci", "cs.DB"], "comment": "100 pages (18 pages main text, 82 pages supplementary material), 5 figures. Supplementary material starts from page 19", "summary": "Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21413", "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Ke\u00dfler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21448", "pdf": "https://arxiv.org/pdf/2511.21448", "abs": "https://arxiv.org/abs/2511.21448", "authors": ["Rebeka Toth", "Tamas Bisztray", "Richard Dubniczky"], "title": "Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework", "categories": ["cs.CR", "cs.AI", "cs.DB"], "comment": null, "summary": "Phishing and spam emails remain a major cybersecurity threat, with attackers increasingly leveraging Large Language Models (LLMs) to craft highly deceptive content. This study presents a comprehensive email dataset containing phishing, spam, and legitimate messages, explicitly distinguishing between human- and LLM-generated content. Each email is annotated with its category, emotional appeal (e.g., urgency, fear, authority), and underlying motivation (e.g., link-following, credential theft, financial fraud). We benchmark multiple LLMs on their ability to identify these emotional and motivational cues and select the most reliable model to annotate the full dataset. To evaluate classification robustness, emails were also rephrased using several LLMs while preserving meaning and intent. A state-of-the-art LLM was then assessed on its performance across both original and rephrased emails using expert-labeled ground truth. The results highlight strong phishing detection capabilities but reveal persistent challenges in distinguishing spam from legitimate emails. Our dataset and evaluation framework contribute to improving AI-assisted email security systems. To support open science, all code, templates, and resources are available on our project site.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21661", "pdf": "https://arxiv.org/pdf/2511.21661", "abs": "https://arxiv.org/abs/2511.21661", "authors": ["Beth Plale", "Neelesh Karthikeyan", "Isuru Gamage", "Joe Stubbs", "Sachith Withana"], "title": "AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI", "categories": ["cs.DC", "cs.DB"], "comment": null, "summary": "AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
