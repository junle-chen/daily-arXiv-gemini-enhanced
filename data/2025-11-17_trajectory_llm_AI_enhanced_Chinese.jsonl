{"id": "2511.10203", "pdf": "https://arxiv.org/pdf/2511.10203", "abs": "https://arxiv.org/abs/2511.10203", "authors": ["Stephane Da Silva Martins", "Emanuel Aldea", "Sylvie Le H\u00e9garat-Mascle"], "title": "VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Paper accepted at WACV 2026", "summary": "Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant to trajectory prediction as it directly addresses multi-agent trajectory forecasting using a novel transformer-based approach. While it doesn't explicitly use or mention Large Language Models (LLMs), the use of a transformer architecture, a common backbone for LLMs, and the focus on intent understanding and social interaction modeling suggest a connection to the broader field of AI and potentially future integration with LLMs for enhanced contextual understanding. The paper focuses on vision and intent, making it related to understanding the context surrounding trajectory prediction.", "keywords": ["trajectory prediction", "multi-agent", "transformer", "social attention", "intent", "goal-conditioned"]}, "AI": {"tldr": "VISTA\u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52\u76ee\u6807\u6761\u4ef6Transformer\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u78b0\u649e\u5e76\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6355\u6349\u667a\u80fd\u4f53\u7684\u957f\u671f\u76ee\u6807\u548c\u7ec6\u7c92\u5ea6\u7684\u793e\u4ea4\u4e92\u52a8\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0d\u771f\u5b9e\u3002", "method": "VISTA\u7ed3\u5408\u4e86\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\u3001\u793e\u4ea4Token\u6ce8\u610f\u529b\u673a\u5236\u548c\u6210\u5bf9\u6ce8\u610f\u529b\u56fe\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u3002", "result": "\u5728MADRAS\u548cSDD\u6570\u636e\u96c6\u4e0a\uff0cVISTA\u53d6\u5f97\u4e86state-of-the-art\u7684\u51c6\u786e\u7387\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u78b0\u649e\u3002", "conclusion": "VISTA\u80fd\u591f\u751f\u6210\u7b26\u5408\u793e\u4ea4\u89c4\u8303\u3001\u611f\u77e5\u76ee\u6807\u4e14\u53ef\u89e3\u91ca\u7684\u8f68\u8ff9\uff0c\u4f7f\u5176\u5728\u5b89\u5168\u6538\u5173\u7684\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\u3002", "summary_zh": "\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u5bf9\u4e8e\u5728\u5bc6\u96c6\u3001\u4ea4\u4e92\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u81ea\u4e3b\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u65e0\u6cd5\u8054\u5408\u6355\u6349\u667a\u80fd\u4f53\u7684\u957f\u671f\u76ee\u6807\u548c\u4ed6\u4eec\u7684\u7ec6\u7c92\u5ea6\u793e\u4ea4\u4e92\u52a8\uff0c\u8fd9\u5bfc\u81f4\u4e86\u4e0d\u5207\u5b9e\u9645\u7684\u591a\u667a\u80fd\u4f53\u672a\u6765\u3002\u6211\u4eec\u63d0\u51fa\u4e86VISTA\uff0c\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u7684\u9012\u5f52\u76ee\u6807\u6761\u4ef6Transformer\u3002VISTA\u7ed3\u5408\u4e86\uff08i\uff09\u4e00\u4e2a\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u6a21\u5757\uff0c\u5b83\u5c06\u957f\u65f6\u7a0b\u610f\u56fe\u4e0e\u8fc7\u53bb\u8fd0\u52a8\u6574\u5408\uff0c\uff08ii\uff09\u4e00\u4e2a\u793e\u4ea4token\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u4e8e\u8de8\u667a\u80fd\u4f53\u7684\u7075\u6d3b\u4ea4\u4e92\u5efa\u6a21\uff0c\u4ee5\u53ca\uff08iii\uff09\u6210\u5bf9\u6ce8\u610f\u529b\u56fe\uff0c\u4f7f\u5f97\u793e\u4ea4\u5f71\u54cd\u6a21\u5f0f\u5728\u63a8\u7406\u65f6\u53ef\u89e3\u91ca\u3002\u6211\u4eec\u7684\u6a21\u578b\u5c06\u5355\u667a\u80fd\u4f53\u76ee\u6807\u6761\u4ef6\u9884\u6d4b\u8f6c\u5316\u4e3a\u4e00\u4e2a\u8fde\u8d2f\u7684\u591a\u667a\u80fd\u4f53\u9884\u6d4b\u6846\u67b6\u3002\u9664\u4e86\u6807\u51c6\u7684\u4f4d\u79fb\u6307\u6807\u5916\uff0c\u6211\u4eec\u8fd8\u8bc4\u4f30\u8f68\u8ff9\u78b0\u649e\u7387\u4f5c\u4e3a\u8054\u5408\u771f\u5b9e\u6027\u7684\u5ea6\u91cf\u3002\u5728\u9ad8\u5bc6\u5ea6\u7684MADRAS\u57fa\u51c6\u6d4b\u8bd5\u548cSDD\u4e0a\uff0cVISTA\u5b9e\u73b0\u4e86state-of-the-art\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5927\u5e45\u51cf\u5c11\u4e86\u78b0\u649e\u3002\u5728MADRAS\u4e0a\uff0c\u5b83\u5c06\u5f3a\u57fa\u7ebf\u7684\u5e73\u5747\u78b0\u649e\u7387\u4ece2.14%\u964d\u4f4e\u52300.03%\uff0c\u5728SDD\u4e0a\uff0c\u5b83\u5b9e\u73b0\u4e86\u96f6\u78b0\u649e\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86ADE\u3001FDE\u548cminFDE\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0cVISTA\u751f\u6210\u4e86\u7b26\u5408\u793e\u4f1a\u89c4\u8303\u3001\u611f\u77e5\u76ee\u6807\u4e14\u53ef\u89e3\u91ca\u7684\u8f68\u8ff9\uff0c\u4f7f\u5176\u5728\u5b89\u5168\u5173\u952e\u7684\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u5177\u6709\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2511.10418", "pdf": "https://arxiv.org/pdf/2511.10418", "abs": "https://arxiv.org/abs/2511.10418", "authors": ["Yaqiao Zhu", "Hongkai Wen", "Mark Birkin", "Man Luo"], "title": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models", "categories": ["cs.DB"], "comment": null, "summary": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u57ce\u5e02\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u9884\u6d4b\u5206\u6790\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u201c\u57ce\u5e02\u8ba1\u7b97\u201d\u3001\u201c\u9884\u6d4b\u5206\u6790\u201d\u7b49\u5173\u952e\u8bcd\u6697\u793a\u4e86\u53ef\u80fd\u5305\u542b\u4e0e\u79fb\u52a8\u5bf9\u8c61\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u7684\u5e94\u7528\u573a\u666f\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u63d0\u5230\u7684\u201c\u52a8\u6001\u6a21\u62df\u201d\u548c\u201c\u611f\u77e5\u3001\u7a7a\u95f4\u7406\u89e3\u3001\u63a8\u7406\u548c\u9884\u6d4b\u201d\u4efb\u52a1\u5c42\u7ea7\u4e5f\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002", "keywords": ["Large Language Models", "LLMs", "urban computing", "predictive analytics", "prediction", "dynamic simulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.09735", "pdf": "https://arxiv.org/pdf/2511.09735", "abs": "https://arxiv.org/abs/2511.09735", "authors": ["Ahmed Alia", "Mohcine Chraibi", "Armin Seyfried"], "title": "Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 9 figures, 4 tables", "summary": "In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86Social LSTM\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "pedestrian trajectory prediction", "Social LSTM", "deep learning", "collision avoidance"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.10411", "pdf": "https://arxiv.org/pdf/2511.10411", "abs": "https://arxiv.org/abs/2511.10411", "authors": ["Benjamin Stoler", "Jonathan Francis", "Jean Oh"], "title": "LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction", "categories": ["cs.RO"], "comment": "8 pages, 3 figures", "summary": "Methods for trajectory prediction in Autonomous Driving must contend with rare, safety-critical scenarios that make reliance on real-world data collection alone infeasible. To assess robustness under such conditions, we propose new long-tail evaluation settings that repartition datasets to create challenging out-of-distribution (OOD) test sets. We first introduce a safety-informed scenario factorization framework, which disentangles scenarios into discrete ego and social contexts. Building on analogies to compositional zero-shot image-labeling in Computer Vision, we then hold out novel context combinations to construct challenging closed-world and open-world settings. This process induces OOD performance gaps in future motion prediction of 5.0% and 14.7% in closed-world and open-world settings, respectively, relative to in-distribution performance for a state-of-the-art baseline. To improve generalization, we extend task-modular gating networks to operate within trajectory prediction models, and develop an auxiliary, difficulty-prediction head to refine internal representations. Our strategies jointly reduce the OOD performance gaps to 2.8% and 11.5% in the two settings, respectively, while still improving in-distribution performance.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u5173\u6ce8\u7684\u573a\u666f\u548c\u6cdb\u5316\u80fd\u529b\u4e0e\u5927\u578b\u6a21\u578b\u7684\u7814\u7a76\u65b9\u5411\u6709\u4e00\u5b9a\u5173\u8054\u6027\u3002", "keywords": ["trajectory prediction", "Autonomous Driving", "long-tail", "zero-shot generalization", "out-of-distribution", "motion prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.10110", "pdf": "https://arxiv.org/pdf/2511.10110", "abs": "https://arxiv.org/abs/2511.10110", "authors": ["Kamil Dreczkowski", "Pietro Vitiello", "Vitalis Vosylius", "Edward Johns"], "title": "Learning a Thousand Tasks in a Day", "categories": ["cs.RO"], "comment": "This is the author's version of the work. It is posted here by permission of the AAAS for personal use, not for redistribution. The definitive version was published in Science Robotics on 12 November 2025, DOI: https://www.science.org/doi/10.1126/scirobotics.adv7594. Link to project website: https://www.robot-learning.uk/learning-1000-tasks", "summary": "Humans are remarkably efficient at learning tasks from demonstrations, but today's imitation learning methods for robot manipulation often require hundreds or thousands of demonstrations per task. We investigate two fundamental priors for improving learning efficiency: decomposing manipulation trajectories into sequential alignment and interaction phases, and retrieval-based generalisation. Through 3,450 real-world rollouts, we systematically study this decomposition. We compare different design choices for the alignment and interaction phases, and examine generalisation and scaling trends relative to today's dominant paradigm of behavioural cloning with a single-phase monolithic policy. In the few-demonstrations-per-task regime (<10 demonstrations), decomposition achieves an order of magnitude improvement in data efficiency over single-phase learning, with retrieval consistently outperforming behavioural cloning for both alignment and interaction. Building on these insights, we develop Multi-Task Trajectory Transfer (MT3), an imitation learning method based on decomposition and retrieval. MT3 learns everyday manipulation tasks from as little as a single demonstration each, whilst also generalising to novel object instances. This efficiency enables us to teach a robot 1,000 distinct everyday tasks in under 24 hours of human demonstrator time. Through 2,200 additional real-world rollouts, we reveal MT3's capabilities and limitations across different task families. Videos of our experiments can be found on at https://www.robot-learning.uk/learning-1000-tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u6a21\u4eff\u5b66\u4e60\uff0c\u76ee\u6807\u662f\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002\u867d\u7136\u63d0\u5230\u4e86\u8f68\u8ff9\u5206\u89e3\u548c\u8f68\u8ff9\u8fc1\u79fb\uff08MT3\uff09\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u3002 \u76f8\u5173\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u8f68\u8ff9\u7684\u5efa\u6a21\u548c\u5b66\u4e60\u4e0a\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u660e\u786e\u5173\u8054\u3002", "keywords": ["trajectory transfer", "imitation learning", "robot manipulation", "trajectories", "decomposition"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.10403", "pdf": "https://arxiv.org/pdf/2511.10403", "abs": "https://arxiv.org/abs/2511.10403", "authors": ["Mingxing Peng", "Ruoyu Yao", "Xusen Guo", "Jun Ma"], "title": "nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 3 figures", "summary": "Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on closed-loop planning for autonomous driving and introduces a new benchmark (nuPlan-R) that incorporates learning-based reactive multi-agent simulation. While it doesn't directly use large language models, the use of diffusion-based reactive agents and the overall focus on realistic multi-agent interactions related to trajectory prediction increase its relevance. The connection to trajectory prediction is stronger than to large language models, but the use of learning-based methods warrants a moderate relevance score.", "keywords": ["trajectory prediction", "autonomous driving", "closed-loop planning", "multi-agent simulation", "diffusion models", "reactive agents"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.09820", "pdf": "https://arxiv.org/pdf/2511.09820", "abs": "https://arxiv.org/abs/2511.09820", "authors": ["Jeongho Min", "Dongyoung Kim", "Jaehyup Lee"], "title": "From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to WACV 2026, 10pages, 4 figures", "summary": "Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a large language model (LLM) to infer location from street-view images, which is then used to retrieve satellite images. While not directly related to trajectory prediction, the application (autonomous navigation, localization) is relevant, and the use of LLMs is a key component. The connection to trajectory prediction is indirect, but the use of LLMs in a navigation-related task makes it moderately relevant.", "keywords": ["Large Language Model", "LLM", "autonomous navigation", "localization"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.10598", "pdf": "https://arxiv.org/pdf/2511.10598", "abs": "https://arxiv.org/abs/2511.10598", "authors": ["Raghav Adhikari", "Sachet Khatiwada", "Suman Poudel"], "title": "Optimizing the flight path for a scouting Uncrewed Aerial Vehicle", "categories": ["cs.RO", "eess.SY"], "comment": "This paper was prepared as an end of semester project for ME8710: Engineering Optimization, Clemson University. Consists of 7 pages and 8 figures", "summary": "Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u7684\u8def\u5f84\u4f18\u5316\uff0c\u5c5e\u4e8e\u79fb\u52a8\u7269\u4f53\u8def\u5f84\u89c4\u5212\u7684\u8303\u7574\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u4f46\u8bba\u6587\u6ca1\u6709\u63d0\u53ca\u4efb\u4f55\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5bb9\u3002", "keywords": ["trajectory prediction", "path planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.09737", "pdf": "https://arxiv.org/pdf/2511.09737", "abs": "https://arxiv.org/abs/2511.09737", "authors": ["Bram Grooten", "Patrick MacAlpine", "Kaushik Subramanian", "Peter Stone", "Peter R. Wurman"], "title": "Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted as an oral at AAAI 2026. For code and videos, please see https://github.com/bramgrooten/sparc", "summary": "Generalization to unseen environments is a significant challenge in the field of robotics and control. In this work, we focus on contextual reinforcement learning, where agents act within environments with varying contexts, such as self-driving cars or quadrupedal robots that need to operate in different terrains or weather conditions than they were trained for. We tackle the critical task of generalizing to out-of-distribution (OOD) settings, without access to explicit context information at test time. Recent work has addressed this problem by training a context encoder and a history adaptation module in separate stages. While promising, this two-phase approach is cumbersome to implement and train. We simplify the methodology and introduce SPARC: single-phase adaptation for robust control. We test SPARC on varying contexts within the high-fidelity racing simulator Gran Turismo 7 and wind-perturbed MuJoCo environments, and find that it achieves reliable and robust OOD generalization.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u8f66\u8f86\u63a7\u5236\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u201c\u8f68\u8ff9\u9884\u6d4b\u201d\u6216\u201c\u5927\u8bed\u8a00\u6a21\u578b\u201d\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u5b66\u4e60\u5982\u4f55\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u63a7\u5236\u8f66\u8f86\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u7684\u201cself-driving cars\u201d\u548c\u201cracing simulator\u201d\u6697\u793a\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u5e94\u7528\u573a\u666f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7ed3\u5408\u7684\u7814\u7a76\u65b9\u5411\u662f\u76f8\u5173\u7684\u3002", "keywords": ["self-driving cars", "robotics", "control", "reinforcement learning", "generalization"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.10586", "pdf": "https://arxiv.org/pdf/2511.10586", "abs": "https://arxiv.org/abs/2511.10586", "authors": ["Omid Mirzaeedodangeh", "Eliot Shekhtman", "Nikolai Matni", "Lars Lindemann"], "title": "Safe Planning in Interactive Environments via Iterative Policy Updates and Adversarially Robust Conformal Prediction", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "Safe planning of an autonomous agent in interactive environments -- such as the control of a self-driving vehicle among pedestrians and human-controlled vehicles -- poses a major challenge as the behavior of the environment is unknown and reactive to the behavior of the autonomous agent. This coupling gives rise to interaction-driven distribution shifts where the autonomous agent's control policy may change the environment's behavior, thereby invalidating safety guarantees in existing work. Indeed, recent works have used conformal prediction (CP) to generate distribution-free safety guarantees using observed data of the environment. However, CP's assumption on data exchangeability is violated in interactive settings due to a circular dependency where a control policy update changes the environment's behavior, and vice versa. To address this gap, we propose an iterative framework that robustly maintains safety guarantees across policy updates by quantifying the potential impact of a planned policy update on the environment's behavior. We realize this via adversarially robust CP where we perform a regular CP step in each episode using observed data under the current policy, but then transfer safety guarantees across policy updates by analytically adjusting the CP result to account for distribution shifts. This adjustment is performed based on a policy-to-trajectory sensitivity analysis, resulting in a safe, episodic open-loop planner. We further conduct a contraction analysis of the system providing conditions under which both the CP results and the policy updates are guaranteed to converge. We empirically demonstrate these safety and convergence guarantees on a two-dimensional car-pedestrian case study. To the best of our knowledge, these are the first results that provide valid safety guarantees in such interactive settings.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5b89\u5168\u89c4\u5212\uff0c\u6d89\u53ca\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u884c\u4eba\u7b49\u4ea4\u4e92\uff0c\u4f7f\u7528\u4e86Conformal Prediction\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u5b89\u5168\u89c4\u5212\u548c\u8f68\u8ff9\u9884\u6d4b\u7684\u80cc\u666f\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e3b\u9898\u76f8\u5173\u3002\u5173\u952e\u8bcd\u5982\u201csafe planning\u201d\u3001\u201cinteractive environments\u201d\u548c\u201cpedestrian\u201d\u7b49\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u3002", "keywords": ["safe planning", "interactive environments", "trajectory", "pedestrian", "autonomous agent", "self-driving vehicle"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.09955", "pdf": "https://arxiv.org/pdf/2511.09955", "abs": "https://arxiv.org/abs/2511.09955", "authors": ["Uday Bhaskar", "Rishabh Bhattacharya", "Avinash Patel", "Sarthak Khoche", "Praveen Anil Kulkarni", "Naresh Manwani"], "title": "Robust Object Detection with Pseudo Labels from VLMs using Per-Object Co-teaching", "categories": ["cs.CV"], "comment": null, "summary": "Foundation models, especially vision-language models (VLMs), offer compelling zero-shot object detection for applications like autonomous driving, a domain where manual labelling is prohibitively expensive. However, their detection latency and tendency to hallucinate predictions render them unsuitable for direct deployment. This work introduces a novel pipeline that addresses this challenge by leveraging VLMs to automatically generate pseudo-labels for training efficient, real-time object detectors. Our key innovation is a per-object co-teaching-based training strategy that mitigates the inherent noise in VLM-generated labels. The proposed per-object coteaching approach filters noisy bounding boxes from training instead of filtering the entire image. Specifically, two YOLO models learn collaboratively, filtering out unreliable boxes from each mini-batch based on their peers' per-object loss values. Overall, our pipeline provides an efficient, robust, and scalable approach to train high-performance object detectors for autonomous driving, significantly reducing reliance on costly human annotation. Experimental results on the KITTI dataset demonstrate that our method outperforms a baseline YOLOv5m model, achieving a significant mAP@0.5 boost ($31.12\\%$ to $46.61\\%$) while maintaining real-time detection latency. Furthermore, we show that supplementing our pseudo-labelled data with a small fraction of ground truth labels ($10\\%$) leads to further performance gains, reaching $57.97\\%$ mAP@0.5 on the KITTI dataset. We observe similar performance improvements for the ACDC and BDD100k datasets.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Vision-Language Models (VLMs) to generate pseudo-labels for object detection, particularly in the context of autonomous driving. While it doesn't directly address trajectory prediction, the application to autonomous driving suggests a potential link, as object detection is often a component of trajectory prediction systems. The use of VLMs also makes it relevant to the large language model theme. However, the primary focus is on object detection, not trajectory prediction itself.", "keywords": ["Vision-Language Models", "VLMs", "Foundation Models", "Autonomous Driving", "Object Detection"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.10017", "pdf": "https://arxiv.org/pdf/2511.10017", "abs": "https://arxiv.org/abs/2511.10017", "authors": ["Xinyi Wang", "Xun Yang", "Yanlong Xu", "Yuchen Wu", "Zhen Li", "Na Zhao"], "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models", "categories": ["cs.CV"], "comment": "NeurIPS 2025", "summary": "Effective human-agent collaboration in physical environments requires understanding not only what to act upon, but also where the actionable elements are and how to interact with them. Existing approaches often operate at the object level or disjointedly handle fine-grained affordance reasoning, lacking coherent, instruction-driven grounding and reasoning. In this work, we introduce a new task: Fine-grained 3D Embodied Reasoning, which requires an agent to predict, for each referenced affordance element in a 3D scene, a structured triplet comprising its spatial location, motion type, and motion axis, based on a task instruction. To solve this task, we propose AffordBot, a novel framework that integrates Multimodal Large Language Models (MLLMs) with a tailored chain-of-thought (CoT) reasoning paradigm. To bridge the gap between 3D input and 2D-compatible MLLMs, we render surround-view images of the scene and project 3D element candidates into these views, forming a rich visual representation aligned with the scene geometry. Our CoT pipeline begins with an active perception stage, prompting the MLLM to select the most informative viewpoint based on the instruction, before proceeding with step-by-step reasoning to localize affordance elements and infer plausible interaction motions. Evaluated on the SceneFun3D dataset, AffordBot achieves state-of-the-art performance, demonstrating strong generalization and physically grounded reasoning with only 3D point cloud input and MLLMs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u901a\u8fc7\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c3D\u573a\u666f\u4e2d\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\uff0c\u4ee5\u7406\u89e3\u5982\u4f55\u4e0e\u73af\u5883\u4e2d\u7684\u53ef\u64cd\u4f5c\u5143\u7d20\u8fdb\u884c\u4ea4\u4e92\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u4e2d\u9884\u6d4b\u53ef\u64cd\u4f5c\u5143\u7d20\u7684\u8fd0\u52a8\u7c7b\u578b\u548c\u8fd0\u52a8\u8f74\u4e0e\u52a8\u4f5c\u9884\u6d4b\u548c\u8def\u5f84\u89c4\u5212\u7684\u6982\u5ff5\u76f8\u5173\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "MLLMs", "motion type", "motion axis", "reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.10020", "pdf": "https://arxiv.org/pdf/2511.10020", "abs": "https://arxiv.org/abs/2511.10020", "authors": ["Yuxin Jiang", "Wei Luo", "Hui Zhang", "Qiyu Chen", "Haiming Yao", "Weiming Shen", "Yunkang Cao"], "title": "Anomagic: Crossmodal Prompt-driven Zero-shot Anomaly Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose Anomagic, a zero-shot anomaly generation method that produces semantically coherent anomalies without requiring any exemplar anomalies. By unifying both visual and textual cues through a crossmodal prompt encoding scheme, Anomagic leverages rich contextual information to steer an inpainting-based generation pipeline. A subsequent contrastive refinement strategy enforces precise alignment between synthesized anomalies and their masks, thereby bolstering downstream anomaly detection accuracy. To facilitate training, we introduce AnomVerse, a collection of 12,987 anomaly-mask-caption triplets assembled from 13 publicly available datasets, where captions are automatically generated by multimodal large language models using structured visual prompts and template-based textual hints. Extensive experiments demonstrate that Anomagic trained on AnomVerse can synthesize more realistic and varied anomalies than prior methods, yielding superior improvements in downstream anomaly detection. Furthermore, Anomagic can generate anomalies for any normal-category image using user-defined prompts, establishing a versatile foundation model for anomaly generation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on anomaly generation using crossmodal prompts and large language models. While not directly related to trajectory prediction, it utilizes large language models for caption generation and can be considered somewhat related due to the use of multimodal models and generation tasks. The connection to trajectory prediction is weak, but the use of LLMs warrants a moderate relevance score.", "keywords": ["large language models", "LLMs", "multimodal"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.10091", "pdf": "https://arxiv.org/pdf/2511.10091", "abs": "https://arxiv.org/abs/2511.10091", "authors": ["Qilang Ye", "Yu Zhou", "Lian He", "Jie Zhang", "Xuanming Guo", "Jiayu Zhang", "Mingkui Tan", "Weicheng Xie", "Yue Sun", "Tao Tan", "Xiaochen Yuan", "Ghada Khoriba", "Zitong Yu"], "title": "SUGAR: Learning Skeleton Representation with Visual-Motion Knowledge for Action Recognition", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2026 Main Track", "summary": "Large Language Models (LLMs) hold rich implicit knowledge and powerful transferability. In this paper, we explore the combination of LLMs with the human skeleton to perform action classification and description. However, when treating LLM as a recognizer, two questions arise: 1) How can LLMs understand skeleton? 2) How can LLMs distinguish among actions? To address these problems, we introduce a novel paradigm named learning Skeleton representation with visUal-motion knowledGe for Action Recognition (SUGAR). In our pipeline, we first utilize off-the-shelf large-scale video models as a knowledge base to generate visual, motion information related to actions. Then, we propose to supervise skeleton learning through this prior knowledge to yield discrete representations. Finally, we use the LLM with untouched pre-training weights to understand these representations and generate the desired action targets and descriptions. Notably, we present a Temporal Query Projection (TQP) module to continuously model the skeleton signals with long sequences. Experiments on several skeleton-based action classification benchmarks demonstrate the efficacy of our SUGAR. Moreover, experiments on zero-shot scenarios show that SUGAR is more versatile than linear-based methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on action recognition using skeleton data and Large Language Models. While it doesn't directly address trajectory prediction, the use of LLMs and the modeling of motion information (though in the context of action recognition) suggest a moderate level of relevance. The paper explores how to represent motion in a way that LLMs can understand, which could potentially be relevant to trajectory prediction in the future.", "keywords": ["Large Language Models", "LLMs", "skeleton", "action recognition", "motion information"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
