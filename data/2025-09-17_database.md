# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-09-17

## 目录

- [cs.CR (1)](#cs-cr)
- [cs.DB (5)](#cs-db)
- [cs.SE (1)](#cs-se)

## cs.CR [cs.CR]
### [1] [ORQ: Complex Analytics on Private Data with Strong Security Guarantees](https://arxiv.org/abs/2509.10793)
*Eli Baum, Sam Buxbaum, Nitin Mathai, Muhammad Faisal, Vasiliki Kalavri, Mayank Varia, John Liagouris*

Main category: cs.CR

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: We present ORQ, a system that enables collaborative analysis of large private datasets using cryptographically secure multi-party computation (MPC). ORQ protects data against semi-honest or malicious parties and can efficiently evaluate relational queries with multi-way joins and aggregations that have been considered notoriously expensive under MPC. To do so, ORQ eliminates the quadratic cost of secure joins by leveraging the fact that, in practice, the structure of many real queries allows us to join records and apply the aggregations "on the fly" while keeping the result size bounded. On the system side, ORQ contributes generic oblivious operators, a data-parallel vectorized query engine, a communication layer that amortizes MPC network costs, and a dataflow API for expressing relational analytics -- all built from the ground up.   We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads, including complex queries with multiple joins and custom aggregations. When compared to state-of-the-art solutions, ORQ significantly reduces MPC execution times and can process one order of magnitude larger datasets. For our most challenging workload, the full TPC-H benchmark, we report results entirely under MPC with Scale Factor 10 -- a scale that had previously been achieved only with information leakage or the use of trusted third parties.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.10793) | **Categories:** cs.CR, cs.DB

---


## cs.DB [cs.DB]
### [1] [Dynamic read & write optimization with TurtleKV](https://arxiv.org/abs/2509.10714)
*Tony Astolfi, Vidya Silai, Darby Huye, Lan Liu, Raja R. Sambasivan, Johes Bater*

Main category: cs.DB

TL;DR: TurtleKV通过动态调整内存使用，在读写性能之间取得平衡，显著提升了键值存储的吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有键值存储在内存、读性能和写性能之间存在tradeoff，难以兼顾高读写性能和动态适应工作负载。

Method: 提出了一种新的无偏数据结构，允许动态地将内存分配给读或写操作，从而优化性能。

Result: 在YCSB测试中，TurtleKV的写吞吐量比RocksDB高8倍，读吞吐量高5倍，空间放大相似。与SplinterDB相比，点查询性能提升40%，范围扫描性能提升6倍，写性能相似，空间放大减少50%。

Conclusion: TurtleKV通过动态内存管理和新的数据结构，实现了高性能和灵活的键值存储。

Abstract: 对于现代应用和数据库至关重要的通用键值存储来说，高读写性能至关重要。然而，由于传统上有限的内存以及在内存使用、读性能和写性能之间三选二的权衡，同时实现高读写性能极具挑战。目前最先进的方法限制了内存使用，并选择一个主要维度（读或写）来优化其磁盘结构。它们通过其他机制来恢复剩余维度的性能。这种方法限制了数据库在剩余维度上的最大性能，以及它们动态（在线）调整以响应不断变化的工作负载的能力。我们探索了一种不同的方法，可以根据需要动态地用内存换取读或写性能。我们提出了TurtleKV，它包括一种用于磁盘存储的新型无偏数据结构。它包括一个旋钮，可以动态增加为提高读取或写入性能而保留的内存。在YCSB上进行评估时，TurtleKV的写入吞吐量是行业领导者RocksDB的8倍，读取吞吐量是其5倍，同时产生的空间放大率相似。与最先进的系统SplinterDB相比，TurtleKV在点查询上的性能提高了40%，在范围扫描上的性能提高了6倍，并且实现了相似的写入性能，同时空间放大减少了50%。

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.10714) | **Categories:** cs.DB

---

### [2] [The Space-Time Complexity of Sum-Product Queries](https://arxiv.org/abs/2509.11920)
*Kyle Deeds, Timo Camillo Merkl, Reinhard Pichler, Dan Suciu*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: While extensive research on query evaluation has achieved consistent improvements in the time complexity of algorithms, the space complexity of query evaluation has been largely ignored. This is a particular challenge in settings with strict pre-defined space constraints. In this paper, we examine the combined space-time complexity of conjunctive queries (CQs) and, more generally, of sum-product queries (SPQs). We propose several classes of space-efficient algorithms for evaluating SPQs, and we show that the optimal time complexity is almost always achievable with asymptotically lower space complexity than traditional approaches.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.11920) | **Categories:** cs.DB

---

### [3] [Query Answering under Volume-Based Diversity Functions](https://arxiv.org/abs/2509.11929)
*Marcelo Arenas, Timo Camillo Merkl, Reinhard Pichler, Cristian Riveros*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: When query evaluation produces too many tuples, a new approach in query answering is to retrieve a diverse subset of them. The standard approach for measuring the diversity of a set of tuples is to use a distance function between tuples, which measures the dissimilarity between them, to then aggregate the pairwise distances of the set into a score (e.g., by using sum or min aggregation). However, as we will point out in this work, the resulting diversity measures may display some unintuitive behavior. Moreover, even in very simple settings, finding a maximally diverse subset of the answers of fixed size is, in general, intractable and little is known about approximations apart from some hand-picked distance-aggregator pairs.   In this work, we introduce a novel approach for computing the diversity of tuples based on volume instead of distance. We present a framework for defining volume-based diversity functions and provide several examples of these measures applied to relational data. Although query answering of conjunctive queries (CQ) under this setting is intractable in general, we show that one can always compute a (1-1/e)-approximation for any volume-based diversity function. Furthermore, in terms of combined complexity, we connect the evaluation of CQs under volume-based diversity functions with the ranked enumeration of solutions, finding general conditions under which a (1-1/e)-approximation can be computed in polynomial time.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.11929) | **Categories:** cs.DB

---

### [4] [SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation](https://arxiv.org/abs/2509.12086)
*Hui Li, Shiyuan Deng, Xiao Yan, Xiangyu Zhi, James Cheng*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Approximate Nearest Neighbor Search (ANNS) plays a critical role in applications such as search engines, recommender systems, and RAG for LLMs. Vector quantization (VQ), a crucial technique for ANNS, is commonly used to reduce space overhead and accelerate distance computations. However, despite significant research advances, state-of-the-art VQ methods still face challenges in balancing encoding efficiency and quantization accuracy. To address these limitations, we propose a novel VQ method called SAQ. To improve accuracy, SAQ employs a new dimension segmentation technique to strategically partition PCA-projected vectors into segments along their dimensions. By prioritizing leading dimension segments with larger magnitudes, SAQ allocates more bits to high-impact segments, optimizing the use of the available space quota. An efficient dynamic programming algorithm is developed to optimize dimension segmentation and bit allocation, ensuring minimal quantization error. To speed up vector encoding, SAQ devises a code adjustment technique to first quantize each dimension independently and then progressively refine quantized vectors using a coordinate-descent-like approach to avoid exhaustive enumeration. Extensive experiments demonstrate SAQ's superiority over classical methods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ, Extended RabitQ). SAQ achieves up to 80% reduction in quantization error and accelerates encoding speed by over 80x compared to Extended RabitQ.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.12086) | **Categories:** cs.DB, cs.DS, cs.IR

---

### [5] [Towards a Standard for JSON Document Databases](https://arxiv.org/abs/2509.12189)
*Elena Botoeva, Julien Corman*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: In this technical report, we present a formalisation of the MongoDB aggregation framework. Our aim is to identify a fragment that could serve as the starting point for an industry-wide standard for querying JSON document databases. We provide a syntax and formal semantics for a set of selected operators, We show how this fragment relates to known relational query languages. We explain how our semantics differs from the current implementation of MongoDB, and justify our choices. We provide a set of algebraic transformations that can be used for query optimisation.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.12189) | **Categories:** cs.DB

---


## cs.SE [cs.SE]
### [1] [Quality Assessment of Tabular Data using Large Language Models and Code Generation](https://arxiv.org/abs/2509.10572)
*Ashlesha Akella, Akshar Kaul, Krishnasuri Narayanam, Sameep Mehta*

Main category: cs.SE

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Reliable data quality is crucial for downstream analysis of tabular datasets, yet rule-based validation often struggles with inefficiency, human intervention, and high computational costs. We present a three-stage framework that combines statistical inliner detection with LLM-driven rule and code generation. After filtering data samples through traditional clustering, we iteratively prompt LLMs to produce semantically valid quality rules and synthesize their executable validators through code-generating LLMs. To generate reliable quality rules, we aid LLMs with retrieval-augmented generation (RAG) by leveraging external knowledge sources and domain-specific few-shot examples. Robust guardrails ensure the accuracy and consistency of both rules and code snippets. Extensive evaluations on benchmark datasets confirm the effectiveness of our approach.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.10572) | **Categories:** cs.SE, cs.AI, cs.DB

---
