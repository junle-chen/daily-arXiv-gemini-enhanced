{"id": "2506.23306", "pdf": "https://arxiv.org/pdf/2506.23306", "abs": "https://arxiv.org/abs/2506.23306", "authors": ["Qi Liu", "Can Li", "Wanjing Ma"], "title": "GATSim: Urban Mobility Simulation with Generative Agents", "categories": ["cs.AI"], "comment": null, "summary": "Traditional agent-based urban mobility simulations rely on rigid rule-based\nsystems that fail to capture the complexity, adaptability, and behavioral\ndiversity characteristic of human travel decision-making. Recent advances in\nlarge language models and AI agent technology offer opportunities to create\nagents with reasoning capabilities, persistent memory, and adaptive learning\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\nframework that leverages these advances to create generative agents with rich\nbehavioral characteristics for urban mobility simulation. Unlike conventional\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\nlifestyles, and evolving preferences that shape their mobility decisions\nthrough psychologically-informed memory systems, tool usage capabilities, and\nlifelong learning mechanisms. The main contributions of this study include: (1)\na comprehensive architecture combining an urban mobility foundation model with\nagent cognitive systems and transport simulation environment, (2) a fully\nfunctional prototype implementation, and (3) systematic validation\ndemonstrating that generative agents produce believable travel behaviors.\nThrough designed reflection processes, generative agents in this study can\ntransform specific travel experiences into generalized insights, enabling\nrealistic behavioral adaptation over time with specialized mechanisms for\nactivity planning and real-time reactive behaviors tailored to urban mobility\ncontexts. Experiments show that generative agents perform competitively with\nhuman annotators in mobility scenarios while naturally producing macroscopic\ntraffic evolution patterns. The code for the prototype system is shared at\nhttps://github.com/qiliuchn/gatsim.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528\u751f\u6210\u5f0fAgent\u8fdb\u884c\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\uff0c\u5e76\u4e14\u5229\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548cAI Agent\u6280\u672f\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90fd\u9ad8\u5ea6\u76f8\u5173\u3002\u8bba\u6587\u5173\u6ce8\u4e8e\u6a21\u62df\u4eba\u7c7b\u51fa\u884c\u51b3\u7b56\u7684\u590d\u6742\u6027\uff0c\u81ea\u9002\u5e94\u6027\u548c\u884c\u4e3a\u591a\u6837\u6027\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u76ee\u6807\u4e00\u81f4\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e5f\u660e\u786e\u63d0\u5230\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u521b\u5efa\u5177\u6709\u63a8\u7406\u80fd\u529b\u7684Agent\u3002", "keywords": ["urban mobility simulation", "generative agents", "large language models", "AI agent technology", "mobility decisions", "behavioral adaptation", "foundation model"]}, "AI": {"tldr": "GATSim \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5229\u7528\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u8fdb\u884c\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\uff0c\u80fd\u591f\u4ea7\u751f\u53ef\u4fe1\u7684\u51fa\u884c\u884c\u4e3a\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\u4f9d\u8d56\u4e8e\u521a\u6027\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u51fa\u884c\u51b3\u7b56\u7684\u590d\u6742\u6027\u3001\u9002\u5e94\u6027\u548c\u884c\u4e3a\u591a\u6837\u6027\u3002", "method": "GATSim \u6846\u67b6\uff0c\u7ed3\u5408\u57ce\u5e02\u4ea4\u901a\u57fa\u7840\u6a21\u578b\u4e0e\u667a\u80fd\u4f53\u8ba4\u77e5\u7cfb\u7edf\u548c\u4ea4\u901a\u4eff\u771f\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u51fa\u884c\u573a\u666f\u4e2d\uff0c\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u7684\u6027\u80fd\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u4e14\u80fd\u591f\u81ea\u7136\u5730\u4ea7\u751f\u5b8f\u89c2\u7684\u4ea4\u901a\u6f14\u53d8\u6a21\u5f0f\u3002", "conclusion": "\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u80fd\u591f\u4ea7\u751f\u53ef\u4fe1\u7684\u51fa\u884c\u884c\u4e3a\uff0c\u5e76\u5728\u51fa\u884c\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u8868\u73b0\u51fa\u7ade\u4e89\u6027\uff0c\u540c\u65f6\u81ea\u7136\u5730\u4ea7\u751f\u5b8f\u89c2\u4ea4\u901a\u6f14\u5316\u6a21\u5f0f\u3002", "summary_zh": "\u4f20\u7edf\u7684\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\u4f9d\u8d56\u4e8e\u521a\u6027\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u51fa\u884c\u51b3\u7b56\u7684\u590d\u6742\u6027\u3001\u9002\u5e94\u6027\u548c\u884c\u4e3a\u591a\u6837\u6027\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u4eba\u5de5\u667a\u80fd\u667a\u80fd\u4f53\u6280\u672f\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u521b\u5efa\u5177\u6709\u63a8\u7406\u80fd\u529b\u3001\u6301\u4e45\u8bb0\u5fc6\u548c\u81ea\u9002\u5e94\u5b66\u4e60\u673a\u5236\u7684\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002\u6211\u4eec\u63d0\u51fa\u4e86 GATSim\uff08\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u4ea4\u901a\u6a21\u62df\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5b83\u5229\u7528\u8fd9\u4e9b\u8fdb\u5c55\u6765\u521b\u5efa\u5177\u6709\u4e30\u5bcc\u884c\u4e3a\u7279\u5f81\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u57ce\u5e02\u4ea4\u901a\u6a21\u62df\u3002\u4e0e\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0cGATSim \u667a\u80fd\u4f53\u5177\u6709\u4e0d\u540c\u7684\u793e\u4f1a\u7ecf\u6d4e\u5c5e\u6027\u3001\u4e2a\u4eba\u751f\u6d3b\u65b9\u5f0f\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u504f\u597d\uff0c\u8fd9\u4e9b\u5c5e\u6027\u901a\u8fc7\u5fc3\u7406\u5b66\u4e0a\u7684\u8bb0\u5fc6\u7cfb\u7edf\u3001\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u548c\u7ec8\u8eab\u5b66\u4e60\u673a\u5236\u6765\u5f71\u54cd\u4ed6\u4eec\u7684\u51fa\u884c\u51b3\u7b56\u3002\u672c\u7814\u7a76\u7684\u4e3b\u8981\u8d21\u732e\u5305\u62ec\uff1a\uff081\uff09\u4e00\u4e2a\u7efc\u5408\u7684\u67b6\u6784\uff0c\u5c06\u57ce\u5e02\u4ea4\u901a\u57fa\u7840\u6a21\u578b\u4e0e\u667a\u80fd\u4f53\u8ba4\u77e5\u7cfb\u7edf\u548c\u4ea4\u901a\u4eff\u771f\u73af\u5883\u76f8\u7ed3\u5408\uff0c\uff082\uff09\u4e00\u4e2a\u529f\u80fd\u9f50\u5168\u7684\u539f\u578b\u5b9e\u73b0\uff0c\u4ee5\u53ca\uff083\uff09\u7cfb\u7edf\u7684\u9a8c\u8bc1\uff0c\u8bc1\u660e\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u80fd\u591f\u4ea7\u751f\u53ef\u4fe1\u7684\u51fa\u884c\u884c\u4e3a\u3002\u901a\u8fc7\u8bbe\u8ba1\u7684\u53cd\u601d\u8fc7\u7a0b\uff0c\u672c\u7814\u7a76\u4e2d\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u53ef\u4ee5\u5c06\u7279\u5b9a\u7684\u51fa\u884c\u4f53\u9a8c\u8f6c\u5316\u4e3a\u5e7f\u4e49\u7684\u89c1\u89e3\uff0c\u4ece\u800c\u5b9e\u73b0\u73b0\u5b9e\u7684\u884c\u4e3a\u968f\u65f6\u95f4\u63a8\u79fb\u7684\u9002\u5e94\uff0c\u5e76\u5177\u6709\u4e13\u95e8\u7684\u6d3b\u52a8\u8ba1\u5212\u673a\u5236\u548c\u9488\u5bf9\u57ce\u5e02\u4ea4\u901a\u73af\u5883\u91cf\u8eab\u5b9a\u5236\u7684\u5b9e\u65f6\u53cd\u5e94\u884c\u4e3a\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u5728\u51fa\u884c\u573a\u666f\u4e2d\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u6807\u6ce8\u8005\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u81ea\u7136\u5730\u4ea7\u751f\u5b8f\u89c2\u7684\u4ea4\u901a\u6f14\u53d8\u6a21\u5f0f\u3002\u539f\u578b\u7cfb\u7edf\u7684\u4ee3\u7801\u5df2\u5728 https://github.com/qiliuchn/gatsim \u4e0a\u5171\u4eab\u3002"}}
{"id": "2506.23316", "pdf": "https://arxiv.org/pdf/2506.23316", "abs": "https://arxiv.org/abs/2506.23316", "authors": ["Zhenghao Peng", "Yuxin Liu", "Bolei Zhou"], "title": "InfGen: Scenario Generation as Next Token Group Prediction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Realistic and interactive traffic simulation is essential for training and\nevaluating autonomous driving systems. However, most existing data-driven\nsimulation methods rely on static initialization or log-replay data, limiting\ntheir ability to model dynamic, long-horizon scenarios with evolving agent\npopulations. We propose InfGen, a scenario generation framework that outputs\nagent states and trajectories in an autoregressive manner. InfGen represents\nthe entire scene as a sequence of tokens, including traffic light signals,\nagent states, and motion vectors, and uses a transformer model to simulate\ntraffic over time. This design enables InfGen to continuously insert new agents\ninto traffic, supporting infinite scene generation. Experiments demonstrate\nthat InfGen produces realistic, diverse, and adaptive traffic behaviors.\nFurthermore, reinforcement learning policies trained in InfGen-generated\nscenarios achieve superior robustness and generalization, validating its\nutility as a high-fidelity simulation environment for autonomous driving. More\ninformation is available at https://metadriverse.github.io/infgen/.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u8868\u660e\u5176\u5173\u6ce8\u573a\u666f\u751f\u6210\uff0c\u5e76\u4e14\u4f7f\u7528Transformer\u6a21\u578b\u8fdb\u884c\u4ea4\u901a\u573a\u666f\u6a21\u62df\u548c\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6458\u8981\u4e2d\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u201cLarge Language Model\u201d\uff0c\u4f46Transformer\u6a21\u578b\u662fLLM\u7684\u57fa\u7840\u67b6\u6784\uff0c\u4e14\u8be5\u8bba\u6587\u4f7f\u7528Transformer\u6a21\u578b\u8fdb\u884c\u5e8f\u5217\u9884\u6d4b\uff08token group prediction\uff09\uff0c\u4e0eLLM\u6709\u4e00\u5b9a\u5173\u8054\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "scenario generation", "transformer model", "traffic simulation", "autonomous driving", "motion vectors", "autoregressive"]}, "AI": {"tldr": "InfGen\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u4ea4\u901a\u4eff\u771f\u6846\u67b6\uff0c\u53ef\u4ee5\u751f\u6210\u903c\u771f\u3001\u591a\u6837\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u4ea4\u901a\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u4eff\u771f\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u521d\u59cb\u5316\u6216\u65e5\u5fd7\u56de\u653e\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6a21\u62df\u5177\u6709\u4e0d\u65ad\u53d8\u5316\u7684\u667a\u80fd\u4f53\u7fa4\u4f53\u7684\u52a8\u6001\u3001\u957f\u65f6\u7a0b\u573a\u666f\u7684\u80fd\u529b\u3002", "method": "InfGen\u91c7\u7528\u81ea\u56de\u5f52\u65b9\u5f0f\u8f93\u51fa\u667a\u80fd\u4f53\u7684\u72b6\u6001\u548c\u8f68\u8ff9\uff0c\u5c06\u6574\u4e2a\u573a\u666f\u8868\u793a\u4e3a\u4ee4\u724c\u5e8f\u5217\uff0c\u5e76\u4f7f\u7528Transformer\u6a21\u578b\u6765\u6a21\u62df\u4ea4\u901a\u3002", "result": "InfGen\u80fd\u591f\u751f\u6210\u903c\u771f\u3001\u591a\u6837\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u4ea4\u901a\u884c\u4e3a\u3002", "conclusion": "InfGen\u751f\u6210\u7684\u573a\u666f\u80fd\u591f\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u7684\u6709\u6548\u6027\u3002", "summary_zh": "\u903c\u771f\u4e14\u4ea4\u4e92\u5f0f\u7684\u4ea4\u901a\u4eff\u771f\u5bf9\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5927\u591a\u6570\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u4eff\u771f\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u521d\u59cb\u5316\u6216\u65e5\u5fd7\u56de\u653e\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u6a21\u62df\u5177\u6709\u4e0d\u65ad\u53d8\u5316\u7684\u667a\u80fd\u4f53\u7fa4\u4f53\u7684\u52a8\u6001\u3001\u957f\u65f6\u7a0b\u573a\u666f\u7684\u80fd\u529b\u3002\u6211\u4eec\u63d0\u51fa\u4e86InfGen\uff0c\u4e00\u4e2a\u4ee5\u81ea\u56de\u5f52\u65b9\u5f0f\u8f93\u51fa\u667a\u80fd\u4f53\u72b6\u6001\u548c\u8f68\u8ff9\u7684\u573a\u666f\u751f\u6210\u6846\u67b6\u3002InfGen\u5c06\u6574\u4e2a\u573a\u666f\u8868\u793a\u4e3a\u4ee4\u724c\u5e8f\u5217\uff0c\u5305\u62ec\u4ea4\u901a\u706f\u4fe1\u53f7\u3001\u667a\u80fd\u4f53\u72b6\u6001\u548c\u8fd0\u52a8\u5411\u91cf\uff0c\u5e76\u4f7f\u7528Transformer\u6a21\u578b\u6765\u6a21\u62df\u4ea4\u901a\u968f\u65f6\u95f4\u7684\u53d8\u5316\u3002\u8fd9\u79cd\u8bbe\u8ba1\u4f7fInfGen\u80fd\u591f\u4e0d\u65ad\u5730\u5c06\u65b0\u7684\u667a\u80fd\u4f53\u63d2\u5165\u5230\u4ea4\u901a\u4e2d\uff0c\u652f\u6301\u65e0\u9650\u7684\u573a\u666f\u751f\u6210\u3002\u5b9e\u9a8c\u8868\u660e\uff0cInfGen\u80fd\u591f\u751f\u6210\u903c\u771f\u3001\u591a\u6837\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u4ea4\u901a\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u5728InfGen\u751f\u6210\u7684\u573a\u666f\u4e2d\u8bad\u7ec3\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u7684\u6548\u7528\u3002"}}
{"id": "2506.22716", "pdf": "https://arxiv.org/pdf/2506.22716", "abs": "https://arxiv.org/abs/2506.22716", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor R\u00fchle"], "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Accepted to ICML 2025 (main conference)", "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on efficiently routing queries to different large language models (LLMs) to optimize cost and performance. While it doesn't directly address trajectory prediction, it heavily involves LLMs and their deployment, making it relevant to the large language model aspect. The 'adaptive routing' concept could potentially be applied to trajectory prediction scenarios where different models or algorithms might be chosen based on the characteristics of the input trajectory.", "keywords": ["Large Language Models", "LLMs", "routing", "query routing", "adaptive routing", "compute"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBEST-Route\u7684\u65b0\u578b\u8def\u7531\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6a21\u578b\u548c\u91c7\u6837\u54cd\u5e94\u7684\u6570\u91cf\u6765\u964d\u4f4eLLM\u7684\u90e8\u7f72\u6210\u672c\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u5f71\u54cd\u5f88\u5c0f\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u4f46\u901a\u5e38\u5927\u89c4\u6a21\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u3002LLM\u67e5\u8be2\u8def\u7531\u901a\u8fc7\u5c06\u67e5\u8be2\u52a8\u6001\u5206\u914d\u7ed9\u4e0d\u540c\u6210\u672c\u548c\u8d28\u91cf\u7684\u6a21\u578b\u6765\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\uff0c\u4ee5\u83b7\u5f97\u6240\u9700\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8def\u7531\u6846\u67b6BEST-Route\uff0c\u8be5\u6846\u67b6\u6839\u636e\u67e5\u8be2\u96be\u5ea6\u548c\u8d28\u91cf\u9608\u503c\u9009\u62e9\u6a21\u578b\u4ee5\u53ca\u4ece\u4e2d\u91c7\u6837\u7684\u54cd\u5e94\u6570\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e1%\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u5c06\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe60%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6027\u80fd\u4e0b\u964d\u5c0f\u4e8e1%\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5c06\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe60%\u3002", "summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u662f\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u4f46\u901a\u5e38\u5927\u89c4\u6a21\u90e8\u7f72\u6210\u672c\u9ad8\u6602\u3002LLM\u67e5\u8be2\u8def\u7531\u901a\u8fc7\u5c06\u67e5\u8be2\u52a8\u6001\u5206\u914d\u7ed9\u4e0d\u540c\u6210\u672c\u548c\u8d28\u91cf\u7684\u6a21\u578b\u6765\u7f13\u89e3\u8fd9\u79cd\u60c5\u51b5\uff0c\u4ee5\u83b7\u5f97\u6240\u9700\u7684\u6743\u8861\u3002\u5148\u524d\u7684\u67e5\u8be2\u8def\u7531\u65b9\u6cd5\u4ec5\u4ece\u6240\u9009\u6a21\u578b\u751f\u6210\u4e00\u4e2a\u54cd\u5e94\uff0c\u800c\u6765\u81ea\u5c0f\u578b\uff08\u5ec9\u4ef7\uff09\u6a21\u578b\u7684\u5355\u4e2a\u54cd\u5e94\u901a\u5e38\u4e0d\u8db3\u4ee5\u80dc\u8fc7\u6765\u81ea\u5927\u578b\uff08\u6602\u8d35\uff09\u6a21\u578b\u7684\u54cd\u5e94\uff0c\u56e0\u6b64\u5b83\u4eec\u6700\u7ec8\u8fc7\u5ea6\u4f7f\u7528\u5927\u578b\u6a21\u578b\uff0c\u9519\u5931\u4e86\u6f5c\u5728\u7684\u6210\u672c\u8282\u7ea6\u3002\u7136\u800c\uff0c\u4f17\u6240\u5468\u77e5\uff0c\u5bf9\u4e8e\u5c0f\u578b\u6a21\u578b\uff0c\u751f\u6210\u591a\u4e2a\u54cd\u5e94\u5e76\u9009\u62e9\u6700\u4f73\u54cd\u5e94\u53ef\u4ee5\u63d0\u9ad8\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6bd4\u5355\u4e2a\u5927\u578b\u6a21\u578b\u54cd\u5e94\u66f4\u4fbf\u5b9c\u3002\u6211\u4eec\u5229\u7528\u8fd9\u4e00\u60f3\u6cd5\u63d0\u51fa\u4e86BEST-Route\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u8def\u7531\u6846\u67b6\uff0c\u5b83\u6839\u636e\u67e5\u8be2\u96be\u5ea6\u548c\u8d28\u91cf\u9608\u503c\u9009\u62e9\u6a21\u578b\u4ee5\u53ca\u4ece\u4e2d\u91c7\u6837\u7684\u54cd\u5e94\u6570\u91cf\u3002\u5728\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u5c06\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe60%\uff0c\u800c\u6027\u80fd\u4e0b\u964d\u4e0d\u52301%\u3002"}}
{"id": "2506.22554", "pdf": "https://arxiv.org/pdf/2506.22554", "abs": "https://arxiv.org/abs/2506.22554", "authors": ["Vasu Agrawal", "Akinniyi Akinyemi", "Kathryn Alvero", "Morteza Behrooz", "Julia Buffalini", "Fabio Maria Carlucci", "Joy Chen", "Junming Chen", "Zhang Chen", "Shiyang Cheng", "Praveen Chowdary", "Joe Chuang", "Antony D'Avirro", "Jon Daly", "Ning Dong", "Mark Duppenthaler", "Cynthia Gao", "Jeff Girard", "Martin Gleize", "Sahir Gomez", "Hongyu Gong", "Srivathsan Govindarajan", "Brandon Han", "Sen He", "Denise Hernandez", "Yordan Hristov", "Rongjie Huang", "Hirofumi Inaguma", "Somya Jain", "Raj Janardhan", "Qingyao Jia", "Christopher Klaiber", "Dejan Kovachev", "Moneish Kumar", "Hang Li", "Yilei Li", "Pavel Litvin", "Wei Liu", "Guangyao Ma", "Jing Ma", "Martin Ma", "Xutai Ma", "Lucas Mantovani", "Sagar Miglani", "Sreyas Mohan", "Louis-Philippe Morency", "Evonne Ng", "Kam-Woh Ng", "Tu Anh Nguyen", "Amia Oberai", "Benjamin Peloquin", "Juan Pino", "Jovan Popovic", "Omid Poursaeed", "Fabian Prada", "Alice Rakotoarison", "Alexander Richard", "Christophe Ropers", "Safiyyah Saleem", "Vasu Sharma", "Alex Shcherbyna", "Jia Shen", "Jie Shen", "Anastasis Stathopoulos", "Anna Sun", "Paden Tomasello", "Tuan Tran", "Arina Turkatenko", "Bo Wan", "Chao Wang", "Jeff Wang", "Mary Williamson", "Carleigh Wood", "Tao Xiang", "Yilin Yang", "Julien Yao", "Chen Zhang", "Jiemin Zhang", "Xinyue Zhang", "Jason Zheng", "Pavlo Zhyzheria", "Jan Zikes", "Michael Zollhoefer"], "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper discusses modeling dyadic audiovisual motion, including gestures and facial expressions, which can be considered a form of trajectory prediction in a broader sense. It also mentions using speech from an LLM model and developing virtual agents, indicating a connection to large language models. While not directly focused on traditional trajectory prediction (e.g., pedestrian or vehicle), the motion modeling aspect and the use of LLMs justify a relatively high relevance score.", "keywords": ["motion modeling", "dyadic motion", "LLM", "virtual agents", "facial expressions", "gesture generation"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u9762\u5bf9\u9762\u4e92\u52a8\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u5957\u6a21\u578b\u6765\u751f\u6210\u4e0e\u4eba\u7c7b\u8bed\u97f3\u5bf9\u9f50\u7684\u4e8c\u5143\u8fd0\u52a8\u624b\u52bf\u548c\u9762\u90e8\u8868\u60c5\uff0c\u4ece\u800c\u63a8\u8fdb\u66f4\u76f4\u89c2\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "Developing socially intelligent AI technologies requires models that can comprehend and generate dyadic behavioral dynamics.", "method": "The authors develop a suite of models that utilize the Seamless Interaction Dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors.", "result": "The paper introduces the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage. The developed models generate dyadic motion gestures and facial expressions aligned with human speech.", "conclusion": "The paper demonstrates the potential for more intuitive and responsive human-AI interactions through the development of dyadic motion models.", "summary_zh": "\u4eba\u7c7b\u4ea4\u6d41\u6d89\u53ca\u8bed\u8a00\u548c\u975e\u8bed\u8a00\u4fe1\u53f7\u7684\u590d\u6742\u4e92\u52a8\uff0c\u8fd9\u5bf9\u4e8e\u4f20\u8fbe\u610f\u4e49\u548c\u5b9e\u73b0\u4eba\u9645\u76ee\u6807\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u4e86\u5f00\u53d1\u5177\u6709\u793e\u4ea4\u667a\u80fd\u7684AI\u6280\u672f\uff0c\u81f3\u5173\u91cd\u8981\u7684\u662f\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u548c\u751f\u6210\u4e8c\u5143\u884c\u4e3a\u52a8\u6001\u7684\u6a21\u578b\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63a8\u51fa\u4e86Seamless Interaction Dataset\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea4000\u591a\u540d\u53c2\u4e0e\u8005\u5728\u5404\u79cd\u60c5\u5883\u4e0b\u76844000\u591a\u4e2a\u5c0f\u65f6\u7684\u9762\u5bf9\u9762\u4e92\u52a8\u89c6\u9891\u3002\u8be5\u6570\u636e\u96c6\u80fd\u591f\u5f00\u53d1\u7406\u89e3\u4e8c\u5143\u5177\u8eab\u52a8\u6001\u7684AI\u6280\u672f\uff0c\u4ece\u800c\u5728\u865a\u62df\u4ee3\u7406\u3001\u8fdc\u7a0b\u5448\u73b0\u4f53\u9a8c\u548c\u591a\u6a21\u6001\u5185\u5bb9\u5206\u6790\u5de5\u5177\u4e2d\u53d6\u5f97\u7a81\u7834\u3002\u6211\u4eec\u8fd8\u5f00\u53d1\u4e86\u4e00\u5957\u6a21\u578b\uff0c\u5229\u7528\u8be5\u6570\u636e\u96c6\u751f\u6210\u4e0e\u4eba\u7c7b\u8bed\u97f3\u5bf9\u9f50\u7684\u4e8c\u5143\u8fd0\u52a8\u624b\u52bf\u548c\u9762\u90e8\u8868\u60c5\u3002\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u5c06\u5bf9\u8bdd\u8005\u7684\u8bed\u97f3\u548c\u89c6\u89c9\u884c\u4e3a\u4f5c\u4e3a\u8f93\u5165\u3002\u6211\u4eec\u5c55\u793a\u4e86\u4e00\u4e2a\u5e26\u6709\u6765\u81eaLLM\u6a21\u578b\u7684\u8bed\u97f3\u7684\u53d8\u4f53\uff0c\u4ee5\u53ca\u4e0e2D\u548c3D\u6e32\u67d3\u65b9\u6cd5\u7684\u96c6\u6210\uff0c\u4f7f\u6211\u4eec\u66f4\u63a5\u8fd1\u4ea4\u4e92\u5f0f\u865a\u62df\u4ee3\u7406\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63cf\u8ff0\u4e86\u6211\u4eec\u8fd0\u52a8\u6a21\u578b\u7684\u53ef\u63a7\u53d8\u4f53\uff0c\u8fd9\u4e9b\u53d8\u4f53\u53ef\u4ee5\u9002\u5e94\u60c5\u7eea\u53cd\u5e94\u548c\u8868\u8fbe\u6c34\u5e73\uff0c\u4ee5\u53ca\u751f\u6210\u66f4\u591a\u8bed\u4e49\u76f8\u5173\u7684\u624b\u52bf\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u8bc4\u4f30\u8fd9\u4e9b\u4e8c\u5143\u8fd0\u52a8\u6a21\u578b\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5c55\u793a\u4e86\u66f4\u76f4\u89c2\u548c\u54cd\u5e94\u5f0f\u4eba\u673a\u4ea4\u4e92\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.23164", "pdf": "https://arxiv.org/pdf/2506.23164", "abs": "https://arxiv.org/abs/2506.23164", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 8 figures, submitted to a journal", "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684mode collapse\u95ee\u9898\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u81ea\u52a8\u9a7e\u9a76\u7b49\u9700\u8981\u590d\u6742\u73af\u5883\u7406\u89e3\u7684\u5e94\u7528\u573a\u666f\u5bc6\u5207\u76f8\u5173\uff0c\u800c\u5927\u6a21\u578b\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "multi-agent trajectory prediction", "mode collapse", "autonomous vehicle", "interaction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.23793", "pdf": "https://arxiv.org/pdf/2506.23793", "abs": "https://arxiv.org/abs/2506.23793", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper discusses multi-agent pathfinding (MAPF) and introduces MAPF-GPT-DDG, which fine-tunes a pre-trained MAPF model. While it doesn't directly focus on trajectory prediction in the way that predicting human or vehicle trajectories does, MAPF is a related area. The use of 'MAPF-GPT' suggests a connection to large language models or similar architectures, although the exact nature of the model isn't fully clear from the abstract.", "keywords": ["multi-agent pathfinding", "MAPF", "MAPF-GPT", "trajectory planning", "imitation learning", "pre-trained model"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.23999", "pdf": "https://arxiv.org/pdf/2506.23999", "abs": "https://arxiv.org/abs/2506.23999", "authors": ["Zeyu Han", "Mengchi Cai", "Chaoyi Chen", "Qingwen Meng", "Guangwei Wang", "Ying Liu", "Qing Xu", "Jianqiang Wang", "Keqiang Li"], "title": "Predictive Risk Analysis and Safe Trajectory Planning for Intelligent and Connected Vehicles", "categories": ["cs.RO"], "comment": null, "summary": "The safe trajectory planning of intelligent and connected vehicles is a key\ncomponent in autonomous driving technology. Modeling the environment risk\ninformation by field is a promising and effective approach for safe trajectory\nplanning. However, existing risk assessment theories only analyze the risk by\ncurrent information, ignoring future prediction. This paper proposes a\npredictive risk analysis and safe trajectory planning framework for intelligent\nand connected vehicles. This framework first predicts future trajectories of\nobjects by a local risk-aware algorithm, following with a\nspatiotemporal-discretised predictive risk analysis using the prediction\nresults. Then the safe trajectory is generated based on the predictive risk\nanalysis. Finally, simulation and vehicle experiments confirm the efficacy and\nreal-time practicability of our approach.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on safe trajectory planning for intelligent and connected vehicles, which falls under the trajectory prediction domain (specifically, predicting the trajectories of other objects to enable safer planning). While it doesn't explicitly mention large language models, the core problem of trajectory prediction is central to its content.", "keywords": ["trajectory prediction", "safe trajectory planning", "intelligent and connected vehicles", "risk analysis", "autonomous driving"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22604", "pdf": "https://arxiv.org/pdf/2506.22604", "abs": "https://arxiv.org/abs/2506.22604", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "title": "Bootstrapping Human-Like Planning via LLMs", "categories": ["cs.AI", "cs.HC", "cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4eba\u7c7b\u884c\u4e3a\u5e8f\u5217\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u751f\u6210action sequences\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5e7f\u4e49\u7684\u8def\u5f84\u89c4\u5212\u6216\u884c\u4e3a\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86LLM\u3002", "keywords": ["LLMs", "large language model", "action sequences", "natural language programming", "planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22441", "pdf": "https://arxiv.org/pdf/2506.22441", "abs": "https://arxiv.org/abs/2506.22441", "authors": ["Lei Yang"], "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Intelligent transportation systems (ITS) rely heavily on complete and\nhigh-quality spatiotemporal traffic data to achieve optimal performance.\nNevertheless, in real-word traffic data collection processes, issues such as\ncommunication failures and sensor malfunctions often lead to incomplete or\ncorrupted datasets, thereby posing significant challenges to the advancement of\nITS. Among various methods for imputing missing spatiotemporal traffic data,\nthe latent factorization of tensors (LFT) model has emerged as a widely adopted\nand effective solution. However, conventional LFT models typically employ the\nstandard L2-norm in their learning objective, which makes them vulnerable to\nthe influence of outliers. To overcome this limitation, this paper proposes a\nthreshold distance weighted (TDW) loss-incorporated Latent Factorization of\nTensors (TDWLFT) model. The proposed loss function effectively reduces the\nmodel's sensitivity to outliers by assigning differentiated weights to\nindividual samples. Extensive experiments conducted on two traffic speed\ndatasets sourced from diverse urban environments confirm that the proposed\nTDWLFT model consistently outperforms state-of-the-art approaches in terms of\nboth in both prediction accuracy and computational efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u6570\u636e\u8865\u5168\u548c\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86\u5f20\u91cf\u5206\u89e3\u7684\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u4e86\u4ea4\u901a\u6570\u636e\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u5173\u952e\u8bcd\u5305\u62ec\u4ea4\u901a\u6570\u636e\u548c\u9884\u6d4b\u3002", "keywords": ["traffic data", "prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22494", "pdf": "https://arxiv.org/pdf/2506.22494", "abs": "https://arxiv.org/abs/2506.22494", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using a vision-language model (built upon BLIP2) to generate explanations for autonomous driving scenarios. While it doesn't explicitly perform trajectory prediction, the context of autonomous driving inherently involves predicting the future states of vehicles and other agents. The use of a large language model (BLIP2) is a key component. Therefore, it has moderate relevance to both trajectory prediction and large language models.", "keywords": ["autonomous driving", "vision-language models", "BLIP2", "explanation generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22788", "pdf": "https://arxiv.org/pdf/2506.22788", "abs": "https://arxiv.org/abs/2506.22788", "authors": ["Xuao Hou", "Yongquan Jia", "Shijin Zhang", "Yuqiang Wu"], "title": "SPI-BoTER: Error Compensation for Industrial Robots via Sparse Attention Masking and Hybrid Loss with Spatial-Physical Information", "categories": ["cs.RO"], "comment": null, "summary": "The widespread application of industrial robots in fields such as cutting and\nwelding has imposed increasingly stringent requirements on the trajectory\naccuracy of end-effectors. However, current error compensation methods face\nseveral critical challenges, including overly simplified mechanism modeling, a\nlack of physical consistency in data-driven approaches, and substantial data\nrequirements. These issues make it difficult to achieve both high accuracy and\nstrong generalization simultaneously. To address these challenges, this paper\nproposes a Spatial-Physical Informed Attention Residual Network (SPI-BoTER).\nThis method integrates the kinematic equations of the robotic manipulator with\na Transformer architecture enhanced by sparse self-attention masks. A\nparameter-adaptive hybrid loss function incorporating spatial and physical\ninformation is employed to iteratively optimize the network during training,\nenabling high-precision error compensation under small-sample conditions.\nAdditionally, inverse joint angle compensation is performed using a gradient\ndescent-based optimization method. Experimental results on a small-sample\ndataset from a UR5 robotic arm (724 samples, with a train:test:validation split\nof 8:1:1) demonstrate the superior performance of the proposed method. It\nachieves a 3D absolute positioning error of 0.2515 mm with a standard deviation\nof 0.15 mm, representing a 35.16\\% reduction in error compared to conventional\ndeep neural network (DNN) methods. Furthermore, the inverse angle compensation\nalgorithm converges to an accuracy of 0.01 mm within an average of 147\niterations. This study presents a solution that combines physical\ninterpretability with data adaptability for high-precision control of\nindustrial robots, offering promising potential for the reliable execution of\nprecision tasks in intelligent manufacturing.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5de5\u4e1a\u673a\u5668\u4eba\u7684\u8f68\u8ff9\u7cbe\u5ea6\u8bef\u5dee\u8865\u507f\uff0c\u4f7f\u7528\u4e86Transformer\u67b6\u6784\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u7ed3\u5408\u4e86\u7269\u7406\u4fe1\u606f\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u63a7\u5236\u548cTransformer\uff08\u4e00\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u4ee5\u88ab\u89c6\u4e3a\u5927\u6a21\u578b\u9886\u57df\u7684\u76f8\u5173\u6280\u672f\uff09\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4e5f\u6ca1\u6709\u660e\u786e\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\uff08\u800c\u662f\u8f68\u8ff9\u8bef\u5dee\u8865\u507f\uff09\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory", "Transformer", "attention"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22827", "pdf": "https://arxiv.org/pdf/2506.22827", "abs": "https://arxiv.org/abs/2506.22827", "authors": ["Andr\u00e9 Schakkal", "Ben Zandonati", "Zhutian Yang", "Navid Azizan"], "title": "Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation", "categories": ["cs.RO"], "comment": "Accepted at the RSS 2025 Workshop on Robot Planning in the Era of\n  Foundation Models", "summary": "Enabling humanoid robots to reliably execute complex multi-step manipulation\ntasks is crucial for their effective deployment in industrial and household\nenvironments. This paper presents a hierarchical planning and control framework\ndesigned to achieve reliable multi-step humanoid manipulation. The proposed\nsystem comprises three layers: (1) a low-level RL-based controller responsible\nfor tracking whole-body motion targets; (2) a mid-level set of skill policies\ntrained via imitation learning that produce motion targets for different steps\nof a task; and (3) a high-level vision-language planning module that determines\nwhich skills should be executed and also monitors their completion in real-time\nusing pretrained vision-language models (VLMs). Experimental validation is\nperformed on a Unitree G1 humanoid robot executing a non-prehensile\npick-and-place task. Over 40 real-world trials, the hierarchical system\nachieved a 72.5% success rate in completing the full manipulation sequence.\nThese experiments confirm the feasibility of the proposed hierarchical system,\nhighlighting the benefits of VLM-based skill planning and monitoring for\nmulti-step manipulation scenarios. See https://vlp-humanoid.github.io/ for\nvideo demonstrations of the policy rollout.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on hierarchical planning for humanoid manipulation. While it doesn't directly deal with trajectory prediction in the traditional sense (e.g., predicting future trajectories of agents), it involves planning and control of robot movements, which is related. The use of vision-language models (VLMs), a type of large model, for skill planning and monitoring also contributes to the relevance, though the focus isn't primarily on LLMs themselves.", "keywords": ["vision-language models", "VLMs", "hierarchical planning", "humanoid manipulation", "motion planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22894", "pdf": "https://arxiv.org/pdf/2506.22894", "abs": "https://arxiv.org/abs/2506.22894", "authors": ["Bei Zhou", "Baha Zarrouki", "Mattia Piccinini", "Cheng Hu", "Lei Xie", "Johannes Betz"], "title": "Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous drifting is a complex and crucial maneuver for safety-critical\nscenarios like slippery roads and emergency collision avoidance, requiring\nprecise motion planning and control. Traditional motion planning methods often\nstruggle with the high instability and unpredictability of drifting,\nparticularly when operating at high speeds. Recent learning-based approaches\nhave attempted to tackle this issue but often rely on expert knowledge or have\nlimited exploration capabilities. Additionally, they do not effectively address\nsafety concerns during learning and deployment. To overcome these limitations,\nwe propose a novel Safe Reinforcement Learning (RL)-based motion planner for\nautonomous drifting. Our approach integrates an RL agent with model-based drift\ndynamics to determine desired drift motion states, while incorporating a\nPredictive Safety Filter (PSF) that adjusts the agent's actions online to\nprevent unsafe states. This ensures safe and efficient learning, and stable\ndrift operation. We validate the effectiveness of our method through\nsimulations on a Matlab-Carsim platform, demonstrating significant improvements\nin drift performance, reduced tracking errors, and computational efficiency\ncompared to traditional methods. This strategy promises to extend the\ncapabilities of autonomous vehicles in safety-critical maneuvers.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on motion planning and control for autonomous drifting vehicles using reinforcement learning with a predictive safety filter. While it doesn't directly involve large language models, it is highly relevant to trajectory prediction due to its emphasis on predicting and controlling the vehicle's future states for safe and efficient navigation. The 'Predictive Safety Filter' is a key component that suggests a focus on anticipating future trajectories.", "keywords": ["trajectory prediction", "motion planning", "reinforcement learning", "predictive safety filter", "autonomous vehicles", "drifting"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22992", "pdf": "https://arxiv.org/pdf/2506.22992", "abs": "https://arxiv.org/abs/2506.22992", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbi\u0107", "Michael Moor"], "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multimodal reasoning and planning, which can be related to trajectory prediction as it involves planning paths under spatial and physical constraints. The paper also explicitly mentions and evaluates Multimodal Language Models (MLLMs). However, it doesn't directly deal with trajectory prediction tasks or methodologies, but the reasoning and planning aspects, along with the use of LLMs, contribute to a moderate relevance.", "keywords": ["multimodal reasoning", "planning", "multimodal language models (MLLMs)", "spatial reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.23126", "pdf": "https://arxiv.org/pdf/2506.23126", "abs": "https://arxiv.org/abs/2506.23126", "authors": ["Suning Huang", "Qianzhong Chen", "Xiaohan Zhang", "Jiankai Sun", "Mac Schwager"], "title": "ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "3D world models (i.e., learning-based 3D dynamics models) offer a promising\napproach to generalizable robotic manipulation by capturing the underlying\nphysics of environment evolution conditioned on robot actions. However,\nexisting 3D world models are primarily limited to single-material dynamics\nusing a particle-based Graph Neural Network model, and often require\ntime-consuming 3D scene reconstruction to obtain 3D particle tracks for\ntraining. In this work, we present ParticleFormer, a Transformer-based point\ncloud world model trained with a hybrid point cloud reconstruction loss,\nsupervising both global and local dynamics features in multi-material,\nmulti-object robot interactions. ParticleFormer captures fine-grained\nmulti-object interactions between rigid, deformable, and flexible materials,\ntrained directly from real-world robot perception data without an elaborate\nscene reconstruction. We demonstrate the model's effectiveness both in 3D scene\nforecasting tasks, and in downstream manipulation tasks using a Model\nPredictive Control (MPC) policy. In addition, we extend existing dynamics\nlearning benchmarks to include diverse multi-material, multi-object interaction\nscenarios. We validate our method on six simulation and three real-world\nexperiments, where it consistently outperforms leading baselines by achieving\nsuperior dynamics prediction accuracy and less rollout error in downstream\nvisuomotor tasks. Experimental videos are available at\nhttps://particleformer.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528Transformer\u67b6\u6784\u8fdb\u884c3D\u70b9\u4e91\u4e16\u754c\u7684\u5efa\u6a21\uff0c\u5e76\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u867d\u7136\u5b83\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86Transformer\u8fd9\u79cd\u67b6\u6784\uff0c\u4e14\u6d89\u53ca\u5230\u4e86\u73af\u5883\u52a8\u6001\u9884\u6d4b\uff08dynamics prediction\uff09\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u63d0\u5230\u4e86Model Predictive Control (MPC)\uff0c\u8fd9\u4e5f\u4e0e\u8f68\u8ff9\u89c4\u5212\u76f8\u5173\u3002", "keywords": ["dynamics prediction", "Transformer", "world model", "Model Predictive Control", "MPC"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.23129", "pdf": "https://arxiv.org/pdf/2506.23129", "abs": "https://arxiv.org/abs/2506.23129", "authors": ["Hossein B. Jond", "Logan Beaver", "Martin Jirou\u0161ek", "Naiemeh Ahmadlou", "Veli Bak\u0131rc\u0131o\u011flu", "Martin Saska"], "title": "Flatness-based Finite-Horizon Multi-UAV Formation Trajectory Planning and Directionally Aware Collision Avoidance Tracking", "categories": ["cs.RO"], "comment": null, "summary": "Collision-free optimal formation control of unmanned aerial vehicle (UAV)\nteams is challenging. The state-of-the-art optimal control approaches often\nrely on numerical methods sensitive to initial guesses. This paper presents an\ninnovative collision-free finite-time formation control scheme for multiple\nUAVs leveraging the differential flatness of the UAV dynamics, eliminating the\nneed for numerical methods. We formulate a finite-time optimal control problem\nto plan a formation trajectory for feasible initial states. This formation\ntrajectory planning optimal control problem involves a collective performance\nindex to meet the formation requirements of achieving relative positions and\nvelocity consensus. It is solved by applying Pontryagin's principle.\nSubsequently, a collision-constrained regulating problem is addressed to ensure\ncollision-free tracking of the planned formation trajectory. The tracking\nproblem incorporates a directionally aware collision avoidance strategy that\nprioritizes avoiding UAVs in the forward path and relative approach. It assigns\nlower priority to those on the sides with an oblique relative approach and\ndisregards UAVs behind and not in the relative approach. The simulation results\nfor a four-UAV team (re)formation problem confirm the efficacy of the proposed\ncontrol scheme.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u7f16\u961f\u8f68\u8ff9\u89c4\u5212\u548c\u907f\u969c\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u4f46\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u5173\u8054\uff0c\u4f46\u8f68\u8ff9\u9884\u6d4b\u662f\u5176\u6838\u5fc3\u5185\u5bb9\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "trajectory planning", "collision avoidance", "UAV", "\u65e0\u4eba\u673a", "\u8def\u5f84\u89c4\u5212"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.23433", "pdf": "https://arxiv.org/pdf/2506.23433", "abs": "https://arxiv.org/abs/2506.23433", "authors": ["Tim Puphal", "Vipul Ramtekkar", "Kenji Nishimiya"], "title": "Risk-Based Filtering of Valuable Driving Situations in the Waymo Open Motion Dataset", "categories": ["cs.RO"], "comment": null, "summary": "Improving automated vehicle software requires driving data rich in valuable\nroad user interactions. In this paper, we propose a risk-based filtering\napproach that helps identify such valuable driving situations from large\ndatasets. Specifically, we use a probabilistic risk model to detect high-risk\nsituations. Our method stands out by considering a) first-order situations\n(where one vehicle directly influences another and induces risk) and b)\nsecond-order situations (where influence propagates through an intermediary\nvehicle). In experiments, we show that our approach effectively selects\nvaluable driving situations in the Waymo Open Motion Dataset. Compared to the\ntwo baseline interaction metrics of Kalman difficulty and Tracks-To-Predict\n(TTP), our filtering approach identifies complex and complementary situations,\nenriching the quality in automated vehicle testing. The risk data is made\nopen-source: https://github.com/HRI-EU/RiskBasedFiltering.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u98ce\u9669\u8bc4\u4f30\u548c\u6570\u636e\u96c6\u7b5b\u9009\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u8f6f\u4ef6\uff0c\u800c\u5927\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u6709\u6f5c\u5728\u5e94\u7528\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b", "Waymo Open Motion Dataset", "\u81ea\u52a8\u9a7e\u9a76", "\u98ce\u9669\u8bc4\u4f30"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.23573", "pdf": "https://arxiv.org/pdf/2506.23573", "abs": "https://arxiv.org/abs/2506.23573", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "title": "Online Human Action Detection during Escorting", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted in IEEE RO-MAN '25", "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on human action detection during escorting, which is related to predicting human movement and behavior. While it doesn't directly involve large language models, the problem of predicting human actions can be framed as a trajectory prediction problem, especially considering the robot's need to adjust its movement based on the escortee's actions. The paper also mentions person re-identification, a common task in trajectory prediction scenarios. However, the absence of any mention or application of LLMs significantly reduces the relevance score.", "keywords": ["action detection", "human movement", "escorting", "trajectory prediction", "person re-identification"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22756", "pdf": "https://arxiv.org/pdf/2506.22756", "abs": "https://arxiv.org/abs/2506.22756", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "ICCV 2025", "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robot manipulation and uses large language models (LLMs) to automate the simulation production process. While it doesn't directly address trajectory prediction, the use of LLMs and the simulation of robot movements suggest a moderate level of relevance.", "keywords": ["Large Language Models", "LLMs", "robot manipulation", "simulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.23739", "pdf": "https://arxiv.org/pdf/2506.23739", "abs": "https://arxiv.org/abs/2506.23739", "authors": ["Lisa Marie Otto", "Michael Kaiser", "Daniel Seebacher", "Steffen M\u00fcller"], "title": "Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment", "categories": ["cs.RO", "cs.CE", "cs.HC"], "comment": "6 pages, 5 figures, Preprint for 2025 IEEE IAVVC (International\n  Automated Vehicle Validation Conference)", "summary": "Ensuring safe and realistic interactions between automated driving systems\nand vulnerable road users (VRUs) in urban environments requires advanced\ntesting methodologies. This paper presents a test environment that combines a\nVehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the\nfeasibility of cyber-physical (CP) testing of vehicle-pedestrian and\nvehicle-cyclist interactions. Building upon previous work focused on pedestrian\nlocalization, we further validate a human pose estimation (HPE) approach\nthrough a comparative analysis of real-world (RW) and virtual representations\nof VRUs. The study examines the perception of full-body motion using a\ncommercial monocular camera-based 3Dskeletal detection AI. The virtual scene is\ngenerated in Unreal Engine 5, where VRUs are animated in real time and\nprojected onto a screen to stimulate the camera. The proposed stimulation\ntechnique ensures the correct perspective, enabling realistic vehicle\nperception. To assess the accuracy and consistency of HPE across RW and CP\ndomains, we analyze the reliability of detections as well as variations in\nmovement trajectories and joint estimation stability. The validation includes\ndynamic test scenarios where human avatars, both walking and cycling, are\nmonitored under controlled conditions. Our results show a strong alignment in\nHPE between RW and CP test conditions for stable motion patterns, while notable\ninaccuracies persist under dynamic movements and occlusions, particularly for\ncomplex cyclist postures. These findings contribute to refining CP testing\napproaches for evaluating next-generation AI-based vehicle perception and to\nenhancing interaction models of automated vehicles and VRUs in CP environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on validating AI-based human pose estimation for interactions between automated driving systems and vulnerable road users. While it doesn't explicitly mention trajectory prediction or large language models, it involves analyzing movement trajectories and uses AI for perception, suggesting a moderate relevance to trajectory prediction. The use of AI models for pose estimation is central to the research.", "keywords": ["human pose estimation", "movement trajectories", "AI", "vehicle perception", "cyber-physical"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22927", "pdf": "https://arxiv.org/pdf/2506.22927", "abs": "https://arxiv.org/abs/2506.22927", "authors": ["Jaeyun Woo", "Jiseok Lee", "Brian Kenji Iwana"], "title": "Towards Time Series Generation Conditioned on Unstructured Natural Language", "categories": ["cs.LG"], "comment": null, "summary": "Generative Artificial Intelligence (AI) has rapidly become a powerful tool,\ncapable of generating various types of data, such as images and text. However,\ndespite the significant advancement of generative AI, time series generative AI\nremains underdeveloped, even though the application of time series is essential\nin finance, climate, and numerous fields. In this research, we propose a novel\nmethod of generating time series conditioned on unstructured natural language\ndescriptions. We use a diffusion model combined with a language model to\ngenerate time series from the text. Through the proposed method, we demonstrate\nthat time series generation based on natural language is possible. The proposed\nmethod can provide various applications such as custom forecasting, time series\nmanipulation, data augmentation, and transfer learning. Furthermore, we\nconstruct and propose a new public dataset for time series generation,\nconsisting of 63,010 time series-description pairs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on generating time series data conditioned on natural language descriptions. While it doesn't explicitly mention trajectory prediction, time series data is fundamental to trajectory prediction. The paper also utilizes a language model, indicating a connection to Large Language Models. The relevance is moderate because it bridges time series generation with language models, a connection that *could* be leveraged for trajectory prediction, but isn't directly focused on it.", "keywords": ["time series", "generative AI", "language model", "diffusion model", "forecasting"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.23771", "pdf": "https://arxiv.org/pdf/2506.23771", "abs": "https://arxiv.org/abs/2506.23771", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Ran Yu", "Lu Xiong"], "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, Submitted to IEEE Robotics and Automation Letters", "summary": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u884c\u4e3a\u548c\u63a7\u5236\u3002\u5b83\u6d89\u53ca\u5230\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u56e0\u4e3a\u6d89\u53ca\u5230\u751f\u6210\u548c\u4f18\u5316\u8f66\u8f86\u7684\u8f68\u8ff9\u3002\u7136\u800c\uff0c\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e0d\u9ad8\u3002\u5173\u952e\u8bcd'motion guidance'\u6697\u793a\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Reinforcement Learning", "autonomous driving", "motion guidance", "policy structure"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.22984", "pdf": "https://arxiv.org/pdf/2506.22984", "abs": "https://arxiv.org/abs/2506.22984", "authors": ["Prathyush Kumar Reddy Lebaku", "Lu Gao", "Yunpeng Zhang", "Zhixia Li", "Yongxin Liu", "Tanvir Arafin"], "title": "Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Anomaly detection in connected autonomous vehicles (CAVs) is crucial for\nmaintaining safe and reliable transportation networks, as CAVs can be\nsusceptible to sensor malfunctions, cyber-attacks, and unexpected environmental\ndisruptions. This study explores an anomaly detection approach by simulating\nvehicle behavior, generating a dataset that represents typical and atypical\nvehicular interactions. The dataset includes time-series data of position,\nspeed, and acceleration for multiple connected autonomous vehicles. We utilized\nmachine learning models to effectively identify abnormal driving patterns.\nFirst, we applied a stacked Long Short-Term Memory (LSTM) model to capture\ntemporal dependencies and sequence-based anomalies. The stacked LSTM model\nprocessed the sequential data to learn standard driving behaviors.\nAdditionally, we deployed a Random Forest model to support anomaly detection by\noffering ensemble-based predictions, which enhanced model interpretability and\nperformance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,\nand a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model\nattained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly\nthreshold of 265.63. These results demonstrate the models' effectiveness in\naccurately predicting vehicle trajectories and detecting anomalies in\nautonomous driving scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e92\u8054\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u5176\u4e2d\u6d89\u53ca\u5230\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u4f5c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u7840\u3002\u4f7f\u7528\u4e86LSTM\u6765\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u5e76\u9884\u6d4b\u8f66\u8f86\u8f68\u8ff9\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u5f02\u5e38\u68c0\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "LSTM", "anomaly detection", "autonomous vehicles", "time-series data"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
