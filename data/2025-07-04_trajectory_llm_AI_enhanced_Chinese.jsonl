{"id": "2507.01284", "pdf": "https://arxiv.org/pdf/2507.01284", "abs": "https://arxiv.org/abs/2507.01284", "authors": ["Cristian Gariboldi", "Hayato Tokida", "Ken Kinjo", "Yuki Asada", "Alexander Carballo"], "title": "VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.LG"], "comment": "2025 IEEE 28th International Conference on Intelligent Transportation\n  Systems (ITSC)", "summary": "Recent advancements in open-source Visual Language Models (VLMs) such as\nLLaVA, Qwen-VL, and Llama have catalyzed extensive research on their\nintegration with diverse systems. The internet-scale general knowledge\nencapsulated within these models presents significant opportunities for\nenhancing autonomous driving perception, prediction, and planning capabilities.\nIn this paper we propose VLAD, a vision-language autonomous driving model,\nwhich integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end\nsystem. We implement a specialized fine-tuning approach using custom\nquestion-answer datasets designed specifically to improve the spatial reasoning\ncapabilities of the model. The enhanced VLM generates high-level navigational\ncommands that VAD subsequently processes to guide vehicle operation.\nAdditionally, our system produces interpretable natural language explanations\nof driving decisions, thereby increasing transparency and trustworthiness of\nthe traditionally black-box end-to-end architecture. Comprehensive evaluation\non the real-world nuScenes dataset demonstrates that our integrated system\nreduces average collision rates by 31.82% compared to baseline methodologies,\nestablishing a new benchmark for VLM-augmented autonomous driving systems.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper is highly relevant as it explicitly combines Visual Language Models (VLMs) with autonomous driving, focusing on enhancing perception, prediction, and planning capabilities. The use of VLMs for generating high-level navigational commands directly relates to trajectory prediction and planning, and the system's performance is evaluated on a real-world autonomous driving dataset (nuScenes).", "keywords": ["Visual Language Models", "VLMs", "autonomous driving", "prediction", "planning", "trajectory prediction", "nuScenes"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01378", "pdf": "https://arxiv.org/pdf/2507.01378", "abs": "https://arxiv.org/abs/2507.01378", "authors": ["Ziyao Wang", "Rongpeng Li", "Sizhao Li", "Yuming Xiang", "Haiping Wang", "Zhifeng Zhao", "Honggang Zhang"], "title": "RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": null, "summary": "Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u7684\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528LLM\u6765\u63a7\u5236\u65e0\u4eba\u673a\u7fa4\u7684\u5bfc\u822a\uff0c\u800c\u65e0\u4eba\u673a\u5bfc\u822a\u672c\u8d28\u4e0a\u662f\u8f68\u8ff9\u9884\u6d4b\u548c\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002\u8bba\u6587\u7ed3\u5408\u4e86LLM\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6765\u63d0\u9ad8\u5bfc\u822a\u6027\u80fd\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u90fd\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "Large Language Models (LLMs)", "UAV Swarms", "navigation", "Multi-Agent Reinforcement Learning (MARL)", "role-adaptive", "path planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01264", "pdf": "https://arxiv.org/pdf/2507.01264", "abs": "https://arxiv.org/abs/2507.01264", "authors": ["Yongjie Fu", "Ruijian Zha", "Pei Tian", "Xuan Di"], "title": "LLM-based Realistic Safety-Critical Driving Video Generation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Designing diverse and safety-critical driving scenarios is essential for\nevaluating autonomous driving systems. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) for few-shot code\ngeneration to automatically synthesize driving scenarios within the CARLA\nsimulator, which has flexibility in scenario scripting, efficient code-based\ncontrol of traffic participants, and enforcement of realistic physical\ndynamics. Given a few example prompts and code samples, the LLM generates\nsafety-critical scenario scripts that specify the behavior and placement of\ntraffic participants, with a particular focus on collision events. To bridge\nthe gap between simulation and real-world appearance, we integrate a video\ngeneration pipeline using Cosmos-Transfer1 with ControlNet, which converts\nrendered scenes into realistic driving videos. Our approach enables\ncontrollable scenario generation and facilitates the creation of rare but\ncritical edge cases, such as pedestrian crossings under occlusion or sudden\nvehicle cut-ins. Experimental results demonstrate the effectiveness of our\nmethod in generating a wide range of realistic, diverse, and safety-critical\nscenarios, offering a promising tool for simulation-based testing of autonomous\nvehicles.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u9a7e\u9a76\u573a\u666f\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u751f\u6210\u7684\u573a\u666f\u53ef\u4ee5\u7528\u4e8e\u6d4b\u8bd5\u548c\u8bc4\u4f30\u8f68\u8ff9\u9884\u6d4b\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u90fd\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "driving scenarios", "autonomous driving systems", "scenario generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01308", "pdf": "https://arxiv.org/pdf/2507.01308", "abs": "https://arxiv.org/abs/2507.01308", "authors": ["Muhammad Atta ur Rahman", "Dooseop Choi", "KyoungWook Min"], "title": "LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted at the 17th IEEE International Conference on Advanced\n  Computational Intelligence (ICACI 2025)", "summary": "Accurate motion forecasting is critical for safe and efficient autonomous\ndriving, enabling vehicles to predict future trajectories and make informed\ndecisions in complex traffic scenarios. Most of the current designs of motion\nprediction models are based on the major representation of lane centerlines,\nwhich limits their capability to capture critical road environments and traffic\nrules and constraints. In this work, we propose an enhanced motion forecasting\nmodel informed by multiple vector map elements, including lane boundaries and\nroad edges, that facilitates a richer and more complete representation of\ndriving environments. An effective feature fusion strategy is developed to\nmerge information in different vector map components, where the model learns\nholistic information on road structures and their interactions with agents.\nSince encoding more information about the road environment increases memory\nusage and is computationally expensive, we developed an effective pruning\nmechanism that filters the most relevant map connections to the target agent,\nensuring computational efficiency while maintaining essential spatial and\nsemantic relationships for accurate trajectory prediction. Overcoming the\nlimitations of lane centerline-based models, our method provides a more\ninformative and efficient representation of the driving environment and\nadvances the state of the art for autonomous vehicle motion forecasting. We\nverify our approach with extensive experiments on the Argoverse 2 motion\nforecasting dataset, where our method maintains competitiveness on AV2 while\nachieving improved performance.\n  Index Terms-Autonomous driving, trajectory prediction, vector map elements,\nroad topology, connection pruning, Argoverse 2.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u8868\u660e\u5176\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86\u8f66\u9053\u8fb9\u754c\u4fe1\u606f\u6765\u63d0\u9ad8\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "autonomous driving", "motion forecasting", "lane boundaries", "vector map"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01930", "pdf": "https://arxiv.org/pdf/2507.01930", "abs": "https://arxiv.org/abs/2507.01930", "authors": ["Wenhao Wang", "Yanyan Li", "Long Jiao", "Jiawei Yuan"], "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations", "categories": ["cs.RO"], "comment": "10 pages", "summary": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper explicitly utilizes Large Language Models (LLMs) for UAV operation, which involves controlling the UAV's trajectory. The abstract mentions translating human instructions into executable control code and using LLMs for feedback and refinement based on trajectory descriptions. While not directly performing trajectory *prediction*, it uses LLMs to *control* trajectories, making it relevant. The 'trajectory descriptions' part links it more closely to trajectory analysis.", "keywords": ["Large Language Models", "LLMs", "UAV operation", "trajectory descriptions", "closed-loop control"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01099", "pdf": "https://arxiv.org/pdf/2507.01099", "abs": "https://arxiv.org/abs/2507.01099", "authors": ["Zeyi Liu", "Shuang Li", "Eric Cousineau", "Siyuan Feng", "Benjamin Burchfiel", "Shuran Song"], "title": "Geometry-aware 4D Video Generation for Robot Manipulation", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project website: https://robot4dgen.github.io", "summary": "Understanding and predicting the dynamics of the physical world can enhance a\nrobot's ability to plan and interact effectively in complex environments. While\nrecent video generation models have shown strong potential in modeling dynamic\nscenes, generating videos that are both temporally coherent and geometrically\nconsistent across camera views remains a significant challenge. To address\nthis, we propose a 4D video generation model that enforces multi-view 3D\nconsistency of videos by supervising the model with cross-view pointmap\nalignment during training. This geometric supervision enables the model to\nlearn a shared 3D representation of the scene, allowing it to predict future\nvideo sequences from novel viewpoints based solely on the given RGB-D\nobservations, without requiring camera poses as inputs. Compared to existing\nbaselines, our method produces more visually stable and spatially aligned\npredictions across multiple simulated and real-world robotic datasets. We\nfurther show that the predicted 4D videos can be used to recover robot\nend-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting\nrobust robot manipulation and generalization to novel camera viewpoints.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u751f\u6210\u548c\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u6d89\u53ca\u9884\u6d4b\u7269\u7406\u4e16\u754c\u7684\u52a8\u6001\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u5e7f\u4e49\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u89c6\u9891\u5e8f\u5217\uff0c\u5e76\u53ef\u4ee5\u7528\u4e8e\u6062\u590d\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u8f68\u8ff9\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u548c\u65b9\u6cd5\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["video generation", "robot manipulation", "dynamics prediction", "trajectory recovery", "4D video generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01376", "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "categories": ["cs.AI"], "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses AI agents, large language models (LLMs), and their application in manufacturing. While it doesn't directly address trajectory prediction, the mention of agents navigating dynamic environments suggests a potential connection to path planning and decision-making, which are relevant to trajectory prediction in a broader sense. The strong focus on LLMs contributes to the overall relevance.", "keywords": ["AI agents", "large language models", "LLMs", "Agentic AI", "autonomous decision-making", "dynamic environments"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01206", "pdf": "https://arxiv.org/pdf/2507.01206", "abs": "https://arxiv.org/abs/2507.01206", "authors": ["Kathy Zhuang", "Zixun Huang", "Yukun Song", "Rui Li", "Yinuo Zhou", "Allen Y. Yang"], "title": "2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "As modern computing advances, new interaction paradigms have emerged,\nparticularly in Augmented Reality (AR), which overlays virtual interfaces onto\nphysical objects. This evolution poses challenges in machine perception,\nespecially for tasks like 3D object pose estimation in complex, dynamic\nenvironments. Our project addresses critical issues in human-robot interaction\nwithin mobile AR, focusing on non-intrusive, spatially aware interfaces. We\npresent URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024\nSUITS challenge, targeting future spaceflight needs such as the Artemis\nmissions. URSA integrates three core technologies: a head-mounted AR device\n(e.g., HoloLens) for intuitive visual feedback, voice control powered by large\nlanguage models for hands-free interaction, and robot tracking algorithms that\nenable accurate 3D localization in dynamic settings. To enhance precision, we\nleverage digital twin localization technologies, using datasets like\nDTTD-Mobile and specialized hardware such as the ZED2 camera for real-world\ntracking under noise and occlusion. Our system enables real-time robot control\nand monitoring via an AR interface, even in the absence of ground-truth\nsensors--vital for hazardous or remote operations. Key contributions include:\n(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based\ndataset tailored for non-rigid robotic bodies; (3) a Local Mission Control\nConsole (LMCC) for mission visualization; (4) a transformer-based 6DoF pose\nestimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)\nend-to-end integration for astronaut mission support. This work advances\ndigital twin applications in robotics, offering scalable solutions for both\naerospace and industrial domains.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on an LLM-driven AR system for robotics and space exploration. It involves robot tracking and 3D localization, which are related to trajectory prediction. The use of LLMs is also explicit. However, the primary focus is on AR interface and robot control, rather than directly on trajectory prediction algorithms or LLM applications in trajectory prediction.", "keywords": ["LLM", "Large Language Models", "robot tracking", "3D localization"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01424", "pdf": "https://arxiv.org/pdf/2507.01424", "abs": "https://arxiv.org/abs/2507.01424", "authors": ["Zhenyang Liu", "Yongchong Gu", "Sixiao Zheng", "Xiangyang Xue", "Yanwei Fu"], "title": "TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control", "categories": ["cs.RO"], "comment": null, "summary": "Recent advancements in vision-language models (VLMs) for common-sense\nreasoning have led to the development of vision-language-action (VLA) models,\nenabling robots to perform generalized manipulation. Although existing\nautoregressive VLA methods design a specific architecture like dual-system to\nleverage large-scale pretrained knowledge, they tend to capture static\ninformation, often neglecting the dynamic aspects vital for embodied tasks. To\nthis end, we propose TriVLA, a unified Vision-Language-Action model with a\ntriple-system architecture for general robot control. The vision-language\nmodule (System 2) interprets the environment through vision and language\ninstructions. The dynamics perception module (System 3) inherently produces\nvisual representations that encompass both current static information and\npredicted future dynamics, thereby providing valuable guidance for policy\nlearning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained\nvideo foundation model on robot datasets along with internet human manipulation\ndata. The subsequent policy learning module (System 1) generates fluid motor\nactions in real time. Experimental evaluation demonstrates that TriVLA operates\nat approximately 36 Hz and surpasses state-of-the-art imitation learning\nbaselines on standard simulation benchmarks as well as challenging real-world\nmanipulation tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses a Vision-Language-Action model (TriVLA) for robot control, leveraging pre-trained vision-language models and video foundation models. While it doesn't directly focus on trajectory prediction in the traditional sense, the \"dynamics perception module\" and the generation of \"predicted future dynamics\" suggest a relevance to predicting future states, which is related to trajectory prediction. It also leverages large language models. Therefore, the relevance score is moderate.", "keywords": ["vision-language models", "VLA models", "video foundation model", "dynamics perception", "predicted future dynamics", "robot control", "large language models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01462", "pdf": "https://arxiv.org/pdf/2507.01462", "abs": "https://arxiv.org/abs/2507.01462", "authors": ["Eneko Osaba", "Estibaliz Garrote", "Pablo Miranda-Rodriguez", "Alessia Ciacco", "Itziar Cabanes", "Aitziber Mancisidor"], "title": "Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0", "categories": ["cs.RO", "cs.AI", "cs.ET"], "comment": "2 pages, 1 figure, paper accepted for presentation at the IEEE\n  International Conference on Quantum Computing and Engineering (QCE)", "summary": "This work explores the application of hybrid quantum-classical algorithms to\noptimize robotic inspection trajectories derived from Computer-Aided Design\n(CAD) models in industrial settings. By modeling the task as a 3D variant of\nthe Traveling Salesman Problem, incorporating incomplete graphs and open-route\nconstraints, this study evaluates the performance of two D-Wave-based solvers\nagainst classical methods such as GUROBI and Google OR-Tools. Results across\nfive real-world cases demonstrate competitive solution quality with\nsignificantly reduced computation times, highlighting the potential of quantum\napproaches in automation under Industry 4.0.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on path planning for robotic inspection, which falls under the broader category of trajectory prediction. However, it does not involve large language models. The path planning is based on quantum-assisted algorithms and classical optimization methods, rather than LLMs.", "keywords": ["path planning", "trajectory", "optimization"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01485", "pdf": "https://arxiv.org/pdf/2507.01485", "abs": "https://arxiv.org/abs/2507.01485", "authors": ["Yibo Qiu", "Zan Huang", "Zhiyu Wang", "Handi Liu", "Yiling Qiao", "Yifeng Hu", "Shu'ang Sun", "Hangke Peng", "Ronald X Xu", "Mingzhai Sun"], "title": "BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments", "categories": ["cs.RO", "cs.AI", "cs.MA", "q-bio.QM"], "comment": null, "summary": "Large language models (LLMs) and vision-language models (VLMs) have the\npotential to transform biological research by enabling autonomous\nexperimentation. Yet, their application remains constrained by rigid protocol\ndesign, limited adaptability to dynamic lab conditions, inadequate error\nhandling, and high operational complexity. Here we introduce BioMARS\n(Biological Multi-Agent Robotic System), an intelligent platform that\nintegrates LLMs, VLMs, and modular robotics to autonomously design, plan, and\nexecute biological experiments. BioMARS uses a hierarchical architecture: the\nBiologist Agent synthesizes protocols via retrieval-augmented generation; the\nTechnician Agent translates them into executable robotic pseudo-code; and the\nInspector Agent ensures procedural integrity through multimodal perception and\nanomaly detection. The system autonomously conducts cell passaging and culture\ntasks, matching or exceeding manual performance in viability, consistency, and\nmorphological integrity. It also supports context-aware optimization,\noutperforming conventional strategies in differentiating retinal pigment\nepithelial cells. A web interface enables real-time human-AI collaboration,\nwhile a modular backend allows scalable integration with laboratory hardware.\nThese results highlight the feasibility of generalizable, AI-driven laboratory\nautomation and the transformative role of language-based reasoning in\nbiological research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Large Language Models (LLMs) to automate biological experiments using a multi-agent robotic system. While it doesn't directly address trajectory prediction, the robotic system's planning and execution of experiments could potentially involve trajectory planning for the robots. The connection is indirect, but the use of LLMs is prominent.", "keywords": ["Large language models", "LLMs", "vision-language models", "VLMs", "multi-agent robotic system", "autonomous experimentation", "robotic pseudo-code", "anomaly detection", "AI-driven laboratory automation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01843", "pdf": "https://arxiv.org/pdf/2507.01843", "abs": "https://arxiv.org/abs/2507.01843", "authors": ["Dmytro Kuzmenko", "Nadiya Shvai"], "title": "MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics", "categories": ["cs.RO"], "comment": "Preprint of a manuscript submitted for peer review", "summary": "Mixture-of-Experts (MoE) approaches have recently gained traction in robotics\napplications due to their ability to dynamically allocate computational\nresources and specialize sub-networks for distinct tasks or environmental\ncontexts, enabling more efficient decision-making. Such systems often comprise\nsparsely activated experts combined under a single monolithic architecture and\nrequire a well-configured internal routing mechanism, which does not allow for\nselective low-level expert and router customization and requires additional\ntraining. We propose MoIRA, an architecture-agnostic modular MoE framework\ndesigned to coordinate existing experts with an external text-based router.\nMoIRA incorporates two zero-shot routing options: embedding-based similarity\nand prompt-driven language model inference. In our experiments, we choose large\nVision-Language-Action models, gr00t-N1 and $\\pi_0$, as the underlying experts,\nand train low-rank adapters for low-overhead inference. We evaluate MoIRA on\nvarious GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it\nconsistently outperforms generalist models and competes with other MoE\npipelines. Additionally, we analyse the robustness of the proposed approach to\nthe variations of the instructions. While relying solely on textual\ndescriptions of tasks and experts, MoIRA demonstrates the practical viability\nof modular deployment with precise, low-effort routing and provides an\nalternative, scalable foundation for future multi-expert robotic systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on a modular architecture (MoIRA) for multi-task robotics using a Mixture-of-Experts approach. While it doesn't directly address trajectory prediction, it uses large Vision-Language-Action models (gr00t-N1 and \u03c00) as experts and incorporates prompt-driven language model inference for routing. This indicates a connection to large language models in the context of robotic tasks which could indirectly involve trajectory planning or execution. The connection to trajectory prediction is weak but possible depending on the tasks performed by the robots.", "keywords": ["Large Language Models", "LLMs", "Mixture-of-Experts", "Robotics", "Vision-Language-Action models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01857", "pdf": "https://arxiv.org/pdf/2507.01857", "abs": "https://arxiv.org/abs/2507.01857", "authors": ["Yuhao Lin", "Yi-Lin Wei", "Haoran Liao", "Mu Lin", "Chengyi Xing", "Hao Li", "Dandan Zhang", "Mark Cutkosky", "Wei-Shi Zheng"], "title": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types", "categories": ["cs.RO"], "comment": "Project Page: https://isee-laboratory.github.io/TypeTele", "summary": "Dexterous teleoperation plays a crucial role in robotic manipulation for\nreal-world data collection and remote robot control. Previous dexterous\nteleoperation mostly relies on hand retargeting to closely mimic human hand\npostures. However, these approaches may fail to fully leverage the inherent\ndexterity of dexterous hands, which can execute unique actions through their\nstructural advantages compared to human hands. To address this limitation, we\npropose TypeTele, a type-guided dexterous teleoperation system, which enables\ndexterous hands to perform actions that are not constrained by human motion\npatterns. This is achieved by introducing dexterous manipulation types into the\nteleoperation system, allowing operators to employ appropriate types to\ncomplete specific tasks. To support this system, we build an extensible\ndexterous manipulation type library to cover comprehensive dexterous postures\nused in manipulation tasks. During teleoperation, we employ a MLLM\n(Multi-modality Large Language Model)-assisted type retrieval module to\nidentify the most suitable manipulation type based on the specific task and\noperator commands. Extensive experiments of real-world teleoperation and\nimitation learning demonstrate that the incorporation of manipulation types\nsignificantly takes full advantage of the dexterous robot's ability to perform\ndiverse and complex tasks with higher success rates.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on dexterous teleoperation and introduces a system that uses a Multi-modality Large Language Model (MLLM) to assist in selecting manipulation types. While it doesn't directly address trajectory prediction, the use of MLLM connects it to the large language model domain. The robotic manipulation aspect could be loosely related to trajectory planning, but it's not a central theme.", "keywords": ["Large Language Model", "MLLM", "robotic manipulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01925", "pdf": "https://arxiv.org/pdf/2507.01925", "abs": "https://arxiv.org/abs/2507.01925", "authors": ["Yifan Zhong", "Fengshuo Bai", "Shaofei Cai", "Xuchuan Huang", "Zhang Chen", "Xiaowei Zhang", "Yuanfei Wang", "Shaoyang Guo", "Tianrui Guan", "Ka Nam Lui", "Zhiquan Qi", "Yitao Liang", "Yuanpei Chen", "Yaodong Yang"], "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective", "categories": ["cs.RO"], "comment": "70 pages, 5 figures", "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof \\textit{action tokens} that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper surveys Vision-Language-Action (VLA) models, which generate executable actions based on vision and language inputs. While it doesn't explicitly focus on trajectory prediction as the primary action, the abstract mentions \"trajectory\" as one type of action token. The mention of vision and language foundation models (which includes LLMs) and the general theme of connecting language models to action in the physical world indicates a moderate level of relevance.", "keywords": ["Large Language Models", "foundation models", "action", "trajectory", "Vision-Language-Action Models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01567", "pdf": "https://arxiv.org/pdf/2507.01567", "abs": "https://arxiv.org/abs/2507.01567", "authors": ["Patrick Benito Eberhard", "Johannes K\u00f6hler", "Oliver H\u00fcsser", "Melanie N. Zeilinger", "Andrea Carron"], "title": "Time-Varying Coverage Control: A Distributed Tracker-Planner MPC Framework", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Time-varying coverage control addresses the challenge of coordinating\nmultiple agents covering an environment where regions of interest change over\ntime. This problem has broad applications, including the deployment of\nautonomous taxis and coordination in search and rescue operations. The\nachievement of effective coverage is complicated by the presence of\ntime-varying density functions, nonlinear agent dynamics, and stringent system\nand safety constraints. In this paper, we present a distributed multi-agent\ncontrol framework for time-varying coverage under nonlinear constrained\ndynamics. Our approach integrates a reference trajectory planner and a tracking\nmodel predictive control (MPC) scheme, which operate at different frequencies\nwithin a multi-rate framework. For periodic density functions, we demonstrate\nclosed-loop convergence to an optimal configuration of trajectories and provide\nformal guarantees regarding constraint satisfaction, collision avoidance, and\nrecursive feasibility. Additionally, we propose an efficient algorithm capable\nof handling nonperiodic density functions, making the approach suitable for\npractical applications. Finally, we validate our method through hardware\nexperiments using a fleet of four miniature race cars.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory planning and control for multi-agent systems, which is related to trajectory prediction. It involves planning optimal trajectories for coverage control, but it does not explicitly mention or utilize large language models. The connection to trajectory prediction is through the trajectory planning aspect.", "keywords": ["trajectory planning", "MPC", "multi-agent control", "coverage control"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01381", "pdf": "https://arxiv.org/pdf/2507.01381", "abs": "https://arxiv.org/abs/2507.01381", "authors": ["Tong Liu", "Yinuo Wang", "Xujie Song", "Wenjun Zou", "Liangfa Chen", "Likun Wang", "Bin Shuai", "Jingliang Duan", "Shengbo Eben Li"], "title": "Distributional Soft Actor-Critic with Diffusion Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted IEEE ITSC 2025", "summary": "Reinforcement learning has been proven to be highly effective in handling\ncomplex control tasks. Traditional methods typically use unimodal\ndistributions, such as Gaussian distributions, to model the output of value\ndistributions. However, unimodal distribution often and easily causes bias in\nvalue function estimation, leading to poor algorithm performance. This paper\nproposes a distributional reinforcement learning algorithm called DSAC-D\n(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges\nof estimating bias in value functions and obtaining multimodal policy\nrepresentations. A multimodal distributional policy iteration framework that\ncan converge to the optimal policy was established by introducing policy\nentropy and value distribution function. A diffusion value network that can\naccurately characterize the distribution of multi peaks was constructed by\ngenerating a set of reward samples through reverse sampling using a diffusion\nmodel. Based on this, a distributional reinforcement learning algorithm with\ndual diffusion of the value network and the policy network was derived. MuJoCo\ntesting tasks demonstrate that the proposed algorithm not only learns\nmultimodal policy, but also achieves state-of-the-art (SOTA) performance in all\n9 control tasks, with significant suppression of estimation bias and total\naverage return improvement of over 10\\% compared to existing mainstream\nalgorithms. The results of real vehicle testing show that DSAC-D can accurately\ncharacterize the multimodal distribution of different driving styles, and the\ndiffusion policy network can characterize multimodal trajectories.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper primarily focuses on reinforcement learning with diffusion policies for control tasks. While it mentions \"multimodal trajectories\" and includes real vehicle testing, the core contribution revolves around distributional reinforcement learning and diffusion models for policy optimization. It doesn't directly address trajectory prediction in the sense of predicting future trajectories of agents (pedestrians, vehicles, etc.). It also doesn't mention or utilize large language models. The connection to trajectory prediction is only through the application to vehicle control and the generation of multimodal trajectories as a *result* of the learned policy, not as the primary focus.", "keywords": ["diffusion policy", "multimodal trajectories", "reinforcement learning", "vehicle control"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01274", "pdf": "https://arxiv.org/pdf/2507.01274", "abs": "https://arxiv.org/abs/2507.01274", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance", "categories": ["cs.HC", "cs.AI"], "comment": "Accepted and Presented at 11th International Maritime Science\n  Conference", "summary": "Traditional simulator-based training for maritime professionals is critical\nfor ensuring safety at sea but often depends on subjective trainer assessments\nof technical skills, behavioral focus, communication, and body language, posing\nchallenges such as subjectivity, difficulty in measuring key features, and\ncognitive limitations. Addressing these issues, this study develops an\nAI-driven framework to enhance maritime training by objectively assessing\ntrainee performance through visual focus tracking, speech recognition, and\nstress detection, improving readiness for high-risk scenarios. The system\nintegrates AI techniques, including visual focus determination using eye\ntracking, pupil dilation analysis, and computer vision; communication analysis\nthrough a maritime-specific speech-to-text model and natural language\nprocessing; communication correctness using large language models; and mental\nstress detection via vocal pitch. Models were evaluated on data from simulated\nmaritime scenarios with seafarers exposed to controlled high-stress events. The\nAI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for\nmaritime speech recognition, and ~90% for stress detection, surpassing existing\nbenchmarks. The system provides insights into visual attention, adherence to\ncommunication checklists, and stress levels under demanding conditions. This\nstudy demonstrates how AI can transform maritime training by delivering\nobjective performance analytics, enabling personalized feedback, and improving\npreparedness for real-world operational challenges.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes large language models for communication correctness analysis within a maritime training simulator. While the core application is not directly trajectory prediction, the use of LLMs places it within the scope of large language model research. The connection to trajectory prediction is weak, if any.", "keywords": ["large language models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
