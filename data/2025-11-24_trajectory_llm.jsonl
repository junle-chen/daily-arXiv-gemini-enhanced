{"id": "2511.16048", "pdf": "https://arxiv.org/pdf/2511.16048", "abs": "https://arxiv.org/abs/2511.16048", "authors": ["Qing Zhang", "Jing Huang", "Mingyang Xu", "Jun Rekimoto"], "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems", "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant because it explicitly uses a Multimodal Large Language Model for navigation, which can be considered a form of trajectory prediction or path planning. While it rejects conventional sensors and focuses on qualitative understanding, the core idea involves using LLMs to guide the robot's movement.", "keywords": ["Large Language Model", "navigation", "autonomous", "robot", "Multimodal Large Language Model", "autonomous pipeline"]}}
{"id": "2511.15992", "pdf": "https://arxiv.org/pdf/2511.15992", "abs": "https://arxiv.org/abs/2511.15992", "authors": ["Shahin Zanbaghi", "Ryan Rostampour", "Farhan Abid", "Salim Al Jarmakani"], "title": "Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis", "categories": ["cs.AI"], "comment": "7 pages, 3 figures, 1 table", "summary": "Large Language Models (LLMs) can be backdoored to exhibit malicious behavior under specific deployment conditions while appearing safe during training a phenomenon known as \"sleeper agents.\" Recent work by Hubinger et al. demonstrated that these backdoors persist through safety training, yet no practical detection methods exist. We present a novel dual-method detection system combining semantic drift analysis with canary baseline comparison to identify backdoored LLMs in real-time. Our approach uses Sentence-BERT embeddings to measure semantic deviation from safe baselines, complemented by injected canary questions that monitor response consistency. Evaluated on the official Cadenza-Labs dolphin-llama3-8B sleeper agent model, our system achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall. The combined detection method operates in real-time (<1s per query), requires no model modification, and provides the first practical solution to LLM backdoor detection. Our work addresses a critical security gap in AI deployment and demonstrates that embedding-based detection can effectively identify deceptive model behavior without sacrificing deployment efficiency.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper focuses on detecting backdoors in Large Language Models (LLMs), which is highly relevant to the 'Large Language Models' theme. While it doesn't directly address trajectory prediction, the security and reliability of LLMs are crucial in all applications, including those involving trajectory prediction. The use of semantic analysis and embeddings is also relevant to understanding how LLMs process and generate information, which could indirectly inform trajectory prediction models that utilize LLMs.", "keywords": ["Large Language Models", "LLMs", "semantic drift analysis", "embeddings", "backdoor detection", "security", "AI deployment"]}}
{"id": "2511.16200", "pdf": "https://arxiv.org/pdf/2511.16200", "abs": "https://arxiv.org/abs/2511.16200", "authors": ["Kewei Chen", "Yayu Long", "Mingsheng Shang"], "title": "PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks", "categories": ["cs.RO"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Multi-robot systems in complex physical collaborations face a \"shared brain dilemma\": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace \"raw data communication\" with \"semantic communication\" by performing \"semantic distillation\" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the \"shared brain dilemma\" in resource-constrained multi-robot systems.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u591a\u673a\u5668\u4eba\u534f\u540c\uff0c\u901a\u8fc7\u7269\u7406\u4ea4\u4e92\u9884\u6d4b\u7f51\u7edc\uff08PIPN\uff09\u8fdb\u884c\u8bed\u4e49\u901a\u4fe1\uff0c\u5176\u4e2dPIPN\u662f\u57fa\u4e8e\u5927\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u6784\u5efa\u7684\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u7269\u7406\u4ea4\u4e92\u9884\u6d4b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u9884\u6d4b\u673a\u5668\u4eba\u8fd0\u52a8\u72b6\u6001\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u5e76\u4f7f\u7528\u4e86\u5927\u6a21\u578b\u6280\u672f\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["physical interaction prediction", "knowledge distillation", "large model", "multi-robot systems", "semantic communication"]}}
{"id": "2511.16518", "pdf": "https://arxiv.org/pdf/2511.16518", "abs": "https://arxiv.org/abs/2511.16518", "authors": ["Xiaoshuai Hao", "Lei Zhou", "Zhijian Huang", "Zhiwen Hou", "Yingbo Tang", "Lingfeng Zhang", "Guang Li", "Zheng Lu", "Shuhuai Ren", "Xianhui Meng", "Yuchen Zhang", "Jing Wu", "Jinghui Lu", "Chenxu Dang", "Jiayi Guan", "Jianhua Wu", "Zhiyi Hou", "Hanbing Li", "Shumeng Xia", "Mingliang Zhou", "Yinan Zheng", "Zihao Yue", "Shuhao Gu", "Hao Tian", "Yuannan Shen", "Jianwei Cui", "Wen Zhang", "Shaoqing Xu", "Bing Wang", "Haiyang Sun", "Zeyu Zhu", "Yuncheng Jiang", "Zibin Guo", "Chuhong Gong", "Chaofan Zhang", "Wenbo Ding", "Kun Ma", "Guang Chen", "Rui Cai", "Diyun Xiang", "Heng Qu", "Fuli Luo", "Hangjun Ye", "Long Chen"], "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "Code: https://github.com/XiaomiMiMo/MiMo-Embodied Model: https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B", "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u5177\u8eab\u57fa\u7840\u6a21\u578b MiMo-Embodied\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eab\u667a\u80fd\u9886\u57df\u90fd\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002\u867d\u7136\u6458\u8981\u4e2d\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u201c\u8f68\u8ff9\u9884\u6d4b\u201d\uff0c\u4f46\u201cStatus Prediction\u201d\u548c\u201cDriving Planning\u201d\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8be5\u8bba\u6587\u660e\u786e\u63d0\u5230\u4e86\u201cFoundation Model\u201d\uff0c\u8868\u660e\u5176\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Foundation Model", "Autonomous Driving", "Driving Planning", "Status Prediction"]}}
{"id": "2511.16105", "pdf": "https://arxiv.org/pdf/2511.16105", "abs": "https://arxiv.org/abs/2511.16105", "authors": ["Yuanbo Tang", "Yan Tang", "Zixuan Zhang", "Zihui Zhao", "Yang Li"], "title": "Pathlet Variational Auto-Encoder for Robust Trajectory Generation", "categories": ["cs.LG"], "comment": null, "summary": "Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints.\n  Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\\%$ and $26.3\\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\\%$ of the time and $56.5\\%$ of GPU memory compared to previous approaches.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on trajectory generation using a variational autoencoder (VAE). While it doesn't directly use large language models, it is relevant to trajectory prediction and utilizes a deep generative model for trajectory generation, which is a related field. The abstract also mentions trajectory prediction as a downstream task.", "keywords": ["trajectory generation", "trajectory prediction", "variational autoencoder", "deep generative model", "pathlet representation"]}}
{"id": "2511.15914", "pdf": "https://arxiv.org/pdf/2511.15914", "abs": "https://arxiv.org/abs/2511.15914", "authors": ["Debasmita Ghose", "Oz Gitelson", "Ryan Jin", "Grace Abawe", "Marynel Vazquez", "Brian Scassellati"], "title": "I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration", "categories": ["cs.RO"], "comment": "Accepted to RA-L", "summary": "For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robots adapting to changing human goals during collaboration, which involves predicting human actions and intentions. While it doesn't directly use Large Language Models, the underlying problem of predicting future human behavior aligns with trajectory prediction concepts. The 'policy bank' could potentially be implemented with or informed by insights from large models, though this isn't explicitly stated. The connection to trajectory prediction is stronger than the connection to large language models.", "keywords": ["action prediction", "goal prediction", "human-robot collaboration", "receding horizon planning"]}}
{"id": "2511.16223", "pdf": "https://arxiv.org/pdf/2511.16223", "abs": "https://arxiv.org/abs/2511.16223", "authors": ["Vincenzo Pomponi", "Paolo Franceschi", "Stefano Baraldo", "Loris Roveda", "Oliver Avram", "Luca Maria Gambardella", "Anna Valente"], "title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating trajectories for robot manipulation using Dynamic Movement Primitives (DMPs). While it doesn't directly involve Large Language Models, the generation of trajectories and adaptation to dynamic environments relate to the broader field of trajectory prediction. The use of DMPs for generating realistic trajectories pushes the relevance score above 0.5, but the lack of any LLM component prevents a higher score.", "keywords": ["trajectory", "Dynamic Movement Primitives", "robot learning", "dynamic environments", "data generation"]}}
{"id": "2511.16233", "pdf": "https://arxiv.org/pdf/2511.16233", "abs": "https://arxiv.org/abs/2511.16233", "authors": ["Kewei Chen", "Yayu Long", "Shuai Li", "Mingsheng Shang"], "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models", "categories": ["cs.RO"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action (VLA) models and data distillation for efficient training. While it doesn't explicitly mention trajectory prediction, the \"Action\" component of VLA models can potentially involve predicting future actions or movements, which is related to trajectory prediction. The mention of large models (VLA models) also contributes to the relevance. However, the core focus is on data distillation rather than trajectory prediction itself.", "keywords": ["Vision-Language-Action models", "VLA models", "data distillation", "large models"]}}
{"id": "2511.16372", "pdf": "https://arxiv.org/pdf/2511.16372", "abs": "https://arxiv.org/abs/2511.16372", "authors": ["Bowen Xu", "Zexuan Yan", "Minghao Lu", "Xiyu Fan", "Yi Luo", "Youshen Lin", "Zhiqiang Chen", "Yeke Chen", "Qiyuan Qiao", "Peng Lu"], "title": "Flow-Aided Flight Through Dynamic Clutters From Point To Motion", "categories": ["cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L), November, 2025", "summary": "Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u98de\u884c\uff0c\u6d89\u53ca\u8f68\u8ff9\u89c4\u5212\u548c\u907f\u969c\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u4e0e\u5927\u89c4\u6a21AI\u6a21\u578b\u76f8\u5173\u7684\u4e00\u4e2a\u9886\u57df\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u4e86\u73af\u5883\u52a8\u6001\u7684\u611f\u77e5\u548c\u89c4\u907f\u884c\u4e3a\u7684\u751f\u6210\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u8fd0\u52a8\u9884\u6d4b\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "motion prediction", "reinforcement learning", "dynamic environment", "obstacle avoidance", "point flow"]}}
{"id": "2511.16651", "pdf": "https://arxiv.org/pdf/2511.16651", "abs": "https://arxiv.org/abs/2511.16651", "authors": ["Yang Tian", "Yuyin Yang", "Yiman Xie", "Zetao Cai", "Xu Shi", "Ning Gao", "Hangxu Liu", "Xuekun Jiang", "Zherui Qiu", "Feng Yuan", "Yaping Li", "Ping Wang", "Junhao Cai", "Jia Zeng", "Hao Dong", "Jiangmiao Pang"], "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy", "categories": ["cs.RO"], "comment": null, "summary": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $\u03c0$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $\u03c0_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $\u03c0_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating synthetic data for pre-training Vision-Language-Action (VLA) models. While it doesn't directly mention trajectory prediction, the generation of trajectories for robotic manipulation tasks is a core component. Furthermore, the use of large-scale data for pre-training aligns with the principles of large language models. The connection to trajectory prediction is implicit rather than explicit, hence the score.", "keywords": ["VLA models", "pre-training", "synthetic data", "trajectories", "embodied AI", "large-scale"]}}
{"id": "2511.16049", "pdf": "https://arxiv.org/pdf/2511.16049", "abs": "https://arxiv.org/abs/2511.16049", "authors": ["Pei Liu", "Songtao Wang", "Lang Zhang", "Xingyue Peng", "Yuandong Lyu", "Jiaxin Deng", "Songxin Lu", "Weiliang Ma", "Xueyang Zhang", "Yifei Zhan", "XianPeng Lang", "Jun Ma"], "title": "LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d4D LiDAR\u6570\u636e\u7684\u751f\u6210\u548c\u9884\u6d4b\uff0c\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff08prediction\uff09\u76f8\u5173\u7684\u6982\u5ff5\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u5b83\u4f7f\u7528\u4e86Transformer\u67b6\u6784\uff0c\u4f46Transformer\u5728\u8fd9\u91cc\u4e3b\u8981\u7528\u4e8e\u65f6\u7a7a\u6ce8\u610f\u529b\u5efa\u6a21\uff0c\u800c\u975e\u4f5c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u3002", "keywords": ["trajectory prediction", "prediction", "autonomous driving", "4D LiDAR", "Transformer"]}}
{"id": "2511.16043", "pdf": "https://arxiv.org/pdf/2511.16043", "abs": "https://arxiv.org/abs/2511.16043", "authors": ["Peng Xia", "Kaide Zeng", "Jiaqi Liu", "Can Qin", "Fang Wu", "Yiyang Zhou", "Caiming Xiong", "Huaxiu Yao"], "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8LLM Agents\u7684\u81ea\u8fdb\u5316\uff0c\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u6d89\u53caagent\u7684reasoning\u548ctool use\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46agent\u7684\u5b66\u4e60\u548c\u8fdb\u5316\u8fc7\u7a0b\u53ef\u4ee5\u88ab\u5e94\u7528\u5230\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Model", "LLM Agents", "reasoning", "self-evolution", "tool use"]}}
{"id": "2511.16183", "pdf": "https://arxiv.org/pdf/2511.16183", "abs": "https://arxiv.org/abs/2511.16183", "authors": ["Jeremie Ochin", "Raphael Chekroun", "Bogdan Stanciulescu", "Sotiris Manitsaris"], "title": "FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8db3\u7403\u6bd4\u8d5b\u89c6\u9891\u4e2d\u7403\u5458\u52a8\u4f5c\u7684\u8bc6\u522b\u548c\u5206\u6790\uff0c\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e0b\u7684\u65f6\u7a7a\u52a8\u4f5c\u68c0\u6d4b\u548c\u76ee\u6807\u8ddf\u8e2a\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201ctrajectory forecasting\u201d\uff08\u8f68\u8ff9\u9884\u6d4b\uff09\uff0c\u8868\u660e\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u6ca1\u6709\u63d0\u53ca\u5927\u6a21\u578b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e0d\u662f\u5f88\u9ad8\u3002\u8bba\u6587\u5229\u7528\u6218\u672f\u77e5\u8bc6\u4f5c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u6765\u652f\u6301\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u9884\u6d4b\uff0c\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u5728\u7279\u5b9a\u9886\u57df\u7684\u5e94\u7528\u3002", "keywords": ["trajectory forecasting", "multi-agent", "action spotting"]}}
{"id": "2511.16150", "pdf": "https://arxiv.org/pdf/2511.16150", "abs": "https://arxiv.org/abs/2511.16150", "authors": ["Chunxu Liu", "Jiyuan Yang", "Ruopeng Gao", "Yuhan Zhu", "Feng Zhu", "Rui Zhao", "Limin Wang"], "title": "Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on improving multimodal retrieval using Multimodal Large Language Models (MLLMs) and reasoning. While it doesn't directly address trajectory prediction, the use of MLLMs is a strong indicator of relevance to the 'Large Language Models' theme. The techniques developed could potentially be adapted for trajectory prediction tasks involving multimodal data.", "keywords": ["Multimodal Large Language Models", "MLLMs", "reasoning", "multimodal retrieval", "embeddings"]}}
{"id": "2511.16160", "pdf": "https://arxiv.org/pdf/2511.16160", "abs": "https://arxiv.org/abs/2511.16160", "authors": ["Yibin Huang", "Wang Xu", "Wanyue Zhang", "Helu Zhi", "Jingjing Huang", "Yangbin Xu", "Yangang Sun", "Conghui Zhu", "Tiejun Zhao"], "title": "Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on spatial reasoning for Multimodal Large Language Models (MLLMs) and uses video input to reconstruct spatial layouts. While it doesn't directly address trajectory prediction, the spatial reasoning aspect and the use of video input are relevant to understanding movement and potential trajectories within a scene. The mention of MLLMs contributes to the relevance.", "keywords": ["Multimodal Large Language Models", "MLLMs", "spatial reasoning", "video"]}}
{"id": "2511.16166", "pdf": "https://arxiv.org/pdf/2511.16166", "abs": "https://arxiv.org/abs/2511.16166", "authors": ["Zeting Liu", "Zida Yang", "Zeyu Zhang", "Hao Tang"], "title": "EvoVLA: Self-Evolving Vision-Language-Action Model", "categories": ["cs.CV"], "comment": null, "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53caVision-Language-Action\u6a21\u578b\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u5176\u7814\u7a76\u7684 robotic manipulation \u548c\u52a8\u4f5c\u89c4\u5212\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u540c\u65f6\uff0c\u6458\u8981\u4e2d\u63d0\u5230\u4e86Gemini\uff0c\u8868\u660e\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "Vision-Language-Action", "robotic manipulation", "action", "Gemini"]}}
{"id": "2511.16175", "pdf": "https://arxiv.org/pdf/2511.16175", "abs": "https://arxiv.org/abs/2511.16175", "authors": ["Yi Yang", "Xueqi Li", "Yiyang Chen", "Jin Song", "Yihan Wang", "Zipeng Xiao", "Jiadi Su", "You Qiaoben", "Pengfei Liu", "Zhijie Deng"], "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $\u03c0_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53caVision-Language-Action\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u89c6\u89c9\u4fe1\u53f7\u8fdb\u884c\u52a8\u4f5c\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176Disentangled Visual Foresight (DVF) \u6a21\u5757\u9884\u6d4b\u89c6\u89c9\u8f68\u8ff9\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u4e86Diffusion Transformer (DiT)\uff0c\u5c5e\u4e8e\u4e00\u79cd\u5927\u578b\u6a21\u578b\u3002\u4f46\u8bba\u6587\u91cd\u70b9\u5e76\u975e\u5b8c\u5168\u805a\u7126\u4e8e\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u8bed\u8a00\u6a21\u578b\u672c\u8eab\uff0c\u800c\u662fVLA\u6a21\u578b\u53ca\u5176\u5728\u52a8\u4f5c\u9884\u6d4b\u4e0a\u7684\u5e94\u7528\u3002", "keywords": ["Vision-Language-Action model", "visual foresight", "action prediction", "diffusion Transformer", "DiT"]}}
{"id": "2511.16258", "pdf": "https://arxiv.org/pdf/2511.16258", "abs": "https://arxiv.org/abs/2511.16258", "authors": ["Yang Xu", "Zuliang Yang", "Kai Ming Ting"], "title": "GeoPTH: A Lightweight Approach to Category-Based Trajectory Retrieval via Geometric Prototype Trajectory Hashing", "categories": ["cs.LG"], "comment": null, "summary": "Trajectory similarity retrieval is an important part of spatiotemporal data mining, however, existing methods have the following limitations: traditional metrics are computationally expensive, while learning-based methods suffer from substantial training costs and potential instability. This paper addresses these problems by proposing \\textbf{Geo}metric \\textbf{P}rototype \\textbf{T}rajectory \\textbf{H}ashing (GeoPTH), a novel, lightweight, and non-learning framework for efficient category-based trajectory retrieval. GeoPTH constructs data-dependent hash functions by using representative trajectory prototypes, i.e., small point sets preserving geometric characteristics, as anchors. The hashing process is efficient, which involves mapping a new trajectory to its closest prototype via a robust, \\textit{Hausdorff} metric. Extensive experiments show that GeoPTH's retrieval accuracy is highly competitive with both traditional metrics and state-of-the-art learning methods, and it significantly outperforms binary codes generated through simple binarization of the learned embeddings. Critically, GeoPTH consistently outperforms all competitors in terms of efficiency. Our work demonstrates that a lightweight, prototype-centric approach offers a practical and powerful alternative, achieving an exceptional retrieval performance and computational efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u68c0\u7d22\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u4e0b\u6e38\u5e94\u7528\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\uff0c\u4f46\u4e0e\u8f68\u8ff9\u6570\u636e\u5904\u7406\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory retrieval", "spatiotemporal data mining", "trajectory hashing", "geometric prototype"]}}
{"id": "2511.16203", "pdf": "https://arxiv.org/pdf/2511.16203", "abs": "https://arxiv.org/abs/2511.16203", "authors": ["Yuping Yan", "Yuhan Xie", "Yinxin Zhang", "Lingjuan Lyu", "Yaochu Jin"], "title": "When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action models (VLAs) and their robustness against multimodal adversarial attacks. While it doesn't directly address trajectory prediction, the 'action' component of VLAs can be related to controlling agents in embodied environments, which can indirectly involve trajectory planning or prediction. The paper also explicitly mentions using a fine-tuned OpenVLA model, indicating the use of a large model architecture. Therefore, there is a moderate degree of relevance to both trajectory prediction (through the 'action' aspect) and large language models (through the use of OpenVLA).", "keywords": ["Vision-Language-Action models", "VLA", "OpenVLA", "multimodal", "embodied environments", "action"]}}
{"id": "2511.16333", "pdf": "https://arxiv.org/pdf/2511.16333", "abs": "https://arxiv.org/abs/2511.16333", "authors": ["Mohammad Areeb Qazi", "Maryam Nadeem", "Mohammad Yaqub"], "title": "Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning", "categories": ["cs.LG"], "comment": "2 Figures, 1 Table", "summary": "Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses world models for clinical prediction, counterfactuals, and planning in healthcare. While it doesn't directly focus on trajectory prediction in the typical sense (e.g., pedestrian or vehicle), it does involve predicting future states based on actions and time, which aligns with the core concepts of trajectory prediction. It also mentions generative models like transformers, diffusion models, and VAEs, which are related to large models, although the main focus is not on their language capabilities. The connection to trajectory prediction is more conceptual (predicting future states/trajectories in a different domain) than direct.", "keywords": ["world models", "prediction", "temporal prediction", "action-conditioned prediction", "trajectory", "transformers", "diffusion models", "VAE", "generative models"]}}
{"id": "2511.16227", "pdf": "https://arxiv.org/pdf/2511.16227", "abs": "https://arxiv.org/abs/2511.16227", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "SwiTrack: Tri-State Switch for Cross-Modal Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\\% and 4.3\\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8de8\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a\uff0c\u5176\u4e2d\u6d89\u53ca\u4f7f\u7528\u8f68\u8ff9\u9884\u6d4b\u6a21\u5757\u6765\u5904\u7406\u65e0\u6548\u6a21\u6001\u4e0b\u7684\u76ee\u6807\u6f02\u79fb\u95ee\u9898\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u70b9\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\uff0c\u4f46\u8f68\u8ff9\u9884\u6d4b\u662f\u8be5\u65b9\u6cd5\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u76ee\u6807\u8ddf\u8e2a", "\u52a8\u4f5c\u9884\u6d4b"]}}
{"id": "2511.16264", "pdf": "https://arxiv.org/pdf/2511.16264", "abs": "https://arxiv.org/abs/2511.16264", "authors": ["Sinan Mutlu", "Georgios F. Angelis", "Savas Ozkan", "Paul Wisbey", "Anastasios Drosou", "Mete Ozay"], "title": "Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs", "categories": ["cs.CV"], "comment": null, "summary": "Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating 3D human motion from sparse inputs, which is related to trajectory prediction (specifically, human motion prediction). It uses a neural network (MLP) architecture. However, the paper does not explicitly mention or utilize large language models. The connection to trajectory prediction is stronger than the connection to large language models.", "keywords": ["3D human motion generation", "motion prediction", "neural network", "MLP", "temporal consistency"]}}
{"id": "2511.16426", "pdf": "https://arxiv.org/pdf/2511.16426", "abs": "https://arxiv.org/abs/2511.16426", "authors": ["Seyed Mohamad Moghadas", "Bruno Cornelis", "Adrian Munteanu"], "title": "FreqFlow: Long-term forecasting using lightweight flow matching", "categories": ["cs.LG"], "comment": "Accepted at EurIPS, 2025", "summary": "Multivariate time-series (MTS) forecasting is fundamental to applications ranging from urban mobility and resource management to climate modeling. While recent generative models based on denoising diffusion have advanced state-of-the-art performance in capturing complex data distributions, they suffer from significant computational overhead due to iterative stochastic sampling procedures that limit real-time deployment. Moreover, these models can be brittle when handling high-dimensional, non-stationary, and multi-scale periodic patterns characteristic of real-world sensor networks. We introduce FreqFlow, a novel framework that leverages conditional flow matching in the frequency domain for deterministic MTS forecasting. Unlike conventional approaches that operate in the time domain, FreqFlow transforms the forecasting problem into the spectral domain, where it learns to model amplitude and phase shifts through a single complex-valued linear layer. This frequency-domain formulation enables the model to efficiently capture temporal dynamics via complex multiplication, corresponding to scaling and temporal translations. The resulting architecture is exceptionally lightweight with only 89k parameters - an order of magnitude smaller than competing diffusion-based models-while enabling single-pass deterministic sampling through ordinary differential equation (ODE) integration. Our approach decomposes MTS signals into trend, seasonal, and residual components, with the flow matching mechanism specifically designed for residual learning to enhance long-term forecasting accuracy. Extensive experiments on real-world traffic speed, volume, and flow datasets demonstrate that FreqFlow achieves state-of-the-art forecasting performance, on average 7\\% RMSE improvements, while being significantly faster and more parameter-efficient than existing methods", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7279\u522b\u662f\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u9884\u6d4b\u4ea4\u901a\u6d41\u91cf\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u9884\u6d4b\u8f66\u8f86\u8f68\u8ff9\u7684\u4e00\u79cd\u5f62\u5f0f\u3002\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["time-series forecasting", "traffic speed", "traffic volume", "traffic flow"]}}
