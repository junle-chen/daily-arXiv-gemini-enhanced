{"id": "2509.25249", "pdf": "https://arxiv.org/pdf/2509.25249", "abs": "https://arxiv.org/abs/2509.25249", "authors": ["Guancheng Chen", "Sheng Yang", "Tong Zhan", "Jian Wang"], "title": "BEV-VLM: Trajectory Planning via Unified BEV Abstraction", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper introduces BEV-VLM, a novel framework for trajectory planning in\nautonomous driving that leverages Vision-Language Models (VLMs) with Bird's-Eye\nView (BEV) feature maps as visual inputs. Unlike conventional approaches that\nrely solely on raw visual data such as camera images, our method utilizes\nhighly compressed and informative BEV representations, which are generated by\nfusing multi-modal sensor data (e.g., camera and LiDAR) and aligning them with\nHD Maps. This unified BEV-HD Map format provides a geometrically consistent and\nrich scene description, enabling VLMs to perform accurate trajectory planning.\nExperimental results on the nuScenes dataset demonstrate 44.8% improvements in\nplanning accuracy and complete collision avoidance. Our work highlights that\nVLMs can effectively interpret processed visual representations like BEV\nfeatures, expanding their applicability beyond raw images in trajectory\nplanning.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper is highly relevant because it combines trajectory planning with Vision-Language Models (VLMs), which fall under the umbrella of Large Language Models. The paper explicitly uses VLMs for trajectory planning in autonomous driving, indicating a strong connection between the two fields.", "keywords": ["trajectory planning", "Vision-Language Models", "VLMs", "autonomous driving", "BEV", "Bird's-Eye View"]}}
{"id": "2509.26324", "pdf": "https://arxiv.org/pdf/2509.26324", "abs": "https://arxiv.org/abs/2509.26324", "authors": ["Ruiyang Wang", "Haolun Tsu", "David Hunt", "Shaocheng Luo", "Jiwoo Kim", "Miroslav Pajic"], "title": "LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Autonomous exploration and object search in unknown indoor environments\nremain challenging for multi-robot systems (MRS). Traditional approaches often\nrely on greedy frontier assignment strategies with limited inter-robot\ncoordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot\nCoordinated Exploration and Search), a novel framework that leverages Large\nLanguage Models (LLMs) for intelligent coordination of both homogeneous and\nheterogeneous robot teams tasked with efficient exploration and target object\nsearch. Our approach combines real-time LiDAR scan processing for frontier\ncluster extraction and doorway detection with multimodal LLM reasoning (e.g.,\nGPT-4o) to generate coordinated waypoint assignments based on shared\nenvironment maps and robot states. LLM-MCoX demonstrates superior performance\ncompared to existing methods, including greedy and Voronoi-based planners,\nachieving 22.7% faster exploration times and 50% improved search efficiency in\nlarge environments with 6 robots. Notably, LLM-MCoX enables natural\nlanguage-based object search capabilities, allowing human operators to provide\nhigh-level semantic guidance that traditional algorithms cannot interpret.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u591a\u673a\u5668\u4eba\u534f\u540c\u63a2\u7d22\u548c\u641c\u7d22\uff0c\u6d89\u53ca\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8f68\u8ff9\u9884\u6d4b\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002\u867d\u7136\u8bba\u6587\u7684\u6838\u5fc3\u662f\u591a\u673a\u5668\u4eba\u534f\u540c\uff0c\u4f46LLM\u5728\u5176\u4e2d\u626e\u6f14\u4e86\u751f\u6210\u534f\u8c03\u8def\u5f84\u7684\u89d2\u8272\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "multi-robot", "coordinated exploration", "waypoint assignments", "robot states", "path planning"]}}
{"id": "2509.25586", "pdf": "https://arxiv.org/pdf/2509.25586", "abs": "https://arxiv.org/abs/2509.25586", "authors": ["Jihye Choi", "Jinsung Yoon", "Jiefeng Chen", "Somesh Jha", "Tomas Pfister"], "title": "ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) have shown remarkable advancements in\nreasoning and tool use, they often fail to generate optimal, grounded solutions\nunder complex constraints. Real-world travel planning exemplifies these\nchallenges, evaluating agents' abilities to handle constraints that are\nexplicit, implicit, and even evolving based on interactions with dynamic\nenvironments and user needs. In this paper, we present ATLAS, a general\nmulti-agent framework designed to effectively handle such complex nature of\nconstraints awareness in real-world travel planning tasks. ATLAS introduces a\nprincipled approach to address the fundamental challenges of constraint-aware\nplanning through dedicated mechanisms for dynamic constraint management,\niterative plan critique, and adaptive interleaved search. ATLAS demonstrates\nstate-of-the-art performance on the TravelPlanner benchmark, improving the\nfinal pass rate from 23.3% to 44.4% over its best alternative. More\nimportantly, our work is the first to demonstrate quantitative effectiveness on\nreal-world travel planning tasks with live information search and multi-turn\nfeedback. In this realistic setting, ATLAS showcases its superior overall\nplanning performance, achieving an 84% final pass rate which significantly\noutperforms baselines including ReAct (59%) and a monolithic agent (27%).", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper discusses real-world travel planning, which is related to trajectory prediction (especially for agents moving in a travel network). Furthermore, it explicitly mentions and utilizes Large Language Models (LLMs) for this task, indicating a significant connection to both topics. The focus on constraint-aware planning and multi-agent collaboration strengthens the link to trajectory prediction in complex environments.", "keywords": ["Large Language Models", "LLMs", "travel planning", "multi-agent", "constraints", "real-world"]}}
{"id": "2509.25794", "pdf": "https://arxiv.org/pdf/2509.25794", "abs": "https://arxiv.org/abs/2509.25794", "authors": ["Haotian Xue", "Yunhao Ge", "Yu Zeng", "Zhaoshuo Li", "Ming-Yu Liu", "Yongxin Chen", "Jiaojiao Fan"], "title": "Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated impressive world knowledge\nacross a wide range of tasks, making them promising candidates for embodied\nreasoning applications. However, existing benchmarks primarily evaluate the\nembodied reasoning ability of VLMs through multiple-choice questions based on\nimage annotations -- for example, selecting which trajectory better describes\nan event in the image. In this work, we introduce the Point-It-Out (PIO)\nbenchmark, a novel benchmark designed to systematically assess the embodied\nreasoning abilities of VLMs through precise visual grounding. We propose a\nhierarchical evaluation protocol spanning three stages (S1: referred-object\nlocalization, S2: task-driven pointing, and S3: visual trace prediction), with\ndata collected from critical domains for embodied intelligence, including\nindoor, kitchen, driving, and robotic manipulation scenarios. Extensive\nexperiments with over ten state-of-the-art VLMs reveal several interesting\nfindings. For example, strong general-purpose models such as GPT-4o, while\nexcelling on many benchmarks (e.g., language, perception, and reasoning),\nunderperform compared to some open-source models in precise visual grounding;\nmodels such as MoLMO perform well in S1 and S2 but struggle in S3, where\nrequires grounding combined with visual trace planning.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5177\u8eab\u63a8\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u540d\u4e3a\u201c\u89c6\u89c9\u8f68\u8ff9\u9884\u6d4b\u201d\u7684\u9636\u6bb5\uff08S3\uff09\u3002\u867d\u7136\u8bba\u6587\u7684\u6838\u5fc3\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\u7b97\u6cd5\u672c\u8eab\uff0c\u4f46\u5b83\u4f7f\u7528\u8f68\u8ff9\u9884\u6d4b\u4f5c\u4e3a\u8bc4\u4f30VLMs\u5728\u5177\u8eab\u73af\u5883\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u4e00\u90e8\u5206\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e5f\u63d0\u5230\u4e86GPT-4o\u7b49\u5927\u578b\u6a21\u578b\u3002", "keywords": ["Vision-Language Models", "VLMs", "embodied reasoning", "visual trace prediction", "GPT-4o", "foundation models"]}}
{"id": "2509.25304", "pdf": "https://arxiv.org/pdf/2509.25304", "abs": "https://arxiv.org/abs/2509.25304", "authors": ["Haozhe Jia", "Wenshuo Chen", "Yuqi Lin", "Yang Yang", "Lei Wang", "Mang Ning", "Bowen Tian", "Songning Lai", "Nanqian Jia", "Yifan Chen", "Yutao Yue"], "title": "LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "While current diffusion-based models, typically built on U-Net architectures,\nhave shown promising results on the text-to-motion generation task, they still\nsuffer from semantic misalignment and kinematic artifacts. Through analysis, we\nidentify severe gradient attenuation in the deep layers of the network as a key\nbottleneck, leading to insufficient learning of high-level features. To address\nthis issue, we propose \\textbf{LUMA} (\\textit{\\textbf{L}ow-dimension\n\\textbf{U}nified \\textbf{M}otion \\textbf{A}lignment}), a text-to-motion\ndiffusion model that incorporates dual-path anchoring to enhance semantic\nalignment. The first path incorporates a lightweight MoCLIP model trained via\ncontrastive learning without relying on external data, offering semantic\nsupervision in the temporal domain. The second path introduces complementary\nalignment signals in the frequency domain, extracted from low-frequency DCT\ncomponents known for their rich semantic content. These two anchors are\nadaptively fused through a temporal modulation mechanism, allowing the model to\nprogressively transition from coarse alignment to fine-grained semantic\nrefinement throughout the denoising process. Experimental results on HumanML3D\nand KIT-ML demonstrate that LUMA achieves state-of-the-art performance, with\nFID scores of 0.035 and 0.123, respectively. Furthermore, LUMA accelerates\nconvergence by 1.4$\\times$ compared to the baseline, making it an efficient and\nscalable solution for high-fidelity text-to-motion generation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53catext-to-motion\u751f\u6210\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u7279\u6b8a\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u5373\u6839\u636e\u6587\u672c\u751f\u6210\u52a8\u4f5c\u8f68\u8ff9\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86MoCLIP\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u5bf9\u9f50\uff0c\u5e76\u4e14\u7814\u7a76\u4e86diffusion model\u7684\u8bad\u7ec3\u95ee\u9898\uff0c\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002", "keywords": ["text-to-motion", "diffusion model", "motion alignment", "MoCLIP"]}}
{"id": "2509.25452", "pdf": "https://arxiv.org/pdf/2509.25452", "abs": "https://arxiv.org/abs/2509.25452", "authors": ["Suhala Rabab Saba", "Sakib Khan", "Minhaj Uddin Ahmad", "Jiahe Cao", "Mizanur Rahman", "Li Zhao", "Nathan Huynh", "Eren Erman Ozguven"], "title": "Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Infrastructure-based sensing and real-time trajectory generation show promise\nfor improving safety in high-risk roadway segments such as work zones, yet\npractical deployments are hindered by perspective distortion, complex geometry,\nocclusions, and costs. This study tackles these barriers by integrating\nroadside camera and LiDAR sensors into a cosimulation environment to develop a\nscalable, cost-effective vehicle detection and localization framework, and\nemploying a Kalman Filter-based late fusion strategy to enhance trajectory\nconsistency and accuracy. In simulation, the fusion algorithm reduced\nlongitudinal error by up to 70 percent compared to individual sensors while\npreserving lateral accuracy within 1 to 3 meters. Field validation in an active\nwork zone, using LiDAR, a radar-camera rig, and RTK-GPS as ground truth,\ndemonstrated that the fused trajectories closely match real vehicle paths, even\nwhen single-sensor data are intermittent or degraded. These results confirm\nthat KF based sensor fusion can reliably compensate for individual sensor\nlimitations, providing precise and robust vehicle tracking capabilities. Our\napproach thus offers a practical pathway to deploy infrastructure-enabled\nmulti-sensor systems for proactive safety measures in complex traffic\nenvironments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on vehicle trajectory generation and tracking using sensor fusion. While it doesn't directly involve large language models, the core topic of trajectory prediction is present. The use of Kalman Filter for trajectory consistency relates to trajectory prediction methodologies.", "keywords": ["trajectory generation", "sensor fusion", "vehicle tracking", "Kalman Filter"]}}
{"id": "2509.25681", "pdf": "https://arxiv.org/pdf/2509.25681", "abs": "https://arxiv.org/abs/2509.25681", "authors": ["Junjie Wen", "Minjie Zhu", "Jiaming Liu", "Zhiyuan Liu", "Yicun Yang", "Linfeng Zhang", "Shanghang Zhang", "Yichen Zhu", "Yi Xu"], "title": "dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought", "categories": ["cs.RO", "cs.CV"], "comment": "technique report", "summary": "Vision-Language-Action (VLA) models are emerging as a next-generation\nparadigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages\na multimodal chain-of-thought to unify visual perception, language reasoning,\nand robotic control in a single system. dVLA jointly optimizes perception,\nlanguage understanding, and action under a single diffusion objective, enabling\nstronger cross-modal reasoning and better generalization to novel instructions\nand objects. For practical deployment, we mitigate inference latency by\nincorporating two acceleration strategies, a prefix attention mask and KV\ncaching, yielding up to around times speedup at test-time inference. We\nevaluate dVLA in both simulation and the real world: on the LIBERO benchmark,\nit achieves state-of-the-art performance with a 96.4% average success rate,\nconsistently surpassing both discrete and continuous action policies; on a real\nFranka robot, it succeeds across a diverse task suite, including a challenging\nbin-picking task that requires multi-step planning, demonstrating robust\nreal-world performance. Together, these results underscore the promise of\nunified diffusion frameworks for practical, high-performance VLA robotics.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action models for robotics, which inherently involves action prediction and control. It leverages diffusion models and chain-of-thought reasoning, indicating the use of a large model architecture. While not directly focused on trajectory prediction in the traditional sense (e.g., predicting pedestrian or vehicle trajectories), the robotic control aspect touches upon similar principles of predicting future states and actions. The mention of 'multi-step planning' also hints at trajectory-like considerations.", "keywords": ["Vision-Language-Action models", "diffusion models", "robotics", "chain-of-thought", "large models", "action prediction", "planning"]}}
{"id": "2509.25685", "pdf": "https://arxiv.org/pdf/2509.25685", "abs": "https://arxiv.org/abs/2509.25685", "authors": ["Amelie Minji Kim", "Anqi Wu", "Ye Zhao"], "title": "Hierarchical Diffusion Motion Planning with Task-Conditioned Uncertainty-Aware Priors", "categories": ["cs.RO"], "comment": null, "summary": "We propose a novel hierarchical diffusion planner that embeds task and motion\nstructure directly in the noise model. Unlike standard diffusion-based planners\nthat use zero-mean, isotropic Gaussian noise, we employ a family of\ntask-conditioned structured Gaussians whose means and covariances are derived\nfrom Gaussian Process Motion Planning (GPMP): sparse, task-centric key states\nor their associated timings (or both) are treated as noisy observations to\nproduce a prior instance. We first generalize the standard diffusion process to\nbiased, non-isotropic corruption with closed-form forward and posterior\nexpressions. Building on this, our hierarchy separates prior instantiation from\ntrajectory denoising: the upper level instantiates a task-conditioned\nstructured Gaussian (mean and covariance), and the lower level denoises the\nfull trajectory under that fixed prior. Experiments on Maze2D goal-reaching and\nKUKA block stacking show improved success rates, smoother trajectories, and\nstronger task alignment compared to isotropic baselines. Ablation studies\nindicate that explicitly structuring the corruption process offers benefits\nbeyond simply conditioning the neural network. Overall, our method concentrates\nprobability mass of prior near feasible, smooth, and semantically meaningful\ntrajectories while maintaining tractability. Our project page is available at\nhttps://hta-diffusion.github.io.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8fd0\u52a8\u89c4\u5212\uff0c\u4f7f\u7528\u4e86\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u751f\u6210\u548c\u89c4\u5212\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u8054\u7cfb\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u9ad8\u65af\u8fc7\u7a0b\u8fd0\u52a8\u89c4\u5212(GPMP)\u6765\u5f15\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u4ece\u800c\u4f18\u5316\u8f68\u8ff9\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u6027\uff0c\u4f46\u662f\u4e0e\u5927\u6a21\u578b\u65e0\u5173\u3002", "keywords": ["motion planning", "diffusion model", "trajectory", "Gaussian Process Motion Planning", "GPMP"]}}
{"id": "2509.25687", "pdf": "https://arxiv.org/pdf/2509.25687", "abs": "https://arxiv.org/abs/2509.25687", "authors": ["Xinda Xue", "Junjun Hu", "Minghua Luo", "Xie Shichao", "Jintao Chen", "Zixun Xie", "Quan Kuichen", "Guo Wei", "Mu Xu", "Zedong Chu"], "title": "OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Embodied navigation presents a core challenge for intelligent robots,\nrequiring the comprehension of visual environments, natural language\ninstructions, and autonomous exploration. Existing models often fall short in\noffering a unified solution across diverse navigation paradigms, resulting in\nlow success rates and limited generalization. We introduce OmniNav, a unified\nframework addressing instruct-goal, object-goal, point-goal navigation, and\nfrontier-based exploration within a single architecture. Our approach features\na lightweight, low-latency policy that accurately predicts continuous-space\nwaypoints (coordinates and orientations). This policy surpasses action-chunk\nmethods in precision and supports real-world deployment at control frequencies\nup to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast\nmodule generates waypoints using short-horizon visual context and subtasks,\nwhile a slow module performs deliberative planning with long-horizon\nobservations and candidate frontiers to select subsequent subgoals and\nsubtasks. This collaboration enhances path efficiency and maintains trajectory\ncoherence, particularly in exploration and memory-intensive scenarios.\nCrucially, we identify that the primary bottleneck isn't merely navigation\npolicy learning, but a robust understanding of general instructions and\nobjects. To boost generalization, OmniNav integrates large-scale,\ngeneral-purpose training datasets, including those for image captioning and\nvisual recognition, into a joint multi-task regimen. This significantly\nimproves success rates and robustness. Extensive experiments confirm OmniNav's\nstate-of-the-art performance across various navigation benchmarks, with\nreal-world deployment further validating its efficacy. OmniNav provides\npractical insights for embodied navigation, charting a scalable path towards\nversatile, highly generalizable robotic intelligence.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5177\u8eab\u5bfc\u822a\uff0c\u5176\u4e2d\u6d89\u53ca\u9884\u6d4b\u8fde\u7eed\u7a7a\u95f4\u7684\u822a\u8def\u70b9\uff08\u5750\u6807\u548c\u65b9\u5411\uff09\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u5230\u4f7f\u7528\u5927\u578b\u901a\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u6765\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e14\u9884\u6d4b\u7684\u662f\u8fde\u7eed\u8f68\u8ff9\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "navigation", "large-scale training datasets", "waypoints"]}}
{"id": "2509.25528", "pdf": "https://arxiv.org/pdf/2509.25528", "abs": "https://arxiv.org/abs/2509.25528", "authors": ["Pranav Saxena", "Avigyan Bhattacharya", "Ji Zhang", "Wenshan Wang"], "title": "LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Referential grounding in outdoor driving scenes is challenging due to large\nscene variability, many visually similar objects, and dynamic elements that\ncomplicate resolving natural-language references (e.g., \"the black car on the\nright\"). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf\nvision-language models for fine-grained attribute extraction with large\nlanguage models for symbolic reasoning. LLM-RG processes an image and a\nfree-form referring expression by using an LLM to extract relevant object types\nand attributes, detecting candidate regions, generating rich visual descriptors\nwith a VLM, and then combining these descriptors with spatial metadata into\nnatural-language prompts that are input to an LLM for chain-of-thought\nreasoning to identify the referent's bounding box. Evaluated on the Talk2Car\nbenchmark, LLM-RG yields substantial gains over both LLM and VLM-based\nbaselines. Additionally, our ablations show that adding 3D spatial cues further\nimproves grounding. Our results demonstrate the complementary strengths of VLMs\nand LLMs, applied in a zero-shot manner, for robust outdoor referential\ngrounding.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on referential grounding in outdoor driving scenes, leveraging large language models (LLMs) for reasoning and vision-language models (VLMs) for attribute extraction. While it doesn't directly address trajectory prediction, the context of outdoor driving scenes and the use of LLMs suggest a potential connection. The grounding task could be a component in a larger trajectory prediction system.", "keywords": ["Large Language Models", "LLMs", "Vision-Language Models", "VLMs", "outdoor driving scenes", "referential grounding"]}}
{"id": "2509.25346", "pdf": "https://arxiv.org/pdf/2509.25346", "abs": "https://arxiv.org/abs/2509.25346", "authors": ["Lawrence Phillips", "Marc Boubnovski Martell", "Aditya Misra", "Josefa Lia Stoisser", "Cesar A. Prada-Medina", "Rory Donovan-Maiye", "Kaspar M\u00e4rtens"], "title": "SynthPert: Enhancing LLM Biological Reasoning via Synthetic Reasoning Traces for Cellular Perturbation Prediction", "categories": ["cs.AI", "cs.LG", "q-bio.CB", "q-bio.GN"], "comment": null, "summary": "Predicting cellular responses to genetic perturbations represents a\nfundamental challenge in systems biology, critical for advancing therapeutic\ndiscovery and virtual cell modeling. While large language models (LLMs) show\npromise for biological reasoning, their application to perturbation prediction\nremains underexplored due to challenges in adapting them to structured\nexperimental data. We present SynthPert, a novel method that enhances LLM\nperformance through supervised fine-tuning on synthetic reasoning traces\ngenerated by frontier models. Using the PerturbQA benchmark, we demonstrate\nthat our approach not only achieves state-of-the-art performance but surpasses\nthe capabilities of the frontier model that generated the training data. Our\nresults reveal three key insights: (1) Synthetic reasoning traces effectively\ndistill biological knowledge even when partially inaccurate, (2) This approach\nenables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells,\nand (3) Performance gains persist despite using only 2% of quality-filtered\ntraining data. This work shows the effectiveness of synthetic reasoning\ndistillation for enhancing domain-specific reasoning in LLMs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using LLMs for predicting cellular responses to genetic perturbations in systems biology, which is not directly related to trajectory prediction. However, it does heavily involve LLMs and biological reasoning/prediction, so there is some relevance. The 'prediction' aspect is related to cellular response, not trajectory.", "keywords": ["Large Language Models (LLMs)", "prediction", "reasoning"]}}
{"id": "2509.25822", "pdf": "https://arxiv.org/pdf/2509.25822", "abs": "https://arxiv.org/abs/2509.25822", "authors": ["Jing Wang", "Weiting Peng", "Jing Tang", "Zeyu Gong", "Xihua Wang", "Bo Tao", "Li Cheng"], "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies", "categories": ["cs.RO"], "comment": "42 pages, 17 figures, 39th Conference on Neural Information\n  Processing Systems (NeurIPS 2025)", "summary": "Existing imitation learning methods decouple perception and action, which\noverlooks the causal reciprocity between sensory representations and action\nexecution that humans naturally leverage for adaptive behaviors. To bridge this\ngap, we introduce Action--Guided Diffusion Policy (DP--AG), a unified\nrepresentation learning that explicitly models a dynamic interplay between\nperception and action through probabilistic latent dynamics. DP--AG encodes\nlatent observations into a Gaussian posterior via variational inference and\nevolves them using an action-guided SDE, where the Vector-Jacobian Product\n(VJP) of the diffusion policy's noise predictions serves as a structured\nstochastic force driving latent updates. To promote bidirectional learning\nbetween perception and action, we introduce a cycle--consistent contrastive\nloss that organizes the gradient flow of the noise predictor into a coherent\nperception--action loop, enforcing mutually consistent transitions in both\nlatent updates and action refinements. Theoretically, we derive a variational\nlower bound for the action-guided SDE, and prove that the contrastive objective\nenhances continuity in both latent and action trajectories. Empirically, DP--AG\nsignificantly outperforms state--of--the--art methods across simulation\nbenchmarks and real-world UR5 manipulation tasks. As a result, our DP--AG\noffers a promising step toward bridging biological adaptability and artificial\npolicy learning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on action-guided policy learning using diffusion models. While it doesn't explicitly mention trajectory prediction or large language models, the core idea of learning adaptive policies and modeling the interplay between perception and action is relevant to trajectory prediction, especially in scenarios where the agent needs to adapt to dynamic environments. The use of diffusion models connects to the broader theme of generative models, which are sometimes used in conjunction with LLMs or as alternatives. However, the absence of direct mentions of trajectory prediction or LLMs lowers the relevance score.", "keywords": ["diffusion policy", "perception-action", "adaptive policies", "policy learning"]}}
{"id": "2509.25224", "pdf": "https://arxiv.org/pdf/2509.25224", "abs": "https://arxiv.org/abs/2509.25224", "authors": ["Qichen Liao", "Chengqiu Hu", "Fangzheng Miao", "Bao Li", "Yiyang Liu", "Junlong Lyu", "Lirui Jiang", "Jun Wang", "Lingchao Zheng", "Jun Li", "Yuwei Fan"], "title": "AMLA: MUL by ADD in FlashAttention Rescaling", "categories": ["cs.LG"], "comment": "21 pages, 11 figures", "summary": "Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage\nin Large Language Models while introducing substantial computational overhead\nand intermediate variable expansion. This poses challenges for efficient\nhardware implementation -- especially during the decode phase. This paper\nintroduces Ascend MLA (AMLA), a high-performance kernel specifically optimized\nfor Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel\nFlashAttention-based algorithm that replaces floating-point multiplications\nwith integer additions for output block rescaling, leveraging binary\ncorrespondence between FP32 and INT32 representations; (2) A Preload Pipeline\nstrategy with hierarchical tiling that maximizes FLOPS utilization: the Preload\nPipeline achieves Cube-bound performance, while hierarchical tiling overlaps\ndata movement and computation within the Cube core. Experiments show that on\nAscend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS,\nreaching 86.8% of the theoretical maximum FLOPS, outperforming the\nstate-of-the-art open-source FlashMLA implementation, whose FLOPS utilization\nis up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into\nHuawei's CANN and will be released soon.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on optimizing the performance of Multi-head Latent Attention (MLA) in Large Language Models, specifically for Huawei's Ascend NPUs. While it does not directly address trajectory prediction, it is relevant to the broader field of large language models.", "keywords": ["Large Language Models", "MLA", "FlashAttention", "Ascend NPUs"]}}
{"id": "2509.25966", "pdf": "https://arxiv.org/pdf/2509.25966", "abs": "https://arxiv.org/abs/2509.25966", "authors": ["Peilong Han", "Fan Jia", "Min Zhang", "Yutao Qiu", "Hongyao Tang", "Yan Zheng", "Tiancai Wang", "Jianye Hao"], "title": "MUVLA: Learning to Explore Object Navigation via Map Understanding", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we present MUVLA, a Map Understanding Vision-Language-Action\nmodel tailored for object navigation. It leverages semantic map abstractions to\nunify and structure historical information, encoding spatial context in a\ncompact and consistent form. MUVLA takes the current and history observations,\nas well as the semantic map, as inputs and predicts the action sequence based\non the description of goal object. Furthermore, it amplifies supervision\nthrough reward-guided return modeling based on dense short-horizon progress\nsignals, enabling the model to develop a detailed understanding of action value\nfor reward maximization. MUVLA employs a three-stage training pipeline:\nlearning map-level spatial understanding, imitating behaviors from\nmixed-quality demonstrations, and reward amplification. This strategy allows\nMUVLA to unify diverse demonstrations into a robust spatial representation and\ngenerate more rational exploration strategies. Experiments on HM3D and Gibson\nbenchmarks demonstrate that MUVLA achieves great generalization and learns\neffective exploration behaviors even from low-quality or partially successful\ntrajectories.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper describes a model for object navigation that uses a vision-language-action model (MUVLA). While it doesn't explicitly mention trajectory prediction, the action sequence prediction can be considered a form of trajectory prediction in a navigation context. It also uses a Vision-Language model which is related to Large Language Models. However, the focus is more on navigation and spatial understanding than on general trajectory prediction or the architecture of the language model itself.", "keywords": ["object navigation", "action sequence", "vision-language model", "trajectory"]}}
{"id": "2509.26375", "pdf": "https://arxiv.org/pdf/2509.26375", "abs": "https://arxiv.org/abs/2509.26375", "authors": ["Zichao Shen", "Chen Gao", "Jiaqi Yuan", "Tianchen Zhu", "Xingcheng Fu", "Qingyun Sun"], "title": "SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Embodied task planning requires agents to produce executable actions in a\nclose-loop manner within the environment. With progressively improving\ncapabilities of LLMs in task decomposition, planning, and generalization,\ncurrent embodied task planning methods adopt LLM-based architecture.However,\nexisting LLM-based planners remain limited in three aspects, i.e., fixed\nplanning paradigms, lack of action sequence constraints, and error-agnostic. In\nthis work, we propose SDA-PLANNER, enabling an adaptive planning paradigm,\nstate-dependency aware and error-aware mechanisms for comprehensive embodied\ntask planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to\nexplicitly model action preconditions and effects, guiding the dynamic\nrevision. To handle execution error, it employs an error-adaptive replanning\nstrategy consisting of Error Backtrack and Diagnosis and Adaptive Action\nSubTree Generation, which locally reconstructs the affected portion of the plan\nbased on the current environment state. Experiments demonstrate that\nSDA-PLANNER consistently outperforms baselines in success rate and goal\ncompletion, particularly under diverse error conditions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5177\u8eab\u4efb\u52a1\u89c4\u5212\uff08Embodied Task Planning\uff09\uff0c\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u4efb\u52a1\u5206\u89e3\u548c\u89c4\u5212\uff0c\u4f46\u5e76\u672a\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u89c4\u5212\u7ed3\u679c\u53ef\u80fd\u6d89\u53ca\u4e00\u7cfb\u5217\u52a8\u4f5c\uff0c\u4f46\u8bba\u6587\u7684\u91cd\u70b9\u4e0d\u5728\u4e8e\u9884\u6d4b\u79fb\u52a8\u7269\u4f53\u7684\u8f68\u8ff9\uff0c\u800c\u662f\u5982\u4f55\u5229\u7528LLM\u8fdb\u884c\u66f4\u6709\u6548\u7684\u4efb\u52a1\u89c4\u5212\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u5c5e\u4e8e\u4e2d\u7b49\u3002", "keywords": ["LLMs", "Large Language Models", "task planning", "embodied task planning"]}}
{"id": "2509.26428", "pdf": "https://arxiv.org/pdf/2509.26428", "abs": "https://arxiv.org/abs/2509.26428", "authors": ["Mattia Piazza", "Mattia Piccinini", "Sebastiano Taddei", "Francesco Biral", "Enrico Bertolazzi"], "title": "Real-time Velocity Profile Optimization for Time-Optimal Maneuvering with Generic Acceleration Constraints", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "comment": null, "summary": "The computation of time-optimal velocity profiles along prescribed paths,\nsubject to generic acceleration constraints, is a crucial problem in robot\ntrajectory planning, with particular relevance to autonomous racing. However,\nthe existing methods either support arbitrary acceleration constraints at high\ncomputational cost or use conservative box constraints for computational\nefficiency. We propose FBGA, a new \\underline{F}orward-\\underline{B}ackward\nalgorithm with \\underline{G}eneric \\underline{A}cceleration constraints, which\nachieves both high accuracy and low computation time. FBGA operates forward and\nbackward passes to maximize the velocity profile in short, discretized path\nsegments, while satisfying user-defined performance limits. Tested on five\nracetracks and two vehicle classes, FBGA handles complex, non-convex\nacceleration constraints with custom formulations. Its maneuvers and lap times\nclosely match optimal control baselines (within $0.11\\%$-$0.36\\%$), while being\nup to three orders of magnitude faster. FBGA maintains high accuracy even with\ncoarse discretization, making it well-suited for online multi-query trajectory\nplanning. Our open-source \\texttt{C++} implementation is available at:\nhttps://anonymous.4open.science/r/FB_public_RAL.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u901f\u5ea6\u5256\u9762\u4f18\u5316\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u901a\u7528\u52a0\u901f\u5ea6\u7ea6\u675f\u7684\u65f6\u95f4\u6700\u4f18\u673a\u52a8\u3002\u867d\u7136\u5b83\u5c5e\u4e8e\u8f68\u8ff9\u89c4\u5212\u7684\u8303\u7574\uff0c\u4f46\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u76f8\u5173\u6027\u4f53\u73b0\u5728\u5b83\u662f\u8f68\u8ff9\u9884\u6d4b/\u89c4\u5212\u9886\u57df\u7684\u4e00\u4e2a\u7814\u7a76\u3002", "keywords": ["trajectory planning", "velocity profile", "acceleration constraints", "time-optimal maneuvering"]}}
{"id": "2509.26459", "pdf": "https://arxiv.org/pdf/2509.26459", "abs": "https://arxiv.org/abs/2509.26459", "authors": ["Akshay Jaitly", "Devesh K. Jha", "Kei Ota", "Yuki Shirai"], "title": "Analytic Conditions for Differentiable Collision Detection in Trajectory Optimization", "categories": ["cs.RO", "cs.CG"], "comment": "8 pages, 8 figures. Accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "summary": "Optimization-based methods are widely used for computing fast, diverse\nsolutions for complex tasks such as collision-free movement or planning in the\npresence of contacts. However, most of these methods require enforcing\nnon-penetration constraints between objects, resulting in a non-trivial and\ncomputationally expensive problem. This makes the use of optimization-based\nmethods for planning and control challenging. In this paper, we present a\nmethod to efficiently enforce non-penetration of sets while performing\noptimization over their configuration, which is directly applicable to problems\nlike collision-aware trajectory optimization. We introduce novel differentiable\nconditions with analytic expressions to achieve this. To enforce non-collision\nbetween non-smooth bodies using these conditions, we introduce a method to\napproximate polytopes as smooth semi-algebraic sets. We present several\nnumerical experiments to demonstrate the performance of the proposed method and\ncompare the performance with other baseline methods recently proposed in the\nliterature.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u4f18\u5316\u4e2d\u7684\u78b0\u649e\u68c0\u6d4b\u95ee\u9898\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u9886\u57df\uff0c\u4f46\u672a\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u5927\u6a21\u578b\u7684\u5173\u8054\u3002", "keywords": ["trajectory optimization", "collision detection", "planning"]}}
{"id": "2509.26513", "pdf": "https://arxiv.org/pdf/2509.26513", "abs": "https://arxiv.org/abs/2509.26513", "authors": ["Saad Abdul Ghani", "Kameron Lee", "Xuesu Xiao"], "title": "Learning from Hallucinating Critical Points for Navigation in Dynamic Environments", "categories": ["cs.RO"], "comment": null, "summary": "Generating large and diverse obstacle datasets to learn motion planning in\nenvironments with dynamic obstacles is challenging due to the vast space of\npossible obstacle trajectories. Inspired by hallucination-based data synthesis\napproaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a\nself-supervised framework for creating rich dynamic obstacle datasets based on\nexisting optimal motion plans without requiring expensive expert demonstrations\nor trial-and-error exploration. LfH-CP factorizes hallucination into two\nstages: first identifying when and where obstacles must appear in order to\nresult in an optimal motion plan, i.e., the critical points, and then\nprocedurally generating diverse trajectories that pass through these points\nwhile avoiding collisions. This factorization avoids generative failures such\nas mode collapse and ensures coverage of diverse dynamic behaviors. We further\nintroduce a diversity metric to quantify dataset richness and show that LfH-CP\nproduces substantially more varied training data than existing baselines.\nExperiments in simulation demonstrate that planners trained on LfH-CP datasets\nachieves higher success rates compared to a prior hallucination method.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on generating obstacle datasets for motion planning in dynamic environments. While it doesn't directly use or discuss large language models, it is highly relevant to trajectory prediction and path planning, specifically in scenarios with dynamic obstacles. The core idea revolves around learning from hallucinated critical points to improve navigation, which is a form of trajectory prediction/planning.", "keywords": ["motion planning", "dynamic environments", "obstacle trajectories", "trajectory prediction", "navigation"]}}
{"id": "2509.25591", "pdf": "https://arxiv.org/pdf/2509.25591", "abs": "https://arxiv.org/abs/2509.25591", "authors": ["Zekai Chen", "Arda Pekis", "Kevin Brown"], "title": "Building the EHR Foundation Model via Next Event Prediction", "categories": ["cs.AI", "cs.CL", "q-bio.OT"], "comment": null, "summary": "Electronic Health Records (EHRs) contain rich temporal dynamics that\nconventional encoding approaches fail to adequately capture. While Large\nLanguage Models (LLMs) show promise for EHR modeling, they struggle to reason\nabout sequential clinical events and temporal dependencies. We propose Next\nEvent Prediction (NEP), a framework that enhances LLMs' temporal reasoning\nthrough autoregressive fine-tuning on clinical event sequences. By\nreformulating EHRs as timestamped event chains and predicting future medical\nevents, NEP explicitly models disease progression patterns and causal\nrelationships. Extensive evaluations across oncology survival prediction and\nclinical diagnosis tasks demonstrate NEP's superiority, outperforming\nspecialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index\nin temporal reasoning tasks. Our analyses reveal dual benefits:\nstate-of-the-art prediction accuracy combined with clinically interpretable\nattention patterns that align with known disease pathways.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses using Large Language Models (LLMs) for Electronic Health Records (EHRs) modeling and introduces Next Event Prediction (NEP) to improve temporal reasoning. While it doesn't directly deal with trajectory prediction in the traditional sense (e.g., pedestrian or vehicle trajectories), the concept of predicting the next event in a sequence shares similarities with trajectory prediction. The use of LLMs is a strong indicator of relevance.", "keywords": ["Large Language Models", "LLMs", "Next Event Prediction", "temporal reasoning", "sequence prediction"]}}
{"id": "2509.26642", "pdf": "https://arxiv.org/pdf/2509.26642", "abs": "https://arxiv.org/abs/2509.26642", "authors": ["Zhuoyang Liu", "Jiaming Liu", "Jiadong Xu", "Nuowei Han", "Chenyang Gu", "Hao Chen", "Kaichen Zhou", "Renrui Zhang", "Kai Chin Hsieh", "Kun Wu", "Zhengping Che", "Jian Tang", "Shanghang Zhang"], "title": "MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Vision-language-action models (VLAs) have shown generalization capabilities\nin robotic manipulation tasks by inheriting from vision-language models (VLMs)\nand learning action generation. Most VLA models focus on interpreting vision\nand language to generate actions, whereas robots must perceive and interact\nwithin the spatial-physical world. This gap highlights the need for a\ncomprehensive understanding of robotic-specific multisensory information, which\nis crucial for achieving complex and contact-rich control. To this end, we\nintroduce a multisensory language-action (MLA) model that collaboratively\nperceives heterogeneous sensory modalities and predicts future multisensory\nobjectives to facilitate physical world modeling. Specifically, to enhance\nperceptual representations, we propose an encoder-free multimodal alignment\nscheme that innovatively repurposes the large language model itself as a\nperception module, directly interpreting multimodal cues by aligning 2D images,\n3D point clouds, and tactile tokens through positional correspondence. To\nfurther enhance MLA's understanding of physical dynamics, we design a future\nmultisensory generation post-training strategy that enables MLA to reason about\nsemantic, geometric, and interaction information, providing more robust\nconditions for action generation. For evaluation, the MLA model outperforms the\nprevious state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex,\ncontact-rich real-world tasks, respectively, while also demonstrating improved\ngeneralization to unseen configurations. Project website:\nhttps://sites.google.com/view/open-mla", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on robotic manipulation and uses a large language model as a perception module. While not directly addressing trajectory prediction, the forecasting of future multisensory objectives and action generation are related concepts. The use of a large language model connects it to the 'large models' theme, but the primary focus is not trajectory prediction in the traditional sense.", "keywords": ["large language model", "multisensory", "action generation", "forecasting"]}}
