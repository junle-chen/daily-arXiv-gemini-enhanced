{"id": "2512.02844", "pdf": "https://arxiv.org/pdf/2512.02844", "abs": "https://arxiv.org/abs/2512.02844", "authors": ["Xinzheng Wu", "Junyi Chen", "Naiting Zhong", "Yong Shen"], "title": "VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion", "categories": ["cs.RO", "cs.LG"], "comment": "25 pages, 9 figures", "summary": "The safe deployment of autonomous driving systems (ADSs) relies on comprehensive testing and evaluation. However, safety-critical scenarios that can effectively expose system vulnerabilities are extremely sparse in the real world. Existing scenario generation methods face challenges in efficiently constructing long-tail scenarios that ensure fidelity, criticality, and interactivity, while particularly lacking real-time dynamic response capabilities to the vehicle under test (VUT). To address these challenges, this paper proposes a safety-critical testing scenario generation framework that integrates the high-level semantic understanding capabilities of Vision Language Models (VLMs) with the fine-grained generation capabilities of adaptive guided diffusion models. The framework establishes a three-layer hierarchical architecture comprising a strategic layer for VLM-directed scenario generation objective determination, a tactical layer for guidance function formulation, and an operational layer for guided diffusion execution. We first establish a high-quality fundamental diffusion model that learns the data distribution of real driving scenarios. Next, we design an adaptive guided diffusion method that enables real-time, precise control of background vehicles (BVs) in closed-loop simulation. The VLM is then incorporated to autonomously generate scenario generation objectives and guidance functions through deep scenario understanding and risk reasoning, ultimately guiding the diffusion model to achieve VLM-directed scenario generation. Experimental results demonstrate that the proposed method can efficiently generate realistic, diverse, and highly interactive safety-critical testing scenarios. Furthermore, case studies validate the adaptability and VLM-directed generation performance of the proposed method.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper discusses the generation of safety-critical testing scenarios for autonomous driving systems, which inherently involves trajectory prediction. It leverages Vision Language Models (VLMs) for high-level semantic understanding and adaptive guided diffusion models for fine-grained generation. The use of VLMs connects it to the large language model domain, and the scenario generation is directly related to trajectory prediction for background vehicles.", "keywords": ["trajectory prediction", "autonomous driving systems", "Vision Language Models", "VLMs", "diffusion models", "scenario generation"]}}
{"id": "2512.02777", "pdf": "https://arxiv.org/pdf/2512.02777", "abs": "https://arxiv.org/abs/2512.02777", "authors": ["Heye Huang", "Yibin Yang", "Mingfeng Fan", "Haoran Wang", "Xiaocong Zhao", "Jianqiang Wang"], "title": "CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy", "categories": ["cs.RO", "cs.MA"], "comment": "25 pages, 6 figures", "summary": "Safe autonomous driving in mixed traffic requires a unified understanding of multimodal interactions and dynamic planning under uncertainty. Existing learning based approaches struggle to capture rare but safety critical behaviors, while rule based systems often lack adaptability in complex interactions. To address these limitations, CogDrive introduces a cognition driven multimodal prediction and planning framework that integrates explicit modal reasoning with safety aware trajectory optimization. The prediction module adopts cognitive representations of interaction modes based on topological motion semantics and nearest neighbor relational encoding. With a differentiable modal loss and multimodal Gaussian decoding, CogDrive learns sparse and unbalanced interaction behaviors and improves long horizon trajectory prediction. The planning module incorporates an emergency response concept and optimizes safety stabilized trajectories, where short term consistent branches ensure safety during replanning cycles and long term branches support smooth and collision free motion under low probability switching modes. Experiments on Argoverse2 and INTERACTION datasets show that CogDrive achieves strong performance in trajectory accuracy and miss rate, while closed loop simulations confirm adaptive behavior in merge and intersection scenarios. By combining cognitive multimodal prediction with safety oriented planning, CogDrive offers an interpretable and reliable paradigm for safe autonomy in complex traffic.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u591a\u6a21\u6001\u9884\u6d4b\u548c\u89c4\u5212\uff0c\u4ee5\u53ca\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u201ccognition-driven\u201d \u548c\u201ccognitive representations\u201d\u6697\u793a\u4e86\u53ef\u80fd\u4f7f\u7528\u4e86\u67d0\u79cd\u7c7b\u578b\u7684\u8ba4\u77e5\u6a21\u578b\uff0c\u4f46\u5177\u4f53\u5b9e\u73b0\u65b9\u5f0f\u672a\u77e5\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u5f97\u5206\u4e2d\u7b49\u504f\u4e0a\u3002", "keywords": ["trajectory prediction", "multimodal prediction", "planning", "autonomous driving", "interaction", "motion semantics", "trajectory optimization"]}}
{"id": "2512.02851", "pdf": "https://arxiv.org/pdf/2512.02851", "abs": "https://arxiv.org/abs/2512.02851", "authors": ["Iana Zhura", "Sausar Karaf", "Faryal Batool", "Nipun Dhananjaya Weerakkodi Mudalige", "Valerii Serpiva", "Ali Alridha Abdulkarim", "Aleksey Fedoseev", "Didar Seyidov", "Amjad Hajira", "Dzmitry Tsetserukou"], "title": "SwarmDiffusion: End-To-End Traversability-Guided Diffusion for Embodiment-Agnostic Navigation of Heterogeneous Robots", "categories": ["cs.RO"], "comment": "This work has been submitted for publication and is currently under review", "summary": "Visual traversability estimation is critical for autonomous navigation, but existing VLM-based methods rely on hand-crafted prompts, generalize poorly across embodiments, and output only traversability maps, leaving trajectory generation to slow external planners. We propose SwarmDiffusion, a lightweight end-to-end diffusion model that jointly predicts traversability and generates a feasible trajectory from a single RGB image. To remove the need for annotated or planner-produced paths, we introduce a planner-free trajectory construction pipeline based on randomized waypoint sampling, Bezier smoothing, and regularization enforcing connectivity, safety, directionality, and path thinness. This enables learning stable motion priors without demonstrations. SwarmDiffusion leverages VLM-derived supervision without prompt engineering and conditions the diffusion process on a compact embodiment state, producing physically consistent, traversable paths that transfer across different robot platforms. Across indoor environments and two embodiments (quadruped and aerial), the method achieves 80-100\\% navigation success and 0.09 s inference, and adapts to a new robot using only-500 additional visual samples. It generalizes reliably to unseen environments in simulation and real-world trials, offering a scalable, prompt-free approach to unified traversability reasoning and trajectory generation.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u4f7f\u7528\u4e86diffusion model\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u662f\u4f7f\u7528\u4e86VLM-derived supervision\uff0c\u5e76\u4e14\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory generation", "traversability", "diffusion model", "navigation", "VLM"]}}
{"id": "2512.02368", "pdf": "https://arxiv.org/pdf/2512.02368", "abs": "https://arxiv.org/abs/2512.02368", "authors": ["Wenyi Xiong", "Jian Chen"], "title": "Multi-Domain Enhanced Map-Free Trajectory Prediction with Selective Attention", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Trajectory prediction is crucial for the reliability and safety of autonomous driving systems, yet it remains a challenging task in complex interactive scenarios. Existing methods often struggle to efficiently extract valuable scene information from redundant data, thereby reducing computational efficiency and prediction accuracy, especially when dealing with intricate agent interactions. To address these challenges, we propose a novel map-free trajectory prediction algorithm that achieves trajectory prediction across the temporal, spatial, and frequency domains. Specifically, in temporal information processing, We utilize a Mixture of Experts (MoE) mechanism to adaptively select critical frequency components. Concurrently, we extract these components and integrate multi-scale temporal features. Subsequently, a selective attention module is proposed to filter out redundant information in both temporal sequences and spatial interactions. Finally, we design a multimodal decoder. Under the supervision of patch-level and point-level losses, we obtain reasonable trajectory results. Experiments on Nuscences datasets demonstrate the superiority of our algorithm, validating its effectiveness in handling complex interactive scenarios.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u65e0\u5730\u56fe\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u4e86\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u6a21\u6001\u89e3\u7801\u5668\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7d27\u5bc6\u76f8\u5173\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86MoE\uff0c\u8fd9\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u548c\u5927\u578b\u6a21\u578b\u7684\u67b6\u6784\u6709\u76f8\u4f3c\u4e4b\u5904\u3002", "keywords": ["trajectory prediction", "autonomous driving", "attention mechanism", "multi-modal decoder", "MoE", "motion prediction"]}}
{"id": "2512.02417", "pdf": "https://arxiv.org/pdf/2512.02417", "abs": "https://arxiv.org/abs/2512.02417", "authors": ["Huiqian Li", "Wei Pan", "Haodong Zhang", "Jin Huang", "Zhihua Zhong"], "title": "Vehicle Dynamics Embedded World Models for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "World models have gained significant attention as a promising approach for autonomous driving. By emulating human-like perception and decision-making processes, these models can predict and adapt to dynamic environments. Existing methods typically map high-dimensional observations into compact latent spaces and learn optimal policies within these latent representations. However, prior work usually jointly learns ego-vehicle dynamics and environmental transition dynamics from the image input, leading to inefficiencies and a lack of robustness to variations in vehicle dynamics. To address these issues, we propose the Vehicle Dynamics embedded Dreamer (VDD) method, which decouples the modeling of ego-vehicle dynamics from environmental transition dynamics. This separation allows the world model to generalize effectively across vehicles with diverse parameters. Additionally, we introduce two strategies to further enhance the robustness of the learned policy: Policy Adjustment during Deployment (PAD) and Policy Augmentation during Training (PAT). Comprehensive experiments in simulated environments demonstrate that the proposed model significantly improves both driving performance and robustness to variations in vehicle dynamics, outperforming existing approaches.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on world models for autonomous driving, which involves predicting future states of the environment and the ego-vehicle, thus relating to trajectory prediction. While it does not directly use or mention large language models, the concept of \"world models\" is increasingly being explored in conjunction with large models, and the task of predicting future states shares similarities with the generative capabilities of LLMs. The paper's emphasis on vehicle dynamics and policy learning further strengthens its connection to trajectory prediction in the context of autonomous vehicles.", "keywords": ["trajectory prediction", "autonomous driving", "world models", "vehicle dynamics", "policy learning"]}}
{"id": "2512.02535", "pdf": "https://arxiv.org/pdf/2512.02535", "abs": "https://arxiv.org/abs/2512.02535", "authors": ["Jeric Lew", "Yuhong Cao", "Derek Ming Siang Tan", "Guillaume Sartoretti"], "title": "AID: Agent Intent from Diffusion for Multi-Agent Informative Path Planning", "categories": ["cs.RO"], "comment": null, "summary": "Information gathering in large-scale or time-critical scenarios (e.g., environmental monitoring, search and rescue) requires broad coverage within limited time budgets, motivating the use of multi-agent systems. These scenarios are commonly formulated as multi-agent informative path planning (MAIPP), where multiple agents must coordinate to maximize information gain while operating under budget constraints. A central challenge in MAIPP is ensuring effective coordination while the belief over the environment evolves with incoming measurements. Recent learning-based approaches address this by using distributions over future positions as \"intent\" to support coordination. However, these autoregressive intent predictors are computationally expensive and prone to compounding errors. Inspired by the effectiveness of diffusion models as expressive, long-horizon policies, we propose AID, a fully decentralized MAIPP framework that leverages diffusion models to generate long-term trajectories in a non-autoregressive manner. AID first performs behavior cloning on trajectories produced by existing MAIPP planners and then fine-tunes the policy using reinforcement learning via Diffusion Policy Policy Optimization (DPPO). This two-stage pipeline enables the policy to inherit expert behavior while learning improved coordination through online reward feedback. Experiments demonstrate that AID consistently improves upon the MAIPP planners it is trained from, achieving up to 4x faster execution and 17% increased information gain, while scaling effectively to larger numbers of agents. Our implementation is publicly available at https://github.com/marmotlab/AID.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u4fe1\u606f\u8def\u5f84\u89c4\u5212\uff08MAIPP\uff09\uff0c\u5176\u4e2d\u6d89\u53ca\u5230\u667a\u80fd\u4f53\u8f68\u8ff9\u7684\u9884\u6d4b\u548c\u89c4\u5212\u3002\u8bba\u6587\u4f7f\u7528\u4e86\u6269\u6563\u6a21\u578b\u751f\u6210\u957f\u671f\u8f68\u8ff9\uff0c\u867d\u7136\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5173\u8054\u6027\u76f8\u5bf9\u8f83\u5f31\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4e0e\u5927\u6a21\u578b\u7684\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u6574\u4f53\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory prediction", "multi-agent informative path planning", "diffusion models", "long-term trajectories", "behavior cloning", "reinforcement learning"]}}
{"id": "2512.02609", "pdf": "https://arxiv.org/pdf/2512.02609", "abs": "https://arxiv.org/abs/2512.02609", "authors": ["Shengkai Wu", "Jinrong Yang", "Wenqiu Luo", "Linfeng Gao", "Chaohui Shang", "Meiyu Zhi", "Mingshan Sun", "Fangping Yang", "Liangliang Ren", "Yong Zhao"], "title": "SAM2Grasp: Resolve Multi-modal Grasping via Prompt-conditioned Temporal Action Prediction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Imitation learning for robotic grasping is often plagued by the multimodal problem: when a scene contains multiple valid targets, demonstrations of grasping different objects create conflicting training signals. Standard imitation learning policies fail by averaging these distinct actions into a single, invalid action. In this paper, we introduce SAM2Grasp, a novel framework that resolves this issue by reformulating the task as a uni-modal, prompt-conditioned prediction problem. Our method leverages the frozen SAM2 model to use its powerful visual temporal tracking capability and introduces a lightweight, trainable action head that operates in parallel with its native segmentation head. This design allows for training only the small action head on pre-computed temporal-visual features from SAM2. During inference, an initial prompt, such as a bounding box provided by an upstream object detection model, designates the specific object to be grasped. This prompt conditions the action head to predict a unique, unambiguous grasp trajectory for that object alone. In all subsequent video frames, SAM2's built-in temporal tracking capability automatically maintains stable tracking of the selected object, enabling our model to continuously predict the grasp trajectory from the video stream without further external guidance. This temporal-prompted approach effectively eliminates ambiguity from the visuomotor policy. We demonstrate through extensive experiments that SAM2Grasp achieves state-of-the-art performance in cluttered, multi-object grasping tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper is related to trajectory prediction because it focuses on predicting the grasp trajectory of a robot. While it doesn't directly use a Large Language Model, it leverages SAM2, a foundation model with visual temporal tracking capabilities. The action prediction component can be seen as a form of trajectory prediction conditioned on visual prompts.", "keywords": ["trajectory prediction", "action prediction", "foundation models", "SAM2", "temporal action prediction"]}}
{"id": "2512.02834", "pdf": "https://arxiv.org/pdf/2512.02834", "abs": "https://arxiv.org/abs/2512.02834", "authors": ["Siyuan Yang", "Yang Zhang", "Haoran He", "Ling Pan", "Xiu Li", "Chenjia Bai", "Xuelong Li"], "title": "Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach", "categories": ["cs.RO", "cs.AI"], "comment": "The first two authors contributed equally. Yang Zhang leads the whole project", "summary": "Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \\textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action (VLA) models, which are a type of large model, and action prediction/execution. While it doesn't directly deal with trajectory prediction in the traditional sense (e.g., predicting the path of a pedestrian or vehicle), the core idea of predicting and executing actions based on vision and language inputs is related. The use of 'action modes' and improving 'inference stability and success rates in downstream-task adaptations' suggests a connection to the broader concept of predicting and controlling movement.", "keywords": ["Vision-Language-Action models", "VLA", "large models", "action prediction", "action modes", "inference", "policy"]}}
{"id": "2512.02273", "pdf": "https://arxiv.org/pdf/2512.02273", "abs": "https://arxiv.org/abs/2512.02273", "authors": ["Peng Kang", "Xijun Wang", "Yu Yuan"], "title": "Progressive Image Restoration via Text-Conditioned Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "First two authors contributed equally to this work. IEEE ICNC Accepted", "summary": "Recent text-to-video models have demonstrated strong temporal generation capabilities, yet their potential for image restoration remains underexplored. In this work, we repurpose CogVideo for progressive visual restoration tasks by fine-tuning it to generate restoration trajectories rather than natural video motion. Specifically, we construct synthetic datasets for super-resolution, deblurring, and low-light enhancement, where each sample depicts a gradual transition from degraded to clean frames. Two prompting strategies are compared: a uniform text prompt shared across all samples, and a scene-specific prompting scheme generated via LLaVA multi-modal LLM and refined with ChatGPT. Our fine-tuned model learns to associate temporal progression with restoration quality, producing sequences that improve perceptual metrics such as PSNR, SSIM, and LPIPS across frames. Extensive experiments show that CogVideo effectively restores spatial detail and illumination consistency while maintaining temporal coherence. Moreover, the model generalizes to real-world scenarios on the ReLoBlur dataset without additional training, demonstrating strong zero-shot robustness and interpretability through temporal restoration.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses a large language model (LLaVA, ChatGPT) to generate text prompts for a video generation model (CogVideo) which is then fine-tuned to perform progressive image restoration. While the primary focus is image restoration and video generation, the use of LLMs for prompt generation and the concept of 'restoration trajectories' connect it to the broader theme of using large models for sequential data processing, which can be loosely related to trajectory prediction. The 'restoration trajectory' itself can be seen as a form of predicted sequence, albeit in the image space.", "keywords": ["Large Language Models", "LLaVA", "ChatGPT", "video generation", "text-to-video", "progressive restoration", "CogVideo", "restoration trajectories"]}}
{"id": "2512.02358", "pdf": "https://arxiv.org/pdf/2512.02358", "abs": "https://arxiv.org/abs/2512.02358", "authors": ["Ran Zhang", "Kun Ouyang", "Tiancheng Ma", "Yida Yang", "Dong Fang"], "title": "Beyond Playtesting: A Generative Multi-Agent Simulation System for Massively Multiplayer Online Games", "categories": ["cs.AI"], "comment": null, "summary": "Optimizing numerical systems and mechanism design is crucial for enhancing player experience in Massively Multiplayer Online (MMO) games. Traditional optimization approaches rely on large-scale online experiments or parameter tuning over predefined statistical models, which are costly, time-consuming, and may disrupt player experience. Although simplified offline simulation systems are often adopted as alternatives, their limited fidelity prevents agents from accurately mimicking real player reasoning and reactions to interventions. To address these limitations, we propose a generative agent-based MMO simulation system empowered by Large Language Models (LLMs). By applying Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on large-scale real player behavioral data, we adapt LLMs from general priors to game-specific domains, enabling realistic and interpretable player decision-making. In parallel, a data-driven environment model trained on real gameplay logs reconstructs dynamic in-game systems. Experiments demonstrate strong consistency with real-world player behaviors and plausible causal responses under interventions, providing a reliable, interpretable, and cost-efficient framework for data-driven numerical design optimization.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u6a21\u62dfMMO\u6e38\u620f\u4e2d\u7684\u73a9\u5bb6\u884c\u4e3a\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u6a21\u62df\u73a9\u5bb6\u884c\u4e3a\u53ef\u4ee5\u95f4\u63a5\u7528\u4e8e\u9884\u6d4b\u73a9\u5bb6\u7684\u79fb\u52a8\u8def\u5f84\u548c\u52a8\u4f5c\uff0c\u5e76\u4e14\u660e\u786e\u4f7f\u7528\u4e86LLMs\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "agent-based simulation", "player behavior"]}}
{"id": "2512.03044", "pdf": "https://arxiv.org/pdf/2512.03044", "abs": "https://arxiv.org/abs/2512.03044", "authors": ["Yueru Jia", "Jiaming Liu", "Shengbang Liu", "Rui Zhou", "Wanhe Yu", "Yuyang Yan", "Xiaowei Chi", "Yandong Guo", "Boxin Shi", "Shanghang Zhang"], "title": "Video2Act: A Dual-System Video Diffusion Policy with Robotic Spatio-Motional Modeling", "categories": ["cs.RO"], "comment": null, "summary": "Robust perception and dynamics modeling are fundamental to real-world robotic policy learning. Recent methods employ video diffusion models (VDMs) to enhance robotic policies, improving their understanding and modeling of the physical world. However, existing approaches overlook the coherent and physically consistent motion representations inherently encoded across frames in VDMs. To this end, we propose Video2Act, a framework that efficiently guides robotic action learning by explicitly integrating spatial and motion-aware representations. Building on the inherent representations of VDMs, we extract foreground boundaries and inter-frame motion variations while filtering out background noise and task-irrelevant biases. These refined representations are then used as additional conditioning inputs to a diffusion transformer (DiT) action head, enabling it to reason about what to manipulate and how to move. To mitigate inference inefficiency, we propose an asynchronous dual-system design, where the VDM functions as the slow System 2 and the DiT head as the fast System 1, working collaboratively to generate adaptive actions. By providing motion-aware conditions to System 1, Video2Act maintains stable manipulation even with low-frequency updates from the VDM. For evaluation, Video2Act surpasses previous state-of-the-art VLA methods by 7.7% in simulation and 21.7% in real-world tasks in terms of average success rate, further exhibiting strong generalization capabilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u5229\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDM\uff09\u8fdb\u884c\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u63d0\u53d6\u5e27\u95f4\u8fd0\u52a8\u4fe1\u606f\u6765\u6307\u5bfc\u52a8\u4f5c\u751f\u6210\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u5bf9\u8fd0\u52a8\u5efa\u6a21\u548c\u52a8\u4f5c\u9884\u6d4b\u7684\u80fd\u529b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86Diffusion Transformer (DiT)\uff0c\u5c5e\u4e8e\u4e00\u79cd\u5927\u578b\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["video diffusion models", "motion modeling", "action learning", "Diffusion Transformer"]}}
{"id": "2512.02448", "pdf": "https://arxiv.org/pdf/2512.02448", "abs": "https://arxiv.org/abs/2512.02448", "authors": ["Whye Kit Fong", "Venice Erin Liong", "Kok Seang Tan", "Holger Caesar"], "title": "nuScenes Revisited: Progress and Challenges in Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": "18 pages, 17 figures", "summary": "Autonomous Vehicles (AV) and Advanced Driver Assistance Systems (ADAS) have been revolutionized by Deep Learning. As a data-driven approach, Deep Learning relies on vast amounts of driving data, typically labeled in great detail. As a result, datasets, alongside hardware and algorithms, are foundational building blocks for the development of AVs. In this work we revisit one of the most widely used autonomous driving datasets: the nuScenes dataset. nuScenes exemplifies key trends in AV development, being the first dataset to include radar data, to feature diverse urban driving scenes from two continents, and to be collected using a fully autonomous vehicle operating on public roads, while also promoting multi-modal sensor fusion, standardized benchmarks, and a broad range of tasks including perception, localization \\& mapping, prediction and planning. We provide an unprecedented look into the creation of nuScenes, as well as its extensions nuImages and Panoptic nuScenes, summarizing many technical details that have hitherto not been revealed in academic publications. Furthermore, we trace how the influence of nuScenes impacted a large number of other datasets that were released later and how it defined numerous standards that are used by the community to this day. Finally, we present an overview of both official and unofficial tasks using the nuScenes dataset and review major methodological developments, thereby offering a comprehensive survey of the autonomous driving literature, with a particular focus on nuScenes.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses the nuScenes dataset, which includes prediction as one of its tasks, suggesting relevance to trajectory prediction. However, the abstract does not mention Large Language Models (LLMs). The primary focus seems to be on autonomous driving datasets, perception, and planning. The connection to trajectory prediction is present but not dominant.", "keywords": ["prediction", "autonomous driving datasets"]}}
{"id": "2512.02569", "pdf": "https://arxiv.org/pdf/2512.02569", "abs": "https://arxiv.org/abs/2512.02569", "authors": ["Yuchong Zhang", "Yong Ma", "Danica Kragic"], "title": "Reframing Human-Robot Interaction Through Extended Reality: Unlocking Safer, Smarter, and More Empathic Interactions with Virtual Robots and Foundation Models", "categories": ["cs.HC", "cs.RO"], "comment": "This paper is under review", "summary": "This perspective reframes human-robot interaction (HRI) through extended reality (XR), arguing that virtual robots powered by large foundation models (FMs) can serve as cognitively grounded, empathic agents. Unlike physical robots, XR-native agents are unbound by hardware constraints and can be instantiated, adapted, and scaled on demand, while still affording embodiment and co-presence. We synthesize work across XR, HRI, and cognitive AI to show how such agents can support safety-critical scenarios, socially and cognitively empathic interaction across domains, and outreaching physical capabilities with XR and AI integration. We then discuss how multimodal large FMs (e.g., large language model, large vision model, and vision-language model) enable context-aware reasoning, affect-sensitive situations, and long-term adaptation, positioning virtual robots as cognitive and empathic mediators rather than mere simulation assets. At the same time, we highlight challenges and potential risks, including overtrust, cultural and representational bias, privacy concerns around biometric sensing, and data governance and transparency. The paper concludes by outlining a research agenda for human-centered, ethically grounded XR agents - emphasizing multi-layered evaluation frameworks, multi-user ecosystems, mixed virtual-physical embodiment, and societal and ethical design practices to envision XR-based virtual agents powered by FMs as reshaping future HRI into a more efficient and adaptive paradigm.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses human-robot interaction using extended reality and large foundation models. While it doesn't directly focus on trajectory prediction, the use of large language models for context-aware reasoning and adaptation in robot interaction suggests a potential connection to predicting robot behavior and interaction trajectories. The mention of safety-critical scenarios also hints at the importance of predicting robot actions.", "keywords": ["Large Foundation Models", "LLM", "human-robot interaction", "context-aware reasoning", "adaptation"]}}
{"id": "2512.02375", "pdf": "https://arxiv.org/pdf/2512.02375", "abs": "https://arxiv.org/abs/2512.02375", "authors": ["Liyuan Lou", "Wanyun Li", "Wentian Gan", "Yifei Yu", "Tengfei Wang", "Xin Wang", "Zongqian Zhan"], "title": "On-the-fly Feedback SfM: Online Explore-and-Exploit UAV Photogrammetry with Incremental Mesh Quality-Aware Indicator and Predictive Path Planning", "categories": ["cs.CV"], "comment": "This work was submitted to IEEE GRSM Journal for consideration.COPYRIGHT would be transferred once it get accepted", "summary": "Compared with conventional offline UAV photogrammetry, real-time UAV photogrammetry is essential for time-critical geospatial applications such as disaster response and active digital-twin maintenance. However, most existing methods focus on processing captured images or sequential frames in real time, without explicitly evaluating the quality of the on-the-go 3D reconstruction or providing guided feedback to enhance image acquisition in the target area. This work presents On-the-fly Feedback SfM, an explore-and-exploit framework for real-time UAV photogrammetry, enabling iterative exploration of unseen regions and exploitation of already observed and reconstructed areas in near real time. Built upon SfM on-the-fly , the proposed method integrates three modules: (1) online incremental coarse-mesh generation for dynamically expanding sparse 3D point cloud; (2) online mesh quality assessment with actionable indicators; and (3) predictive path planning for on-the-fly trajectory refinement. Comprehensive experiments demonstrate that our method achieves in-situ reconstruction and evaluation in near real time while providing actionable feedback that markedly reduces coverage gaps and re-flight costs. Via the integration of data collection, processing, 3D reconstruction and assessment, and online feedback, our on the-fly feedback SfM could be an alternative for the transition from traditional passive working mode to a more intelligent and adaptive exploration workflow. Code is now available at https://github.com/IRIS-LAB-whu/OntheflySfMFeedback.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u6444\u5f71\u6d4b\u91cf\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u548c\u4e09\u7ef4\u91cd\u5efa\uff0c\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff08\u901a\u8fc7\u8def\u5f84\u89c4\u5212\u5b9e\u73b0\uff09\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cpredictive path planning\u201d\uff0c\u8868\u660e\u5b58\u5728\u8f68\u8ff9\u9884\u6d4b\u7684\u6210\u5206\u3002", "keywords": ["predictive path planning", "trajectory refinement"]}}
{"id": "2512.02392", "pdf": "https://arxiv.org/pdf/2512.02392", "abs": "https://arxiv.org/abs/2512.02392", "authors": ["Yuqing Shao", "Yuchen Yang", "Rui Yu", "Weilong Li", "Xu Guo", "Huaicheng Yan", "Wei Wang", "Xiao Sun"], "title": "From Detection to Association: Learning Discriminative Object Embeddings for Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end multi-object tracking (MOT) methods have recently achieved remarkable progress by unifying detection and association within a single framework. Despite their strong detection performance, these methods suffer from relatively low association accuracy. Through detailed analysis, we observe that object embeddings produced by the shared DETR architecture display excessively high inter-object similarity, as it emphasizes only category-level discrimination within single frames. In contrast, tracking requires instance-level distinction across frames with spatial and temporal continuity, for which current end-to-end approaches insufficiently optimize object embeddings. To address this, we introduce FDTA (From Detection to Association), an explicit feature refinement framework that enhances object discriminativeness across three complementary perspectives. Specifically, we introduce a Spatial Adapter (SA) to integrate depth-aware cues for spatial continuity, a Temporal Adapter (TA) to aggregate historical information for temporal dependencies, and an Identity Adapter (IA) to leverage quality-aware contrastive learning for instance-level separability. Extensive experiments demonstrate that FDTA achieves state-of-the-art performance on multiple challenging MOT benchmarks, including DanceTrack, SportsMOT, and BFT, highlighting the effectiveness of our proposed discriminative embedding enhancement strategy. The code is available at https://github.com/Spongebobbbbbbbb/FDTA.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Multi-Object Tracking (MOT), which is closely related to trajectory prediction as it involves associating objects across frames and predicting their future locations. While the paper doesn't directly use Large Language Models, the underlying concept of tracking and associating objects can be relevant in scenarios where LLMs are used for understanding and predicting agent behaviors. The paper focuses on improving object embeddings for better association, a crucial step in trajectory prediction.", "keywords": ["multi-object tracking", "object embeddings", "association", "temporal dependencies", "spatial continuity"]}}
{"id": "2512.02336", "pdf": "https://arxiv.org/pdf/2512.02336", "abs": "https://arxiv.org/abs/2512.02336", "authors": ["Sai Siddharth Nalamalpu", "Kaining Yuan", "Aiden Zhou", "Eugene Pinsky"], "title": "Forecasting MBTA Transit Dynamics: A Performance Benchmarking of Statistical and Machine Learning Models", "categories": ["cs.LG"], "comment": "14 pages 9 figures", "summary": "The Massachusetts Bay Transportation Authority (MBTA) is the main public transit provider in Boston, operating multiple means of transport, including trains, subways, and buses. However, the system often faces delays and fluctuations in ridership volume, which negatively affect efficiency and passenger satisfaction. To further understand this phenomenon, this paper compares the performance of existing and unique methods to determine the best approach in predicting gated station entries in the subway system (a proxy for subway usage) and the number of delays in the overall MBTA system. To do so, this research considers factors that tend to affect public transportation, such as day of week, season, pressure, wind speed, average temperature, and precipitation. This paper evaluates the performance of 10 statistical and machine learning models on predicting next-day subway usage. On predicting delay count, the number of models is extended to 11 per day by introducing a self-exciting point process model, representing a unique application of a point-process framework for MBTA delay modeling. This research involves experimenting with the selective inclusion of features to determine feature importance, testing model accuracy via Root Mean Squared Error (RMSE). Remarkably, it is found that providing either day of week or season data has a more substantial benefit to predictive accuracy compared to weather data; in fact, providing weather data generally worsens performance, suggesting a tendency of models to overfit.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u7edf\u8ba1\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u5730\u94c1\u7cfb\u7edf\u7684\u5ba2\u6d41\u91cf\u548c\u5ef6\u8bef\u60c5\u51b5\u3002\u867d\u7136\u6d89\u53ca\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u548c\u673a\u5668\u5b66\u4e60\uff0c\u4f46\u5e76\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u6216\u8005\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u5ba2\u6d41\u91cf\u9884\u6d4b\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5e7f\u4e49\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["machine learning models", "forecasting", "prediction", "time series prediction"]}}
{"id": "2512.02458", "pdf": "https://arxiv.org/pdf/2512.02458", "abs": "https://arxiv.org/abs/2512.02458", "authors": ["Zhongyi Cai", "Yi Du", "Chen Wang", "Yu Kong"], "title": "Vision to Geometry: 3D Spatial Memory for Sequential Embodied MLLM Reasoning and Exploration", "categories": ["cs.CV"], "comment": null, "summary": "Existing research on indoor embodied tasks typically requires agents to actively explore unknown environments and reason about the scene to achieve a specific goal. However, when deployed in real life, agents often face sequential tasks, where each new sub-task follows the completion of the previous one, and certain sub-tasks may be infeasible, such as searching for a non-existent object. Compared with the single-task setting, the core challenge lies in reusing spatial knowledge accumulated from previous explorations to support subsequent reasoning and exploration. In this work, we investigate this underexplored yet practically significant embodied AI challenge. To evaluate this challenge, we introduce SEER-Bench, a new Sequential Embodied Exploration and Reasoning Benchmark encompassing encompassing two classic embodied tasks: Embodied Question Answering (EQA) and Embodied Multi-modal Navigation (EMN). Building on SEER-Bench, we propose 3DSPMR, a 3D SPatial Memory Reasoning approach that exploits relational, visual, and geometric cues from explored regions to augment Multi-Modal Large Language Models (MLLMs) for reasoning and exploration in sequential embodied tasks. To the best of our knowledge, this is the first work to explicitly incorporate geometric information into MLLM-based spatial understanding and reasoning. Extensive experiments verify that 3DSPMR achieves substantial performance gains on both sequential EQA and EMN tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on embodied AI tasks involving exploration and reasoning in sequential environments. It leverages Multi-Modal Large Language Models (MLLMs) and incorporates geometric information for spatial understanding. While not directly focused on trajectory prediction, the embodied multi-modal navigation aspect has some overlap. The use of MLLMs is a strong indicator of relevance to large language models.", "keywords": ["Multi-Modal Large Language Models", "MLLMs", "embodied multi-modal navigation", "spatial reasoning", "embodied AI"]}}
{"id": "2512.02473", "pdf": "https://arxiv.org/pdf/2512.02473", "abs": "https://arxiv.org/abs/2512.02473", "authors": ["Yuta Oshima", "Yusuke Iwasawa", "Masahiro Suzuki", "Yutaka Matsuo", "Hiroki Furuta"], "title": "WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u5173\u6ce8\u4e8e\u89c6\u9891\u4e16\u754c\u5efa\u6a21\uff0c\u6d89\u53ca\u5bf9\u672a\u6765\u89c6\u89c9\u89c2\u6d4b\u7684\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u4e86\u8f68\u8ff9\u6253\u5305\uff08trajectory packing\uff09\u6765\u63d0\u9ad8\u7a7a\u95f4\u4e00\u81f4\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u8003\u8651\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u5efa\u6a21\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u867d\u7136\u6ca1\u6709\u660e\u786e\u63d0\u5230\u5927\u6a21\u578b\uff0c\u4f46\u8003\u8651\u5230\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7684\u590d\u6742\u6027\uff0c\u5e95\u5c42\u6a21\u578b\u5f88\u53ef\u80fd\u5177\u6709\u76f8\u5f53\u7684\u89c4\u6a21\u3002", "keywords": ["trajectory packing", "world modeling", "long-term generation", "spatial consistency", "video world model", "future visual observations"]}}
{"id": "2512.02631", "pdf": "https://arxiv.org/pdf/2512.02631", "abs": "https://arxiv.org/abs/2512.02631", "authors": ["Zhengcheng Wang", "Zichuan Lin", "Yijun Yang", "Haobo Fu", "Deheng Ye"], "title": "SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization", "categories": ["cs.LG"], "comment": "12 pages,6 figures", "summary": "Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language Navigation (VLN) agents, which involves navigating through environments based on visual and textual cues. While not directly trajectory prediction, VLN implicitly requires agents to plan and execute trajectories. The paper also explicitly mentions and utilizes Large Vision-Language Models (LVLMs). Therefore, it has relevance to both trajectory prediction (through navigation) and large language models.", "keywords": ["Vision-Language Navigation", "LVLMs", "Large Vision-Language Models", "navigation", "planning"]}}
