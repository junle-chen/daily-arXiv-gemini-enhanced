{"id": "2510.01639", "pdf": "https://arxiv.org/pdf/2510.01639", "abs": "https://arxiv.org/abs/2510.01639", "authors": ["Thinh Hung Truong", "Jey Han Lau", "Jianzhong Qi"], "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective", "categories": ["cs.AI"], "comment": null, "summary": "We explore the geospatial reasoning capabilities of Large Language Models\n(LLMs), specifically, whether LLMs can read road network maps and perform\nnavigation. We frame trajectory recovery as a proxy task, which requires models\nto reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with\nover 4,000 real-world trajectories across diverse regions and transportation\nmodes. Using road network as context, our prompting framework enables LLMs to\ngenerate valid paths without accessing any external navigation tools.\nExperiments show that LLMs outperform off-the-shelf baselines and specialized\ntrajectory recovery models, with strong zero-shot generalization. Fine-grained\nanalysis shows that LLMs have strong comprehension of the road network and\ncoordinate systems, but also pose systematic biases with respect to regions and\ntransportation modes. Finally, we demonstrate how LLMs can enhance navigation\nexperiences by reasoning over maps in flexible ways to incorporate user\npreferences.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "The paper explicitly explores the geospatial reasoning capabilities of Large Language Models (LLMs) in the context of trajectory recovery, framing it as a proxy task for navigation. It introduces a dataset for trajectory recovery and demonstrates how LLMs can generate valid paths using road networks, showcasing a strong connection between trajectory prediction and large language models.", "keywords": ["trajectory recovery", "trajectory prediction", "Large Language Models", "LLMs", "geospatial reasoning", "navigation", "road network", "GPS traces"]}}
{"id": "2510.01272", "pdf": "https://arxiv.org/pdf/2510.01272", "abs": "https://arxiv.org/abs/2510.01272", "authors": ["Kunal Jha", "Aydan Yuenan Huang", "Eric Ye", "Natasha Jaques", "Max Kleiman-Weiner"], "title": "Modeling Others' Minds as Code", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Accurate prediction of human behavior is essential for robust and safe\nhuman-AI collaboration. However, existing approaches for modeling people are\noften data-hungry and brittle because they either make unrealistic assumptions\nabout rationality or are too computationally demanding to adapt rapidly. Our\nkey insight is that many everyday social interactions may follow predictable\npatterns; efficient \"scripts\" that minimize cognitive load for actors and\nobservers, e.g., \"wait for the green light, then go.\" We propose modeling these\nroutines as behavioral programs instantiated in computer code rather than\npolicies conditioned on beliefs and desires. We introduce ROTE, a novel\nalgorithm that leverages both large language models (LLMs) for synthesizing a\nhypothesis space of behavioral programs, and probabilistic inference for\nreasoning about uncertainty over that space. We test ROTE in a suite of\ngridworld tasks and a large-scale embodied household simulator. ROTE predicts\nhuman and AI behaviors from sparse observations, outperforming competitive\nbaselines -- including behavior cloning and LLM-based methods -- by as much as\n50% in terms of in-sample accuracy and out-of-sample generalization. By\ntreating action understanding as a program synthesis problem, ROTE opens a path\nfor AI systems to efficiently and effectively predict human behavior in the\nreal-world.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u5efa\u6a21\u548c\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\uff0c\u5e76\u4e0e\u884c\u4e3a\u514b\u9686\u7b49\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u867d\u7136\u6ca1\u6709\u660e\u786e\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\uff0c\u5c24\u5176\u662f\u5728\u5177\u8eab\u73af\u5883\u4e2d\u3002\u8be5\u8bba\u6587\u91cd\u70b9\u5728\u4e8e\u9884\u6d4b\u884c\u4e3a\u7684\u7a0b\u5e8f\u5316\u6a21\u578b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e2d\u9884\u6d4b\u672a\u6765\u8def\u5f84\u7684\u601d\u60f3\u76f8\u7b26\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["large language models", "LLMs", "behavior prediction", "human behavior", "program synthesis", "embodied environment"]}}
{"id": "2510.01388", "pdf": "https://arxiv.org/pdf/2510.01388", "abs": "https://arxiv.org/abs/2510.01388", "authors": ["Arthur Zhang", "Xiangyun Meng", "Luca Calliari", "Dong-Ki Kim", "Shayegan Omidshafiei", "Joydeep Biswas", "Ali Agha", "Amirreza Shaban"], "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 6 figures, 3 tables", "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u5229\u7528\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8def\u5f84\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u8303\u7574\u3002\u867d\u7136\u63d0\u5230\u4e86Vision-Language models (VLMs)\uff0c\u4f46\u4e3b\u8981\u662f\u5229\u7528\u5176\u751f\u6210caption\u6765\u8f85\u52a9\u8bad\u7ec3\uff0c\u5927\u6a21\u578b\u672c\u8eab\u4e0d\u662f\u6838\u5fc3\u7814\u7a76\u5bf9\u8c61\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4f46\u5e76\u975e\u5b8c\u5168\u805a\u7126\u4e8e\u5927\u6a21\u578b\u3002", "keywords": ["trajectory prediction", "path planning", "Vision-Language models", "navigation", "robot behaviors"]}}
{"id": "2510.01348", "pdf": "https://arxiv.org/pdf/2510.01348", "abs": "https://arxiv.org/abs/2510.01348", "authors": ["Michal Werner", "David \u010capek", "Tom\u00e1\u0161 Musil", "Ond\u0159ej Fran\u011bk", "Tom\u00e1\u0161 B\u00e1\u010da", "Martin Saska"], "title": "Kilometer-Scale GNSS-Denied UAV Navigation via Heightmap Gradients: A Winning System from the SPRIN-D Challenge", "categories": ["cs.RO", "x", "I.2.9"], "comment": "8 pages", "summary": "Reliable long-range flight of unmanned aerial vehicles (UAVs) in GNSS-denied\nenvironments is challenging: integrating odometry leads to drift, loop closures\nare unavailable in previously unseen areas and embedded platforms provide\nlimited computational power. We present a fully onboard UAV system developed\nfor the SPRIN-D Funke Fully Autonomous Flight Challenge, which required 9 km\nlong-range waypoint navigation below 25 m AGL (Above Ground Level) without GNSS\nor prior dense mapping. The system integrates perception, mapping, planning,\nand control with a lightweight drift-correction method that matches\nLiDAR-derived local heightmaps to a prior geo-data heightmap via\ngradient-template matching and fuses the evidence with odometry in a clustered\nparticle filter. Deployed during the competition, the system executed\nkilometer-scale flights across urban, forest, and open-field terrain and\nreduced drift substantially relative to raw odometry, while running in real\ntime on CPU-only hardware. We describe the system architecture, the\nlocalization pipeline, and the competition evaluation, and we report practical\ninsights from field deployment that inform the design of GNSS-denied UAV\nautonomy.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on UAV navigation in GNSS-denied environments, which is related to trajectory prediction and path planning. It involves perception, mapping, planning, and control to achieve autonomous flight. While it doesn't directly use or mention large language models, the problem it addresses (autonomous navigation) is a relevant application area where LLMs could potentially be integrated in the future for tasks like scene understanding or decision-making. The use of particle filters for localization also connects to trajectory estimation.", "keywords": ["UAV navigation", "trajectory prediction", "path planning", "autonomous flight", "particle filter"]}}
{"id": "2510.01357", "pdf": "https://arxiv.org/pdf/2510.01357", "abs": "https://arxiv.org/abs/2510.01357", "authors": ["Alejandro Gonzalez-Garcia", "Wei Xiao", "Wei Wang", "Alejandro Astudillo", "Wilm Decr\u00e9", "Jan Swevers", "Carlo Ratti", "Daniela Rus"], "title": "Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels", "categories": ["cs.RO"], "comment": "IROS 2025", "summary": "Safe motion planning is essential for autonomous vessel operations,\nespecially in challenging spaces such as narrow inland waterways. However,\nconventional motion planning approaches are often computationally intensive or\noverly conservative. This paper proposes a safe motion planning strategy\ncombining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).\nWe introduce a time-varying inflated ellipse obstacle representation, where the\ninflation radius is adjusted depending on the relative position and attitude\nbetween the vessel and the obstacle. The proposed adaptive inflation reduces\nthe conservativeness of the controller compared to traditional fixed-ellipsoid\nobstacle formulations. The MPC solution provides an approximate motion plan,\nand high-order CBFs ensure the vessel's safety using the varying inflation\nradius. Simulation and real-world experiments demonstrate that the proposed\nstrategy enables the fully-actuated autonomous robot vessel to navigate through\nnarrow spaces in real time and resolve potential deadlocks, all while ensuring\nsafety.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u4e3b\u6c34\u9762\u8239\u53ea\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\uff0c\u91c7\u7528\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBFs\uff09\u7b49\u65b9\u6cd5\u3002\u867d\u7136\u6d89\u53ca\u8fd0\u52a8\u89c4\u5212\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u4e2a\u5e94\u7528\u573a\u666f\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["motion planning", "model predictive control", "trajectory prediction", "autonomous vessel"]}}
{"id": "2510.01262", "pdf": "https://arxiv.org/pdf/2510.01262", "abs": "https://arxiv.org/abs/2510.01262", "authors": ["Koyena Chowdhury", "Paramita Koley", "Abhijnan Chakraborty", "Saptarshi Ghosh"], "title": "RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of train delays is critical for efficient railway\noperations, enabling better scheduling and dispatching decisions. While earlier\napproaches have largely focused on forecasting the exact delays of individual\ntrains, recent studies have begun exploring station-level delay prediction to\nsupport higher-level traffic management. In this paper, we propose the\nRailway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed\nto forecast average arrival delays of all the incoming trains at railway\nstations for a particular time period. Our approach incorporates several\narchitectural innovations and novel feature integrations, including train\nfrequency-aware spatial attention, which significantly enhances predictive\nperformance. To support this effort, we curate and release a comprehensive\ndataset for the entire Indian Railway Network (IRN), spanning 4,735 stations\nacross 17 zones - the largest and most diverse railway network studied to date.\nWe conduct extensive experiments using multiple state-of-the-art baselines,\ndemonstrating consistent improvements across standard metrics. Our work not\nonly advances the modeling of average delay prediction in large-scale rail\nnetworks but also provides an open dataset to encourage further research in\nthis critical domain.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on train delay prediction, which can be considered a form of trajectory prediction in a transportation context. It uses a graph convolutional network (GCN) to model spatio-temporal dependencies. However, it doesn't explicitly involve or mention large language models. Therefore, the relevance is moderate due to the connection to trajectory prediction but the absence of any large language model component.", "keywords": ["trajectory prediction", "train delay prediction", "spatio-temporal graph convolutional network", "GCN"]}}
{"id": "2510.01454", "pdf": "https://arxiv.org/pdf/2510.01454", "abs": "https://arxiv.org/abs/2510.01454", "authors": ["Nilay Naharas", "Dang Nguyen", "Nesihan Bulut", "Mohammadhossein Bateni", "Vahab Mirrokni", "Baharan Mirzasoleiman"], "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories", "categories": ["cs.CV", "cs.LG"], "comment": "30 pages, 10 figures, 5 tables, link:\n  https://bigml-cs-ucla.github.io/XMAS-project-page/", "summary": "Data-efficient learning aims to eliminate redundancy in large training\ndatasets by training models on smaller subsets of the most informative\nexamples. While data selection has been extensively explored for vision models\nand large language models (LLMs), it remains underexplored for Large\nVision-Language Models (LVLMs). Notably, none of existing methods can\noutperform random selection at different subset sizes. In this work, we propose\nthe first principled method for data-efficient instruction tuning of LVLMs. We\nprove that examples with similar cross-modal attention matrices during\ninstruction tuning have similar gradients. Thus, they influence model\nparameters in a similar manner and convey the same information to the model\nduring training. Building on this insight, we propose XMAS, which clusters\nexamples based on the trajectories of the top singular values of their\nattention matrices obtained from fine-tuning a small proxy LVLM. By sampling a\nbalanced subset from these clusters, XMAS effectively removes redundancy in\nlarge-scale LVLM training data. Extensive experiments show that XMAS can\ndiscard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while\nfully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and\nspeeding up its training by 1.2x. This is 30% more data reduction compared to\nthe best baseline for LLaVA-665k. The project's website can be found at\nhttps://bigml-cs-ucla.github.io/XMAS-project-page/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7684\u6570\u636e\u9009\u62e9\u548c\u9ad8\u6548\u5fae\u8c03\uff0c\u4f7f\u7528\u4e86 attention matrices \u7684\u8f68\u8ff9\u8fdb\u884c\u6570\u636e\u805a\u7c7b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4f7f\u7528\u4e86\u201c\u8f68\u8ff9\u201d\u7684\u6982\u5ff5\u6765\u5206\u6790\u6a21\u578b\u7684\u884c\u4e3a\uff0c\u5e76\u4e14\u660e\u786e\u6d89\u53ca\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "Vision Language Models", "LVLMs", "attention matrices", "trajectories"]}}
{"id": "2510.01483", "pdf": "https://arxiv.org/pdf/2510.01483", "abs": "https://arxiv.org/abs/2510.01483", "authors": ["Mohamad Al Mdfaa", "Svetlana Lukina", "Timur Akhtyamov", "Arthur Nigmatzyanov", "Dmitrii Nalberskii", "Sergey Zagoruyko", "Gonzalo Ferrer"], "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs", "categories": ["cs.RO", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Vision-language models (VLMs) have shown potential for robot navigation but\nencounter fundamental limitations: they lack persistent scene memory, offer\nlimited spatial reasoning, and do not scale effectively with video duration for\nreal-time application. We present VL-KnG, a Visual Scene Understanding system\nthat tackles these challenges using spatiotemporal knowledge graph construction\nand computationally efficient query processing for navigation goal\nidentification. Our approach processes video sequences in chunks utilizing\nmodern VLMs, creates persistent knowledge graphs that maintain object identity\nover time, and enables explainable spatial reasoning through queryable graph\nstructures. We also introduce WalkieKnowledge, a new benchmark with about 200\nmanually annotated questions across 8 diverse trajectories spanning\napproximately 100 minutes of video data, enabling fair comparison between\nstructured approaches and general-purpose VLMs. Real-world deployment on a\ndifferential drive robot demonstrates practical applicability, with our method\nachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5\nPro performance while providing explainable reasoning supported by the\nknowledge graph, computational efficiency for real-time deployment across\ndifferent tasks, such as localization, navigation and planning. Code and\ndataset will be released after acceptance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses Vision-Language Models (VLMs) for robot navigation, which can be related to trajectory prediction, especially in the context of planning. The use of knowledge graphs for spatial reasoning also connects to the broader scope of trajectory prediction, although indirectly. While it mentions VLMs, the focus is more on visual scene understanding and knowledge graph construction rather than directly leveraging LLMs for trajectory prediction. Navigation goal identification is a related task, thus the moderate score.", "keywords": ["Vision-language models", "VLMs", "robot navigation", "spatiotemporal knowledge graphs", "navigation", "planning"]}}
{"id": "2510.01398", "pdf": "https://arxiv.org/pdf/2510.01398", "abs": "https://arxiv.org/abs/2510.01398", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "categories": ["cs.AI"], "comment": null, "summary": "Modern engineering increasingly relies on vast datasets generated by\nexperiments and simulations, driving a growing demand for efficient, reliable,\nand broadly applicable modeling strategies. There is also heightened interest\nin developing data-driven approaches, particularly neural network models, for\neffective prediction and analysis of scientific datasets. Traditional\ndata-driven methods frequently involve extensive manual intervention, limiting\ntheir ability to scale effectively and generalize to diverse applications. In\nthis study, we propose an innovative pipeline utilizing Large Language Model\n(LLM) agents to automate data-driven modeling and analysis, with a particular\nemphasis on regression tasks. We evaluate two LLM-agent frameworks: a\nmulti-agent system featuring specialized collaborative agents, and a\nsingle-agent system based on the Reasoning and Acting (ReAct) paradigm. Both\nframeworks autonomously handle data preprocessing, neural network development,\ntraining, hyperparameter optimization, and uncertainty quantification (UQ). We\nvalidate our approach using a critical heat flux (CHF) prediction benchmark,\ninvolving approximately 25,000 experimental data points from the OECD/NEA\nbenchmark dataset. Results indicate that our LLM-agent-developed model\nsurpasses traditional CHF lookup tables and delivers predictive accuracy and UQ\non par with state-of-the-art Bayesian optimized deep neural network models\ndeveloped by human experts. These outcomes underscore the significant potential\nof LLM-based agents to automate complex engineering modeling tasks, greatly\nreducing human workload while meeting or exceeding existing standards of\npredictive performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses Large Language Models to automate data-driven modeling and analysis, focusing on regression tasks. While it doesn't directly address trajectory prediction, the use of LLMs for prediction tasks and data analysis aligns with the broader application of these models in areas like trajectory prediction. The 'prediction' aspect contributes to the relevance.", "keywords": ["Large Language Model", "LLM", "prediction", "data-driven modeling"]}}
{"id": "2510.01642", "pdf": "https://arxiv.org/pdf/2510.01642", "abs": "https://arxiv.org/abs/2510.01642", "authors": ["Zijun Lin", "Jiafei Duan", "Haoquan Fang", "Dieter Fox", "Ranjay Krishna", "Cheston Tan", "Bihan Wen"], "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models", "categories": ["cs.RO"], "comment": "Project Page: https://jimntu.github.io/FailSafe/", "summary": "Recent advances in robotic manipulation have integrated low-level robotic\ncontrol into Vision-Language Models (VLMs), extending them into\nVision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve\nstrong performance in downstream robotic applications, supported by large-scale\ncrowd-sourced robot training data, they still inevitably encounter failures\nduring execution. Enabling robots to reason about and recover from\nunpredictable and abrupt failures remains a critical challenge. Existing\nrobotic manipulation datasets, collected in either simulation or the real\nworld, primarily provide only ground-truth trajectories, leaving robots unable\nto recover once failures occur. Moreover, the few datasets that address failure\ndetection typically offer only textual explanations, which are difficult to\nutilize directly in VLA models. To address this gap, we introduce FailSafe, a\nnovel failure generation and recovery system that automatically produces\ndiverse failure cases paired with executable recovery actions. FailSafe can be\nseamlessly applied to any manipulation task in any simulator, enabling scalable\ncreation of failure-action data. To demonstrate its effectiveness, we fine-tune\nLLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results\nshow that FailSafe-VLM successfully helps robotic arm detect and recover from\npotential failures, improving the performance of three state-of-the-art VLA\nmodels pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several\ntasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different\nspatial configurations, camera viewpoints, and robotic embodiments. We plan to\nrelease the FailSafe code to the community.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on failure recovery in robotic manipulation using Vision-Language-Action models (VLA), which are built upon Large Language Models (LLMs) like LLaVa. While not directly addressing trajectory prediction, the robotic manipulation context often involves planning and executing trajectories. The connection to LLMs is clear. The paper mentions generating executable recovery actions, which could be interpreted as a form of trajectory correction or replanning, thus having some relevance to trajectory prediction.", "keywords": ["Large Language Models", "LLMs", "Vision-Language-Action Models", "VLA", "robotic manipulation", "failure recovery", "LLaVa"]}}
{"id": "2510.01531", "pdf": "https://arxiv.org/pdf/2510.01531", "abs": "https://arxiv.org/abs/2510.01531", "authors": ["Djengo Cyun-Jyun Fang", "Tsung-Wei Ke"], "title": "Information Seeking for Robust Decision Making under Partial Observability", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "The project page is available at https://infoseekerllm.github.io", "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6458\u8981\u63d0\u5230\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e0d\u5b8c\u5168\u53ef\u89c2\u5bdf\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u5236\u5b9a\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u5173\u6ce8\u7684planning\u548cdecision-making\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u76f8\u5173\u3002InfoSeeker\u6846\u67b6\u7684\u76ee\u6807\u662f\u4f7fLLM\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u548c\u9002\u5e94\u73af\u5883\u52a8\u6001\uff0c\u8fd9\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u4e5f\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002", "keywords": ["Large Language Model", "LLM", "planning", "decision-making", "uncertainty", "partially observable environments"]}}
{"id": "2510.01623", "pdf": "https://arxiv.org/pdf/2510.01623", "abs": "https://arxiv.org/abs/2510.01623", "authors": ["Angen Ye", "Zeyu Zhang", "Boyuan Wang", "Xiaofeng Wang", "Dapeng Zhang", "Zheng Zhu"], "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action (VLA) models, which aim to unify perception, language understanding, and action generation. While it mentions 'trajectory consistency' and leverages datasets with 'trajectory annotations', the primary focus is not on trajectory prediction as an independent research problem. The 'reasoning-enhanced VLA' aspect connects it to Large Language Models, as VLAs often incorporate LLMs. The mention of 'chain-of-thought' further strengthens this connection. However, the core contribution is on improving VLA model reasoning and execution, rather than specifically advancing trajectory prediction techniques using LLMs.", "keywords": ["Vision-Language-Action models", "VLA", "trajectory consistency", "chain-of-thought", "reasoning", "action generation"]}}
{"id": "2510.01586", "pdf": "https://arxiv.org/pdf/2510.01586", "abs": "https://arxiv.org/abs/2510.01586", "authors": ["Zhenyu Pan", "Yiting Zhang", "Zhuo Liu", "Yolo Yunlong Tang", "Zeliang Zhang", "Haozheng Luo", "Yuwei Han", "Jianshu Zhang", "Dennis Wu", "Hong-Yu Chen", "Haoran Lu", "Haoyang Fang", "Manling Li", "Chenliang Xu", "Philip S. Yu", "Han Liu"], "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u57fa\u4e8eLLM\u7684\u5b89\u5168\u95ee\u9898\uff0c\u7279\u522b\u662f\u9632\u5fa1\u5bf9\u6297\u6027\u653b\u51fb\u3002\u867d\u7136\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548cLLM\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\u3002\u76f8\u5173\u6027\u4f53\u73b0\u5728\u5b83\u4f7f\u7528\u4e86LLM\u4f5c\u4e3a\u667a\u80fd\u4f53\uff0c\u5e76\u6d89\u53ca\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u548c\u51b3\u7b56\uff0c\u53ef\u4ee5\u89c6\u4e3a\u4e00\u79cd\u5e7f\u4e49\u7684\u201c\u8f68\u8ff9\u201d\u6216\u884c\u4e3a\u6a21\u5f0f\u7684\u9884\u6d4b\u4e0e\u63a7\u5236\u3002", "keywords": ["LLM", "multi-agent systems", "reinforcement learning", "adversarial learning"]}}
{"id": "2510.01795", "pdf": "https://arxiv.org/pdf/2510.01795", "abs": "https://arxiv.org/abs/2510.01795", "authors": ["Haibo Hu", "Lianming Huang", "Xinyu Wang", "Yufei Cui", "Nan Guan", "Chun Jason Xue"], "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving\nfor unified perception and reasoning, but high inference latency hinders\nreal-time deployment. Early-exit reduces latency by terminating inference at\nintermediate layers, yet its task-dependent nature limits generalization across\ndiverse scenarios. We observe that this limitation aligns with autonomous\ndriving: navigation systems can anticipate upcoming contexts (e.g.,\nintersections, traffic lights), indicating which tasks will be required. We\npropose Nav-EE, a navigation-guided early-exit framework that precomputes\ntask-specific exit layers offline and dynamically applies them online based on\nnavigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE\nachieves accuracy comparable to full inference while reducing latency by up to\n63.9%. Real-vehicle integration with Autoware Universe further demonstrates\nreduced inference latency (600ms to 300ms), supporting faster decision-making\nin complex scenarios. These results suggest that coupling navigation foresight\nwith early-exit offers a viable path toward efficient deployment of large\nmodels in autonomous systems. Code and data are available at our anonymous\nrepository: https://anonymous.4open.science/r/Nav-EE-BBC4", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses efficient deployment of Vision-Language Models (VLMs) in autonomous driving by leveraging navigation systems to guide early exiting. While it does not directly address trajectory prediction, the context of autonomous driving and the use of navigation information suggest a connection. The focus is more on optimizing VLM inference rather than trajectory prediction itself, but the navigation aspect hints at future trajectory-aware applications. The use of VLMs also connects it to the large model theme.", "keywords": ["Vision-Language Models", "autonomous driving", "navigation", "large models"]}}
{"id": "2510.01843", "pdf": "https://arxiv.org/pdf/2510.01843", "abs": "https://arxiv.org/abs/2510.01843", "authors": ["Wanyue Li", "Ji Ma", "Minghao Lu", "Peng Lu"], "title": "Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots", "categories": ["cs.RO", "I.2.9; I.2.8; G.1.6"], "comment": "8 pages, 8 figures, conference paper", "summary": "Humanoid robot soccer presents several challenges, particularly in\nmaintaining system stability during aggressive kicking motions while achieving\nprecise ball trajectory control. Current solutions, whether traditional\nposition-based control methods or reinforcement learning (RL) approaches,\nexhibit significant limitations. Model predictive control (MPC) is a prevalent\napproach for ordinary quadruped and biped robots. While MPC has demonstrated\nadvantages in legged robots, existing studies often oversimplify the leg swing\nprogress, relying merely on simple trajectory interpolation methods. This\nseverely constrains the foot's environmental interaction capability, hindering\ntasks such as ball kicking. This study innovatively adapts the spatial-temporal\ntrajectory planning method, which has been successful in drone applications, to\nbipedal robotic systems. The proposed approach autonomously generates foot\ntrajectories that satisfy constraints on target kicking position, velocity, and\nacceleration while simultaneously optimizing swing phase duration. Experimental\nresults demonstrate that the optimized trajectories closely mimic human kicking\nbehavior, featuring a backswing motion. Simulation and hardware experiments\nconfirm the algorithm's efficiency, with trajectory planning times under 1 ms,\nand its reliability, achieving nearly 100 % task completion accuracy when the\nsoccer goal is within the range of -90{\\deg} to 90{\\deg}.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u53cc\u8db3\u673a\u5668\u4eba\u7684\u8db3\u7403\u8e22\u7403\u52a8\u4f5c\u7684\u8f68\u8ff9\u4f18\u5316\u548c\u63a7\u5236\uff0c\u4f7f\u7528\u4e86\u7a7a\u95f4-\u65f6\u95f4\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u548c\u63a7\u5236\uff0c\u4f46\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u53ef\u4ee5\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e0d\u662f\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "trajectory planning", "model predictive control", "foot trajectories", "bipedal robots"]}}
{"id": "2510.01869", "pdf": "https://arxiv.org/pdf/2510.01869", "abs": "https://arxiv.org/abs/2510.01869", "authors": ["Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "TACOS: Task Agnostic COordinator of a multi-drone System", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "6 pages, 6 figures, accepted as poster at 2025 IEEE International\n  Symposium on Multi-Robot & Multi-Agent Systems", "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on controlling a multi-drone system using Large Language Models (LLMs) for high-level task delegation through natural language. While it doesn't explicitly mention trajectory prediction, the coordination and control of drones inherently involve path planning and movement, which are related. The heavy reliance on LLMs increases the relevance.", "keywords": ["Large Language Models", "LLMs", "multi-drone system", "natural language control", "autonomous agent", "coordination"]}}
{"id": "2510.01991", "pdf": "https://arxiv.org/pdf/2510.01991", "abs": "https://arxiv.org/abs/2510.01991", "authors": ["Lei Liu", "Can Wang", "Zhenghao Chen", "Dong Xu"], "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on 4D Gaussian Splatting editing. While the core is not directly trajectory prediction, it uses an LLM-based module for user intent understanding and instruction decomposition, which is relevant to Large Language Models. The '4D' aspect might relate to temporal changes which could indirectly involve trajectory-like information.", "keywords": ["Large Language Models", "LLM", "user intent understanding", "4D"]}}
{"id": "2510.01545", "pdf": "https://arxiv.org/pdf/2510.01545", "abs": "https://arxiv.org/abs/2510.01545", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Predictive Preference Learning from Human Interventions", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "NeurIPS 2025 Spotlight. Project page:\n  https://metadriverse.github.io/ppl", "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ece\u4eba\u7c7b\u5e72\u9884\u4e2d\u5b66\u4e60\u9884\u6d4b\u6027\u504f\u597d\uff0c\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u9886\u57df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u5b66\u4e60\u667a\u80fd\u4f53\u5728\u672a\u6765\u72b6\u6001\u7684\u884c\u52a8\u7b56\u7565\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\u3002\u4eba\u7c7b\u5e72\u9884\u53ef\u4ee5\u88ab\u89c6\u4e3a\u5bf9agent\u8f68\u8ff9\u7684\u4fee\u6b63\uff0c\u4ece\u800c\u5b66\u4e60\u5230\u66f4\u4f18\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e2d\u7b49\u7a0b\u5ea6\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "autonomous driving", "human intervention", "action prediction"]}}
{"id": "2510.01538", "pdf": "https://arxiv.org/pdf/2510.01538", "abs": "https://arxiv.org/abs/2510.01538", "authors": ["Haokun Zhao", "Xiang Zhang", "Jiaqi Wei", "Yiwei Xu", "Yuting He", "Siqi Sun", "Chenyu You"], "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting is central to decision-making in domains as diverse\nas energy, finance, climate, and public health. In practice, forecasters face\nthousands of short, noisy series that vary in frequency, quality, and horizon,\nwhere the dominant cost lies not in model fitting, but in the labor-intensive\npreprocessing, validation, and ensembling required to obtain reliable\npredictions. Prevailing statistical and deep learning models are tailored to\nspecific datasets or domains and generalize poorly. A general, domain-agnostic\nframework that minimizes human intervention is urgently in demand. In this\npaper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic\nframework for general time series forecasting. The framework comprises four\nspecialized agents: Curator performs LLM-guided diagnostics augmented by\nexternal tools that reason over data statistics to choose targeted\npreprocessing; Planner narrows the hypothesis space of model choice by\nleveraging multi-modal diagnostics and self-planning over the input; Forecaster\nperforms model fitting and validation and, based on the results, adaptively\nselects the best model configuration as well as ensemble strategy to make final\npredictions; and Reporter synthesizes the whole process into a comprehensive,\ntransparent report. With transparent natural-language rationales and\ncomprehensive reports, TSci transforms the forecasting workflow into a\nwhite-box system that is both interpretable and extensible across tasks.\nEmpirical results on eight established benchmarks demonstrate that TSci\nconsistently outperforms both statistical and LLM-based baselines, reducing\nforecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci\nproduces a clear and rigorous report that makes the forecasting workflow more\ntransparent and interpretable.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on time series forecasting using a large language model (LLM) driven agent. While it doesn't explicitly mention trajectory prediction, time series forecasting is a relevant technique that could be applied in trajectory prediction. The paper heavily involves LLMs, making it relevant to the large language model aspect.", "keywords": ["time series forecasting", "LLM", "large language model", "agent", "forecasting"]}}
