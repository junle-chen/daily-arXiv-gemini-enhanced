{"id": "2512.03444", "pdf": "https://arxiv.org/pdf/2512.03444", "abs": "https://arxiv.org/abs/2512.03444", "authors": ["Davood Soleymanzadeh", "Xiao Liang", "Minghui Zheng"], "title": "PerFACT: Motion Policy with LLM-Powered Dataset Synthesis and Fusion Action-Chunking Transformers", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Deep learning methods have significantly enhanced motion planning for robotic manipulators by leveraging prior experiences within planning datasets. However, state-of-the-art neural motion planners are primarily trained on small datasets collected in manually generated workspaces, limiting their generalizability to out-of-distribution scenarios. Additionally, these planners often rely on monolithic network architectures that struggle to encode critical planning information. To address these challenges, we introduce Motion Policy with Dataset Synthesis powered by large language models (LLMs) and Fusion Action-Chunking Transformers (PerFACT), which incorporates two key components. Firstly, a novel LLM-powered workspace generation method, MotionGeneralizer, enables large-scale planning data collection by producing a diverse set of semantically feasible workspaces. Secondly, we introduce Fusion Motion Policy Networks (MpiNetsFusion), a generalist neural motion planner that uses a fusion action-chunking transformer to better encode planning signals and attend to multiple feature modalities. Leveraging MotionGeneralizer, we collect 3.5M trajectories to train and evaluate MpiNetsFusion against state-of-the-art planners, which shows that the proposed MpiNetsFusion can plan several times faster on the evaluated tasks.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper directly combines large language models (LLMs) with motion planning, which is closely related to trajectory prediction. It uses LLMs for dataset synthesis to improve motion planning performance. The term 'motion policy' and the focus on planning datasets also indicate a strong connection to trajectory prediction.", "keywords": ["large language models", "LLMs", "motion planning", "motion policy", "dataset synthesis", "trajectory prediction"]}}
{"id": "2512.03756", "pdf": "https://arxiv.org/pdf/2512.03756", "abs": "https://arxiv.org/abs/2512.03756", "authors": ["Marlon Steiner", "Royden Wagner", "\u00d6mer Sahin Tas", "Christoph Stiller"], "title": "Prediction-Driven Motion Planning: Route Integration Strategies in Attention-Based Prediction Models", "categories": ["cs.RO"], "comment": "In Proceedings of the IEEE International Conference on Intelligent Transportation Systems (ITSC), Gold Coast, AUSTRALIA, 18-21 November 2025", "summary": "Combining motion prediction and motion planning offers a promising framework for enhancing interactions between automated vehicles and other traffic participants. However, this introduces challenges in conditioning predictions on navigation goals and ensuring stable, kinematically feasible trajectories. Addressing the former challenge, this paper investigates the extension of attention-based motion prediction models with navigation information. By integrating the ego vehicle's intended route and goal pose into the model architecture, we bridge the gap between multi-agent motion prediction and goal-based motion planning. We propose and evaluate several architectural navigation integration strategies to our model on the nuPlan dataset. Our results demonstrate the potential of prediction-driven motion planning, highlighting how navigation information can enhance both prediction and planning tasks. Our implementation is at: https://github.com/KIT-MRT/future-motion.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on motion prediction, specifically in the context of automated vehicles and motion planning. While it doesn't explicitly use Large Language Models, it uses attention-based models for prediction and explores integrating navigation information, which aligns with trajectory prediction themes. The connection to LLMs is indirect but the focus on prediction earns it a moderate relevance score.", "keywords": ["motion prediction", "trajectory prediction", "attention-based models", "motion planning", "automated vehicles", "navigation"]}}
{"id": "2512.03795", "pdf": "https://arxiv.org/pdf/2512.03795", "abs": "https://arxiv.org/abs/2512.03795", "authors": ["Jia Hu", "Zhexi Lian", "Xuerun Yan", "Ruiang Bi", "Dou Shen", "Yu Ruan", "Haoran Wang"], "title": "MPCFormer: A physics-informed data-driven approach for explainable socially-aware autonomous driving", "categories": ["cs.RO", "cs.AI"], "comment": "17 pages, 18 figures", "summary": "Autonomous Driving (AD) vehicles still struggle to exhibit human-like behavior in highly dynamic and interactive traffic scenarios. The key challenge lies in AD's limited ability to interact with surrounding vehicles, largely due to a lack of understanding the underlying mechanisms of social interaction. To address this issue, we introduce MPCFormer, an explainable socially-aware autonomous driving approach with physics-informed and data-driven coupled social interaction dynamics. In this model, the dynamics are formulated into a discrete space-state representation, which embeds physics priors to enhance modeling explainability. The dynamics coefficients are learned from naturalistic driving data via a Transformer-based encoder-decoder architecture. To the best of our knowledge, MPCFormer is the first approach to explicitly model the dynamics of multi-vehicle social interactions. The learned social interaction dynamics enable the planner to generate manifold, human-like behaviors when interacting with surrounding traffic. By leveraging the MPC framework, the approach mitigates the potential safety risks typically associated with purely learning-based methods. Open-looped evaluation on NGSIM dataset demonstrates that MPCFormer achieves superior social interaction awareness, yielding the lowest trajectory prediction errors compared with other state-of-the-art approach. The prediction achieves an ADE as low as 0.86 m over a long prediction horizon of 5 seconds. Close-looped experiments in highly intense interaction scenarios, where consecutive lane changes are required to exit an off-ramp, further validate the effectiveness of MPCFormer. Results show that MPCFormer achieves the highest planning success rate of 94.67%, improves driving efficiency by 15.75%, and reduces the collision rate from 21.25% to 0.5%, outperforming a frontier Reinforcement Learning (RL) based planner.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on trajectory prediction for autonomous driving, specifically addressing social interaction dynamics. It uses a Transformer-based architecture, which, while not a Large Language Model in the traditional sense, shares architectural similarities and represents a large, data-driven model. The paper emphasizes trajectory prediction accuracy and human-like behavior, making it relevant to the trajectory prediction aspect. While it doesn't explicitly use a Large Language Model, the use of a Transformer for learning dynamics coefficients pushes it towards relevance.", "keywords": ["trajectory prediction", "autonomous driving", "transformer", "social interaction", "MPC", "NGSIM dataset", "dynamics"]}}
{"id": "2512.03936", "pdf": "https://arxiv.org/pdf/2512.03936", "abs": "https://arxiv.org/abs/2512.03936", "authors": ["Aron Distelzweig", "Yiwei Wang", "Faris Janjo\u0161", "Marcel Hallgarten", "Mihai Dobre", "Alexander Langmann", "Joschka Boedecker", "Johannes Betz"], "title": "Driving is a Game: Combining Planning and Prediction with Bayesian Iterative Best Response", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous driving planning systems perform nearly perfectly in routine scenarios using lightweight, rule-based methods but still struggle in dense urban traffic, where lane changes and merges require anticipating and influencing other agents. Modern motion predictors offer highly accurate forecasts, yet their integration into planning is mostly rudimental: discarding unsafe plans. Similarly, end-to-end models offer a one-way integration that avoids the challenges of joint prediction and planning modeling under uncertainty. In contrast, game-theoretic formulations offer a principled alternative but have seen limited adoption in autonomous driving. We present Bayesian Iterative Best Response (BIBeR), a framework that unifies motion prediction and game-theoretic planning into a single interaction-aware process. BIBeR is the first to integrate a state-of-the-art predictor into an Iterative Best Response (IBR) loop, repeatedly refining the strategies of the ego vehicle and surrounding agents. This repeated best-response process approximates a Nash equilibrium, enabling bidirectional adaptation where the ego both reacts to and shapes the behavior of others. In addition, our proposed Bayesian confidence estimation quantifies prediction reliability and modulates update strength, more conservative under low confidence and more decisive under high confidence. BIBeR is compatible with modern predictors and planners, combining the transparency of structured planning with the flexibility of learned models. Experiments show that BIBeR achieves an 11% improvement over state-of-the-art planners on highly interactive interPlan lane-change scenarios, while also outperforming existing approaches on standard nuPlan benchmarks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u9884\u6d4b\uff0c\u7279\u522b\u662f\u901a\u8fc7\u535a\u5f08\u8bba\u65b9\u6cd5\u5c06\u4e24\u8005\u7ed3\u5408\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u96c6\u6210\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u5668\uff0c\u5e76\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "motion prediction", "autonomous driving", "planning"]}}
{"id": "2512.03454", "pdf": "https://arxiv.org/pdf/2512.03454", "abs": "https://arxiv.org/abs/2512.03454", "authors": ["Haicheng Liao", "Huanming Shen", "Bonan Wang", "Yongkang Li", "Yihong Tang", "Chengyue Wang", "Dingyi Zhuang", "Kehua Chen", "Hai Yang", "Chengzhong Xu", "Zhenning Li"], "title": "Think Before You Drive: World Model-Inspired Multimodal Grounding for Autonomous Vehicles", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Interpreting natural-language commands to localize target objects is critical for autonomous driving (AD). Existing visual grounding (VG) methods for autonomous vehicles (AVs) typically struggle with ambiguous, context-dependent instructions, as they lack reasoning over 3D spatial relations and anticipated scene evolution. Grounded in the principles of world models, we propose ThinkDeeper, a framework that reasons about future spatial states before making grounding decisions. At its core is a Spatial-Aware World Model (SA-WM) that learns to reason ahead by distilling the current scene into a command-aware latent state and rolling out a sequence of future latent states, providing forward-looking cues for disambiguation. Complementing this, a hypergraph-guided decoder then hierarchically fuses these states with the multimodal input, capturing higher-order spatial dependencies for robust localization. In addition, we present DrivePilot, a multi-source VG dataset in AD, featuring semantic annotations generated by a Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)-prompted LLM pipeline. Extensive evaluations on six benchmarks, ThinkDeeper ranks #1 on the Talk2Car leaderboard and surpasses state-of-the-art baselines on DrivePilot, MoCAD, and RefCOCO/+/g benchmarks. Notably, it shows strong robustness and efficiency in challenging scenes (long-text, multi-agent, ambiguity) and retains superior performance even when trained on 50% of the data.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u95ee\u9898\uff0c\u6d89\u53ca\u5bf9\u672a\u6765\u573a\u666f\u6f14\u5316\u7684\u63a8\u7406\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86Retrieval-Augmented Generation (RAG) \u548c Chain-of-Thought (CoT)-prompted LLM pipeline\u751f\u6210\u6570\u636e\u96c6\u7684\u8bed\u4e49\u6807\u6ce8\uff0c\u8868\u660e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["autonomous driving", "world model", "Retrieval-Augmented Generation", "Chain-of-Thought", "LLM", "future spatial states", "scene evolution"]}}
{"id": "2512.03166", "pdf": "https://arxiv.org/pdf/2512.03166", "abs": "https://arxiv.org/abs/2512.03166", "authors": ["Aya Taourirte", "Md Sohag Mia"], "title": "Multi-Agent Reinforcement Learning and Real-Time Decision-Making in Robotic Soccer for Virtual Environments", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The deployment of multi-agent systems in dynamic, adversarial environments like robotic soccer necessitates real-time decision-making, sophisticated cooperation, and scalable algorithms to avoid the curse of dimensionality. While Reinforcement Learning (RL) offers a promising framework, existing methods often struggle with the multi-granularity of tasks (long-term strategy vs. instant actions) and the complexity of large-scale agent interactions. This paper presents a unified Multi-Agent Reinforcement Learning (MARL) framework that addresses these challenges. First, we establish a baseline using Proximal Policy Optimization (PPO) within a client-server architecture for real-time action scheduling, with PPO demonstrating superior performance (4.32 avg. goals, 82.9% ball control). Second, we introduce a Hierarchical RL (HRL) structure based on the options framework to decompose the problem into a high-level trajectory planning layer (modeled as a Semi-Markov Decision Process) and a low-level action execution layer, improving global strategy (avg. goals increased to 5.26). Finally, to ensure scalability, we integrate mean-field theory into the HRL framework, simplifying many-agent interactions into a single agent vs. the population average. Our mean-field actor-critic method achieves a significant performance boost (5.93 avg. goals, 89.1% ball control, 92.3% passing accuracy) and enhanced training stability. Extensive simulations of 4v4 matches in the Webots environment validate our approach, demonstrating its potential for robust, scalable, and cooperative behavior in complex multi-agent domains.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u8db3\u7403\u4e2d\u7684\u5e94\u7528\uff0c\u5176\u4e2d\u5305\u542b\u8f68\u8ff9\u89c4\u5212\uff08trajectory planning\uff09\u7684\u5185\u5bb9\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u867d\u7136\u8bba\u6587\u4e2d\u63d0\u5230\u4e86\u7b56\u7565\u5b66\u4e60\u548c\u51b3\u7b56\uff0c\u4f46\u91cd\u70b9\u5728\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u5b9e\u73b0\u548c\u4f18\u5316\uff0c\u800c\u975e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u6216\u51b3\u7b56\u3002", "keywords": ["trajectory planning", "reinforcement learning", "multi-agent reinforcement learning", "action execution"]}}
{"id": "2512.03429", "pdf": "https://arxiv.org/pdf/2512.03429", "abs": "https://arxiv.org/abs/2512.03429", "authors": ["Raul Steinmetz", "Fabio Demo Rosa", "Victor Augusto Kich", "Jair Augusto Bottega", "Ricardo Bedin Grando", "Daniel Fernando Tello Gamarra"], "title": "World Models for Autonomous Navigation of Terrestrial Robots from LIDAR Observations", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted for publication in the Journal of Intelligent and Fuzzy Systems", "summary": "Autonomous navigation of terrestrial robots using Reinforcement Learning (RL) from LIDAR observations remains challenging due to the high dimensionality of sensor data and the sample inefficiency of model-free approaches. Conventional policy networks struggle to process full-resolution LIDAR inputs, forcing prior works to rely on simplified observations that reduce spatial awareness and navigation robustness. This paper presents a novel model-based RL framework built on top of the DreamerV3 algorithm, integrating a Multi-Layer Perceptron Variational Autoencoder (MLP-VAE) within a world model to encode high-dimensional LIDAR readings into compact latent representations. These latent features, combined with a learned dynamics predictor, enable efficient imagination-based policy optimization. Experiments on simulated TurtleBot3 navigation tasks demonstrate that the proposed architecture achieves faster convergence and higher success rate compared to model-free baselines such as SAC, DDPG, and TD3. It is worth emphasizing that the DreamerV3-based agent attains a 100% success rate across all evaluated environments when using the full dataset of the Turtlebot3 LIDAR (360 readings), while model-free methods plateaued below 85%. These findings demonstrate that integrating predictive world models with learned latent representations enables more efficient and robust navigation from high-dimensional sensory data.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autonomous navigation of robots using reinforcement learning and a world model to process LIDAR data. While it doesn't directly involve large language models, it does deal with trajectory prediction in the context of robot navigation and uses a model-based approach to predict future states, which is relevant to the broader field of trajectory prediction. The 'world model' aspect has connections to predictive modeling which overlaps with some LLM concepts, though not directly.", "keywords": ["autonomous navigation", "trajectory prediction", "world model", "LIDAR", "reinforcement learning", "dynamics predictor"]}}
{"id": "2512.03538", "pdf": "https://arxiv.org/pdf/2512.03538", "abs": "https://arxiv.org/abs/2512.03538", "authors": ["Yuhang Huang", "Shilong Zou", "Jiazhao Zhang", "Xinwang Liu", "Ruizhen Hu", "Kai Xu"], "title": "AdaPower: Specializing World Foundation Models for Predictive Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "World Foundation Models (WFMs) offer remarkable visual dynamics simulation capabilities, yet their application to precise robotic control remains limited by the gap between generative realism and control-oriented precision. While existing approaches use WFMs as synthetic data generators, they suffer from high computational costs and underutilization of pre-trained VLA policies. We introduce \\textbf{AdaPower} (\\textbf{Ada}pt and Em\\textbf{power}), a lightweight adaptation framework that transforms general-purpose WFMs into specialist world models through two novel components: Temporal-Spatial Test-Time Training (TS-TTT) for inference-time adaptation and Memory Persistence (MP) for long-horizon consistency. Integrated within a Model Predictive Control framework, our adapted world model empowers pre-trained VLAs, achieving over 41\\% improvement in task success rates on LIBERO benchmarks without policy retraining, while preserving computational efficiency and generalist capabilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on adapting World Foundation Models (WFMs) for robotic manipulation using Model Predictive Control (MPC). While it doesn't directly predict trajectories of agents or vehicles, it uses WFMs for simulating visual dynamics and uses MPC which inherently involves predicting future states to control a system. The connection to trajectory prediction is through the use of MPC within the adapted WFM. It is relevant to large models as it uses World Foundation Models.", "keywords": ["World Foundation Models", "WFMs", "Model Predictive Control", "MPC"]}}
{"id": "2512.03639", "pdf": "https://arxiv.org/pdf/2512.03639", "abs": "https://arxiv.org/abs/2512.03639", "authors": ["Kilian Schweppe", "Anne-Kathrin Schmuck"], "title": "Context-Triggered Contingency Games for Strategic Multi-Agent Interaction", "categories": ["cs.RO"], "comment": null, "summary": "We address the challenge of reliable and efficient interaction in autonomous multi-agent systems, where agents must balance long-term strategic objectives with short-term dynamic adaptation. We propose context-triggered contingency games, a novel integration of strategic games derived from temporal logic specifications with dynamic contingency games solved in real time. Our two-layered architecture leverages strategy templates to guarantee satisfaction of high-level objectives, while a new factor-graph-based solver enables scalable, real-time model predictive control of dynamic interactions. The resulting framework ensures both safety and progress in uncertain, interactive environments. We validate our approach through simulations and hardware experiments in autonomous driving and robotic navigation, demonstrating efficient, reliable, and adaptive multi-agent interaction.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u548c\u81ea\u4e3b\u5bfc\u822a\uff0c\u5176\u4e2d\u63d0\u5230\u4e86\u81ea\u4e3b\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5e94\u7528\u573a\u666f\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u63d0\u51fa\u7684\u6846\u67b6\u53ef\u80fd\u53ef\u4ee5\u7ed3\u5408\u5927\u6a21\u578b\u8fdb\u884c\u7b56\u7565\u751f\u6210\u6216\u51b3\u7b56\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5e76\u975e\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["autonomous driving", "robotic navigation", "multi-agent interaction", "model predictive control"]}}
{"id": "2512.03350", "pdf": "https://arxiv.org/pdf/2512.03350", "abs": "https://arxiv.org/abs/2512.03350", "authors": ["Yu Yuan", "Tharindu Wickremasinghe", "Zeeshan Nadir", "Xijun Wang", "Yiheng Chi", "Stanley H. Chan"], "title": "SeeU: Seeing the Unseen World via 4D Dynamics-aware Generation", "categories": ["cs.CV"], "comment": "Project Page: https://yuyuanspace.com/SeeU/", "summary": "Images and videos are discrete 2D projections of the 4D world (3D space + time). Most visual understanding, prediction, and generation operate directly on 2D observations, leading to suboptimal performance. We propose SeeU, a novel approach that learns the continuous 4D dynamics and generate the unseen visual contents. The principle behind SeeU is a new 2D$\\to$4D$\\to$2D learning framework. SeeU first reconstructs the 4D world from sparse and monocular 2D frames (2D$\\to$4D). It then learns the continuous 4D dynamics on a low-rank representation and physical constraints (discrete 4D$\\to$continuous 4D). Finally, SeeU rolls the world forward in time, re-projects it back to 2D at sampled times and viewpoints, and generates unseen regions based on spatial-temporal context awareness (4D$\\to$2D). By modeling dynamics in 4D, SeeU achieves continuous and physically-consistent novel visual generation, demonstrating strong potentials in multiple tasks including unseen temporal generation, unseen spatial generation, and video editing.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u52304D\u52a8\u6001\u5efa\u6a21\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u5c24\u5176\u662f\u5728\u7406\u89e3\u548c\u751f\u6210\u672a\u89c1\u8fc7\u7684\u573a\u666f\u65b9\u9762\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u63d0\u51fa\u7684\u5b66\u4e60\u6846\u67b6\u53ef\u4ee5\u7528\u4e8e\u589e\u5f3a\u89c6\u89c9\u5185\u5bb9\u7684\u751f\u6210\u80fd\u529b\uff0c\u8fd9\u53ef\u80fd\u4e0e\u5229\u7528\u5927\u578b\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u4efb\u52a1\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u5f97\u5206\u4e2d\u7b49\u3002", "keywords": ["4D dynamics", "temporal generation", "spatial generation", "video editing", "prediction", "continuous 4D dynamics"]}}
{"id": "2512.03101", "pdf": "https://arxiv.org/pdf/2512.03101", "abs": "https://arxiv.org/abs/2512.03101", "authors": ["Congjing Zhang", "Feng Lin", "Xinyi Zhao", "Pei Guo", "Wei Li", "Lin Chen", "Chaoyue Zhao", "Shuai Huang"], "title": "ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on anomaly detection using MLLMs in complex environments, which could potentially include scenarios involving moving objects or trajectories. It explicitly mentions the use of Large Language Models (LLMs) and multi-modal LLMs (MLLMs). While it doesn't directly address trajectory prediction, the application of MLLMs to anomaly detection in complex environments suggests a possible connection, especially if the environment involves dynamic elements. The uncertainty quantification aspect could also be relevant to trajectory prediction where future paths are uncertain.", "keywords": ["Large Language Models", "MLLM", "anomaly detection", "complex environments", "uncertainty quantification"]}}
{"id": "2512.03369", "pdf": "https://arxiv.org/pdf/2512.03369", "abs": "https://arxiv.org/abs/2512.03369", "authors": ["Nan Zhou", "Huandong Wang", "Jiahao Li", "Han Li", "Yali Song", "Qiuhua Wang", "Yong Li", "Xinlei Chen"], "title": "FireSentry: A Multi-Modal Spatio-temporal Benchmark Dataset for Fine-Grained Wildfire Spread Forecasting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Fine-grained wildfire spread prediction is crucial for enhancing emergency response efficacy and decision-making precision. However, existing research predominantly focuses on coarse spatiotemporal scales and relies on low-resolution satellite data, capturing only macroscopic fire states while fundamentally constraining high-precision localized fire dynamics modeling capabilities. To bridge this gap, we present FireSentry, a provincial-scale multi-modal wildfire dataset characterized by sub-meter spatial and sub-second temporal resolution. Collected using synchronized UAV platforms, FireSentry provides visible and infrared video streams, in-situ environmental measurements, and manually validated fire masks. Building on FireSentry, we establish a comprehensive benchmark encompassing physics-based, data-driven, and generative models, revealing the limitations of existing mask-only approaches. Our analysis proposes FiReDiff, a novel dual-modality paradigm that first predicts future video sequences in the infrared modality, and then precisely segments fire masks in the mask modality based on the generated dynamics. FiReDiff achieves state-of-the-art performance, with video quality gains of 39.2% in PSNR, 36.1% in SSIM, 50.0% in LPIPS, 29.4% in FVD, and mask accuracy gains of 3.3% in AUPRC, 59.1% in F1 score, 42.9% in IoU, and 62.5% in MSE when applied to generative models. The FireSentry benchmark dataset and FiReDiff paradigm collectively advance fine-grained wildfire forecasting and dynamic disaster simulation. The processed benchmark dataset is publicly available at: https://github.com/Munan222/FireSentry-Benchmark-Dataset.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on wildfire spread forecasting, which can be considered a form of trajectory prediction, specifically the trajectory of the fire's spread. While it doesn't directly mention large language models, the use of generative models and the prediction of future video sequences suggest a connection to sequence modeling, a domain where large models are often applied. The use of multi-modal data also aligns with current trends in large model research.", "keywords": ["trajectory prediction", "generative models", "video sequence prediction", "spatio-temporal", "dynamic disaster simulation"]}}
{"id": "2512.03774", "pdf": "https://arxiv.org/pdf/2512.03774", "abs": "https://arxiv.org/abs/2512.03774", "authors": ["Johannes Fischer", "Marlon Steiner", "\u00d6mer Sahin Tas", "Christoph Stiller"], "title": "Safety Reinforced Model Predictive Control (SRMPC): Improving MPC with Reinforcement Learning for Motion Planning in Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Model predictive control (MPC) is widely used for motion planning, particularly in autonomous driving. Real-time capability of the planner requires utilizing convex approximation of optimal control problems (OCPs) for the planner. However, such approximations confine the solution to a subspace, which might not contain the global optimum. To address this, we propose using safe reinforcement learning (SRL) to obtain a new and safe reference trajectory within MPC. By employing a learning-based approach, the MPC can explore solutions beyond the close neighborhood of the previous one, potentially finding global optima. We incorporate constrained reinforcement learning (CRL) to ensure safety in automated driving, using a handcrafted energy function-based safety index as the constraint objective to model safe and unsafe regions. Our approach utilizes a state-dependent Lagrangian multiplier, learned concurrently with the safe policy, to solve the CRL problem. Through experimentation in a highway scenario, we demonstrate the superiority of our approach over both MPC and SRL in terms of safety and performance measures.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on motion planning in autonomous driving using Model Predictive Control (MPC) and Reinforcement Learning (RL). While it's related to trajectory planning (a subfield of trajectory prediction), it doesn't directly involve Large Language Models. The use of RL could be seen as a connection to large models in a broader sense, but the paper's core contribution is in improving MPC with RL for safety-critical applications, not in leveraging LLMs.", "keywords": ["motion planning", "trajectory planning", "model predictive control", "reinforcement learning", "autonomous driving"]}}
{"id": "2512.03886", "pdf": "https://arxiv.org/pdf/2512.03886", "abs": "https://arxiv.org/abs/2512.03886", "authors": ["Brais Fontan-Costas", "M. Diaz-Cacho", "Ruben Fernandez-Boullon", "Manuel Alonso-Carracedo", "Javier Perez-Robles"], "title": "A Modular Architecture Design for Autonomous Driving Racing in Controlled Environments", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "This paper presents an Autonomous System (AS) architecture for vehicles in a closed circuit. The AS performs precision tasks including computer vision for environment perception, positioning and mapping for accurate localization, path planning for optimal trajectory generation, and control for precise vehicle actuation. Each subsystem operates independently while connecting data through a cohesive pipeline architecture. The system implements a modular design that combines state-of-the-art technologies for real-time autonomous navigation in controlled environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on autonomous driving, specifically path planning and trajectory generation, which are closely related to trajectory prediction. However, it does not mention or utilize large language models. The relevance score reflects the connection to trajectory prediction but the absence of any LLM component.", "keywords": ["trajectory generation", "path planning", "autonomous driving"]}}
{"id": "2512.03724", "pdf": "https://arxiv.org/pdf/2512.03724", "abs": "https://arxiv.org/abs/2512.03724", "authors": ["Ziwen Li", "Xin Wang", "Hanlue Zhang", "Runnan Chen", "Runqi Lin", "Xiao He", "Han Huang", "Yandong Guo", "Fakhri Karray", "Tongliang Liu", "Mingming Gong"], "title": "PosA-VLA: Enhancing Action Generation via Pose-Conditioned Anchor Attention", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The Vision-Language-Action (VLA) models have demonstrated remarkable performance on embodied tasks and shown promising potential for real-world applications. However, current VLAs still struggle to produce consistent and precise target-oriented actions, as they often generate redundant or unstable motions along trajectories, limiting their applicability in time-sensitive scenarios.In this work, we attribute these redundant actions to the spatially uniform perception field of existing VLAs, which causes them to be distracted by target-irrelevant objects, especially in complex environments.To address this issue, we propose an efficient PosA-VLA framework that anchors visual attention via pose-conditioned supervision, consistently guiding the model's perception toward task-relevant regions. The pose-conditioned anchor attention mechanism enables the model to better align instruction semantics with actionable visual cues, thereby improving action generation precision and efficiency. Moreover, our framework adopts a lightweight architecture and requires no auxiliary perception modules (e.g., segmentation or grounding networks), ensuring efficient inference. Extensive experiments verify that our method executes embodied tasks with precise and time-efficient behavior across diverse robotic manipulation benchmarks and shows robust generalization in a variety of challenging environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving action generation in Vision-Language-Action models, which are relevant to trajectory prediction because actions often define trajectories. While it doesn't directly use Large Language Models (LLMs) in the traditional sense, it builds upon models that process language and visual information to generate actions, which can be considered a form of trajectory prediction or control. The connection to trajectory prediction is through the generated actions forming trajectories. The mention of 'Vision-Language-Action' models suggests a connection to large models, even if it's not explicitly an LLM.", "keywords": ["action generation", "Vision-Language-Action models", "embodied tasks", "trajectory", "pose-conditioned"]}}
{"id": "2512.03520", "pdf": "https://arxiv.org/pdf/2512.03520", "abs": "https://arxiv.org/abs/2512.03520", "authors": ["Yiyi Cai", "Yuhan Wu", "Kunhang Li", "You Zhou", "Bo Zheng", "Haiyang Liu"], "title": "FloodDiffusion: Tailored Diffusion Forcing for Streaming Motion Generation", "categories": ["cs.CV"], "comment": "15 pages, 7 figures", "summary": "We present FloodDiffusion, a new framework for text-driven, streaming human motion generation. Given time-varying text prompts, FloodDiffusion generates text-aligned, seamless motion sequences with real-time latency. Unlike existing methods that rely on chunk-by-chunk or auto-regressive model with diffusion head, we adopt a diffusion forcing framework to model this time-series generation task under time-varying control events. We find that a straightforward implementation of vanilla diffusion forcing (as proposed for video models) fails to model real motion distributions. We demonstrate that to guarantee modeling the output distribution, the vanilla diffusion forcing must be tailored to: (i) train with a bi-directional attention instead of casual attention; (ii) implement a lower triangular time scheduler instead of a random one; (iii) utilize a continues time-varying way to introduce text conditioning. With these improvements, we demonstrate in the first time that the diffusion forcing-based framework achieves state-of-the-art performance on the streaming motion generation task, reaching an FID of 0.057 on the HumanML3D benchmark. Models, code, and weights are available. https://shandaai.github.io/FloodDiffusion/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53cahuman motion generation\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u7279\u6b8a\u60c5\u51b5\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4f7f\u7528\u4e86diffusion\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u5927\u578b\u751f\u6210\u6a21\u578b\uff0c\u4e0e\u5927\u6a21\u578b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528LLM\uff0c\u4f46text-driven\u7684\u7279\u6027\u8868\u660e\u4e0eLLM\u6709\u6f5c\u5728\u7684\u7ed3\u5408\u53ef\u80fd\u6027\u3002", "keywords": ["motion generation", "diffusion model", "time-series generation", "text-driven"]}}
{"id": "2512.03532", "pdf": "https://arxiv.org/pdf/2512.03532", "abs": "https://arxiv.org/abs/2512.03532", "authors": ["Zhishan Zhou", "Siyuan Wei", "Zengran Wang", "Chunjie Wang", "Xiaosheng Yan", "Xiao Liu"], "title": "OpenTrack3D: Towards Accurate and Generalizable Open-Vocabulary 3D Instance Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Generalizing open-vocabulary 3D instance segmentation (OV-3DIS) to diverse, unstructured, and mesh-free environments is crucial for robotics and AR/VR, yet remains a significant challenge. We attribute this to two key limitations of existing methods: (1) proposal generation relies on dataset-specific proposal networks or mesh-based superpoints, rendering them inapplicable in mesh-free scenarios and limiting generalization to novel scenes; and (2) the weak textual reasoning of CLIP-based classifiers, which struggle to recognize compositional and functional user queries. To address these issues, we introduce OpenTrack3D, a generalizable and accurate framework. Unlike methods that rely on pre-generated proposals, OpenTrack3D employs a novel visual-spatial tracker to construct cross-view consistent object proposals online. Given an RGB-D stream, our pipeline first leverages a 2D open-vocabulary segmenter to generate masks, which are lifted to 3D point clouds using depth. Mask-guided instance features are then extracted using DINO feature maps, and our tracker fuses visual and spatial cues to maintain instance consistency. The core pipeline is entirely mesh-free, yet we also provide an optional superpoints refinement module to further enhance performance when scene mesh is available. Finally, we replace CLIP with a multi-modal large language model (MLLM), significantly enhancing compositional reasoning for complex user queries. Extensive experiments on diverse benchmarks, including ScanNet200, Replica, ScanNet++, and SceneFun3D, demonstrate state-of-the-art performance and strong generalization capabilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5f00\u653e\u8bcd\u6c473D\u5b9e\u4f8b\u5206\u5272\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u4f7f\u7528\u89c6\u89c9-\u7a7a\u95f4\u8ddf\u8e2a\u5668\u6784\u5efa\u5bf9\u8c61\u63d0\u8bae\u4ee5\u53ca\u4f7f\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6765\u589e\u5f3a\u590d\u6742\u7528\u6237\u67e5\u8be2\u7684\u7ec4\u5408\u63a8\u7406\u3002 \u867d\u7136\u4e3b\u8981\u76ee\u6807\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u8ddf\u8e2a\u5668\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u660e\u786e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["large language model", "MLLM", "tracker"]}}
