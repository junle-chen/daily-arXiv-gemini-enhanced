{"id": "2511.17855", "pdf": "https://arxiv.org/pdf/2511.17855", "abs": "https://arxiv.org/abs/2511.17855", "authors": ["Jordan Abi Nader", "David Lee", "Nathaniel Dennler", "Andreea Bobu"], "title": "QuickLAP: Quick Language-Action Preference Learning for Autonomous Driving Agents", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Robots must learn from both what people do and what they say, but either modality alone is often incomplete: physical corrections are grounded but ambiguous in intent, while language expresses high-level goals but lacks physical grounding. We introduce QuickLAP: Quick Language-Action Preference learning, a Bayesian framework that fuses physical and language feedback to infer reward functions in real time. Our key insight is to treat language as a probabilistic observation over the user's latent preferences, clarifying which reward features matter and how physical corrections should be interpreted. QuickLAP uses Large Language Models (LLMs) to extract reward feature attention masks and preference shifts from free-form utterances, which it integrates with physical feedback in a closed-form update rule. This enables fast, real-time, and robust reward learning that handles ambiguous feedback. In a semi-autonomous driving simulator, QuickLAP reduces reward learning error by over 70% compared to physical-only and heuristic multimodal baselines. A 15-participant user study further validates our approach: participants found QuickLAP significantly more understandable and collaborative, and preferred its learned behavior over baselines. Code is available at https://github.com/MIT-CLEAR-Lab/QuickLAP.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper is highly relevant as it combines Large Language Models (LLMs) with autonomous driving agents, which inherently involves trajectory prediction and path planning. The abstract mentions using LLMs to extract reward feature attention masks and preference shifts from language, which are then integrated with physical feedback for real-time reward learning in a driving simulator. This directly relates to improving the behavior and trajectory planning of autonomous vehicles using language understanding.", "keywords": ["Large Language Models", "LLMs", "autonomous driving", "trajectory prediction", "reward learning", "action preference learning"]}}
{"id": "2511.18270", "pdf": "https://arxiv.org/pdf/2511.18270", "abs": "https://arxiv.org/abs/2511.18270", "authors": ["Zhongkai Chen", "Yihao Sun", "Chao Yan", "Han Zhou", "Xiaojia Xiang", "Jie Jiang"], "title": "Skypilot: Fine-Tuning LLM with Physical Grounding for AAV Coverage Search", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous aerial vehicles (AAVs) have played a pivotal role in coverage operations and search missions. Recent advances in large language models (LLMs) offer promising opportunities to augment AAV intelligence. These advances help address complex challenges like area coverage optimization, dynamic path planning, and adaptive decision-making. However, the absence of physical grounding in LLMs leads to hallucination and reproducibility problems in spatial reasoning and decision-making. To tackle these issues, we present Skypilot, an LLM-enhanced two-stage framework that grounds language models in physical reality by integrating monte carlo tree search (MCTS). In the first stage, we introduce a diversified action space that encompasses generate, regenerate, fine-tune, and evaluate operations, coupled with physics-informed reward functions to ensure trajectory feasibility. In the second stage, we fine-tune Qwen3-4B on 23,000 MCTS-generated samples, achieving substantial inference acceleration while maintaining solution quality. Extensive numerical simulations and real-world flight experiments validate the efficiency and superiority of our proposed approach. Detailed information and experimental results are accessible at https://sky-pilot.top.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper is highly relevant as it explicitly combines Large Language Models (LLMs) with path planning and decision-making for Autonomous Aerial Vehicles (AAVs), which falls under the umbrella of trajectory prediction/path planning for mobile objects. The abstract mentions dynamic path planning, trajectory feasibility, and fine-tuning an LLM (Qwen3-4B) for this purpose.", "keywords": ["trajectory prediction", "large language models", "LLMs", "path planning", "autonomous aerial vehicles", "AAV", "monte carlo tree search", "MCTS", "fine-tuning", "Qwen3-4B"]}}
{"id": "2511.17681", "pdf": "https://arxiv.org/pdf/2511.17681", "abs": "https://arxiv.org/abs/2511.17681", "authors": ["Weiyi Lv", "Ning Zhang", "Hanyang Sun", "Haoran Jiang", "Kai Zhao", "Jing Xiao", "Dan Zeng"], "title": "Vision-Motion-Reference Alignment for Referring Multi-Object Tracking via Multi-Modal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Referring Multi-Object Tracking (RMOT) extends conventional multi-object tracking (MOT) by introducing natural language references for multi-modal fusion tracking. RMOT benchmarks only describe the object's appearance, relative positions, and initial motion states. This so-called static regulation fails to capture dynamic changes of the object motion, including velocity changes and motion direction shifts. This limitation not only causes a temporal discrepancy between static references and dynamic vision modality but also constrains multi-modal tracking performance. To address this limitation, we propose a novel Vision-Motion-Reference aligned RMOT framework, named VMRMOT. It integrates a motion modality extracted from object dynamics to enhance the alignment between vision modality and language references through multi-modal large language models (MLLMs). Specifically, we introduce motion-aware descriptions derived from object dynamic behaviors and, leveraging the powerful temporal-reasoning capabilities of MLLMs, extract motion features as the motion modality. We further design a Vision-Motion-Reference Alignment (VMRA) module to hierarchically align visual queries with motion and reference cues, enhancing their cross-modal consistency. In addition, a Motion-Guided Prediction Head (MGPH) is developed to explore motion modality to enhance the performance of the prediction head. To the best of our knowledge, VMRMOT is the first approach to employ MLLMs in the RMOT task for vision-reference alignment. Extensive experiments on multiple RMOT benchmarks demonstrate that VMRMOT outperforms existing state-of-the-art methods.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5e94\u7528\u4e8e\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u5e76\u4e14\u8003\u8651\u4e86\u7269\u4f53\u7684\u8fd0\u52a8\u4fe1\u606f\uff08motion modality\uff09\uff0c\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u4f46\u662f\u6d89\u53ca\u4e86\u7269\u4f53\u52a8\u6001\u53d8\u5316\uff0c\u5982\u901f\u5ea6\u548c\u65b9\u5411\u53d8\u5316\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u540c\u65f6\uff0c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86multi-modal large language models (MLLMs)\u3002", "keywords": ["multi-object tracking", "large language models", "motion modality", "MLLMs", "object dynamics"]}}
{"id": "2511.18874", "pdf": "https://arxiv.org/pdf/2511.18874", "abs": "https://arxiv.org/abs/2511.18874", "authors": ["Yuzhi Chen", "Yuanchang Xie", "Lei Zhao", "Pan Liu", "Yajie Zou", "Chen Wang"], "title": "GContextFormer: A global context-aware hybrid multi-head attention approach with scaled additive aggregation for multimodal trajectory prediction", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.RO", "cs.SI"], "comment": null, "summary": "Multimodal trajectory prediction generates multiple plausible future trajectories to address vehicle motion uncertainty from intention ambiguity and execution variability. However, HD map-dependent models suffer from costly data acquisition, delayed updates, and vulnerability to corrupted inputs, causing prediction failures. Map-free approaches lack global context, with pairwise attention over-amplifying straight patterns while suppressing transitional patterns, resulting in motion-intention misalignment. This paper proposes GContextFormer, a plug-and-play encoder-decoder architecture with global context-aware hybrid attention and scaled additive aggregation achieving intention-aligned multimodal prediction without map reliance. The Motion-Aware Encoder builds scene-level intention prior via bounded scaled additive aggregation over mode-embedded trajectory tokens and refines per-mode representations under shared global context, mitigating inter-mode suppression and promoting intention alignment. The Hierarchical Interaction Decoder decomposes social reasoning into dual-pathway cross-attention: a standard pathway ensures uniform geometric coverage over agent-mode pairs while a neighbor-context-enhanced pathway emphasizes salient interactions, with gating module mediating their contributions to maintain coverage-focus balance. Experiments on eight highway-ramp scenarios from TOD-VT dataset show GContextFormer outperforms state-of-the-art baselines. Compared to existing transformer models, GContextFormer achieves greater robustness and concentrated improvements in high-curvature and transition zones via spatial distributions. Interpretability is achieved through motion mode distinctions and neighbor context modulation exposing reasoning attribution. The modular architecture supports extensibility toward cross-domain multimodal reasoning tasks. Source: https://fenghy-chen.github.io/sources/.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u4e86Transformer\u67b6\u6784\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u4f7f\u7528\u4e86\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4f46\u4e0d\u662fLLM\u7684\u5e94\u7528\uff0c\u800c\u662f\u7528\u4e8e\u5efa\u6a21\u8f66\u8f86\u4e4b\u95f4\u7684\u4ea4\u4e92\u548c\u573a\u666f\u7406\u89e3\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4f46\u672a\u8fbe\u5230\u5b8c\u5168\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "multimodal trajectory prediction", "transformer", "attention", "vehicle motion", "motion prediction"]}}
{"id": "2511.17941", "pdf": "https://arxiv.org/pdf/2511.17941", "abs": "https://arxiv.org/abs/2511.17941", "authors": ["Xiangyan Kong", "Xuecheng Wu", "Xiongwei Zhao", "Xiaodong Li", "Yunyun Shi", "Gang Wang", "Dingkang Yang", "Yang Liu", "Hong Chen", "Yulong Gao"], "title": "V2X-RECT: An Efficient V2X Trajectory Prediction Framework via Redundant Interaction Filtering and Tracking Error Correction", "categories": ["cs.CV"], "comment": null, "summary": "V2X prediction can alleviate perception incompleteness caused by limited line of sight through fusing trajectory data from infrastructure and vehicles, which is crucial to traffic safety and efficiency. However, in dense traffic scenarios, frequent identity switching of targets hinders cross-view association and fusion. Meanwhile, multi-source information tends to generate redundant interactions during the encoding stage, and traditional vehicle-centric encoding leads to large amounts of repetitive historical trajectory feature encoding, degrading real-time inference performance. To address these challenges, we propose V2X-RECT, a trajectory prediction framework designed for high-density environments. It enhances data association consistency, reduces redundant interactions, and reuses historical information to enable more efficient and accurate prediction. Specifically, we design a multi-source identity matching and correction module that leverages multi-view spatiotemporal relationships to achieve stable and consistent target association, mitigating the adverse effects of mismatches on trajectory encoding and cross-view feature fusion. Then we introduce traffic signal-guided interaction module, encoding trend of traffic light changes as features and exploiting their role in constraining spatiotemporal passage rights to accurately filter key interacting vehicles, while capturing the dynamic impact of signal changes on interaction patterns. Furthermore, a local spatiotemporal coordinate encoding enables reusable features of historical trajectories and map, supporting parallel decoding and significantly improving inference efficiency. Extensive experimental results across V2X-Seq and V2X-Traj datasets demonstrate that our V2X-RECT achieves significant improvements compared to SOTA methods, while also enhancing robustness and inference efficiency across diverse traffic densities.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on trajectory prediction in V2X environments. While it doesn't directly involve large language models, it addresses a core problem in trajectory prediction (V2X prediction) using novel techniques for data association, interaction filtering, and efficient encoding, which are relevant to the broader field.", "keywords": ["trajectory prediction", "V2X", "vehicle trajectory prediction", "data association", "interaction filtering", "traffic signal"]}}
{"id": "2511.17889", "pdf": "https://arxiv.org/pdf/2511.17889", "abs": "https://arxiv.org/abs/2511.17889", "authors": ["Ting Huang", "Dongjian Li", "Rui Yang", "Zeyu Zhang", "Zida Yang", "Hao Tang"], "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u5c06\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fde\u7eed\u63a7\u5236\uff0c\u5176\u4e2d\u6d89\u53ca\u5230\u8f68\u8ff9\u7684\u751f\u6210\u548c\u6267\u884c\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u673a\u5668\u4eba\u63a7\u5236\u548c\u8def\u5f84\u89c4\u5212\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002 \u6458\u8981\u4e2d\u63d0\u5230\u4e86\u5927\u578b\u6570\u636e\u96c6\u548c\u94fe\u5f0f\u601d\u8003(CoT)\uff0c\u8868\u660e\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\u6216\u76f8\u5173\u6280\u672f\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["vision-language-action", "quadruped robots", "continuous control", "embodied trajectories", "chain-of-thought", "reinforcement learning", "large-scale dataset"]}}
{"id": "2511.18112", "pdf": "https://arxiv.org/pdf/2511.18112", "abs": "https://arxiv.org/abs/2511.18112", "authors": ["Min Lin", "Xiwen Liang", "Bingqian Lin", "Liu Jingzhi", "Zijian Jiao", "Kehan Li", "Yuhan Ma", "Yuecheng Liu", "Shen Zhao", "Yuzheng Zhuang", "Xiaodan Liang"], "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $\u03c0_{0.5}$ by +0.08 and +0.11.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u5728\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u6d89\u53ca\u4e86\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u7684\u534f\u8c03\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u79cd\u5e7f\u4e49\u4e0a\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6765\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language-Action model", "mobile manipulation", "navigation", "multimodal large language model", "MLLM", "trajectory"]}}
{"id": "2511.17792", "pdf": "https://arxiv.org/pdf/2511.17792", "abs": "https://arxiv.org/abs/2511.17792", "authors": ["Dingrui Wang", "Hongyuan Ye", "Zhihao Liang", "Zhexiao Sun", "Zhaowei Lu", "Yuchen Zhang", "Yuyu Zhao", "Yuan Gao", "Marvin Seegert", "Finn Sch\u00e4fer", "Haotong Qin", "Wei Li", "Luigi Palmieri", "Felix Jahncke", "Mattia Piccinini", "Johannes Betz"], "title": "Target-Bench: Can World Models Achieve Mapless Path Planning with Semantic Targets?", "categories": ["cs.CV", "cs.RO"], "comment": "10 pages", "summary": "While recent world models generate highly realistic videos, their ability to perform robot path planning remains unclear and unquantified. We introduce Target-Bench, the first benchmark specifically designed to evaluate world models on mapless path planning toward semantic targets in real-world environments. Target-Bench provides 450 robot-collected video sequences spanning 45 semantic categories with SLAM-based ground truth trajectories. Our evaluation pipeline recovers camera motion from generated videos and measures planning performance using five complementary metrics that quantify target-reaching capability, trajectory accuracy, and directional consistency. We evaluate state-of-the-art models including Sora 2, Veo 3.1, and the Wan series. The best off-the-shelf model (Wan2.2-Flash) achieves only 0.299 overall score, revealing significant limitations in current world models for robotic planning tasks. We show that fine-tuning an open-source 5B-parameter model on only 325 scenarios from our dataset achieves 0.345 overall score -- an improvement of more than 400% over its base version (0.066) and 15% higher than the best off-the-shelf model. We will open-source the code and dataset.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on path planning using world models, which are a type of large model. While it doesn't explicitly mention trajectory prediction, path planning is closely related. The benchmark uses robot trajectories as ground truth. The involvement of large models (Sora, Veo, Wan series, and a 5B-parameter model) increases the relevance.", "keywords": ["path planning", "world models", "large models", "robot trajectories"]}}
{"id": "2511.18845", "pdf": "https://arxiv.org/pdf/2511.18845", "abs": "https://arxiv.org/abs/2511.18845", "authors": ["Changxin Huang", "Lv Tang", "Zhaohuan Zhan", "Lisha Yu", "Runhao Zeng", "Zun Liu", "Zhengjie Wang", "Jianqiang Li"], "title": "UNeMo: Collaborative Visual-Language Reasoning and Navigation via a Multimodal World Model", "categories": ["cs.AI"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires agents to autonomously navigate complex environments via visual images and natural language instruction--remains highly challenging. Recent research on enhancing language-guided navigation reasoning using pre-trained large language models (LLMs) has shown promising prospects. However, the reasoning of such methods is limited to the linguistic modality, lacking visual reasoning capabilities. Moreover, existing reasoning modules are optimized separately from navigation policies, leading to incompatibility and potential conflicts in optimization objectives. To tackle these challenges, we introduce UNeMo, a novel framework designed for the collaborative optimization of visual state reasoning and navigational decision-making. It introduces a Multimodal World Model (MWM) that takes visual features, language instructions, and navigational actions as inputs to jointly predict subsequent visual states, enabling cross-modal reasoning. Via a Hierarchical Prediction-Feedback (HPN) mechanism, MWM collaborates with navigation policies: the first layer generates actions using current vision-and-language features; MWM then infers post-action visual states to guide the second layer's fine-grained decisions. This forms a dynamic bidirectional promotion mechanism where MWM reasoning optimizes navigation policies, while policy decisions feedback to improve MWM's reasoning accuracy. Experiments on R2R and REVERIE datasets show UNeMo outperforms state-of-the-art methods by 2.1% and 0.7% in navigation accuracy for unseen scenes, validating its effectiveness.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on Vision-and-Language Navigation (VLN), which involves navigating environments using visual images and natural language instructions. It utilizes large language models (LLMs) for language-guided navigation reasoning and introduces a Multimodal World Model (MWM) that predicts subsequent visual states based on visual features, language instructions, and navigational actions. While not directly trajectory prediction, navigation inherently involves predicting future states and actions. The use of LLMs and the prediction of visual states connect it to the specified themes, although the primary focus is on navigation rather than trajectory prediction in isolation.", "keywords": ["Vision-and-Language Navigation", "LLMs", "large language models", "multimodal world model", "navigation", "action prediction"]}}
{"id": "2511.17609", "pdf": "https://arxiv.org/pdf/2511.17609", "abs": "https://arxiv.org/abs/2511.17609", "authors": ["Linh Van Ma", "Unse Fatima", "Tepy Sokun Chriv", "Haroon Imran", "Moongu Jeon"], "title": "3D Ground Truth Reconstruction from Multi-Camera Annotations Using UKF", "categories": ["cs.CV"], "comment": "International Conference on Control, Automation and Information Sciences (ICCAIS) 2025, October 27 - 29, 2025 | Jeju, Korea", "summary": "Accurate 3D ground truth estimation is critical for applications such as autonomous navigation, surveillance, and robotics. This paper introduces a novel method that uses an Unscented Kalman Filter (UKF) to fuse 2D bounding box or pose keypoint ground truth annotations from multiple calibrated cameras into accurate 3D ground truth. By leveraging human-annotated ground-truth 2D, our proposed method, a multi-camera single-object tracking algorithm, transforms 2D image coordinates into robust 3D world coordinates through homography-based projection and UKF-based fusion. Our proposed algorithm processes multi-view data to estimate object positions and shapes while effectively handling challenges such as occlusion. We evaluate our method on the CMC, Wildtrack, and Panoptic datasets, demonstrating high accuracy in 3D localization compared to the available 3D ground truth. Unlike existing approaches that provide only ground-plane information, our method also outputs the full 3D shape of each object. Additionally, the algorithm offers a scalable and fully automatic solution for multi-camera systems using only 2D image annotations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 3D ground truth reconstruction for object tracking using multi-camera annotations and an Unscented Kalman Filter (UKF). While it doesn't directly involve Large Language Models, it relates to trajectory prediction by providing more accurate 3D ground truth data, which is crucial for training and evaluating trajectory prediction models. The core method focuses on improving the accuracy of the data used in trajectory prediction, therefore has some relevance.", "keywords": ["trajectory prediction", "object tracking", "3D ground truth", "Kalman Filter"]}}
{"id": "2511.17798", "pdf": "https://arxiv.org/pdf/2511.17798", "abs": "https://arxiv.org/abs/2511.17798", "authors": ["Francesco D'Orazio", "Sepehr Samavi", "Xintong Du", "Siqi Zhou", "Giuseppe Oriolo", "Angela P. Schoellig"], "title": "SM$^2$ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control", "categories": ["cs.RO"], "comment": null, "summary": "Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u5b89\u5168\u6027\u548c\u4eba\u7c7b\u4ea4\u4e92\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\u3002\u867d\u7136\u6d89\u53ca\u4e86\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u6216\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u76f8\u5173\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u65b9\u9762\u3002", "keywords": ["human motion prediction", "trajectory prediction", "model predictive control"]}}
{"id": "2511.18170", "pdf": "https://arxiv.org/pdf/2511.18170", "abs": "https://arxiv.org/abs/2511.18170", "authors": ["Kaier Liang", "Licheng Luo", "Yixuan Wang", "Mingyu Cai", "Cristian Ioan Vasile"], "title": "Time-aware Motion Planning in Dynamic Environments with Conformal Prediction", "categories": ["cs.RO"], "comment": null, "summary": "Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u5229\u7528 conformal prediction \u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u4ece\u800c\u5b9e\u73b0\u5b89\u5168\u5bfc\u822a\u3002 \u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u751f\u6210\u548c\u9884\u6d4b\uff08obstacle trajectory predictions\uff09\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002Conformal Prediction\u65b9\u6cd5\u672c\u8eab\u4e0e\u5927\u6a21\u578b\u5e76\u975e\u76f4\u63a5\u76f8\u5173\uff0c\u53ea\u662f\u4f5c\u4e3a\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5de5\u5177\u5e94\u7528\u3002", "keywords": ["motion planning", "trajectory generation", "obstacle trajectory predictions", "conformal prediction", "dynamic environments"]}}
{"id": "2511.18183", "pdf": "https://arxiv.org/pdf/2511.18183", "abs": "https://arxiv.org/abs/2511.18183", "authors": ["Yixuan Jia", "Qingyuan Li", "Jonathan P. How"], "title": "Off-Road Navigation via Implicit Neural Representation of Terrain Traversability", "categories": ["cs.RO"], "comment": "9 pages", "summary": "Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on off-road navigation and trajectory optimization using neural representations. While it doesn't directly involve Large Language Models, it does deal with trajectory prediction and path planning, making it somewhat relevant. The use of neural representation can be considered a form of learned model, however it is not a Large Language Model.", "keywords": ["trajectory optimization", "path planning", "off-road navigation", "neural representation"]}}
{"id": "2511.18368", "pdf": "https://arxiv.org/pdf/2511.18368", "abs": "https://arxiv.org/abs/2511.18368", "authors": ["Yue Hu", "Xiaoming He", "Rui Yuan", "Shahid Mumtaz"], "title": "Wireless Power Transfer and Intent-Driven Network Optimization in AAVs-assisted IoT for 6G Sustainable Connectivity", "categories": ["cs.AI"], "comment": null, "summary": "Autonomous Aerial Vehicle (AAV)-assisted Internet of Things (IoT) represents a collaborative architecture in which AAV allocate resources over 6G links to jointly enhance user-intent interpretation and overall network performance. Owing to this mutual dependence, improvements in intent inference and policy decisions on one component reinforce the efficiency of others, making highly reliable intent prediction and low-latency action execution essential. Although numerous approaches can model intent relationships, they encounter severe obstacles when scaling to high-dimensional action sequences and managing intensive on-board computation. We propose an Intent-Driven Framework for Autonomous Network Optimization comprising prediction and decision modules. First, implicit intent modeling is adopted to mitigate inaccuracies arising from ambiguous user expressions. For prediction, we introduce Hyperdimensional Transformer (HDT), which embeds data into a Hyperdimensional space via Hyperdimensional vector encoding and replaces standard matrix and attention operations with symbolic Hyperdimensional computations. For decision-making, where AAV must respond to user intent while planning trajectories, we design Double Actions based Multi-Agent Proximal Policy Optimization (DA-MAPPO). Building upon MAPPO, it samples actions through two independently parameterized networks and cascades the user-intent network into the trajectory network to maintain action dependencies. We evaluate our framework on a real IoT action dataset with authentic wireless data. Experimental results demonstrate that HDT and DA-MAPPO achieve superior performance across diverse scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on intent-driven network optimization for AAVs, which involves trajectory planning and decision-making. While it doesn't directly use large language models, it employs a 'Hyperdimensional Transformer' for intent prediction and a modified MAPPO for trajectory planning, indicating a connection to the broader field of sequential prediction and potentially leveraging transformer-like architectures. The 'intent prediction' aspect is closely related to predicting future actions, which falls under trajectory prediction. However, the primary focus is on network optimization, not trajectory prediction itself, hence the moderate score.", "keywords": ["intent prediction", "trajectory planning", "Hyperdimensional Transformer", "MAPPO", "action prediction"]}}
{"id": "2511.18450", "pdf": "https://arxiv.org/pdf/2511.18450", "abs": "https://arxiv.org/abs/2511.18450", "authors": ["Rui Xu", "Dakuan Lu", "Zicheng Zhao", "Xiaoyu Tan", "Xintao Wang", "Siyu Yuan", "Jiangjie Chen", "Yinghui Xu"], "title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints", "categories": ["cs.AI"], "comment": null, "summary": "Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models(MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances,each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on evaluating Multimodal Large Language Models (MLLMs) in spatial reasoning, particularly with mathematical constraints. While not directly addressing trajectory prediction, spatial reasoning is a related field. The paper explicitly mentions MLLMs and benchmarks their performance, making it relevant to the large language model theme. The connection to trajectory prediction is weaker, as the spatial reasoning tasks are based on origami, not movement prediction.", "keywords": ["Multimodal Large Language Models", "MLLMs", "spatial reasoning", "foundation models"]}}
{"id": "2511.18509", "pdf": "https://arxiv.org/pdf/2511.18509", "abs": "https://arxiv.org/abs/2511.18509", "authors": ["Ziyu Meng", "Tengyu Liu", "Le Ma", "Yingying Wu", "Ran Song", "Wei Zhang", "Siyuan Huang"], "title": "SafeFall: Learning Protective Control for Humanoid Robots", "categories": ["cs.RO"], "comment": null, "summary": "Bipedal locomotion makes humanoid robots inherently prone to falls, causing catastrophic damage to the expensive sensors, actuators, and structural components of full-scale robots. To address this critical barrier to real-world deployment, we present \\method, a framework that learns to predict imminent, unavoidable falls and execute protective maneuvers to minimize hardware damage. SafeFall is designed to operate seamlessly alongside existing nominal controller, ensuring no interference during normal operation. It combines two synergistic components: a lightweight, GRU-based fall predictor that continuously monitors the robot's state, and a reinforcement learning policy for damage mitigation. The protective policy remains dormant until the predictor identifies a fall as unavoidable, at which point it activates to take control and execute a damage-minimizing response. This policy is trained with a novel, damage-aware reward function that incorporates the robot's specific structural vulnerabilities, learning to shield critical components like the head and hands while absorbing energy with more robust parts of its body. Validated on a full-scale Unitree G1 humanoid, SafeFall demonstrated significant performance improvements over unprotected falls. It reduced peak contact forces by 68.3\\%, peak joint torques by 78.4\\%, and eliminated 99.3\\% of collisions with vulnerable components. By enabling humanoids to fail safely, SafeFall provides a crucial safety net that allows for more aggressive experiments and accelerates the deployment of these robots in complex, real-world environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8dcc\u5012\u4fdd\u62a4\u63a7\u5236\uff0c\u5176\u4e2d\u4f7f\u7528\u4e86GRU\uff08\u4e00\u79cd\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff09\u8fdb\u884c\u8dcc\u5012\u9884\u6d4b\u3002\u867d\u7136\u6d89\u53ca\u5230\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08\u8dcc\u5012\u9884\u6d4b\uff09\uff0c\u53ef\u4ee5\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\u7684\u53d8\u4f53\uff0c\u4f46\u4e0e\u4f20\u7edf\u610f\u4e49\u4e0a\u7684\u8f68\u8ff9\u9884\u6d4b\uff08\u5982\u884c\u4eba\u6216\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff09\u6709\u6240\u4e0d\u540c\uff0c\u4e14\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["fall prediction", "GRU", "humanoid robots", "reinforcement learning"]}}
{"id": "2511.17843", "pdf": "https://arxiv.org/pdf/2511.17843", "abs": "https://arxiv.org/abs/2511.17843", "authors": ["Chenyi Wang", "Zhaowei Li", "Ming F. Li", "Wujie Wen"], "title": "JigsawComm: Joint Semantic Feature Encoding and Transmission for Communication-Efficient Cooperative Perception", "categories": ["cs.CV"], "comment": null, "summary": "Multi-agent cooperative perception (CP) promises to overcome the inherent occlusion and sensing-range limitations of single-agent systems (e.g., autonomous driving). However, its practicality is severely constrained by the limited communication bandwidth. Existing approaches attempt to improve bandwidth efficiency via compression or heuristic message selection, without considering the semantic relevance or cross-agent redundancy of sensory data. We argue that a practical CP system must maximize the contribution of every transmitted bit to the final perception task, by extracting and transmitting semantically essential and non-redundant data. In this paper, we formulate a joint semantic feature encoding and transmission problem, which aims to maximize CP accuracy under limited bandwidth. To solve this problem, we introduce JigsawComm, an end-to-end trained, semantic-aware, and communication-efficient CP framework that learns to ``assemble the puzzle'' of multi-agent feature transmission. It uses a regularized encoder to extract semantically-relevant and sparse features, and a lightweight Feature Utility Estimator to predict the contribution of each agent's features to the final perception task. The resulting meta utility maps are exchanged among agents and leveraged to compute a provably optimal transmission policy, which selects features from agents with the highest utility score for each location. This policy inherently eliminates redundancy and achieves a scalable $\\mathcal{O}(1)$ communication cost as the number of agents increases. On the benchmarks OPV2V and DAIR-V2X, JigsawComm reduces the total data volume by up to $>$500$\\times$ while achieving matching or superior accuracy compared to state-of-the-art methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on cooperative perception for multi-agent systems, particularly in the context of autonomous driving. While it doesn't directly involve Large Language Models, it is relevant to trajectory prediction because improved perception is crucial for accurate trajectory forecasting in autonomous vehicles. The core focus is on communication efficiency and feature encoding for perception tasks, which indirectly supports trajectory prediction. It does not involve LLMs.", "keywords": ["cooperative perception", "autonomous driving", "feature encoding", "perception", "multi-agent systems"]}}
{"id": "2511.17885", "pdf": "https://arxiv.org/pdf/2511.17885", "abs": "https://arxiv.org/abs/2511.17885", "authors": ["Guoyang Xia", "Yifeng Ding", "Fengfa Li", "Lei Ren", "Wei Chen", "Fangxiang Feng", "Xiaojie Wang"], "title": "FastMMoE: Accelerating Multimodal Large Language Models through Dynamic Expert Activation and Routing-Aware Token Pruning", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved impressive performance, but high-resolution visual inputs result in long sequences of visual tokens and substantial inference latency. Reducing redundant visual tokens is critical to ease computational/memory burdens while preserving performance, enabling MLLM deployment in resource-constrained or latency-sensitive scenarios. Current visual token pruning methods mainly rely on attention-based redundancy analysis and are tailored to dense architectures. We propose Fast Multimodal Mixture-of-Experts (FastMMoE), a training-free acceleration framework for mixture-of-experts (MoE) based MLLMs, developed from a routing analysis perspective. FastMMoE combines two complementary strategies: (i) expert activation reduction for visual tokens to minimize unnecessary expert computation; and (ii) routing-aware token pruning that leverages similarity in routing probability distributions to identify and remove highly redundant visual tokens. Experiments on large-scale MoE-MLLMs such as DeepSeek-VL2 and InternVL3.5 demonstrate that FastMMoE can reduce FLOPs by up to 55.0% while retaining approximately 95.5% of the original performance, consistently outperforming dense-model pruning baselines including FastV and SparseVLM across multiple retention rates.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on accelerating multimodal large language models (MLLMs) by reducing redundant visual tokens and optimizing computation. While it heavily involves large language models and their optimization, it doesn't directly address trajectory prediction. The 'multimodal' aspect could potentially involve processing video or visual sequences that *might* relate to trajectories, but this is not explicitly stated or the main focus. Therefore, the relevance is moderate due to its strong focus on LLMs but lack of direct connection to trajectory prediction.", "keywords": ["Multimodal Large Language Models", "MLLMs", "Large Language Models", "MoE", "mixture-of-experts"]}}
{"id": "2511.18703", "pdf": "https://arxiv.org/pdf/2511.18703", "abs": "https://arxiv.org/abs/2511.18703", "authors": ["Ardalan Tajbakhsh", "Augustinos Saravanos", "James Zhu", "Evangelos A. Theodorou", "Lorenz T. Biegler", "Aaron M. Johnson"], "title": "Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication", "categories": ["cs.RO"], "comment": "9 pages, 5 figures", "summary": "This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on multi-robot motion planning and trajectory optimization, which falls under the umbrella of trajectory prediction. However, it does not explicitly mention or utilize large language models. The asynchronous distributed optimization and delay-aware ADMM are related to the challenges in trajectory planning, but the connection to large language models is absent.", "keywords": ["trajectory optimization", "motion planning", "multi-robot systems"]}}
{"id": "2511.18718", "pdf": "https://arxiv.org/pdf/2511.18718", "abs": "https://arxiv.org/abs/2511.18718", "authors": ["Omar Garib", "Jayaprakash D. Kambhampaty", "Olivia J. Pinon Fischer", "Dimitri N. Mavris"], "title": "AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 4 figures, 1 table, 1 algorithm", "summary": "We introduce AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed), a modular and lightweight simulation environment designed to evaluate multimodal pilot and air traffic control (ATC) assistance systems for aviation conflict detection. Built on the open-source Godot engine, AIRHILT synchronizes pilot and ATC radio communications, visual scene understanding from camera streams, and ADS-B surveillance data within a unified, scalable platform. The environment supports pilot- and controller-in-the-loop interactions, providing a comprehensive scenario suite covering both terminal area and en route operational conflicts, including communication errors and procedural mistakes. AIRHILT offers standardized JSON-based interfaces that enable researchers to easily integrate, swap, and evaluate automatic speech recognition (ASR), visual detection, decision-making, and text-to-speech (TTS) models. We demonstrate AIRHILT through a reference pipeline incorporating fine-tuned Whisper ASR, YOLO-based visual detection, ADS-B-based conflict logic, and GPT-OSS-20B structured reasoning, and present preliminary results from representative runway-overlap scenarios, where the assistant achieves an average time-to-first-warning of approximately 7.7 s, with average ASR and vision latencies of approximately 5.9 s and 0.4 s, respectively. The AIRHILT environment and scenario suite are openly available, supporting reproducible research on multimodal situational awareness and conflict detection in aviation; code and scenarios are available at https://github.com/ogarib3/airhilt.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper describes a simulation environment for aviation conflict detection. While it doesn't explicitly focus on trajectory prediction, it uses ADS-B data which implicitly contains trajectory information. The system also incorporates a large language model (GPT-OSS-20B) for structured reasoning. Therefore, it has moderate relevance.", "keywords": ["ADS-B", "conflict detection", "GPT-OSS-20B", "large language model"]}}
{"id": "2511.17952", "pdf": "https://arxiv.org/pdf/2511.17952", "abs": "https://arxiv.org/abs/2511.17952", "authors": ["Liangyang Ouyang", "Yifei Huang", "Mingfang Zhang", "Caixin Kang", "Ryosuke Furuta", "Yoichi Sato"], "title": "Multi-speaker Attention Alignment for Multimodal Social Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Understanding social interaction in video requires reasoning over a dynamic interplay of verbal and non-verbal cues: who is speaking, to whom, and with what gaze or gestures. While Multimodal Large Language Models (MLLMs) are natural candidates, simply adding visual inputs yields surprisingly inconsistent gains on social tasks. Our quantitative analysis of cross-modal attention inside state-of-the-art MLLMs reveals a core failure mode: in multi-speaker scenes, visual and textual tokens lack speaker-consistent alignment, exhibiting substantially weaker cross-modal attention than in object-centric images. To address this, we propose a multimodal multi-speaker attention alignment method that can be integrated into existing MLLMs. First, we introduce dynamic cross-modal head selection to identify attention heads most responsible for grounding. Then, an adaptive social-aware attention bias, computed from existing attention patterns and speaker locations, is injected into the attention mechanism. This bias reinforces alignment between a speaker's visual representation and their utterances without introducing trainable parameters or architectural changes. We integrate our method into three distinct MLLMs (LLaVA-NeXT-Video, Qwen2.5-VL, and InternVL3) and evaluate on three benchmarks (TVQA+, MMSI, OnlineMMSI). Across four social tasks, results demonstrate that our approach improves the ability of MLLMs and achieves state-of-the-art results. Attention visualizations confirm our method successfully focuses the model on speaker-relevant regions, enabling more robust multi-party social reasoning. Our implementation and model will be available at https://github.com/ut-vision/SocialInteraction.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving multimodal large language models (MLLMs) for understanding social interaction in videos. While it doesn't directly deal with trajectory prediction, it uses MLLMs, a core component related to large language models. The connection is indirect but present through the use of MLLMs for analyzing dynamic interactions, which could potentially be extended to predict future actions or trajectories in a social context.", "keywords": ["Multimodal Large Language Models (MLLMs)", "Large Language Models", "attention alignment", "social interaction"]}}
{"id": "2511.17506", "pdf": "https://arxiv.org/pdf/2511.17506", "abs": "https://arxiv.org/abs/2511.17506", "authors": ["Narjes Nourzad", "Mingyu Zong", "Bhaskar Krishnamachari"], "title": "AURA: Adaptive Unified Reasoning and Automation with LLM-Guided MARL for NextG Cellular Networks", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Next-generation (NextG) cellular networks are expected to manage dynamic traffic while sustaining high performance. Large language models (LLMs) provide strategic reasoning for 6G planning, but their computational cost and latency limit real-time use. Multi-agent reinforcement learning (MARL) supports localized adaptation, yet coordination at scale remains challenging. We present AURA, a framework that integrates cloud-based LLMs for high-level planning with base stations modeled as MARL agents for local decision-making. The LLM generates objectives and subgoals from its understanding of the environment and reasoning capabilities, while agents at base stations execute these objectives autonomously, guided by a trust mechanism that balances local learning with external input. To reduce latency, AURA employs batched communication so that agents update the LLM's view of the environment and receive improved feedback. In a simulated 6G scenario, AURA improves resilience, reducing dropped handoff requests by more than half under normal and high traffic and lowering system failures. Agents use LLM input in fewer than 60\\% of cases, showing that guidance augments rather than replaces local adaptability, thereby mitigating latency and hallucination risks. These results highlight the promise of combining LLM reasoning with MARL adaptability for scalable, real-time NextG network management.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses using LLMs for high-level planning in 6G cellular networks and MARL for local decision-making. While it doesn't directly address trajectory prediction, the real-time network management aspect and the mention of planning and adaptation could be loosely related to predicting future network states or user behavior, which could influence trajectory prediction in some contexts. The use of LLMs is a strong indicator of relevance to one of the specified themes.", "keywords": ["Large Language Models (LLMs)", "MARL", "planning", "reasoning", "adaptation"]}}
{"id": "2511.19011", "pdf": "https://arxiv.org/pdf/2511.19011", "abs": "https://arxiv.org/abs/2511.19011", "authors": ["Jiale Zhang", "Yeqiang Qian", "Tong Qin", "Mingyang Jiang", "Siyuan Chen", "Ming Yang"], "title": "End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera", "categories": ["cs.RO"], "comment": null, "summary": "The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on autonomous vehicle following using a monocular fisheye camera. While it doesn't explicitly mention large language models, it involves trajectory tracking and prediction of the preceding vehicle. The 'dynamic sampling mechanism to precisely track the trajectories of preceding vehicles' indicates a component related to trajectory prediction, albeit in a specific application. The end-to-end nature of the system hints at potential applications of learned models, though not necessarily large language models.", "keywords": ["trajectory prediction", "vehicle following", "autonomous vehicle", "dynamic sampling", "trajectory tracking"]}}
{"id": "2511.19135", "pdf": "https://arxiv.org/pdf/2511.19135", "abs": "https://arxiv.org/abs/2511.19135", "authors": ["Pascal Goldschmid", "Aamir Ahmad"], "title": "Autonomous Docking of Multi-Rotor UAVs on Blimps under the Influence of Wind Gusts", "categories": ["cs.RO"], "comment": "13 pages, 8 figures, 8 tables", "summary": "Multi-rotor UAVs face limited flight time due to battery constraints. Autonomous docking on blimps with onboard battery recharging and data offloading offers a promising solution for extended UAV missions. However, the vulnerability of blimps to wind gusts causes trajectory deviations, requiring precise, obstacle-aware docking strategies. To this end, this work introduces two key novelties: (i) a temporal convolutional network that predicts blimp responses to wind gusts, enabling rapid gust detection and estimation of points where the wind gust effect has subsided; (ii) a model predictive controller (MPC) that leverages these predictions to compute collision-free trajectories for docking, enabled by a novel obstacle avoidance method for close-range manoeuvres near the blimp. Simulation results show our method outperforms a baseline constant-velocity model of the blimp significantly across different scenarios. We further validate the approach in real-world experiments, demonstrating the first autonomous multi-rotor docking control strategy on blimps shown outside simulation. Source code is available here https://github.com/robot-perception-group/multi_rotor_airship_docking.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper is related to trajectory prediction because it uses a temporal convolutional network to predict blimp responses to wind gusts, which is then used in a model predictive controller to compute collision-free trajectories for UAV docking. While it doesn't directly involve large language models, the use of a predictive model for trajectory planning gives it some relevance to the topic. The core focus seems to be on control and robotics rather than LLMs.", "keywords": ["trajectory prediction", "temporal convolutional network", "model predictive control", "collision-free trajectories"]}}
{"id": "2511.19204", "pdf": "https://arxiv.org/pdf/2511.19204", "abs": "https://arxiv.org/abs/2511.19204", "authors": ["Fabian Schramm", "Pierre Fabre", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "title": "Reference-Free Sampling-Based Model Predictive Control", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on model predictive control (MPC) and trajectory optimization for robot locomotion. While it involves predicting future states and optimizing trajectories, it doesn't directly use or relate to large language models. The connection to trajectory prediction is present through MPC, but the lack of any mention of LLMs lowers the relevance score.", "keywords": ["trajectory prediction", "model predictive control", "MPC", "motion planning", "trajectory optimization"]}}
{"id": "2511.17986", "pdf": "https://arxiv.org/pdf/2511.17986", "abs": "https://arxiv.org/abs/2511.17986", "authors": ["Lun Huang", "You Xie", "Hongyi Xu", "Tianpei Gu", "Chenxu Zhang", "Guoxian Song", "Zenan Li", "Xiaochen Zhao", "Linjie Luo", "Guillermo Sapiro"], "title": "Plan-X: Instruct Video Generation via Semantic Planning", "categories": ["cs.CV", "cs.AI"], "comment": "The project page is at https://byteaigc.github.io/Plan-X", "summary": "Diffusion Transformers have demonstrated remarkable capabilities in visual synthesis, yet they often struggle with high-level semantic reasoning and long-horizon planning. This limitation frequently leads to visual hallucinations and mis-alignments with user instructions, especially in scenarios involving complex scene understanding, human-object interactions, multi-stage actions, and in-context motion reasoning. To address these challenges, we propose Plan-X, a framework that explicitly enforces high-level semantic planning to instruct video generation process. At its core lies a Semantic Planner, a learnable multimodal language model that reasons over the user's intent from both text prompts and visual context, and autoregressively generates a sequence of text-grounded spatio-temporal semantic tokens. These semantic tokens, complementary to high-level text prompt guidance, serve as structured \"semantic sketches\" over time for the video diffusion model, which has its strength at synthesizing high-fidelity visual details. Plan-X effectively integrates the strength of language models in multimodal in-context reasoning and planning, together with the strength of diffusion models in photorealistic video synthesis. Extensive experiments demonstrate that our framework substantially reduces visual hallucinations and enables fine-grained, instruction-aligned video generation consistent with multimodal context.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses video generation using a multimodal language model for semantic planning. While it doesn't directly address trajectory prediction, the concept of planning and predicting future states (even in the context of video frames) shares similarities. The use of a large language model is a key component, making it relevant to the large language model aspect. The 'motion reasoning' mentioned in the abstract also hints at a potential connection to trajectory prediction, albeit indirectly.", "keywords": ["Large Language Models", "multimodal language model", "planning", "motion reasoning"]}}
{"id": "2511.17526", "pdf": "https://arxiv.org/pdf/2511.17526", "abs": "https://arxiv.org/abs/2511.17526", "authors": ["Honggang Jia", "Nan Cheng", "Xiucheng Wang"], "title": "RadioMapMotion: A Dataset and Baseline for Proactive Spatio-Temporal Radio Environment Prediction", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "Radio maps (RMs), which provide location-based pathloss estimations, are fundamental to enabling proactive, environment-aware communication in 6G networks. However, existing deep learning-based methods for RM construction often model dynamic environments as a series of independent static snapshots, thereby omitting the temporal continuity inherent in signal propagation changes caused by the motion of dynamic entities. To address this limitation, we propose the task of spatio-temporal RM prediction, which involves forecasting a sequence of future maps from historical observations. A key barrier to this predictive approach has been the lack of datasets capturing continuous environmental evolution. To fill this gap, we introduce RadioMapMotion, the first large-scale public dataset of continuous RM sequences generated from physically consistent vehicle trajectories. As a baseline for this task, we propose RadioLSTM, a UNet architecture based on Convolutional Long Short-Term Memory (ConvLSTM) and designed for multi-step sequence forecasting. Experimental evaluations show that RadioLSTM achieves higher prediction accuracy and structural fidelity compared to representative baseline methods. Furthermore, the model exhibits a low inference latency, indicating its potential suitability for real-time network operations. Our project will be publicly released at: https://github.com/UNIC-Lab/RadioMapMotion upon paper acceptance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on spatio-temporal prediction of radio maps based on vehicle trajectories. While it doesn't directly use large language models, it involves trajectory prediction (vehicle trajectories) and sequence forecasting, which are related to the broader field of trajectory prediction. The use of LSTM further strengthens this connection. However, the primary focus is on radio environment prediction, not general trajectory prediction or integration with LLMs.", "keywords": ["trajectory prediction", "spatio-temporal prediction", "sequence forecasting", "LSTM", "vehicle trajectories"]}}
