{"id": "2506.21041", "pdf": "https://arxiv.org/pdf/2506.21041", "abs": "https://arxiv.org/abs/2506.21041", "authors": ["Junwei You", "Pei Li", "Zhuoyu Jiang", "Zilin Huang", "Rui Gan", "Haotian Shi", "Bin Ran"], "title": "V2X-REALM: Vision-Language Model-Based Robust End-to-End Cooperative Autonomous Driving with Adaptive Long-Tail Modeling", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Ensuring robust planning and decision-making under rare, diverse, and\nvisually degraded long-tail scenarios remains a fundamental challenge for\nautonomous driving in urban environments. This issue becomes more critical in\ncooperative settings, where vehicles and infrastructure jointly perceive and\nreason across complex environments. To address this challenge, we propose\nV2X-REALM, a vision-language model (VLM)-based framework with adaptive\nmultimodal learning for robust cooperative autonomous driving under long-tail\nscenarios. V2X-REALM introduces three core innovations: (i) a prompt-driven\nlong-tail scenario generation and evaluation pipeline that leverages foundation\nmodels to synthesize realistic long-tail conditions such as snow and fog across\nvehicle- and infrastructure-side views, enriching training diversity\nefficiently; (ii) a gated multi-scenario adaptive attention module that\nmodulates the visual stream using scenario priors to recalibrate ambiguous or\ncorrupted features; and (iii) a multi-task scenario-aware contrastive learning\nobjective that improves multimodal alignment and promotes cross-scenario\nfeature separability. Extensive experiments demonstrate that V2X-REALM\nsignificantly outperforms existing baselines in robustness, semantic reasoning,\nsafety, and planning accuracy under complex, challenging driving conditions,\nadvancing the scalability of end-to-end cooperative autonomous driving.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86vision-language model (VLM)\uff0c\u5c5e\u4e8e\u5927\u6a21\u578b\u8303\u7574\u3002\u540c\u65f6\uff0c\u8bba\u6587\u5173\u6ce8cooperative autonomous driving\uff0c\u6d89\u53ca\u8f66\u8f86\u7684\u89c4\u5212\u548c\u51b3\u7b56\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u5173\u6ce8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u89c4\u5212\u548c\u51b3\u7b56\uff0c\u867d\u7136\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u76ee\u6807\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["Vision-Language Model", "VLM", "autonomous driving", "cooperative autonomous driving", "foundation models", "planning", "decision-making"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21121", "pdf": "https://arxiv.org/pdf/2506.21121", "abs": "https://arxiv.org/abs/2506.21121", "authors": ["Muleilan Pei", "Shaoshuai Shi", "Lu Zhang", "Peiliang Li", "Shaojie Shen"], "title": "GoIRL: Graph-Oriented Inverse Reinforcement Learning for Multimodal Trajectory Prediction", "categories": ["cs.CV", "cs.RO"], "comment": "Accepted by ICML 2025", "summary": "Trajectory prediction for surrounding agents is a challenging task in\nautonomous driving due to its inherent uncertainty and underlying\nmultimodality. Unlike prevailing data-driven methods that primarily rely on\nsupervised learning, in this paper, we introduce a novel Graph-oriented Inverse\nReinforcement Learning (GoIRL) framework, which is an IRL-based predictor\nequipped with vectorized context representations. We develop a feature adaptor\nto effectively aggregate lane-graph features into grid space, enabling seamless\nintegration with the maximum entropy IRL paradigm to infer the reward\ndistribution and obtain the policy that can be sampled to induce multiple\nplausible plans. Furthermore, conditioned on the sampled plans, we implement a\nhierarchical parameterized trajectory generator with a refinement module to\nenhance prediction accuracy and a probability fusion strategy to boost\nprediction confidence. Extensive experimental results showcase our approach not\nonly achieves state-of-the-art performance on the large-scale Argoverse &\nnuScenes motion forecasting benchmarks but also exhibits superior\ngeneralization abilities compared to existing supervised models.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u8f68\u8ff9\u9884\u6d4b\uff08Trajectory Prediction\uff09\uff0c\u5e76\u4f7f\u7528\u56fe\u7ed3\u6784\uff08Graph\uff09\u6765\u8fdb\u884c\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u9006\u5f3a\u5316\u5b66\u4e60\uff08Inverse Reinforcement Learning\uff09\u65b9\u6cd5\uff0c\u8fd9\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5b66\u4e60\u590d\u6742\u7b56\u7565\u7684\u65b9\u5f0f\uff0c\u4e0e\u5927\u6a21\u578b\u5728\u67d0\u79cd\u7a0b\u5ea6\u4e0a\u5177\u6709\u76f8\u5173\u6027\u3002\u603b\u4f53\u800c\u8a00\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e0e\u5927\u6a21\u578b\u6709\u95f4\u63a5\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "multimodal trajectory prediction", "inverse reinforcement learning", "graph", "autonomous driving"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21030", "pdf": "https://arxiv.org/pdf/2506.21030", "abs": "https://arxiv.org/abs/2506.21030", "authors": ["Zhou Tianxing", "Wang Zhirui", "Ao Haojia", "Chen Guangyan", "Xing Boyang", "Cheng Jingwen", "Yang Yi", "Yue Yufeng"], "title": "STEP Planner: Constructing cross-hierarchical subgoal tree as an embodied long-horizon task planner", "categories": ["cs.RO"], "comment": null, "summary": "The ability to perform reliable long-horizon task planning is crucial for\ndeploying robots in real-world environments. However, directly employing Large\nLanguage Models (LLMs) as action sequence generators often results in low\nsuccess rates due to their limited reasoning ability for long-horizon embodied\ntasks. In the STEP framework, we construct a subgoal tree through a pair of\nclosed-loop models: a subgoal decomposition model and a leaf node termination\nmodel. Within this framework, we develop a hierarchical tree structure that\nspans from coarse to fine resolutions. The subgoal decomposition model\nleverages a foundation LLM to break down complex goals into manageable\nsubgoals, thereby spanning the subgoal tree. The leaf node termination model\nprovides real-time feedback based on environmental states, determining when to\nterminate the tree spanning and ensuring each leaf node can be directly\nconverted into a primitive action. Experiments conducted in both the\nVirtualHome WAH-NL benchmark and on real robots demonstrate that STEP achieves\nlong-horizon embodied task completion with success rates up to 34% (WAH-NL) and\n25% (real robot) outperforming SOTA methods.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper uses Large Language Models (LLMs) for task planning, specifically for subgoal decomposition. While the paper focuses on task planning for robots, which is related to trajectory generation and execution, it doesn't directly address trajectory prediction. However, the use of LLMs and the connection to embodied agents that require trajectory generation justifies a relatively high relevance score.", "keywords": ["Large Language Models", "LLMs", "task planning", "embodied agents", "foundation models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21205", "pdf": "https://arxiv.org/pdf/2506.21205", "abs": "https://arxiv.org/abs/2506.21205", "authors": ["Elia Trevisan", "Khaled A. Mustafa", "Godert Notten", "Xinwei Wang", "Javier Alonso-Mora"], "title": "Dynamic Risk-Aware MPPI for Mobile Robots in Crowds via Efficient Monte Carlo Approximations", "categories": ["cs.RO"], "comment": "Accepted for presentation at IROS 2025. Submitted Version", "summary": "Deploying mobile robots safely among humans requires the motion planner to\naccount for the uncertainty in the other agents' predicted trajectories. This\nremains challenging in traditional approaches, especially with arbitrarily\nshaped predictions and real-time constraints. To address these challenges, we\npropose a Dynamic Risk-Aware Model Predictive Path Integral control (DRA-MPPI),\na motion planner that incorporates uncertain future motions modelled with\npotentially non-Gaussian stochastic predictions. By leveraging MPPI's\ngradient-free nature, we propose a method that efficiently approximates the\njoint Collision Probability (CP) among multiple dynamic obstacles for several\nhundred sampled trajectories in real-time via a Monte Carlo (MC) approach. This\nenables the rejection of samples exceeding a predefined CP threshold or the\nintegration of CP as a weighted objective within the navigation cost function.\nConsequently, DRA-MPPI mitigates the freezing robot problem while enhancing\nsafety. Real-world and simulated experiments with multiple dynamic obstacles\ndemonstrate DRA-MPPI's superior performance compared to state-of-the-art\napproaches, including Scenario-based Model Predictive Control (S-MPC), Frenet\nplanner, and vanilla MPPI.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory prediction and path planning for mobile robots in dynamic environments. While it does not directly involve large language models, it deals with predicting and accounting for the uncertain trajectories of other agents in the environment. The keywords 'trajectory prediction', 'motion planning', and 'dynamic obstacles' indicate its relevance to the trajectory prediction domain. However, the absence of any mention or application of large language models lowers the relevance score.", "keywords": ["trajectory prediction", "motion planning", "dynamic obstacles", "mobile robots"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21250", "pdf": "https://arxiv.org/pdf/2506.21250", "abs": "https://arxiv.org/abs/2506.21250", "authors": ["Jing Bi", "Lianggong Bruce Wen", "Zhang Liu", "Chenliang Xu"], "title": "ACTLLM: Action Consistency Tuned Large Language Model", "categories": ["cs.RO"], "comment": null, "summary": "This paper introduces ACTLLM (Action Consistency Tuned Large Language Model),\na novel approach for robot manipulation in dynamic environments. Traditional\nvision-based systems often struggle to learn visual representations that excel\nin both task execution and spatial reasoning, thereby limiting their\nadaptability in dynamic environments. ACTLLM addresses these challenges by\nharnessing language to craft structured scene descriptors, providing a uniform\ninterface for both spatial understanding and task performance through flexible\nlanguage instructions. Moreover, we introduce a novel action consistency\nconstraint that aligns visual perception with corresponding actions, thereby\nenhancing the learning of actionable visual representations. Additionally, we\nhave reformulated the Markov decision process for manipulation tasks into a\nmulti-turn visual dialogue framework. This approach enables the modeling of\nlong-term task execution with enhanced contextual relevance derived from the\nhistory of task execution. During our evaluation, ACTLLM excels in diverse\nscenarios, proving its effectiveness on challenging vision-based robot\nmanipulation tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robot manipulation using a Large Language Model (ACTLLM). While it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting the future path of pedestrians or vehicles), the robot manipulation task inherently involves planning and executing sequences of actions, which can be seen as a form of trajectory generation. The use of an LLM makes it relevant to the large language model aspect.", "keywords": ["Large Language Model", "robot manipulation", "Markov decision process"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21265", "pdf": "https://arxiv.org/pdf/2506.21265", "abs": "https://arxiv.org/abs/2506.21265", "authors": ["Jelmer van der Saag", "Elia Trevisan", "Wouter Falkena", "Javier Alonso-Mora"], "title": "Active Disturbance Rejection Control for Trajectory Tracking of a Seagoing USV: Design, Simulation, and Field Experiments", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted for presentation at IROS 2025. Submitted version", "summary": "Unmanned Surface Vessels (USVs) face significant control challenges due to\nuncertain environmental disturbances like waves and currents. This paper\nproposes a trajectory tracking controller based on Active Disturbance Rejection\nControl (ADRC) implemented on the DUS V2500. A custom simulation incorporating\nrealistic waves and current disturbances is developed to validate the\ncontroller's performance, supported by further validation through field tests\nin the harbour of Scheveningen, the Netherlands, and at sea. Simulation results\ndemonstrate that ADRC significantly reduces cross-track error across all tested\nconditions compared to a baseline PID controller but increases control effort\nand energy consumption. Field trials confirm these findings while revealing a\nfurther increase in energy consumption during sea trials compared to the\nbaseline.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory tracking of an Unmanned Surface Vessel (USV) using Active Disturbance Rejection Control (ADRC). While it involves trajectory tracking which is related to trajectory prediction, it doesn't involve large language models. The connection to trajectory prediction is through the 'trajectory tracking' aspect, as tracking implies predicting future states to maintain a desired path. However, the core contribution is control-oriented, not prediction-oriented. Therefore, the relevance is moderate.", "keywords": ["trajectory tracking", "USV", "control"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21539", "pdf": "https://arxiv.org/pdf/2506.21539", "abs": "https://arxiv.org/abs/2506.21539", "authors": ["Jun Cen", "Chaohui Yu", "Hangjie Yuan", "Yuming Jiang", "Siteng Huang", "Jiayan Guo", "Xin Li", "Yibing Song", "Hao Luo", "Fan Wang", "Deli Zhao", "Hao Chen"], "title": "WorldVLA: Towards Autoregressive Action World Model", "categories": ["cs.RO", "cs.AI"], "comment": "Code: https://github.com/alibaba-damo-academy/WorldVLA", "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u56fe\u50cf\u548c\u751f\u6210\u52a8\u4f5c\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u52a8\u4f5c\u9884\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u4e2d\u6d89\u53ca\u4e86\u89c6\u89c9\u7406\u89e3\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u4e5f\u63d0\u5230\u4e86\u6a21\u578b\u5728\u81ea\u56de\u5f52\u751f\u6210\u52a8\u4f5c\u5e8f\u5217\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u6697\u793a\u4e86\u5bf9\u672a\u6765\u72b6\u6001\u7684\u9884\u6d4b\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e5f\u63d0\u5230\u4e86Vision-Language-Action (VLA) model\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u662f\u6d89\u53ca\u5230\u5927\u6a21\u578b\u9886\u57df\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["action model", "world model", "Vision-Language-Action (VLA) model", "action prediction", "action generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.20886", "pdf": "https://arxiv.org/pdf/2506.20886", "abs": "https://arxiv.org/abs/2506.20886", "authors": ["Zixian Wang", "Cole Ramos", "Muhammad A. Awad", "Keith Lowery"], "title": "Omniwise: Predicting GPU Kernels Performance with LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In recent years, the rapid advancement of deep neural networks (DNNs) has\nrevolutionized artificial intelligence, enabling models with unprecedented\ncapabilities in understanding, generating, and processing complex data. These\npowerful architectures have transformed a wide range of downstream\napplications, tackling tasks beyond human reach. In this paper, we introduce\nOmniwise, the first end-to-end, self-supervised fine-tuning pipeline that\napplies large language models (LLMs) to GPU kernel performance prediction--a\nnovel use case in performance profiling. Omniwise is model-agnostic and\nlightweight, achieving strong results even with a small 3B-parameter model. It\ncan predict key performance metrics, including memory bandwidth, cache hit\nrates, GFLOPs, and arithmetic intensity, directly from kernel code without the\nneed for code execution or profiling tools. Our approach achieves over 90% of\npredictions within 10% relative error on GPU kernels executed on AMD MI250 and\nMI300X architectures. In addition to the pipeline, we develop an online\ninference server and a Visual Studio Code plugin that seamlessly integrate\nLLM-based performance prediction into developers' workflows.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Large Language Models (LLMs) for predicting the performance of GPU kernels. While it doesn't directly deal with trajectory prediction, it does heavily involve LLMs. The connection to trajectory prediction is weak, but the significant presence of LLMs justifies a moderate relevance score.", "keywords": ["LLMs", "Large Language Models", "performance prediction", "GPU kernels"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21185", "pdf": "https://arxiv.org/pdf/2506.21185", "abs": "https://arxiv.org/abs/2506.21185", "authors": ["Yuheng Zhang", "Mengfei Duan", "Kunyu Peng", "Yuhang Wang", "Ruiping Liu", "Fei Teng", "Kai Luo", "Zhiyong Li", "Kailun Yang"], "title": "Out-of-Distribution Semantic Occupancy Prediction", "categories": ["cs.CV", "cs.RO", "eess.IV"], "comment": "The established datasets and source code will be made publicly\n  available at https://github.com/7uHeng/OccOoD", "summary": "3D Semantic Occupancy Prediction is crucial for autonomous driving, providing\na dense, semantically rich environmental representation. However, existing\nmethods focus on in-distribution scenes, making them susceptible to\nOut-of-Distribution (OoD) objects and long-tail distributions, which increases\nthe risk of undetected anomalies and misinterpretations, posing safety hazards.\nTo address these challenges, we introduce Out-of-Distribution Semantic\nOccupancy Prediction, targeting OoD detection in 3D voxel space. To fill the\ngaps in the dataset, we propose a Synthetic Anomaly Integration Pipeline that\ninjects synthetic anomalies while preserving realistic spatial and occlusion\npatterns, enabling the creation of two datasets: VAA-KITTI and VAA-KITTI-360.\nWe introduce OccOoD, a novel framework integrating OoD detection into 3D\nsemantic occupancy prediction, with Voxel-BEV Progressive Fusion (VBPF)\nleveraging an RWKV-based branch to enhance OoD detection via geometry-semantic\nfusion. Experimental results demonstrate that OccOoD achieves state-of-the-art\nOoD detection with an AuROC of 67.34% and an AuPRCr of 29.21% within a 1.2m\nregion, while maintaining competitive occupancy prediction performance. The\nestablished datasets and source code will be made publicly available at\nhttps://github.com/7uHeng/OccOoD.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce83D\u8bed\u4e49\u5360\u636e\u9884\u6d4b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684Out-of-Distribution (OoD) \u68c0\u6d4b\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86RWKV\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u3002\u867d\u7136\u8bba\u6587\u7684\u6838\u5fc3\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u73af\u5883\u611f\u77e5\u80fd\u529b\u5bf9\u4e8e\u8f68\u8ff9\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u76f8\u5173\u6280\u672f\u3002", "keywords": ["Out-of-Distribution detection", "3D semantic occupancy prediction", "autonomous driving", "RWKV", "Voxel-BEV Progressive Fusion"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21234", "pdf": "https://arxiv.org/pdf/2506.21234", "abs": "https://arxiv.org/abs/2506.21234", "authors": ["Qifei Cui", "Yuang Zhou", "Ruichen Deng"], "title": "Real-Time ESFP: Estimating, Smoothing, Filtering, and Pose-Mapping", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "This paper presents ESFP, an end-to-end pipeline that converts monocular RGB\nvideo into executable joint trajectories for a low-cost 4-DoF desktop arm. ESFP\ncomprises four sequential modules. (1) Estimating: ROMP lifts each frame to a\n24-joint 3-D skeleton. (2) Smoothing: the proposed HPSTM-a sequence-to-sequence\nTransformer with self-attention-combines long-range temporal context with a\ndifferentiable forward-kinematics decoder, enforcing constant bone lengths and\nanatomical plausibility while jointly predicting joint means and full\ncovariances. (3) Filtering: root-normalized trajectories are variance-weighted\naccording to HPSTM's uncertainty estimates, suppressing residual noise. (4)\nPose-Mapping: a geometric retargeting layer transforms shoulder-elbow-wrist\ntriples into the uArm's polar workspace, preserving wrist orientation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a sequence-to-sequence Transformer for trajectory smoothing, which falls under the umbrella of sequence prediction, and can be considered a form of trajectory prediction, specifically joint trajectory prediction for robotic arms. The use of a Transformer is a connection to the architecture used in many Large Language Models, though not directly using or exploring LLMs themselves. Therefore, there is moderate relevance.", "keywords": ["trajectory prediction", "sequence-to-sequence Transformer", "joint trajectories", "sequence prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21552", "pdf": "https://arxiv.org/pdf/2506.21552", "abs": "https://arxiv.org/abs/2506.21552", "authors": ["Yutong Bai", "Danny Tran", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik"], "title": "Whole-Body Conditioned Egocentric Video Prediction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "cs.RO"], "comment": "Project Page: https://dannytran123.github.io/PEVA", "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper predicts egocentric video conditioned on 3D body pose trajectories. While it focuses on video prediction and uses a diffusion transformer (a type of large model), the core contribution lies in leveraging human action, represented as 3D body pose trajectories, to predict future video frames. The connection to trajectory prediction is relatively strong, as the body pose acts as a trajectory. The use of a diffusion transformer suggests a connection to large models, but it's not the primary focus. It does not explicitly use or discuss large language models.", "keywords": ["trajectory prediction", "3D body pose", "diffusion transformer", "action prediction", "video prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21129", "pdf": "https://arxiv.org/pdf/2506.21129", "abs": "https://arxiv.org/abs/2506.21129", "authors": ["Deepak Kumar Panda", "Adolfo Perrusquia", "Weisi Guo"], "title": "Curriculum-Guided Antifragile Reinforcement Learning for Secure UAV Deconfliction under Observation-Space Attacks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) policies deployed in safety-critical systems,\nsuch as unmanned aerial vehicle (UAV) navigation in dynamic airspace, are\nvulnerable to out-ofdistribution (OOD) adversarial attacks in the observation\nspace. These attacks induce distributional shifts that significantly degrade\nvalue estimation, leading to unsafe or suboptimal decision making rendering the\nexisting policy fragile. To address this vulnerability, we propose an\nantifragile RL framework designed to adapt against curriculum of incremental\nadversarial perturbations. The framework introduces a simulated attacker which\nincrementally increases the strength of observation-space perturbations which\nenables the RL agent to adapt and generalize across a wider range of OOD\nobservations and anticipate previously unseen attacks. We begin with a\ntheoretical characterization of fragility, formally defining catastrophic\nforgetting as a monotonic divergence in value function distributions with\nincreasing perturbation strength. Building on this, we define antifragility as\nthe boundedness of such value shifts and derive adaptation conditions under\nwhich forgetting is stabilized. Our method enforces these bounds through\niterative expert-guided critic alignment using Wasserstein distance\nminimization across incrementally perturbed observations. We empirically\nevaluate the approach in a UAV deconfliction scenario involving dynamic 3D\nobstacles. Results show that the antifragile policy consistently outperforms\nstandard and robust RL baselines when subjected to both projected gradient\ndescent (PGD) and GPS spoofing attacks, achieving up to 15% higher cumulative\nreward and over 30% fewer conflict events. These findings demonstrate the\npractical and theoretical viability of antifragile reinforcement learning for\nsecure and resilient decision-making in environments with evolving threat\nscenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5f3a\u5316\u5b66\u4e60\u5728\u65e0\u4eba\u673a\u5bfc\u822a\u4e2d\u7684\u5e94\u7528\uff0c\u6d89\u53ca\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\u548c\u907f\u969c\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u4e86\u79fb\u52a8\u7269\u4f53\u7684\u8def\u5f84\u89c4\u5212\uff08UAV deconfliction\uff09\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u9886\u57df\u3002\u5f3a\u5316\u5b66\u4e60\u90e8\u5206\u4e0e\u5927\u6a21\u578b\u53ef\u4ee5\u6709\u5173\u8054\uff0c\u4f46\u8be5\u8bba\u6587\u6ca1\u6709\u660e\u786e\u4f53\u73b0\u3002", "keywords": ["UAV navigation", "trajectory prediction", "reinforcement learning", "UAV deconfliction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.21101", "pdf": "https://arxiv.org/pdf/2506.21101", "abs": "https://arxiv.org/abs/2506.21101", "authors": ["Caoshuo Li", "Zengmao Ding", "Xiaobin Hu", "Bang Li", "Donghao Luo", "AndyPian Wu", "Chaoyang Wang", "Chengjie Wang", "Taisong Jin", "SevenShu", "Yunsheng Wu", "Yongge Liu", "Rongrong Ji"], "title": "OracleFusion: Assisting the Decipherment of Oracle Bone Script with Structurally Constrained Semantic Typography", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "As one of the earliest ancient languages, Oracle Bone Script (OBS)\nencapsulates the cultural records and intellectual expressions of ancient\ncivilizations. Despite the discovery of approximately 4,500 OBS characters,\nonly about 1,600 have been deciphered. The remaining undeciphered ones, with\ntheir complex structure and abstract imagery, pose significant challenges for\ninterpretation. To address these challenges, this paper proposes a novel\ntwo-stage semantic typography framework, named OracleFusion. In the first\nstage, this approach leverages the Multimodal Large Language Model (MLLM) with\nenhanced Spatial Awareness Reasoning (SAR) to analyze the glyph structure of\nthe OBS character and perform visual localization of key components. In the\nsecond stage, we introduce Oracle Structural Vector Fusion (OSVF),\nincorporating glyph structure constraints and glyph maintenance constraints to\nensure the accurate generation of semantically enriched vector fonts. This\napproach preserves the objective integrity of the glyph structure, offering\nvisually enhanced representations that assist experts in deciphering OBS.\nExtensive qualitative and quantitative experiments demonstrate that\nOracleFusion outperforms state-of-the-art baseline models in terms of\nsemantics, visual appeal, and glyph maintenance, significantly enhancing both\nreadability and aesthetic quality. Furthermore, OracleFusion provides\nexpert-like insights on unseen oracle characters, making it a valuable tool for\nadvancing the decipherment of OBS.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u7532\u9aa8\u6587\u7684\u7834\u8bd1\uff0c\u4f46\u4f7f\u7528\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLM) \u6765\u5206\u6790\u7532\u9aa8\u6587\u5b57\u7b26\u7684\u5b57\u5f62\u7ed3\u6784\uff0c\u5e76\u8fdb\u884c\u5173\u952e\u7ec4\u4ef6\u7684\u89c6\u89c9\u5b9a\u4f4d\u3002 \u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u786e\u5b9e\u5229\u7528\u4e86\u5927\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u5206\u6790\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Multimodal Large Language Model (MLLM)", "Spatial Awareness Reasoning (SAR)"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
