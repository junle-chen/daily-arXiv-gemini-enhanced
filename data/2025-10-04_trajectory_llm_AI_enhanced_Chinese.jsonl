{"id": "2510.01639", "pdf": "https://arxiv.org/pdf/2510.01639", "abs": "https://arxiv.org/abs/2510.01639", "authors": ["Thinh Hung Truong", "Jey Han Lau", "Jianzhong Qi"], "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective", "categories": ["cs.AI"], "comment": null, "summary": "We explore the geospatial reasoning capabilities of Large Language Models\n(LLMs), specifically, whether LLMs can read road network maps and perform\nnavigation. We frame trajectory recovery as a proxy task, which requires models\nto reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with\nover 4,000 real-world trajectories across diverse regions and transportation\nmodes. Using road network as context, our prompting framework enables LLMs to\ngenerate valid paths without accessing any external navigation tools.\nExperiments show that LLMs outperform off-the-shelf baselines and specialized\ntrajectory recovery models, with strong zero-shot generalization. Fine-grained\nanalysis shows that LLMs have strong comprehension of the road network and\ncoordinate systems, but also pose systematic biases with respect to regions and\ntransportation modes. Finally, we demonstrate how LLMs can enhance navigation\nexperiences by reasoning over maps in flexible ways to incorporate user\npreferences.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "This paper directly addresses the geospatial reasoning capabilities of Large Language Models in the context of trajectory recovery. It introduces a dataset for trajectory recovery and evaluates LLMs' performance on this task, indicating a strong connection to both trajectory prediction and large language models.", "keywords": ["trajectory recovery", "Large Language Models", "LLMs", "geospatial reasoning", "navigation", "road network", "trajectory prediction"]}, "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u8f68\u8ff9\u6062\u590d\u4efb\u52a1\u9a8c\u8bc1\u4e86LLMs\u5728\u7406\u89e3\u9053\u8def\u7f51\u7edc\u548c\u6267\u884c\u5bfc\u822a\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u5177\u5907\u9605\u8bfb\u9053\u8def\u7f51\u7edc\u5730\u56fe\u5e76\u6267\u884c\u5bfc\u822a\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u5c06\u8f68\u8ff9\u6062\u590d\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u4ee3\u7406\u4efb\u52a1\uff0c\u8981\u6c42\u6a21\u578b\u91cd\u5efa\u88ab\u63a9\u76d6\u7684GPS\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u4e86\u5305\u542b\u8d85\u8fc74,000\u6761\u771f\u5b9e\u4e16\u754c\u8f68\u8ff9\u7684\u6570\u636e\u96c6GLOBALTRACE\u3002\u5229\u7528\u9053\u8def\u7f51\u7edc\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u672c\u6587\u7684\u63d0\u793a\u6846\u67b6\u4f7fLLMs\u80fd\u591f\u751f\u6210\u6709\u6548\u7684\u8def\u5f84\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u4efb\u4f55\u5916\u90e8\u5bfc\u822a\u5de5\u5177\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMs\u4f18\u4e8e\u73b0\u6210\u7684\u57fa\u7ebf\u6a21\u578b\u548c\u4e13\u95e8\u7684\u8f68\u8ff9\u6062\u590d\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002\u7ec6\u7c92\u5ea6\u5206\u6790\u8868\u660e\uff0cLLMs\u5bf9\u9053\u8def\u7f51\u7edc\u548c\u5750\u6807\u7cfb\u7edf\u6709\u5f88\u5f3a\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u5173\u4e8e\u533a\u57df\u548c\u8fd0\u8f93\u65b9\u5f0f\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86LLMs\u53ef\u4ee5\u901a\u8fc7\u4ee5\u7075\u6d3b\u7684\u65b9\u5f0f\u63a8\u7406\u5730\u56fe\u6765\u589e\u5f3a\u5bfc\u822a\u4f53\u9a8c\uff0c\u4ece\u800c\u6574\u5408\u7528\u6237\u504f\u597d\u3002", "summary_zh": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5730\u7406\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662fLLMs\u662f\u5426\u80fd\u591f\u8bfb\u53d6\u9053\u8def\u7f51\u7edc\u5730\u56fe\u5e76\u6267\u884c\u5bfc\u822a\u3002\u6211\u4eec\u5c06\u8f68\u8ff9\u6062\u590d\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u4ee3\u7406\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u8981\u6c42\u6a21\u578b\u91cd\u5efa\u88ab\u63a9\u76d6\u7684GPS\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u4e86GLOBALTRACE\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b\u8d85\u8fc74,000\u6761\u8de8\u4e0d\u540c\u533a\u57df\u548c\u4ea4\u901a\u65b9\u5f0f\u7684\u771f\u5b9e\u4e16\u754c\u8f68\u8ff9\u7684\u6570\u636e\u96c6\u3002\u901a\u8fc7\u4f7f\u7528\u9053\u8def\u7f51\u7edc\u4f5c\u4e3a\u4e0a\u4e0b\u6587\uff0c\u6211\u4eec\u7684\u63d0\u793a\u6846\u67b6\u4f7fLLMs\u80fd\u591f\u751f\u6210\u6709\u6548\u7684\u8def\u5f84\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u4efb\u4f55\u5916\u90e8\u5bfc\u822a\u5de5\u5177\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u4f18\u4e8e\u73b0\u6210\u7684\u57fa\u7ebf\u6a21\u578b\u548c\u4e13\u95e8\u7684\u8f68\u8ff9\u6062\u590d\u6a21\u578b\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002\u7ec6\u7c92\u5ea6\u5206\u6790\u8868\u660e\uff0cLLMs\u5bf9\u9053\u8def\u7f51\u7edc\u548c\u5750\u6807\u7cfb\u7edf\u6709\u5f88\u5f3a\u7684\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u5173\u4e8e\u533a\u57df\u548c\u8fd0\u8f93\u65b9\u5f0f\u7684\u7cfb\u7edf\u6027\u504f\u5dee\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86LLMs\u5982\u4f55\u901a\u8fc7\u4ee5\u7075\u6d3b\u7684\u65b9\u5f0f\u63a8\u7406\u5730\u56fe\u6765\u589e\u5f3a\u5bfc\u822a\u4f53\u9a8c\uff0c\u4ece\u800c\u6574\u5408\u7528\u6237\u504f\u597d\u3002"}}
{"id": "2510.01272", "pdf": "https://arxiv.org/pdf/2510.01272", "abs": "https://arxiv.org/abs/2510.01272", "authors": ["Kunal Jha", "Aydan Yuenan Huang", "Eric Ye", "Natasha Jaques", "Max Kleiman-Weiner"], "title": "Modeling Others' Minds as Code", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Accurate prediction of human behavior is essential for robust and safe\nhuman-AI collaboration. However, existing approaches for modeling people are\noften data-hungry and brittle because they either make unrealistic assumptions\nabout rationality or are too computationally demanding to adapt rapidly. Our\nkey insight is that many everyday social interactions may follow predictable\npatterns; efficient \"scripts\" that minimize cognitive load for actors and\nobservers, e.g., \"wait for the green light, then go.\" We propose modeling these\nroutines as behavioral programs instantiated in computer code rather than\npolicies conditioned on beliefs and desires. We introduce ROTE, a novel\nalgorithm that leverages both large language models (LLMs) for synthesizing a\nhypothesis space of behavioral programs, and probabilistic inference for\nreasoning about uncertainty over that space. We test ROTE in a suite of\ngridworld tasks and a large-scale embodied household simulator. ROTE predicts\nhuman and AI behaviors from sparse observations, outperforming competitive\nbaselines -- including behavior cloning and LLM-based methods -- by as much as\n50% in terms of in-sample accuracy and out-of-sample generalization. By\ntreating action understanding as a program synthesis problem, ROTE opens a path\nfor AI systems to efficiently and effectively predict human behavior in the\nreal-world.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on predicting human behavior, which is related to trajectory prediction. It also leverages Large Language Models (LLMs) for synthesizing behavioral programs. Therefore, it combines both trajectory prediction and large language models.", "keywords": ["trajectory prediction", "human behavior prediction", "large language models", "LLMs", "behavioral programs"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aROTE\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u65e5\u5e38\u793e\u4ea4\u4e92\u52a8\u5efa\u6a21\u4e3a\u884c\u4e3a\u7a0b\u5e8f\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u6982\u7387\u63a8\u7406\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4eba\u7c7b\u548cAI\u884c\u4e3a\u7684\u51c6\u786e\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u884c\u4e3a\u5efa\u6a21\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u8106\u5f31\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u4e48\u5bf9\u7406\u6027\u505a\u51fa\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\uff0c\u8981\u4e48\u8ba1\u7b97\u91cf\u8fc7\u5927\u800c\u65e0\u6cd5\u5feb\u901f\u9002\u5e94\u3002", "method": "\u63d0\u51faROTE\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5408\u6210\u884c\u4e3a\u7a0b\u5e8f\u7684\u5047\u8bbe\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u6982\u7387\u63a8\u7406\u6765\u63a8\u7406\u8be5\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u7f51\u683c\u4e16\u754c\u4efb\u52a1\u548c\u5927\u578b\u5177\u8eab\u5bb6\u5ead\u6a21\u62df\u5668\u4e2d\u6d4b\u8bd5ROTE\uff0c\u7ed3\u679c\u8868\u660eROTE\u5728\u6837\u672c\u5185\u51c6\u786e\u7387\u548c\u6837\u672c\u5916\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u5305\u62ec\u884c\u4e3a\u514b\u9686\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5185\u7684\u7ade\u4e89\u57fa\u7ebf\uff0c\u9ad8\u8fbe50%\u3002", "conclusion": "\u901a\u8fc7\u5c06\u52a8\u4f5c\u7406\u89e3\u89c6\u4e3a\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\uff0cROTE\u4e3aAI\u7cfb\u7edf\u9ad8\u6548\u6709\u6548\u5730\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u5f00\u8f9f\u4e86\u4e00\u6761\u9053\u8def\u3002", "summary_zh": "\u4e3a\u4e86\u5b9e\u73b0\u7a33\u5065\u548c\u5b89\u5168\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u51c6\u786e\u9884\u6d4b\u4eba\u7c7b\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5efa\u6a21\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\u4e14\u8106\u5f31\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u4e48\u5bf9\u7406\u6027\u505a\u51fa\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\uff0c\u8981\u4e48\u8ba1\u7b97\u91cf\u8fc7\u5927\u800c\u65e0\u6cd5\u5feb\u901f\u9002\u5e94\u3002\u6211\u4eec\u7684\u5173\u952e\u6d1e\u5bdf\u662f\uff0c\u8bb8\u591a\u65e5\u5e38\u793e\u4ea4\u4e92\u52a8\u53ef\u80fd\u9075\u5faa\u53ef\u9884\u6d4b\u7684\u6a21\u5f0f\uff1b\u9ad8\u6548\u7684\u201c\u811a\u672c\u201d\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u53c2\u4e0e\u8005\u548c\u89c2\u5bdf\u8005\u7684\u8ba4\u77e5\u8d1f\u8377\uff0c\u4f8b\u5982\u201c\u7b49\u5f85\u7eff\u706f\uff0c\u7136\u540e\u524d\u8fdb\u201d\u3002\u6211\u4eec\u5efa\u8bae\u5c06\u8fd9\u4e9b\u4f8b\u7a0b\u5efa\u6a21\u4e3a\u5728\u8ba1\u7b97\u673a\u4ee3\u7801\u4e2d\u5b9e\u4f8b\u5316\u7684\u884c\u4e3a\u7a0b\u5e8f\uff0c\u800c\u4e0d\u662f\u4ee5\u4fe1\u5ff5\u548c\u613f\u671b\u4e3a\u6761\u4ef6\u7684\u7b56\u7565\u3002\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aROTE\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u5408\u6210\u884c\u4e3a\u7a0b\u5e8f\u7684\u5047\u8bbe\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u6982\u7387\u63a8\u7406\u6765\u63a8\u7406\u8be5\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u6211\u4eec\u5728\u4e00\u5957\u7f51\u683c\u4e16\u754c\u4efb\u52a1\u548c\u4e00\u4e2a\u5927\u578b\u5177\u8eab\u5bb6\u5ead\u6a21\u62df\u5668\u4e2d\u6d4b\u8bd5ROTE\u3002ROTE\u53ef\u4ee5\u6839\u636e\u7a00\u758f\u7684\u89c2\u5bdf\u7ed3\u679c\u9884\u6d4b\u4eba\u7c7b\u548cAI\u884c\u4e3a\uff0c\u5728\u6837\u672c\u5185\u51c6\u786e\u7387\u548c\u6837\u672c\u5916\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u5305\u62ec\u884c\u4e3a\u514b\u9686\u548c\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5728\u5185\u7684\u7ade\u4e89\u57fa\u7ebf\uff0c\u9ad8\u8fbe50%\u3002\u901a\u8fc7\u5c06\u52a8\u4f5c\u7406\u89e3\u89c6\u4e3a\u7a0b\u5e8f\u5408\u6210\u95ee\u9898\uff0cROTE\u4e3aAI\u7cfb\u7edf\u9ad8\u6548\u6709\u6548\u5730\u9884\u6d4b\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u5f00\u8f9f\u4e86\u4e00\u6761\u9053\u8def\u3002"}}
{"id": "2510.01388", "pdf": "https://arxiv.org/pdf/2510.01388", "abs": "https://arxiv.org/abs/2510.01388", "authors": ["Arthur Zhang", "Xiangyun Meng", "Luca Calliari", "Dong-Ki Kim", "Shayegan Omidshafiei", "Joydeep Biswas", "Ali Agha", "Amirreza Shaban"], "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 6 figures, 3 tables", "summary": "Robots must adapt to diverse human instructions and operate safely in\nunstructured, open-world environments. Recent Vision-Language models (VLMs)\noffer strong priors for grounding language and perception, but remain difficult\nto steer for navigation due to differences in action spaces and pretraining\nobjectives that hamper transferability to robotics tasks. Towards addressing\nthis, we introduce VENTURA, a vision-language navigation system that finetunes\ninternet-pretrained image diffusion models for path planning. Instead of\ndirectly predicting low-level actions, VENTURA generates a path mask (i.e. a\nvisual plan) in image space that captures fine-grained, context-aware\nnavigation behaviors. A lightweight behavior-cloning policy grounds these\nvisual plans into executable trajectories, yielding an interface that follows\nnatural language instructions to generate diverse robot behaviors. To scale\ntraining, we supervise on path masks derived from self-supervised tracking\nmodels paired with VLM-augmented captions, avoiding manual pixel-level\nannotation or highly engineered data collection setups. In extensive real-world\nevaluations, VENTURA outperforms state-of-the-art foundation model baselines on\nobject reaching, obstacle avoidance, and terrain preference tasks, improving\nsuccess rates by 33% and reducing collisions by 54% across both seen and unseen\nscenarios. Notably, we find that VENTURA generalizes to unseen combinations of\ndistinct tasks, revealing emergent compositional capabilities. Videos, code,\nand additional materials: https://venturapath.github.io", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper utilizes Vision-Language models (VLMs) which can be considered a type of large model, and focuses on navigation and path planning, which is related to trajectory prediction. It uses diffusion models for path planning and generates executable trajectories, demonstrating a connection to both large models and trajectory generation.", "keywords": ["Vision-Language models", "VLMs", "navigation", "path planning", "trajectory", "diffusion models"]}, "AI": {"tldr": "VENTURA\u901a\u8fc7\u5fae\u8c03\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u8def\u5f84\u63a9\u7801\uff0c\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\uff0c\u56e0\u4e3a\u52a8\u4f5c\u7a7a\u95f4\u548c\u9884\u8bad\u7ec3\u76ee\u6807\u5b58\u5728\u5dee\u5f02\uff0c\u963b\u788d\u4e86\u8fc1\u79fb\u6027\u3002", "method": "VENTURA\u7cfb\u7edf\u5fae\u8c03\u4e86\u4e92\u8054\u7f51\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7528\u4e8e\u8def\u5f84\u89c4\u5212\uff0c\u751f\u6210\u8def\u5f84\u63a9\u7801\u4f5c\u4e3a\u89c6\u89c9\u8ba1\u5212\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7684\u884c\u4e3a\u514b\u9686\u7b56\u7565\u5c06\u89c6\u89c9\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u8bc4\u4f30\u4e2d\uff0cVENTURA\u5728\u7269\u4f53\u6293\u53d6\u3001\u907f\u969c\u548c\u5730\u5f62\u504f\u597d\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u6210\u529f\u7387\u63d0\u9ad833%\uff0c\u78b0\u649e\u51cf\u5c1154%\uff0c\u5e76\u5c55\u73b0\u51fa\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "VENTURA\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "summary_zh": "\u673a\u5668\u4eba\u5fc5\u987b\u9002\u5e94\u591a\u6837\u5316\u7684\u4eba\u7c7b\u6307\u4ee4\uff0c\u5e76\u5728\u975e\u7ed3\u6784\u5316\u7684\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b89\u5168\u8fd0\u884c\u3002 \u8fd1\u671f\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b (VLMs) \u4e3a\u8bed\u8a00\u548c\u611f\u77e5\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5148\u9a8c\uff0c\u4f46\u7531\u4e8e\u52a8\u4f5c\u7a7a\u95f4\u548c\u9884\u8bad\u7ec3\u76ee\u6807\u7684\u5dee\u5f02\uff0c\u5b83\u4eec\u4ecd\u7136\u96be\u4ee5\u7528\u4e8e\u5bfc\u822a\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5411\u673a\u5668\u4eba\u4efb\u52a1\u7684\u8f6c\u79fb\u3002 \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 VENTURA\uff0c\u4e00\u4e2a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7cfb\u7edf\uff0c\u5b83\u5bf9\u4e92\u8054\u7f51\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u8fdb\u884c\u8def\u5f84\u89c4\u5212\u3002 VENTURA \u6ca1\u6709\u76f4\u63a5\u9884\u6d4b\u4f4e\u7ea7\u52a8\u4f5c\uff0c\u800c\u662f\u5728\u56fe\u50cf\u7a7a\u95f4\u4e2d\u751f\u6210\u8def\u5f84\u63a9\u7801\uff08\u5373\u89c6\u89c9\u8ba1\u5212\uff09\uff0c\u8be5\u8def\u5f84\u63a9\u7801\u6355\u83b7\u7ec6\u7c92\u5ea6\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5bfc\u822a\u884c\u4e3a\u3002 \u8f7b\u91cf\u7ea7\u7684\u884c\u4e3a\u514b\u9686\u7b56\u7565\u5c06\u8fd9\u4e9b\u89c6\u89c9\u8ba1\u5212\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u7684\u8f68\u8ff9\uff0c\u4ece\u800c\u4ea7\u751f\u4e00\u4e2a\u9075\u5faa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u4ee5\u751f\u6210\u5404\u79cd\u673a\u5668\u4eba\u884c\u4e3a\u7684\u754c\u9762\u3002 \u4e3a\u4e86\u6269\u5927\u8bad\u7ec3\u89c4\u6a21\uff0c\u6211\u4eec\u5bf9\u4ece\u81ea\u76d1\u7763\u8ddf\u8e2a\u6a21\u578b\u548c VLM \u589e\u5f3a\u7684\u5b57\u5e55\u4e2d\u5bfc\u51fa\u7684\u8def\u5f84\u63a9\u7801\u8fdb\u884c\u76d1\u7763\uff0c\u907f\u514d\u4e86\u624b\u52a8\u50cf\u7d20\u7ea7\u6ce8\u91ca\u6216\u9ad8\u5ea6\u5de5\u7a0b\u5316\u7684\u6570\u636e\u6536\u96c6\u8bbe\u7f6e\u3002 \u5728\u5e7f\u6cdb\u7684\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cVENTURA \u5728\u7269\u4f53\u5230\u8fbe\u3001\u907f\u969c\u548c\u5730\u5f62\u504f\u597d\u4efb\u52a1\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\uff0c\u5728\u53ef\u89c1\u548c\u4e0d\u53ef\u89c1\u573a\u666f\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u9ad8\u4e86 33%\uff0c\u78b0\u649e\u51cf\u5c11\u4e86 54%\u3002 \u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u53d1\u73b0 VENTURA \u53ef\u4ee5\u63a8\u5e7f\u5230\u4e0d\u540c\u4efb\u52a1\u7684\u672a\u89c1\u7ec4\u5408\uff0c\u4ece\u800c\u63ed\u793a\u4e86\u65b0\u5174\u7684\u7ec4\u5408\u80fd\u529b\u3002 \u89c6\u9891\u3001\u4ee3\u7801\u548c\u5176\u4ed6\u6750\u6599\uff1ahttps://venturapath.github.io"}}
{"id": "2510.01357", "pdf": "https://arxiv.org/pdf/2510.01357", "abs": "https://arxiv.org/abs/2510.01357", "authors": ["Alejandro Gonzalez-Garcia", "Wei Xiao", "Wei Wang", "Alejandro Astudillo", "Wilm Decr\u00e9", "Jan Swevers", "Carlo Ratti", "Daniela Rus"], "title": "Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels", "categories": ["cs.RO"], "comment": "IROS 2025", "summary": "Safe motion planning is essential for autonomous vessel operations,\nespecially in challenging spaces such as narrow inland waterways. However,\nconventional motion planning approaches are often computationally intensive or\noverly conservative. This paper proposes a safe motion planning strategy\ncombining Model Predictive Control (MPC) and Control Barrier Functions (CBFs).\nWe introduce a time-varying inflated ellipse obstacle representation, where the\ninflation radius is adjusted depending on the relative position and attitude\nbetween the vessel and the obstacle. The proposed adaptive inflation reduces\nthe conservativeness of the controller compared to traditional fixed-ellipsoid\nobstacle formulations. The MPC solution provides an approximate motion plan,\nand high-order CBFs ensure the vessel's safety using the varying inflation\nradius. Simulation and real-world experiments demonstrate that the proposed\nstrategy enables the fully-actuated autonomous robot vessel to navigate through\nnarrow spaces in real time and resolve potential deadlocks, all while ensuring\nsafety.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u4e3b\u6c34\u9762\u8230\u8247\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u4f7f\u7528\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u548c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08CBFs\uff09\u7b49\u65b9\u6cd5\u3002\u867d\u7136\u6d89\u53ca\u5230\u8fd0\u52a8\u89c4\u5212\u548c\u9884\u6d4b\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u884c\u4eba\u6216\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6709\u6240\u533a\u522b\uff0c\u4e14\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["motion planning", "Model Predictive Control", "prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01454", "pdf": "https://arxiv.org/pdf/2510.01454", "abs": "https://arxiv.org/abs/2510.01454", "authors": ["Nilay Naharas", "Dang Nguyen", "Nesihan Bulut", "Mohammadhossein Bateni", "Vahab Mirrokni", "Baharan Mirzasoleiman"], "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories", "categories": ["cs.CV", "cs.LG"], "comment": "30 pages, 10 figures, 5 tables, link:\n  https://bigml-cs-ucla.github.io/XMAS-project-page/", "summary": "Data-efficient learning aims to eliminate redundancy in large training\ndatasets by training models on smaller subsets of the most informative\nexamples. While data selection has been extensively explored for vision models\nand large language models (LLMs), it remains underexplored for Large\nVision-Language Models (LVLMs). Notably, none of existing methods can\noutperform random selection at different subset sizes. In this work, we propose\nthe first principled method for data-efficient instruction tuning of LVLMs. We\nprove that examples with similar cross-modal attention matrices during\ninstruction tuning have similar gradients. Thus, they influence model\nparameters in a similar manner and convey the same information to the model\nduring training. Building on this insight, we propose XMAS, which clusters\nexamples based on the trajectories of the top singular values of their\nattention matrices obtained from fine-tuning a small proxy LVLM. By sampling a\nbalanced subset from these clusters, XMAS effectively removes redundancy in\nlarge-scale LVLM training data. Extensive experiments show that XMAS can\ndiscard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while\nfully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and\nspeeding up its training by 1.2x. This is 30% more data reduction compared to\nthe best baseline for LLaVA-665k. The project's website can be found at\nhttps://bigml-cs-ucla.github.io/XMAS-project-page/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on data selection for fine-tuning Vision Language Models (LVLMs). While it doesn't directly address trajectory prediction, it does heavily involve Large Language Models (LLMs) and uses the concept of 'trajectories' in the context of cross-modal attention matrices. The connection to trajectory prediction is metaphorical rather than literal, hence the moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "Vision Language Models", "LVLMs", "fine-tuning", "attention matrices", "trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01262", "pdf": "https://arxiv.org/pdf/2510.01262", "abs": "https://arxiv.org/abs/2510.01262", "authors": ["Koyena Chowdhury", "Paramita Koley", "Abhijnan Chakraborty", "Saptarshi Ghosh"], "title": "RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate prediction of train delays is critical for efficient railway\noperations, enabling better scheduling and dispatching decisions. While earlier\napproaches have largely focused on forecasting the exact delays of individual\ntrains, recent studies have begun exploring station-level delay prediction to\nsupport higher-level traffic management. In this paper, we propose the\nRailway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed\nto forecast average arrival delays of all the incoming trains at railway\nstations for a particular time period. Our approach incorporates several\narchitectural innovations and novel feature integrations, including train\nfrequency-aware spatial attention, which significantly enhances predictive\nperformance. To support this effort, we curate and release a comprehensive\ndataset for the entire Indian Railway Network (IRN), spanning 4,735 stations\nacross 17 zones - the largest and most diverse railway network studied to date.\nWe conduct extensive experiments using multiple state-of-the-art baselines,\ndemonstrating consistent improvements across standard metrics. Our work not\nonly advances the modeling of average delay prediction in large-scale rail\nnetworks but also provides an open dataset to encourage further research in\nthis critical domain.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on train delay prediction, which falls under the umbrella of trajectory prediction (specifically, predicting the arrival time of trains). It uses a spatio-temporal graph convolutional network, a common technique in trajectory prediction. However, it does not mention or utilize large language models.", "keywords": ["trajectory prediction", "train delay prediction", "spatio-temporal graph convolutional network"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01483", "pdf": "https://arxiv.org/pdf/2510.01483", "abs": "https://arxiv.org/abs/2510.01483", "authors": ["Mohamad Al Mdfaa", "Svetlana Lukina", "Timur Akhtyamov", "Arthur Nigmatzyanov", "Dmitrii Nalberskii", "Sergey Zagoruyko", "Gonzalo Ferrer"], "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs", "categories": ["cs.RO", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Vision-language models (VLMs) have shown potential for robot navigation but\nencounter fundamental limitations: they lack persistent scene memory, offer\nlimited spatial reasoning, and do not scale effectively with video duration for\nreal-time application. We present VL-KnG, a Visual Scene Understanding system\nthat tackles these challenges using spatiotemporal knowledge graph construction\nand computationally efficient query processing for navigation goal\nidentification. Our approach processes video sequences in chunks utilizing\nmodern VLMs, creates persistent knowledge graphs that maintain object identity\nover time, and enables explainable spatial reasoning through queryable graph\nstructures. We also introduce WalkieKnowledge, a new benchmark with about 200\nmanually annotated questions across 8 diverse trajectories spanning\napproximately 100 minutes of video data, enabling fair comparison between\nstructured approaches and general-purpose VLMs. Real-world deployment on a\ndifferential drive robot demonstrates practical applicability, with our method\nachieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5\nPro performance while providing explainable reasoning supported by the\nknowledge graph, computational efficiency for real-time deployment across\ndifferent tasks, such as localization, navigation and planning. Code and\ndataset will be released after acceptance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes Vision-Language Models (VLMs) which are related to Large Language Models, and it addresses navigation goal identification using spatiotemporal knowledge graphs constructed from video sequences. While not directly focusing on trajectory prediction, navigation implies trajectory planning and understanding, thus having some relevance. The mention of localization, navigation, and planning further strengthens the connection.", "keywords": ["Vision-language models", "VLMs", "navigation", "spatial reasoning", "knowledge graph", "planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01398", "pdf": "https://arxiv.org/pdf/2510.01398", "abs": "https://arxiv.org/abs/2510.01398", "authors": ["Yang Liu", "Zaid Abulawi", "Abhiram Garimidi", "Doyeong Lim"], "title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents", "categories": ["cs.AI"], "comment": null, "summary": "Modern engineering increasingly relies on vast datasets generated by\nexperiments and simulations, driving a growing demand for efficient, reliable,\nand broadly applicable modeling strategies. There is also heightened interest\nin developing data-driven approaches, particularly neural network models, for\neffective prediction and analysis of scientific datasets. Traditional\ndata-driven methods frequently involve extensive manual intervention, limiting\ntheir ability to scale effectively and generalize to diverse applications. In\nthis study, we propose an innovative pipeline utilizing Large Language Model\n(LLM) agents to automate data-driven modeling and analysis, with a particular\nemphasis on regression tasks. We evaluate two LLM-agent frameworks: a\nmulti-agent system featuring specialized collaborative agents, and a\nsingle-agent system based on the Reasoning and Acting (ReAct) paradigm. Both\nframeworks autonomously handle data preprocessing, neural network development,\ntraining, hyperparameter optimization, and uncertainty quantification (UQ). We\nvalidate our approach using a critical heat flux (CHF) prediction benchmark,\ninvolving approximately 25,000 experimental data points from the OECD/NEA\nbenchmark dataset. Results indicate that our LLM-agent-developed model\nsurpasses traditional CHF lookup tables and delivers predictive accuracy and UQ\non par with state-of-the-art Bayesian optimized deep neural network models\ndeveloped by human experts. These outcomes underscore the significant potential\nof LLM-based agents to automate complex engineering modeling tasks, greatly\nreducing human workload while meeting or exceeding existing standards of\npredictive performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Large Language Models (LLMs) to automate data-driven modeling and analysis in engineering, specifically for regression tasks and critical heat flux prediction. While it doesn't directly address trajectory prediction, the use of LLMs for prediction tasks makes it somewhat relevant. The connection is that the LLM is being used to build predictive models, which is a core component of trajectory prediction, even if the specific application is different.", "keywords": ["Large Language Model", "LLM", "prediction", "data-driven modeling"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01642", "pdf": "https://arxiv.org/pdf/2510.01642", "abs": "https://arxiv.org/abs/2510.01642", "authors": ["Zijun Lin", "Jiafei Duan", "Haoquan Fang", "Dieter Fox", "Ranjay Krishna", "Cheston Tan", "Bihan Wen"], "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models", "categories": ["cs.RO"], "comment": "Project Page: https://jimntu.github.io/FailSafe/", "summary": "Recent advances in robotic manipulation have integrated low-level robotic\ncontrol into Vision-Language Models (VLMs), extending them into\nVision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve\nstrong performance in downstream robotic applications, supported by large-scale\ncrowd-sourced robot training data, they still inevitably encounter failures\nduring execution. Enabling robots to reason about and recover from\nunpredictable and abrupt failures remains a critical challenge. Existing\nrobotic manipulation datasets, collected in either simulation or the real\nworld, primarily provide only ground-truth trajectories, leaving robots unable\nto recover once failures occur. Moreover, the few datasets that address failure\ndetection typically offer only textual explanations, which are difficult to\nutilize directly in VLA models. To address this gap, we introduce FailSafe, a\nnovel failure generation and recovery system that automatically produces\ndiverse failure cases paired with executable recovery actions. FailSafe can be\nseamlessly applied to any manipulation task in any simulator, enabling scalable\ncreation of failure-action data. To demonstrate its effectiveness, we fine-tune\nLLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results\nshow that FailSafe-VLM successfully helps robotic arm detect and recover from\npotential failures, improving the performance of three state-of-the-art VLA\nmodels pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several\ntasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different\nspatial configurations, camera viewpoints, and robotic embodiments. We plan to\nrelease the FailSafe code to the community.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on failure recovery in Vision-Language-Action models for robotic manipulation. While it doesn't directly address trajectory prediction, it involves action prediction and utilizes a Large Language Model (LLaVa-OV-7B) for reasoning and recovery. The connection to trajectory prediction is indirect, as the recovery actions could influence future trajectories. The core focus is on VLA models and failure handling, not explicitly on predicting trajectories.", "keywords": ["Large Language Models", "Vision-Language-Action Models", "LLaVa-OV-7B", "robotic manipulation", "action"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01623", "pdf": "https://arxiv.org/pdf/2510.01623", "abs": "https://arxiv.org/abs/2510.01623", "authors": ["Angen Ye", "Zeyu Zhang", "Boyuan Wang", "Xiaofeng Wang", "Dapeng Zhang", "Zheng Zhu"], "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models aim to unify perception, language\nunderstanding, and action generation, offering strong cross-task and\ncross-scene generalization with broad impact on embodied AI. However, current\nVLA models often lack explicit step-by-step reasoning, instead emitting final\nactions without considering affordance constraints or geometric relations.\nTheir post-training pipelines also rarely reinforce reasoning quality, relying\nprimarily on supervised fine-tuning with weak reward design. To address these\nchallenges, we present VLA-R1, a reasoning-enhanced VLA that integrates\nReinforcement Learning from Verifiable Rewards (RLVR) with Group Relative\nPolicy Optimization (GRPO) to systematically optimize both reasoning and\nexecution. Specifically, we design an RLVR-based post-training strategy with\nverifiable rewards for region alignment, trajectory consistency, and output\nformatting, thereby strengthening reasoning robustness and execution accuracy.\nMoreover, we develop VLA-CoT-13K, a high-quality dataset that provides\nchain-of-thought supervision explicitly aligned with affordance and trajectory\nannotations. Furthermore, extensive evaluations on in-domain, out-of-domain,\nsimulation, and real-robot platforms demonstrate that VLA-R1 achieves superior\ngeneralization and real-world performance compared to prior VLA methods. We\nplan to release the model, code, and dataset following the publication of this\nwork. Code: https://github.com/GigaAI-research/VLA-R1. Website:\nhttps://gigaai-research.github.io/VLA-R1.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action (VLA) models, which can be considered a type of large model. The abstract mentions trajectory consistency and action generation, indicating a tangential relationship to trajectory prediction. While not a direct trajectory prediction paper, the mention of trajectory consistency and the use of reinforcement learning to improve action generation within a VLA model suggests some relevance.", "keywords": ["Vision-Language-Action models", "VLA", "trajectory consistency", "reinforcement learning", "reasoning", "action generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01531", "pdf": "https://arxiv.org/pdf/2510.01531", "abs": "https://arxiv.org/abs/2510.01531", "authors": ["Djengo Cyun-Jyun Fang", "Tsung-Wei Ke"], "title": "Information Seeking for Robust Decision Making under Partial Observability", "categories": ["cs.AI", "cs.CL", "cs.RO"], "comment": "The project page is available at https://infoseekerllm.github.io", "summary": "Explicit information seeking is essential to human problem-solving in\npractical environments characterized by incomplete information and noisy\ndynamics. When the true environmental state is not directly observable, humans\nseek information to update their internal dynamics and inform future\ndecision-making. Although existing Large Language Model (LLM) planning agents\nhave addressed observational uncertainty, they often overlook discrepancies\nbetween their internal dynamics and the actual environment. We introduce\nInformation Seeking Decision Planner (InfoSeeker), an LLM decision-making\nframework that integrates task-oriented planning with information seeking to\nalign internal dynamics and make optimal decisions under uncertainty in both\nagent observations and environmental dynamics. InfoSeeker prompts an LLM to\nactively gather information by planning actions to validate its understanding,\ndetect environmental changes, or test hypotheses before generating or revising\ntask-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark\nsuite featuring partially observable environments with incomplete observations\nand uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74%\nabsolute performance gain over prior methods without sacrificing sample\nefficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms\nbaselines on established benchmarks such as robotic manipulation and web\nnavigation. These findings underscore the importance of tightly integrating\nplanning and information seeking for robust behavior in partially observable\nenvironments. The project page is available at https://infoseekerllm.github.io", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Large Language Models (LLMs) for decision-making in partially observable environments, where the agent needs to actively seek information. While the environment could potentially involve trajectory prediction (e.g., robotic manipulation), the core focus is on information seeking and decision planning with LLMs rather than trajectory prediction itself. The connection to trajectory prediction is implicit but not central. The paper explicitly mentions LLMs.", "keywords": ["Large Language Models", "LLMs", "decision-making", "partially observable environments", "planning", "information seeking"]}, "AI": {"tldr": "InfoSeeker \u901a\u8fc7\u6574\u5408\u4efb\u52a1\u5bfc\u5411\u89c4\u5212\u548c\u4fe1\u606f\u641c\u5bfb\uff0c\u663e\u8457\u63d0\u5347\u4e86 LLM \u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u73b0\u6709 LLM \u89c4\u5212\u667a\u80fd\u4f53\u5ffd\u7565\u4e86\u5176\u5185\u90e8\u52a8\u6001\u4e0e\u5b9e\u9645\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u4e0d\u5b8c\u6574\u4fe1\u606f\u548c\u52a8\u6001\u53d8\u5316\u3002", "method": "InfoSeeker \u6846\u67b6\u63d0\u793a LLM \u4e3b\u52a8\u6536\u96c6\u4fe1\u606f\uff0c\u901a\u8fc7\u89c4\u5212\u884c\u52a8\u6765\u9a8c\u8bc1\u5176\u7406\u89e3\u3001\u68c0\u6d4b\u73af\u5883\u53d8\u5316\u6216\u6d4b\u8bd5\u5047\u8bbe\uff0c\u4ece\u800c\u751f\u6210\u6216\u4fee\u8ba2\u4efb\u52a1\u5bfc\u5411\u8ba1\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cInfoSeeker \u5728\u6027\u80fd\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e86 74%\uff0c\u5e76\u4e14\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u7f51\u9875\u5bfc\u822a\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\uff0c\u7d27\u5bc6\u6574\u5408\u89c4\u5212\u548c\u4fe1\u606f\u641c\u5bfb\u5bf9\u4e8e\u5b9e\u73b0\u7a33\u5065\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002", "summary_zh": "\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u548c\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u4e2d\uff0c\u4eba\u7c7b\u89e3\u51b3\u95ee\u9898\u9700\u8981\u660e\u786e\u7684\u4fe1\u606f\u641c\u5bfb\u3002\u5f53\u65e0\u6cd5\u76f4\u63a5\u89c2\u5bdf\u5230\u771f\u5b9e\u73af\u5883\u72b6\u6001\u65f6\uff0c\u4eba\u7c7b\u4f1a\u5bfb\u6c42\u4fe1\u606f\u6765\u66f4\u65b0\u5176\u5185\u90e8\u52a8\u6001\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u51b3\u7b56\u63d0\u4f9b\u4fe1\u606f\u3002\u867d\u7136\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89c4\u5212\u667a\u80fd\u4f53\u5df2\u7ecf\u89e3\u51b3\u4e86\u89c2\u6d4b\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u7ecf\u5e38\u5ffd\u7565\u5176\u5185\u90e8\u52a8\u6001\u4e0e\u5b9e\u9645\u73af\u5883\u4e4b\u95f4\u7684\u5dee\u5f02\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4fe1\u606f\u641c\u5bfb\u51b3\u7b56\u89c4\u5212\u5668\uff08InfoSeeker\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a LLM \u51b3\u7b56\u6846\u67b6\uff0c\u5b83\u6574\u5408\u4e86\u4efb\u52a1\u5bfc\u5411\u89c4\u5212\u548c\u4fe1\u606f\u641c\u5bfb\uff0c\u4ee5\u5728\u667a\u80fd\u4f53\u89c2\u6d4b\u548c\u73af\u5883\u52a8\u6001\u7684\u4e0d\u786e\u5b9a\u6027\u4e0b\u5bf9\u9f50\u5185\u90e8\u52a8\u6001\u5e76\u505a\u51fa\u6700\u4f73\u51b3\u7b56\u3002InfoSeeker \u63d0\u793a LLM \u901a\u8fc7\u89c4\u5212\u884c\u52a8\u6765\u4e3b\u52a8\u6536\u96c6\u4fe1\u606f\uff0c\u4ee5\u9a8c\u8bc1\u5176\u7406\u89e3\u3001\u68c0\u6d4b\u73af\u5883\u53d8\u5316\u6216\u5728\u751f\u6210\u6216\u4fee\u6539\u4efb\u52a1\u5bfc\u5411\u8ba1\u5212\u4e4b\u524d\u6d4b\u8bd5\u5047\u8bbe\u3002\u4e3a\u4e86\u8bc4\u4f30 InfoSeeker\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5176\u4e2d\u5305\u542b\u5177\u6709\u4e0d\u5b8c\u6574\u89c2\u6d4b\u548c\u4e0d\u786e\u5b9a\u52a8\u6001\u7684\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u3002\u5b9e\u9a8c\u8868\u660e\uff0cInfoSeeker \u7684\u6027\u80fd\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e86 74%\uff0c\u4e14\u4e0d\u727a\u7272\u6837\u672c\u6548\u7387\u3002\u6b64\u5916\uff0cInfoSeeker \u53ef\u4ee5\u5728 LLM \u4e4b\u95f4\u63a8\u5e7f\uff0c\u5e76\u4e14\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u548c Web \u5bfc\u822a\u7b49\u5df2\u5efa\u7acb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u7d27\u5bc6\u6574\u5408\u89c4\u5212\u548c\u4fe1\u606f\u641c\u5bfb\u5bf9\u4e8e\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u884c\u4e3a\u7684\u91cd\u8981\u6027\u3002\u9879\u76ee\u9875\u9762\u4f4d\u4e8e https://infoseekerllm.github.io"}}
{"id": "2510.01586", "pdf": "https://arxiv.org/pdf/2510.01586", "abs": "https://arxiv.org/abs/2510.01586", "authors": ["Zhenyu Pan", "Yiting Zhang", "Zhuo Liu", "Yolo Yunlong Tang", "Zeliang Zhang", "Haozheng Luo", "Yuwei Han", "Jianshu Zhang", "Dennis Wu", "Hong-Yu Chen", "Haoran Lu", "Haoyang Fang", "Manling Li", "Chenliang Xu", "Philip S. Yu", "Han Liu"], "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning", "categories": ["cs.AI"], "comment": null, "summary": "LLM-based multi-agent systems excel at planning, tool use, and role\ncoordination, but their openness and interaction complexity also expose them to\njailbreak, prompt-injection, and adversarial collaboration. Existing defenses\nfall into two lines: (i) self-verification that asks each agent to pre-filter\nunsafe instructions before execution, and (ii) external guard modules that\npolice behaviors. The former often underperforms because a standalone agent\nlacks sufficient capacity to detect cross-agent unsafe chains and\ndelegation-induced risks; the latter increases system overhead and creates a\nsingle-point-of-failure-once compromised, system-wide safety collapses, and\nadding more guards worsens cost and complexity. To solve these challenges, we\npropose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning\nframework that internalizes safety into task agents. Rather than relying on\nexternal guards, AdvEvo-MARL jointly optimizes attackers (which synthesize\nevolving jailbreak prompts) and defenders (task agents trained to both\naccomplish their duties and resist attacks) in adversarial learning\nenvironments. To stabilize learning and foster cooperation, we introduce a\npublic baseline for advantage estimation: agents within the same functional\ngroup share a group-level mean-return baseline, enabling lower-variance updates\nand stronger intra-group coordination. Across representative attack scenarios,\nAdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas\nbaselines reach up to 38.33%, while preserving-and sometimes improving-task\naccuracy (up to +3.67% on reasoning tasks). These results show that safety and\nutility can be jointly improved without relying on extra guard agents or added\nsystem overhead.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u57fa\u4e8eLLM\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5305\u62ec\u5bf9\u6297\u653b\u51fb\u548c\u9632\u5fa1\u3002\u867d\u7136\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u4e0d\u8fc7\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86LLM\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["LLM", "Large Language Models", "multi-agent reinforcement learning", "adversarial learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01795", "pdf": "https://arxiv.org/pdf/2510.01795", "abs": "https://arxiv.org/abs/2510.01795", "authors": ["Haibo Hu", "Lianming Huang", "Xinyu Wang", "Yufei Cui", "Nan Guan", "Chun Jason Xue"], "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving\nfor unified perception and reasoning, but high inference latency hinders\nreal-time deployment. Early-exit reduces latency by terminating inference at\nintermediate layers, yet its task-dependent nature limits generalization across\ndiverse scenarios. We observe that this limitation aligns with autonomous\ndriving: navigation systems can anticipate upcoming contexts (e.g.,\nintersections, traffic lights), indicating which tasks will be required. We\npropose Nav-EE, a navigation-guided early-exit framework that precomputes\ntask-specific exit layers offline and dynamically applies them online based on\nnavigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE\nachieves accuracy comparable to full inference while reducing latency by up to\n63.9%. Real-vehicle integration with Autoware Universe further demonstrates\nreduced inference latency (600ms to 300ms), supporting faster decision-making\nin complex scenarios. These results suggest that coupling navigation foresight\nwith early-exit offers a viable path toward efficient deployment of large\nmodels in autonomous systems. Code and data are available at our anonymous\nrepository: https://anonymous.4open.science/r/Nav-EE-BBC4", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u5bfc\u822a\u4fe1\u606f\u5f15\u5bfc\u63d0\u524d\u9000\u51fa\u673a\u5236\u6765\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5bfc\u822a\u4fe1\u606f\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u672a\u6765\u8f68\u8ff9\u7684\u67d0\u79cd\u5f62\u5f0f\u7684\u9884\u6d4b\uff0c\u5e76\u4e14\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language Models", "autonomous driving", "navigation", "early-exit", "large models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01843", "pdf": "https://arxiv.org/pdf/2510.01843", "abs": "https://arxiv.org/abs/2510.01843", "authors": ["Wanyue Li", "Ji Ma", "Minghao Lu", "Peng Lu"], "title": "Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots", "categories": ["cs.RO", "I.2.9; I.2.8; G.1.6"], "comment": "8 pages, 8 figures, conference paper", "summary": "Humanoid robot soccer presents several challenges, particularly in\nmaintaining system stability during aggressive kicking motions while achieving\nprecise ball trajectory control. Current solutions, whether traditional\nposition-based control methods or reinforcement learning (RL) approaches,\nexhibit significant limitations. Model predictive control (MPC) is a prevalent\napproach for ordinary quadruped and biped robots. While MPC has demonstrated\nadvantages in legged robots, existing studies often oversimplify the leg swing\nprogress, relying merely on simple trajectory interpolation methods. This\nseverely constrains the foot's environmental interaction capability, hindering\ntasks such as ball kicking. This study innovatively adapts the spatial-temporal\ntrajectory planning method, which has been successful in drone applications, to\nbipedal robotic systems. The proposed approach autonomously generates foot\ntrajectories that satisfy constraints on target kicking position, velocity, and\nacceleration while simultaneously optimizing swing phase duration. Experimental\nresults demonstrate that the optimized trajectories closely mimic human kicking\nbehavior, featuring a backswing motion. Simulation and hardware experiments\nconfirm the algorithm's efficiency, with trajectory planning times under 1 ms,\nand its reliability, achieving nearly 100 % task completion accuracy when the\nsoccer goal is within the range of -90{\\deg} to 90{\\deg}.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u53cc\u8db3\u673a\u5668\u4eba\u7684\u8db3\u7403\u8e22\u7403\u63a7\u5236\uff0c\u5176\u4e2d\u6d89\u53ca\u8db3\u90e8\u8f68\u8ff9\u7684\u4f18\u5316\u548c\u89c4\u5212\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\uff08\u7279\u522b\u662f\u8db3\u90e8\u8f68\u8ff9\uff09\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u8fd9\u79cd\u5e38\u89c1\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "foot trajectories", "bipedal robots", "model predictive control", "spatial-temporal trajectory planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01869", "pdf": "https://arxiv.org/pdf/2510.01869", "abs": "https://arxiv.org/abs/2510.01869", "authors": ["Alessandro Nazzari", "Roberto Rubinacci", "Marco Lovera"], "title": "TACOS: Task Agnostic COordinator of a multi-drone System", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "6 pages, 6 figures, accepted as poster at 2025 IEEE International\n  Symposium on Multi-Robot & Multi-Agent Systems", "summary": "When a single pilot is responsible for managing a multi-drone system, the\ntask demands varying levels of autonomy, from direct control of individual\nUAVs, to group-level coordination, to fully autonomous swarm behaviors for\naccomplishing high-level tasks. Enabling such flexible interaction requires a\nframework that supports multiple modes of shared autonomy. As language models\ncontinue to improve in reasoning and planning, they provide a natural\nfoundation for such systems, reducing pilot workload by enabling high-level\ntask delegation through intuitive, language-based interfaces. In this paper we\npresent TACOS (Task-Agnostic COordinator of a multi-drone System), a unified\nframework that enables high-level natural language control of multi-UAV systems\nthrough Large Language Models (LLMs). TACOS integrates three key capabilities\ninto a single architecture: a one-to-many natural language interface for\nintuitive user interaction, an intelligent coordinator for translating user\nintent into structured task plans, and an autonomous agent that executes plans\ninteracting with the real-world. TACOS allows a LLM to interact with a library\nof executable APIs, bridging semantic reasoning with real-time multi-robot\ncoordination. We demonstrate the system in real-world multi-drone system and\nconduct an ablation study to assess the contribution of each module.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Large Language Models (LLMs) for controlling multi-drone systems through natural language. While it doesn't directly address trajectory prediction algorithms, the control of drones inherently involves trajectory planning and execution. The LLM aspect is a strong indicator of relevance. Therefore, it has a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "multi-drone system", "natural language control", "autonomous agent", "task planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01991", "pdf": "https://arxiv.org/pdf/2510.01991", "abs": "https://arxiv.org/abs/2510.01991", "authors": ["Lei Liu", "Can Wang", "Zhenghao Chen", "Dong Xu"], "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges\nwith view, temporal, and non-editing region consistency, as well as with\nhandling complex text instructions. To address these issues, we propose\n4DGS-Craft, a consistent and interactive 4DGS editing framework. We first\nintroduce a 4D-aware InstructPix2Pix model to ensure both view and temporal\nconsistency. This model incorporates 4D VGGT geometry features extracted from\nthe initial scene, enabling it to capture underlying 4D geometric structures\nduring editing. We further enhance this model with a multi-view grid module\nthat enforces consistency by iteratively refining multi-view input images while\njointly optimizing the underlying 4D scene. Furthermore, we preserve the\nconsistency of non-edited regions through a novel Gaussian selection mechanism,\nwhich identifies and optimizes only the Gaussians within the edited regions.\nBeyond consistency, facilitating user interaction is also crucial for effective\n4DGS editing. Therefore, we design an LLM-based module for user intent\nunderstanding. This module employs a user instruction template to define atomic\nediting operations and leverages an LLM for reasoning. As a result, our\nframework can interpret user intent and decompose complex instructions into a\nlogical sequence of atomic operations, enabling it to handle intricate user\ncommands and further enhance editing performance. Compared to related works,\nour approach enables more consistent and controllable 4D scene editing. Our\ncode will be made available upon acceptance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses 4D Gaussian Splatting editing, which involves understanding user intent and decomposing instructions using an LLM. While the primary focus is not trajectory prediction, the LLM-based module for user intent understanding is relevant. The 4D aspect could potentially be extended to dynamic scenes and trajectories in the future, although not directly addressed here.", "keywords": ["Large Language Models", "LLM"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01545", "pdf": "https://arxiv.org/pdf/2510.01545", "abs": "https://arxiv.org/abs/2510.01545", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Predictive Preference Learning from Human Interventions", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "NeurIPS 2025 Spotlight. Project page:\n  https://metadriverse.github.io/ppl", "summary": "Learning from human involvement aims to incorporate the human subject to\nmonitor and correct agent behavior errors. Although most interactive imitation\nlearning methods focus on correcting the agent's action at the current state,\nthey do not adjust its actions in future states, which may be potentially more\nhazardous. To address this, we introduce Predictive Preference Learning from\nHuman Interventions (PPL), which leverages the implicit preference signals\ncontained in human interventions to inform predictions of future rollouts. The\nkey idea of PPL is to bootstrap each human intervention into L future time\nsteps, called the preference horizon, with the assumption that the agent\nfollows the same action and the human makes the same intervention in the\npreference horizon. By applying preference optimization on these future states,\nexpert corrections are propagated into the safety-critical regions where the\nagent is expected to explore, significantly improving learning efficiency and\nreducing human demonstrations needed. We evaluate our approach with experiments\non both autonomous driving and robotic manipulation benchmarks and demonstrate\nits efficiency and generality. Our theoretical analysis further shows that\nselecting an appropriate preference horizon L balances coverage of risky states\nwith label correctness, thereby bounding the algorithmic optimality gap. Demo\nand code are available at: https://metadriverse.github.io/ppl", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ece\u4eba\u7c7b\u5e72\u9884\u4e2d\u5b66\u4e60\u9884\u6d4b\u504f\u597d\uff0c\u4ee5\u6539\u8fdb\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u5728\u81ea\u4e3b\u9a7e\u9a76\u9886\u57df\u7684\u5e94\u7528\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u8bba\u6587\u7684\u6838\u5fc3\u5728\u4e8e\u5229\u7528\u4eba\u7c7b\u5e72\u9884\u4fe1\u53f7\u9884\u6d4b\u672a\u6765\u7684\u884c\u52a8\u8f68\u8ff9\uff0c\u5e76\u4f18\u5316\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u81ea\u4e3b\u9a7e\u9a76", "\u52a8\u4f5c\u9884\u6d4b", "\u9884\u6d4b\u504f\u597d", "\u4eba\u7c7b\u5e72\u9884"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.01538", "pdf": "https://arxiv.org/pdf/2510.01538", "abs": "https://arxiv.org/abs/2510.01538", "authors": ["Haokun Zhao", "Xiang Zhang", "Jiaqi Wei", "Yiwei Xu", "Yuting He", "Siqi Sun", "Chenyu You"], "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis", "categories": ["cs.LG"], "comment": null, "summary": "Time series forecasting is central to decision-making in domains as diverse\nas energy, finance, climate, and public health. In practice, forecasters face\nthousands of short, noisy series that vary in frequency, quality, and horizon,\nwhere the dominant cost lies not in model fitting, but in the labor-intensive\npreprocessing, validation, and ensembling required to obtain reliable\npredictions. Prevailing statistical and deep learning models are tailored to\nspecific datasets or domains and generalize poorly. A general, domain-agnostic\nframework that minimizes human intervention is urgently in demand. In this\npaper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic\nframework for general time series forecasting. The framework comprises four\nspecialized agents: Curator performs LLM-guided diagnostics augmented by\nexternal tools that reason over data statistics to choose targeted\npreprocessing; Planner narrows the hypothesis space of model choice by\nleveraging multi-modal diagnostics and self-planning over the input; Forecaster\nperforms model fitting and validation and, based on the results, adaptively\nselects the best model configuration as well as ensemble strategy to make final\npredictions; and Reporter synthesizes the whole process into a comprehensive,\ntransparent report. With transparent natural-language rationales and\ncomprehensive reports, TSci transforms the forecasting workflow into a\nwhite-box system that is both interpretable and extensible across tasks.\nEmpirical results on eight established benchmarks demonstrate that TSci\nconsistently outperforms both statistical and LLM-based baselines, reducing\nforecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci\nproduces a clear and rigorous report that makes the forecasting workflow more\ntransparent and interpretable.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on time series forecasting using a large language model (LLM) as an agent. While not directly related to trajectory prediction, time series forecasting is a relevant technique that can be applied to trajectory prediction tasks. The paper's use of LLMs and its focus on forecasting make it somewhat relevant.", "keywords": ["Large Language Models", "LLM", "time series forecasting"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
