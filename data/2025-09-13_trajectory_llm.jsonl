{"id": "2509.09200", "pdf": "https://arxiv.org/pdf/2509.09200", "abs": "https://arxiv.org/abs/2509.09200", "authors": ["Ge Sun", "Jun Ma"], "title": "MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network", "categories": ["cs.CV"], "comment": null, "summary": "Accurate human trajectory prediction is crucial for robotics navigation and\nautonomous driving. Recent research has demonstrated that incorporating goal\nguidance significantly enhances prediction accuracy by reducing uncertainty and\nleveraging prior knowledge. Most goal-guided approaches decouple the prediction\ntask into two stages: goal prediction and subsequent trajectory completion\nbased on the predicted goal, which operate at extreme granularities:\ncoarse-grained goal prediction forecasts the overall intention, while\nfine-grained trajectory completion needs to generate the positions for all\nfuture timesteps. The potential utility of intermediate temporal granularity\nremains largely unexplored, which motivates multi-granularity trajectory\nmodeling. While prior work has shown that multi-granularity representations\ncapture diverse scales of human dynamics and motion patterns, effectively\nintegrating this concept into goal-guided frameworks remains challenging. In\nthis paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for\nhuman Trajectory prediction. MGTraj recursively encodes trajectory proposals\nfrom coarse to fine granularity levels. At each level, a transformer-based\nrecursive refinement network (RRN) captures features and predicts progressive\nrefinements. Features across different granularities are integrated using a\nweight-sharing strategy, and velocity prediction is employed as an auxiliary\ntask to further enhance performance. Comprehensive experimental results in\nEHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline\nmethods and achieves state-of-the-art performance among goal-guided methods.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u4e86Transformer\u7ed3\u6784\uff0cTransformer\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u3002\u867d\u7136\u8bba\u6587\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u8ba8\u8bba\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u9886\u57df\u548c\u4f7f\u7528\u7684\u6a21\u578b\u7ed3\u6784\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002", "keywords": ["trajectory prediction", "human trajectory prediction", "goal-guided", "transformer", "motion patterns"]}}
{"id": "2509.09210", "pdf": "https://arxiv.org/pdf/2509.09210", "abs": "https://arxiv.org/abs/2509.09210", "authors": ["Xing Gao", "Zherui Huang", "Weiyao Lin", "Xiao Sun"], "title": "ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Accurate motion prediction of surrounding agents is crucial for the safe\nplanning of autonomous vehicles. Recent advancements have extended prediction\ntechniques from individual agents to joint predictions of multiple interacting\nagents, with various strategies to address complex interactions within future\nmotions of agents. However, these methods overlook the evolving nature of these\ninteractions. To address this limitation, we propose a novel progressive\nmulti-scale decoding strategy, termed ProgD, with the help of dynamic\nheterogeneous graph-based scenario modeling. In particular, to explicitly and\ncomprehensively capture the evolving social interactions in future scenarios,\ngiven their inherent uncertainty, we design a progressive modeling of scenarios\nwith dynamic heterogeneous graphs. With the unfolding of such dynamic\nheterogeneous graphs, a factorized architecture is designed to process the\nspatio-temporal dependencies within future scenarios and progressively\neliminate uncertainty in future motions of multiple agents. Furthermore, a\nmulti-scale decoding procedure is incorporated to improve on the future\nscenario modeling and consistent prediction of agents' future motion. The\nproposed ProgD achieves state-of-the-art performance on the INTERACTION\nmulti-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2\nmulti-world forecasting benchmark.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86\u52a8\u6001\u56fe\u6765\u5efa\u6a21\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u5e76\u53d6\u5f97\u4e86\u8f83\u597d\u7684\u9884\u6d4b\u6548\u679c\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\uff0c\u4e14\u4f7f\u7528\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u4e0e\u4f7f\u7528\u5927\u578b\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\uff0c\u4f8b\u5982\u53ef\u4ee5\u4f7f\u7528LLM\u6765\u8f85\u52a9\u751f\u6210\u66f4\u597d\u7684graph\u7ed3\u6784\u3002", "keywords": ["motion prediction", "trajectory prediction", "multi-agent", "dynamic graphs", "autonomous vehicles"]}}
{"id": "2509.09074", "pdf": "https://arxiv.org/pdf/2509.09074", "abs": "https://arxiv.org/abs/2509.09074", "authors": ["Alice Kate Li", "Thales C Silva", "Victoria Edwards", "Vijay Kumar", "M. Ani Hsieh"], "title": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to CoRL 2025 (Conference on Robot Learning). 15 pages 11\n  figures", "summary": "In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such\nthat it converges to the trajectory's end point. Despite demonstrated efficacy\nin using Koopman operator theory for modeling dynamical systems, Koopman does\nnot inherently enforce convergence to desired trajectories nor to specified\ngoals -- a requirement when learning from demonstrations (LfD). We present\nKoopMotion which represents motion flow fields as dynamical systems,\nparameterized by Koopman Operators to mimic desired trajectories, and leverages\nthe divergence properties of the learnt flow fields to obtain smooth motion\nfields that converge to a desired reference trajectory when a robot is placed\naway from the desired trajectory, and tracks the trajectory until the end\npoint. To demonstrate the effectiveness of our approach, we show evaluations of\nKoopMotion on the LASA human handwriting dataset and a 3D manipulator\nend-effector trajectory dataset, including spectral analysis. We also perform\nexperiments on a physical robot, verifying KoopMotion on a miniature autonomous\nsurface vehicle operating in a non-static fluid flow environment. Our approach\nis highly sample efficient in both space and time, requiring only 3\\% of the\nLASA dataset to generate dense motion plans. Additionally, KoopMotion provides\na significant improvement over baselines when comparing metrics that measure\nspatial and temporal dynamics modeling efficacy.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528Koopman\u7b97\u5b50\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\uff0c\u7279\u522b\u662f\u5b66\u4e60\u51e0\u4e4e\u65e0\u6563\u5ea6\u7684Koopman\u6d41\u573a\u3002 \u867d\u7136\u5b83\u786e\u5b9e\u6d89\u53ca\u8fd0\u52a8\u89c4\u5212\u548c\u8f68\u8ff9\u76f8\u5173\u6982\u5ff5\uff08\u4f8b\u5982\uff0c\u53c2\u8003\u8f68\u8ff9\uff0c\u6536\u655b\uff09\uff0c\u4f46\u5b83\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u8054\u7cfb\u3002 \u5b83\u4f7f\u7528\u6570\u636e\u96c6\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\uff0c\u4f46\u91cd\u70b9\u5728\u4e8e\u8fd0\u52a8\u89c4\u5212\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u3002\u76f8\u5173\u6027\u8bc4\u5206\u4e3a0.6\uff0c\u56e0\u4e3a\u867d\u7136\u6d89\u53ca\u8fd0\u52a8\u89c4\u5212\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u6838\u5fc3\u4efb\u52a1\uff08\u5373\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\uff09\u7565\u6709\u4e0d\u540c\uff0c\u5e76\u4e14\u4e0d\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["motion planning", "trajectory", "Koopman operator", "flow field", "dynamical systems"]}}
{"id": "2509.09206", "pdf": "https://arxiv.org/pdf/2509.09206", "abs": "https://arxiv.org/abs/2509.09206", "authors": ["Farhad Nawaz", "Faizan M. Tariq", "Sangjae Bae", "David Isele", "Avinash Singh", "Nadia Figueroa", "Nikolai Matni", "Jovin D'sa"], "title": "Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Accurately reasoning about future parking spot availability and integrated\nplanning is critical for enabling safe and efficient autonomous valet parking\nin dynamic, uncertain environments. Unlike existing methods that rely solely on\ninstantaneous observations or static assumptions, we present an approach that\npredicts future parking spot occupancy by explicitly distinguishing between\ninitially vacant and occupied spots, and by leveraging the predicted motion of\ndynamic agents. We introduce a probabilistic spot occupancy estimator that\nincorporates partial and noisy observations within a limited Field-of-View\n(FoV) model and accounts for the evolving uncertainty of unobserved regions.\nCoupled with this, we design a strategy planner that adaptively balances\ngoal-directed parking maneuvers with exploratory navigation based on\ninformation gain, and intelligently incorporates wait-and-go behaviors at\npromising spots. Through randomized simulations emulating large parking lots,\nwe demonstrate that our framework significantly improves parking efficiency,\nsafety margins, and trajectory smoothness compared to existing approaches.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u89c4\u5212\uff0c\u7279\u522b\u662f\u9488\u5bf9\u81ea\u52a8\u6cca\u8f66\u573a\u666f\u3002\u867d\u7136\u6d89\u53ca\u52a8\u6001\u73af\u5883\u548c\u79fb\u52a8agent\u7684\u8fd0\u52a8\u9884\u6d4b\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u8f68\u8ff9\u9884\u6d4b\u662f\u5176\u6838\u5fc3\uff0c\u4f46\u6ca1\u6709\u6d89\u53caLLMs\u3002", "keywords": ["trajectory planning", "autonomous valet parking", "trajectory prediction", "dynamic environments", "motion prediction"]}}
{"id": "2509.09332", "pdf": "https://arxiv.org/pdf/2509.09332", "abs": "https://arxiv.org/abs/2509.09332", "authors": ["Yuecheng Liu", "Dafeng Chi", "Shiguang Wu", "Zhanguang Zhang", "Yuzheng Zhuang", "Bowen Yang", "He Zhu", "Lingfeng Zhang", "Pengwei Xie", "David Gamaliel Arcos Bravo", "Yingxue Zhang", "Jianye Hao", "Xingyue Quan"], "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5177\u8eab\u667a\u80fd\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u6d89\u53ca\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u548c\u51b3\u7b56\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4efb\u52a1\u89c4\u5212\u53ef\u4ee5\u770b\u4f5c\u662f\u5e7f\u4e49\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u4e14\u660e\u786e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["large language models", "MLLM", "embodied intelligence", "task planning", "reasoning"]}}
{"id": "2509.09372", "pdf": "https://arxiv.org/pdf/2509.09372", "abs": "https://arxiv.org/abs/2509.09372", "authors": ["Yihao Wang", "Pengxiang Ding", "Lingxiao Li", "Can Cui", "Zirui Ge", "Xinyang Tong", "Wenxuan Song", "Han Zhao", "Wei Zhao", "Pengxu Hou", "Siteng Huang", "Yifan Tang", "Wenhui Wang", "Ru Zhang", "Jianyi Liu", "Donglin Wang"], "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Vision-Language-Action (VLA) models, which are related to robotics and action prediction. While not directly trajectory prediction, the 'action' component can involve predicting future actions or movements. The paper also mentions large-scale Vision-Language Models (VLMs), connecting it to the 'Large Models' theme. However, the primary focus is on efficient training of VLA models, rather than specifically trajectory prediction or the properties of large language models themselves.", "keywords": ["Vision-Language-Action models", "VLA", "Vision-Language Model", "VLM", "action", "Policy module"]}}
{"id": "2509.09284", "pdf": "https://arxiv.org/pdf/2509.09284", "abs": "https://arxiv.org/abs/2509.09284", "authors": ["Bingning Huang", "Tu Nguyen", "Matthieu Zimmer"], "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses using Monte Carlo Tree Search (MCTS) with Large Language Models (LLMs) for improving policy optimization in reinforcement learning. While it doesn't directly focus on trajectory prediction in the traditional sense (e.g., predicting pedestrian or vehicle paths), the MCTS-derived trajectories and policy optimization aspects have some relevance. The use of LLMs is also a key factor.", "keywords": ["Large Language Models", "LLMs", "Monte Carlo Tree Search", "MCTS", "policy optimization", "trajectories"]}}
{"id": "2509.09349", "pdf": "https://arxiv.org/pdf/2509.09349", "abs": "https://arxiv.org/abs/2509.09349", "authors": ["Ian Nell", "Shane Gilroy"], "title": "Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.RO", "eess.IV"], "comment": null, "summary": "Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behavior classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviors such as excessive lateral movement and erratic trajectory patterns by\nimplementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioral analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on driver behavior classification using computer vision techniques, specifically object tracking and trajectory analysis. While it touches upon trajectory patterns, it doesn't explicitly involve large language models. The connection to trajectory prediction is moderate due to the analysis of driver behavior and trajectory patterns, but there's no mention of LLMs.", "keywords": ["trajectory patterns", "object tracking"]}}
{"id": "2509.09496", "pdf": "https://arxiv.org/pdf/2509.09496", "abs": "https://arxiv.org/abs/2509.09496", "authors": ["Ha Linh Nguyen", "Tze Ho Elden Tse", "Angela Yao"], "title": "Improving Human Motion Plausibility with Body Momentum", "categories": ["cs.CV"], "comment": "Accepted at BMVC 2025", "summary": "Many studies decompose human motion into local motion in a frame attached to\nthe root joint and global motion of the root joint in the world frame, treating\nthem separately. However, these two components are not independent. Global\nmovement arises from interactions with the environment, which are, in turn,\ndriven by changes in the body configuration. Motion models often fail to\nprecisely capture this physical coupling between local and global dynamics,\nwhile deriving global trajectories from joint torques and external forces is\ncomputationally expensive and complex. To address these challenges, we propose\nusing whole-body linear and angular momentum as a constraint to link local\nmotion with global movement. Since momentum reflects the aggregate effect of\njoint-level dynamics on the body's movement through space, it provides a\nphysically grounded way to relate local joint behavior to global displacement.\nBuilding on this insight, we introduce a new loss term that enforces\nconsistency between the generated momentum profiles and those observed in\nground-truth data. Incorporating our loss reduces foot sliding and jitter,\nimproves balance, and preserves the accuracy of the recovered motion. Code and\ndata are available at the project page https://hlinhn.github.io/momentum_bmvc.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving the plausibility of human motion by incorporating body momentum. While it doesn't directly use or discuss Large Language Models, it does relate to trajectory prediction and human motion modeling, which are relevant to the broader field of trajectory prediction. The paper aims to improve the accuracy and realism of human motion, which is a common goal in trajectory prediction tasks, especially those involving human agents.", "keywords": ["human motion", "trajectory prediction", "motion modeling", "global motion"]}}
{"id": "2509.09530", "pdf": "https://arxiv.org/pdf/2509.09530", "abs": "https://arxiv.org/abs/2509.09530", "authors": ["Paul F. R. Wilson", "Matteo Ronchetti", "R\u00fcdiger G\u00f6bl", "Viktoria Markova", "Sebastian Rosenzweig", "Raphael Prevost", "Parvin Mousavi", "Oliver Zettinig"], "title": "DualTrack: Sensorless 3D Ultrasound needs Local and Global Context", "categories": ["cs.CV"], "comment": null, "summary": "Three-dimensional ultrasound (US) offers many clinical advantages over\nconventional 2D imaging, yet its widespread adoption is limited by the cost and\ncomplexity of traditional 3D systems. Sensorless 3D US, which uses deep\nlearning to estimate a 3D probe trajectory from a sequence of 2D US images, is\na promising alternative. Local features, such as speckle patterns, can help\npredict frame-to-frame motion, while global features, such as coarse shapes and\nanatomical structures, can situate the scan relative to anatomy and help\npredict its general shape. In prior approaches, global features are either\nignored or tightly coupled with local feature extraction, restricting the\nability to robustly model these two complementary aspects. We propose\nDualTrack, a novel dual-encoder architecture that leverages decoupled local and\nglobal encoders specialized for their respective scales of feature extraction.\nThe local encoder uses dense spatiotemporal convolutions to capture\nfine-grained features, while the global encoder utilizes an image backbone\n(e.g., a 2D CNN or foundation model) and temporal attention layers to embed\nhigh-level anatomical features and long-range dependencies. A lightweight\nfusion module then combines these features to estimate the trajectory.\nExperimental results on a large public benchmark show that DualTrack achieves\nstate-of-the-art accuracy and globally consistent 3D reconstructions,\noutperforming previous methods and yielding an average reconstruction error\nbelow 5 mm.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses the estimation of 3D probe trajectory from 2D ultrasound images using deep learning. While the primary focus is on 3D ultrasound reconstruction, it involves predicting the trajectory of the probe. The abstract also mentions the use of a \"foundation model\" in the global encoder, suggesting a connection to larger pre-trained models. However, the paper doesn't explicitly use or discuss Large Language Models in the context of trajectory prediction, so the relevance is moderate.", "keywords": ["trajectory prediction", "foundation model", "deep learning"]}}
