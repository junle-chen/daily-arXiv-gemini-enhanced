{"id": "2510.03386", "pdf": "https://arxiv.org/pdf/2510.03386", "abs": "https://arxiv.org/abs/2510.03386", "authors": ["Zixuan Yi", "Sami Abu-el-Haija", "Yawen Wang", "Teja Vemparala", "Yannis Chronis", "Yu Gan", "Michael Burrows", "Carsten Binnig", "Bryan Perozzi", "Ryan Marcus", "Fatma Ozcan"], "title": "Is it Bigger than a Breadbox: Efficient Cardinality Estimation for Real World Workloads", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "DB engines produce efficient query execution plans by relying on cost models.\nPractical implementations estimate cardinality of queries using heuristics,\nwith magic numbers tuned to improve average performance on benchmarks.\nEmpirically, estimation error significantly grows with query complexity.\nAlternatively, learning-based estimators offer improved accuracy, but add\noperational complexity preventing their adoption in-practice. Recognizing that\nquery workloads contain highly repetitive subquery patterns, we learn many\nsimple regressors online, each localized to a pattern. The regressor\ncorresponding to a pattern can be randomly-accessed using hash of graph\nstructure of the subquery. Our method has negligible overhead and competes with\nSoTA learning-based approaches on error metrics. Further, amending PostgreSQL\nwith our method achieves notable accuracy and runtime improvements over\ntraditional methods and drastically reduces operational costs compared to other\nlearned cardinality estimators, thereby offering the most practical and\nefficient solution on the Pareto frontier. Concretely, simulating JOB-lite\nworkload on IMDb speeds-up execution by 7.5 minutes (>30%) while incurring only\n37 seconds overhead for online learning."}
{"id": "2510.04014", "pdf": "https://arxiv.org/pdf/2510.04014", "abs": "https://arxiv.org/abs/2510.04014", "authors": ["Kai Cao", "Yucong Duan", "Wensheng Gan"], "title": "Dual Pruning and Sorting-Free Overestimation for Average-Utility Sequential Pattern Mining", "categories": ["cs.DB"], "comment": "preprint, 13 figures, 4 tables", "summary": "In a quantitative sequential database, numerous efficient algorithms have\nbeen developed for high-utility sequential pattern mining (HUSPM). HUSPM\nestablishes a relationship between frequency and significance in the real world\nand reflects more crucial information than frequent pattern mining. However,\nhigh average-utility sequential pattern mining (HAUSPM) is deemed fairer and\nmore valuable than HUSPM. It provides a reasonable measure for longer patterns\nby considering their length. In contrast to scenarios in retail business\nanalysis, some pattern mining applications, such as cybersecurity or artificial\nintelligence (AI), often involve much longer sequences. Consequently, pruning\nstrategies can exert a more pronounced impact on efficiency. This paper\nproposes a novel algorithm named HAUSP-PG, which adopts two complementary\nstrategies to independently process pattern prefixes and remaining sequences,\nthereby achieving a dual pruning effect. Additionally, the proposed method\ncalculates average utility upper bounds without requiring item sorting,\nsignificantly reducing computational time and memory consumption compared to\nalternative approaches. Through experiments conducted on both real-life and\nsynthetic datasets, we demonstrate that the proposed algorithm could achieve\nsatisfactory performance."}
{"id": "2510.04249", "pdf": "https://arxiv.org/pdf/2510.04249", "abs": "https://arxiv.org/abs/2510.04249", "authors": ["Yu-Ting Lin", "Hsin-Po Wang"], "title": "Ambidextrous Degree Sequence Bounds for Pessimistic Cardinality Estimation", "categories": ["cs.DB", "cs.IT", "math.IT"], "comment": "25 pages, 16 figures", "summary": "In a large database system, upper-bounding the cardinality of a join query is\na crucial task called $\\textit{pessimistic cardinality estimation}$. Recently,\nAbo Khamis, Nakos, Olteanu, and Suciu unified related works into the following\ndexterous framework. Step 1: Let $(X_1, \\dotsc, X_n)$ be a random row of the\njoin, equating $H(X_1, \\dotsc, X_n)$ to the log of the join cardinality. Step\n2: Upper-bound $H(X_1, \\dotsc, X_n)$ using Shannon-type inequalities such as\n$H(X, Y, Z) \\le H(X) + H(Y|X) + H(Z|Y)$. Step 3: Upper-bound $H(X_i) + p H(X_j\n| X_i)$ using the $p$-norm of the degree sequence of the underlying graph of a\nrelation.\n  While old bound in step 3 count \"claws $\\in$\" in the underlying graph, we\nproposed $\\textit{ambidextrous}$ bounds that count \"claw pairs\n${\\ni}\\!{-}\\!{\\in}$\". The new bounds are provably not looser and empirically\ntighter: they overestimate by $x^{3/4}$ times when the old bounds overestimate\nby $x$ times. An example is counting friend triples in the\n$\\texttt{com-Youtube}$ dataset, the best dexterous bound is $1.2 \\cdot 10^9$,\nthe best ambidextrous bound is $5.1 \\cdot 10^8$, and the actual cardinality is\n$1.8 \\cdot 10^7$."}
{"id": "2510.04776", "pdf": "https://arxiv.org/pdf/2510.04776", "abs": "https://arxiv.org/abs/2510.04776", "authors": ["Ebenezer Awotoro", "Chisom Ezekannagha", "Florian Schwarz", "Johannes Tauscher", "Dominik Heider", "Katharina Ladewig", "Christel Le Bon", "Karine Moncoq", "Bruno Miroux", "Georges Hattab"], "title": "MetaMP: Seamless Metadata Enrichment and AI Application Framework for Enhanced Membrane Protein Visualization and Analysis", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "Structural biology has made significant progress in determining membrane\nproteins, leading to a remarkable increase in the number of available\nstructures in dedicated databases. The inherent complexity of membrane protein\nstructures, coupled with challenges such as missing data, inconsistencies, and\ncomputational barriers from disparate sources, underscores the need for\nimproved database integration. To address this gap, we present MetaMP, a\nframework that unifies membrane-protein databases within a web application and\nuses machine learning for classification. MetaMP improves data quality by\nenriching metadata, offering a user-friendly interface, and providing eight\ninteractive views for streamlined exploration. MetaMP was effective across\ntasks of varying difficulty, demonstrating advantages across different levels\nwithout compromising speed or accuracy, according to user evaluations.\nMoreover, MetaMP supports essential functions such as structure classification\nand outlier detection.\n  We present three practical applications of Artificial Intelligence (AI) in\nmembrane protein research: predicting transmembrane segments, reconciling\nlegacy databases, and classifying structures with explainable AI support. In a\nvalidation focused on statistics, MetaMP resolved 77% of data discrepancies and\naccurately predicted the class of newly identified membrane proteins 98% of the\ntime and overtook expert curation. Altogether, MetaMP is a much-needed resource\nthat harmonizes current knowledge and empowers AI-driven exploration of\nmembrane-protein architecture."}
{"id": "2510.04919", "pdf": "https://arxiv.org/pdf/2510.04919", "abs": "https://arxiv.org/abs/2510.04919", "authors": ["Davood Rafiei", "Morgan Lindsay Heisler", "Weiwei Zhang", "Mohammadreza Pourreza", "Yong Zhang"], "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks."}
