# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-07-07

## 目录

- [人工智能 (Artificial Intelligence) (1)](#cs-ai)
- [计算机视觉 (Computer Vision) (4)](#cs-cv)
- [机器学习 (Machine Learning) (3)](#cs-lg)
- [机器人学 (Robotics) (4)](#cs-ro)

## 人工智能 (Artificial Intelligence) [cs.AI]
### [1] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado, Felipe Parodi, Jordan K. Matelsky, Michael L. Platt, Eva B. Dyer, Konrad P. Kording*

Main category: cs.AI

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Recent advances in machine learning have dramatically improved our ability to model language, vision, and other high-dimensional data, yet they continue to struggle with one of the most fundamental aspects of biological systems: movement. Across neuroscience, medicine, robotics, and ethology, movement is essential for interpreting behavior, predicting intent, and enabling interaction. Despite its core significance in our intelligence, movement is often treated as an afterthought rather than as a rich and structured modality in its own right. This reflects a deeper fragmentation in how movement data is collected and modeled, often constrained by task-specific goals and domain-specific assumptions. But movement is not domain-bound. It reflects shared physical constraints, conserved morphological structures, and purposeful dynamics that cut across species and settings. We argue that movement should be treated as a primary modeling target for AI. It is inherently structured and grounded in embodiment and physics. This structure, often allowing for compact, lower-dimensional representations (e.g., pose), makes it more interpretable and computationally tractable to model than raw, high-dimensional sensory inputs. Developing models that can learn from and generalize across diverse movement data will not only advance core capabilities in generative modeling and control, but also create a shared foundation for understanding behavior across biological and artificial systems. Movement is not just an outcome, it is a window into how intelligent systems engage with the world.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02771) | **Categories:** cs.AI, cs.CV, cs.LG, cs.RO

---


## 计算机视觉 (Computer Vision) [cs.CV]
### [1] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter, Ibne Farabi Shihab, Anuj Sharma*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Crash detection from video feeds is a critical problem in intelligent transportation systems. Recent developments in large language models (LLMs) and vision-language models (VLMs) have transformed how we process, reason about, and summarize multimodal information. This paper surveys recent methods leveraging LLMs for crash detection from video data. We present a structured taxonomy of fusion strategies, summarize key datasets, analyze model architectures, compare performance benchmarks, and discuss ongoing challenges and opportunities. Our review provides a foundation for future research in this fast-growing intersection of video understanding and foundation models.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02074) | **Categories:** cs.CV, cs.AI

---

### [2] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei, Hongyuan Yu, Jinlin Wu, Zhen Chen*

Main category: cs.CV

TL;DR: SurgVisAgent 是一个基于多模态大型语言模型的手术视觉代理，能够动态识别和增强内窥镜图像，优于传统单任务模型。


<details>
  <summary>Details</summary>
Motivation: 现有算法通常针对特定场景中的单一任务设计，限制了它们在复杂现实情况中的有效性。

Method: 我们提出了 SurgVisAgent，一个基于多模态大型语言模型 (MLLM) 的端到端智能手术视觉代理。

Result: SurgVisAgent 能够动态识别内窥镜图像中的失真类别和严重程度，从而执行各种增强任务，例如弱光增强、过度曝光校正、运动模糊消除和烟雾去除。

Conclusion: SurgVisAgent 优于传统单任务模型，证明了其作为统一手术辅助解决方案的潜力。

Abstract: 精确的手术干预对于患者安全至关重要，并且已经开发了先进的增强算法来帮助外科医生进行决策。尽管取得了显著进展，但这些算法通常是为特定场景中的单一任务而设计的，从而限制了它们在复杂的现实世界情况中的有效性。为了解决这个局限性，我们提出了 SurgVisAgent，一个建立在多模态大型语言模型（MLLM）之上的端到端智能手术视觉代理。SurgVisAgent 能够动态识别内窥镜图像中的失真类别和严重程度，从而能够执行各种增强任务，例如弱光增强、过度曝光校正、运动模糊消除和烟雾去除。具体来说，为了实现卓越的手术场景理解，我们设计了一个提供领域特定知识的先验模型。此外，通过上下文少样本学习和思维链（CoT）推理，SurgVisAgent 能够提供定制的图像增强，以满足各种失真类型和严重程度的需求，从而满足外科医生的多样化需求。此外，我们构建了一个模拟真实世界手术失真的综合基准，在此基础上进行的大量实验表明，SurgVisAgent 优于传统的单任务模型，突出了其作为统一手术辅助解决方案的潜力。

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02252) | **Categories:** cs.CV, cs.AI

---

### [3] [DexVLG: Dexterous Vision-Language-Grasp Model at Scale](https://arxiv.org/abs/2507.02747)
*Jiawei He, Danshi Li, Xinqiang Yu, Zekun Qi, Wenyao Zhang, Jiayi Chen, Zhaoxiang Zhang, Zhizheng Zhang, Li Yi, He Wang*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: As large models gain traction, vision-language-action (VLA) systems are enabling robots to tackle increasingly complex tasks. However, limited by the difficulty of data collection, progress has mainly focused on controlling simple gripper end-effectors. There is little research on functional grasping with large models for human-like dexterous hands. In this paper, we introduce DexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction aligned with language instructions using single-view RGBD input. To accomplish this, we generate a dataset of 170 million dexterous grasp poses mapped to semantic parts across 174,000 objects in simulation, paired with detailed part-level captions. This large-scale dataset, named DexGraspNet 3.0, is used to train a VLM and flow-matching-based pose head capable of producing instruction-aligned grasp poses for tabletop objects. To assess DexVLG's performance, we create benchmarks in physics-based simulations and conduct real-world experiments. Extensive testing demonstrates DexVLG's strong zero-shot generalization capabilities-achieving over 76% zero-shot execution success rate and state-of-the-art part-grasp accuracy in simulation-and successful part-aligned grasps on physical objects in real-world scenarios.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02747) | **Categories:** cs.CV, cs.RO

---

### [4] [CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios](https://arxiv.org/abs/2507.02479)
*Teng Fu, Yuwen Chen, Zhuofan Chen, Mengyang Zhao, Bin Li, Xiangyang Xue*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Multi-object tracking is a classic field in computer vision. Among them, pedestrian tracking has extremely high application value and has become the most popular research category. Existing methods mainly use motion or appearance information for tracking, which is often difficult in complex scenarios. For the motion information, mutual occlusions between objects often prevent updating of the motion state; for the appearance information, non-robust results are often obtained due to reasons such as only partial visibility of the object or blurred images. Although learning how to perform tracking in these situations from the annotated data is the simplest solution, the existing MOT dataset fails to satisfy this solution. Existing methods mainly have two drawbacks: relatively simple scene composition and non-realistic scenarios. Although some of the video sequences in existing dataset do not have the above-mentioned drawbacks, the number is far from adequate for research purposes. To this end, we propose a difficult large-scale dataset for multi-pedestrian tracking, shot mainly from the first-person view and all from real-life complex scenarios. We name it ``CrowdTrack'' because there are numerous objects in most of the sequences. Our dataset consists of 33 videos, containing a total of 5,185 trajectories. Each object is annotated with a complete bounding box and a unique object ID. The dataset will provide a platform to facilitate the development of algorithms that remain effective in complex situations. We analyzed the dataset comprehensively and tested multiple SOTA models on our dataset. Besides, we analyzed the performance of the foundation models on our dataset. The dataset and project code is released at: https://github.com/loseevaya/CrowdTrack .

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02479) | **Categories:** cs.CV, cs.AI

---


## 机器学习 (Machine Learning) [cs.LG]
### [1] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo, Lina Achaji, Stefano Sabatini, Nicola Poerio, Grzegorz Bartyzel, Sascha Hornauer, Fabien Moutarde*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous vehicle. Inaccurate or inconsistent predictions regarding the movement of agents in its surroundings lead to poorly planned maneuvers and potentially dangerous situations for the end-user. Current state-of-the-art deep-learning-based trajectory prediction models can achieve excellent accuracy on public datasets. However, when used in more complex, interactive scenarios, they often fail to capture important interdependencies between agents, leading to inconsistent predictions among agents in the traffic scene. Inspired by the efficacy of incorporating human preference into large language models, this work fine-tunes trajectory prediction models in multi-agent settings using preference optimization. By taking as input automatically calculated preference rankings among predicted futures in the fine-tuning process, our experiments--using state-of-the-art models on three separate datasets--show that we are able to significantly improve scene consistency while minimally sacrificing trajectory prediction accuracy and without adding any excess computational requirements at inference time.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02406) | **Categories:** cs.LG

---

### [2] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long, Xiangzhi Huang, Jiemin Xie, Ming Cai*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Accurate traffic demand forecasting enables transportation management departments to allocate resources more effectively, thereby improving their utilization efficiency. However, complex spatiotemporal relationships in traffic systems continue to limit the performance of demand forecasting models. To improve the accuracy of spatiotemporal traffic demand prediction, we propose a new graph convolutional network structure called DKGCM. Specifically, we first consider the spatial flow distribution of different traffic nodes and propose a novel temporal similarity-based clustering graph convolution method, DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering to group traffic nodes and more effectively capture spatial dependencies. On the temporal scale, we integrate the Fast Fourier Transform (FFT) within the bidirectional Mamba deep learning framework to capture temporal dependencies in traffic demand. To further optimize model training, we incorporate the GRPO reinforcement learning strategy to enhance the loss function feedback mechanism. Extensive experiments demonstrate that our model outperforms several advanced methods and achieves strong results on three public datasets.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.01982) | **Categories:** cs.LG, cs.AI

---

### [3] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao, Jiaqi Han, Siyi Gu, Mingjian Jiang, James Zou, Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda提出了一种SE(3)-等变适配器框架，用于对几何扩散模型进行参数高效的微调，以适应具有不同几何控制的下游任务。


<details>
  <summary>Details</summary>
Motivation: 几何扩散模型在分子动力学和结构生成中表现出了显著的成功。然而，如何有效地对它们进行微调以适应具有不同几何控制的下游任务，仍有待探索。

Method: 该文提出了一种SE(3)-等变适配器框架（GeoAda），它能够为受控生成任务实现灵活且参数高效的微调，而无需修改原始模型架构。GeoAda引入了一种结构化的适配器设计：控制信号首先通过耦合算子进行编码，然后由选定的预训练模型层的可训练副本进行处理，最后通过解耦算子进行投影，然后是等变零初始化的卷积。

Result: 实验结果表明，GeoAda在各种几何控制类型（包括框架控制、全局控制、子图控制）以及广泛的应用领域（如粒子动力学、分子动力学、人体运动预测和分子生成）中，都表现出了广泛的适用性。实验结果表明，GeoAda在保持原始任务准确性的同时，实现了最先进的微调性能，而其他基线方法由于过拟合和灾难性遗忘而出现显著的性能下降。

Conclusion: 实验结果表明，GeoAda在保持原始任务准确性的同时，实现了最先进的微调性能，而其他基线方法由于过拟合和灾难性遗忘而出现显著的性能下降。

Abstract: 几何扩散模型在分子动力学和结构生成中取得了显著的成功。然而，针对具有不同几何控制的下游任务，如何高效地对这些模型进行微调仍然是一个未被充分探索的问题。本文提出了一种SE(3)-等变适配器框架（GeoAda），该框架能够在不修改原始模型架构的情况下，为受控生成任务实现灵活且参数高效的微调。GeoAda引入了一种结构化的适配器设计：控制信号首先通过耦合算子进行编码，然后由选定的预训练模型层的可训练副本进行处理，最后通过解耦算子进行投影，接着进行等变零初始化的卷积。通过仅对这些轻量级的适配器模块进行微调，GeoAda在减轻过拟合和灾难性遗忘的同时，保留了模型的几何一致性。我们在理论上证明了所提出的适配器保持SE(3)-等变性，确保了预训练扩散模型的几何归纳偏置在适配过程中保持不变。我们通过实验证明了GeoAda在各种几何控制类型（包括框架控制、全局控制、子图控制）以及广泛的应用领域（如粒子动力学、分子动力学、人体运动预测和分子生成）中，都表现出了广泛的适用性。实验结果表明，GeoAda在保持原始任务准确性的同时，实现了最先进的微调性能，而其他基线方法由于过拟合和灾难性遗忘而出现显著的性能下降。

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02085) | **Categories:** cs.LG, cs.AI

---


## 机器人学 (Robotics) [cs.RO]
### [1] [RoboBrain 2.0 Technical Report](https://arxiv.org/abs/2507.02029)
*BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, Yi Han, Yingbo Tang, Xiangqi Xu, Wei Guo, Yaoxu Lyu, Yijie Xu, Jiayu Shi, Cheng Chi, Mengdi Zhao, Xiaoshuai Hao, Shanyu Rong, Zhengliang Cai, Bolun Zhang, Shuyi Zhang, Huaihai Lyu, Mengfei Du, Lingfeng Zhang, Xi Feng, Xiaodan Liu, Yance Jiao, Chenrui He, Mengsi Lyu, Zhuo Chen, Yulong Ao, Xue Sun, Zheqi He, Jingshu Zheng, Xi Yang, Donghai Shi, Kunchang Xie, Bochao Zhang, Shaokai Nie, Chunlei Men, Yonghua Lin, Zhongyuan Wang, Tiejun Huang, Shanghang Zhang*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: We introduce RoboBrain 2.0, our latest generation of embodied vision-language foundation models, designed to unify perception, reasoning, and planning for complex embodied tasks in physical environments. It comes in two variants: a lightweight 7B model and a full-scale 32B model, featuring a heterogeneous architecture with a vision encoder and a language model. Despite its compact size, RoboBrain 2.0 achieves strong performance across a wide spectrum of embodied reasoning tasks. On both spatial and temporal benchmarks, the 32B variant achieves leading results, surpassing prior open-source and proprietary models. In particular, it supports key real-world embodied AI capabilities, including spatial understanding (e.g., affordance prediction, spatial referring, trajectory forecasting) and temporal decision-making (e.g., closed-loop interaction, multi-agent long-horizon planning, and scene graph updating). This report details the model architecture, data construction, multi-stage training strategies, infrastructure and practical applications. We hope RoboBrain 2.0 advances embodied AI research and serves as a practical step toward building generalist embodied agents. The code, checkpoint and benchmark are available at https://superrobobrain.github.io.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02029) | **Categories:** cs.RO

---

### [2] [cVLA: Towards Efficient Camera-Space VLAs](https://arxiv.org/abs/2507.02190)
*Max Argus, Jelena Bratulic, Houman Masnavi, Maxim Velikanov, Nick Heppert, Abhinav Valada, Thomas Brox*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Vision-Language-Action (VLA) models offer a compelling framework for tackling complex robotic manipulation tasks, but they are often expensive to train. In this paper, we propose a novel VLA approach that leverages the competitive performance of Vision Language Models (VLMs) on 2D images to directly infer robot end-effector poses in image frame coordinates. Unlike prior VLA models that output low-level controls, our model predicts trajectory waypoints, making it both more efficient to train and robot embodiment agnostic. Despite its lightweight design, our next-token prediction architecture effectively learns meaningful and executable robot trajectories. We further explore the underutilized potential of incorporating depth images, inference-time techniques such as decoding strategies, and demonstration-conditioned action generation. Our model is trained on a simulated dataset and exhibits strong sim-to-real transfer capabilities. We evaluate our approach using a combination of simulated and real data, demonstrating its effectiveness on a real robotic system.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02190) | **Categories:** cs.RO, cs.LG

---

### [3] [Integrating path-planning and control for robotic unicycles](https://arxiv.org/abs/2507.02700)
*Máté B. Vizi, Dénes Tákács, Gábor Stépán, Gábor Orosz*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: This article focuses on integrating path-planning and control with specializing on the unique needs of robotic unicycles. A unicycle design is presented which is capable of accelerating/breaking and carrying out a variety of maneuvers. The proposed path-planning method segments the path into straight and curved path sections dedicated for accelerating/breaking and turning maneuvers, respectively. The curvature profiles of the curved sections are optimized while considering the control performance and the slipping limits of the wheel. The performance of the proposed integrated approach is demonstrated via numerical simulations.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02700) | **Categories:** cs.RO, cs.SY, eess.SY

---

### [4] [Trajectory Optimization for Differential Drive Mobile Manipulators via Topological Paths Search and Arc Length-Yaw Parameterization](https://arxiv.org/abs/2507.02761)
*Long Xu, Choilam Wong, Mengke Zhang, Junxiao Lin, Fei Gao*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: We present an efficient hierarchical motion planning pipeline for differential drive mobile manipulators. Our approach first searches for multiple collisionfree and topologically distinct paths for the mobile base to extract the space in which optimal solutions may exist. Further sampling and optimization are then conducted in parallel to explore feasible whole-body trajectories. For trajectory optimization, we employ polynomial trajectories and arc length-yaw parameterization, enabling efficient handling of the nonholonomic dynamics while ensuring optimality.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.02761) | **Categories:** cs.RO

---
