# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-10-10

## 目录

- [计算语言学 (Computation and Language) (1)](#cs-cl)
- [cs.DB (4)](#cs-db)
- [机器学习 (Machine Learning) (1)](#cs-lg)

## 计算语言学 (Computation and Language) [cs.CL]
### [1] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan, Zhenke Duan, Jiani Tu, Anzhi Cheng, Yanqing Wang*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Industrial question-answering (QA) systems require higher safety and reliability than general-purpose dialogue models, as errors in high-risk scenarios such as equipment fault diagnosis can have severe consequences. Although multi-agent large language models enhance reasoning depth, they suffer from uncontrolled iterations and unverifiable outputs, and conventional distillation methods struggle to transfer collaborative reasoning capabilities to lightweight, deployable student models. To address these challenges, we propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our approach formulates distillation as a Markov Decision Process and incorporates a knowledge graph as a verifiable structured prior to enrich state representation and ensure convergence. By integrating collaborative reasoning with knowledge grounding, KG-MASD generates high-confidence instruction-tuning data and jointly distills reasoning depth and verifiability into compact student models suitable for edge deployment. Experiments on an industrial QA dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent over baselines and significantly enhances reliability, enabling trustworthy AI deployment in safety-critical industrial scenarios. Code and data are available at https://github.com/erwinmsmith/KG-MAD/.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.06240) | **Categories:** cs.CL, cs.AI, cs.DB

---


## cs.DB [cs.DB]
### [1] [Bridging Imperative Process Models and Process Data Queries-Translation and Relaxation](https://arxiv.org/abs/2510.06414)
*Abdur Rehman Anwar Qureshi, Adrian Rebmann, Timotheus Kampik, Matthias Weidlich, Mathias Weske*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Business process management is increasingly practiced using data-driven approaches. Still, classical imperative process models, which are typically formalized using Petri nets, are not straightforwardly applicable to the relational databases that contain much of the available structured process execution data. This creates a gap between the traditional world of process modeling and recent developments around data-driven process analysis, ultimately leading to the under-utilization of often readily available process models. In this paper, we close this gap by providing an approach for translating imperative models into relaxed process data queries, specifically SQL queries executable on relational databases, for conformance checking. Our results show the continued relevance of imperative process models to data-driven process management, as well as the importance of behavioral footprints and other declarative approaches for integrating model-based and data-driven process management.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.06414) | **Categories:** cs.DB, cs.SE

---

### [2] [Automated Discovery of Test Oracles for Database Management Systems Using LLMs](https://arxiv.org/abs/2510.06663)
*Qiuyang Mang, Runyuan He, Suyang Zhong, Xiaoxuan Liu, Huanchen Zhang, Alvin Cheung*

Main category: cs.DB

TL;DR: 本文提出了一种名为Argus的新框架，该框架利用大型语言模型（LLM）自动发现和实例化数据库管理系统（DBMS）的测试预言，从而发现新的bug。


<details>
  <summary>Details</summary>
Motivation: 现有的数据库管理系统（DBMS）的自动测试技术依赖于手动设计的测试预言，效率低且成本高。

Method: Argus框架基于约束抽象查询的概念，使用LLM生成查询骨架对，并通过SQL等价求解器验证其等价性，最后使用LLM生成具体的SQL代码片段来实例化这些骨架。

Result: Argus在五个广泛测试的DBMS上发现了40个以前未知的bug，其中35个是逻辑错误，36个已确认，26个已被开发人员修复。

Conclusion: Argus框架能够有效地利用大型语言模型自动生成高质量的数据库测试预言，显著提高了DBMS测试的效率和bug发现能力。

Abstract: 自2020年以来，数据库管理系统（DBMS）的自动测试蓬勃发展，揭示了广泛使用的系统中的数百个错误。这些技术的基石是测试预言，它通常实现一种生成等效查询对的机制，从而通过检查其结果之间的一致性来识别错误。然而，虽然应用这些预言可以自动化，但它们的设计仍然是一项根本上是手动的工作。本文探讨了使用大型语言模型（LLM）来自动化测试预言的发现和实例化，从而解决了一个长期存在的完全自动化DBMS测试的瓶颈。虽然LLM表现出令人印象深刻的创造力，但它们容易产生幻觉，这可能导致大量误报。此外，它们的高昂的货币成本和延迟意味着LLM的调用应该受到限制，以确保bug检测是高效和经济的。为此，我们介绍Argus，这是一个建立在约束抽象查询核心概念之上的新颖框架——一个包含占位符及其相关实例化条件（例如，要求占位符由布尔列填充）的SQL骨架。Argus使用LLM生成成对的被断言为语义等价的骨架。然后，使用SQL等价求解器正式证明这种等价性，以确保可靠性。最后，经过验证的骨架中的占位符被具体的、可重用的SQL代码段实例化，这些代码段也由LLM合成，以有效地生成复杂的测试用例。我们实现了Argus，并在五个经过广泛测试的DBMS上对其进行了评估，发现了40个以前未知的错误，其中35个是逻辑错误，其中36个已确认，26个已被开发人员修复。

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.06663) | **Categories:** cs.DB, cs.PL, cs.SE

---

### [3] [Relational Database Distillation: From Structured Tables to Condensed Graph Data](https://arxiv.org/abs/2510.06980)
*Xinyi Gao, Jingxi Zhang, Lijian Chen, Tong Chen, Lizhen Cui, Hongzhi Yin*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Relational databases (RDBs) underpin the majority of global data management systems, where information is structured into multiple interdependent tables. To effectively use the knowledge within RDBs for predictive tasks, recent advances leverage graph representation learning to capture complex inter-table relations as multi-hop dependencies. Despite achieving state-of-the-art performance, these methods remain hindered by the prohibitive storage overhead and excessive training time, due to the massive scale of the database and the computational burden of intensive message passing across interconnected tables. To alleviate these concerns, we propose and study the problem of Relational Database Distillation (RDD). Specifically, we aim to distill large-scale RDBs into compact heterogeneous graphs while retaining the predictive power (i.e., utility) required for training graph-based models. Multi-modal column information is preserved through node features, and primary-foreign key relations are encoded via heterogeneous edges, thereby maintaining both data fidelity and relational structure. To ensure adaptability across diverse downstream tasks without engaging the traditional, inefficient bi-level distillation framework, we further design a kernel ridge regression-guided objective with pseudo-labels, which produces quality features for the distilled graph. Extensive experiments on multiple real-world RDBs demonstrate that our solution substantially reduces the data size while maintaining competitive performance on classification and regression tasks, creating an effective pathway for scalable learning with RDBs.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.06980) | **Categories:** cs.DB, cs.LG

---

### [4] [On the Expressiveness of Languages for Querying Property Graphs in Relational Databases](https://arxiv.org/abs/2510.07062)
*Hadar Rotschield, Liat Peterfreund*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: SQL/PGQ is the emerging ISO standard for querying property graphs defined as views over relational data. We formalize its expressive power across three fragments: the read-only core, the read-write extension, and an extended variant with richer view definitions. Our results show that graph creation plays a central role in determining the expressiveness. The read-only fragment is strictly weaker than the read-write fragment, and the latter is still below the complexity class NL. Extending view definitions with arbitrary arity identifiers closes this gap: the extended fragment captures exactly NL. This yields a strict hierarchy of SQL/PGQ fragments, whose union covers all NL queries. On ordered structures the hierarchy collapses: once arity-2 identifiers are allowed, higher arities add no power, mirroring the classical transitive-closure collapse and underscoring the central role of view construction in property graph querying.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.07062) | **Categories:** cs.DB

---


## 机器学习 (Machine Learning) [cs.LG]
### [1] [Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data](https://arxiv.org/abs/2510.06377)
*Rishabh Ranjan, Valter Hudovernik, Mark Znidar, Charilaos Kanatsoulis, Roshan Upendra, Mahmoud Mohammadi, Joe Meyer, Tom Palczewski, Carlos Guestrin, Jure Leskovec*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Pretrained transformers readily adapt to new sequence modeling tasks via zero-shot prompting, but relational domains still lack architectures that transfer across datasets and tasks. The core challenge is the diversity of relational data, with varying heterogeneous schemas, graph structures and functional dependencies. In this paper, we present the Relational Transformer (RT) architecture, which can be pretrained on diverse relational databases and directly applied to unseen datasets and tasks without task- or dataset-specific fine-tuning, or retrieval of in-context examples. RT (i) tokenizes cells with table/column metadata, (ii) is pretrained via masked token prediction, and (iii) utilizes a novel \textit{Relational Attention} mechanism over columns, rows, and primary-foreign key links. Pretrained on RelBench datasets spanning tasks such as churn and sales forecasting, RT attains strong zero-shot performance, averaging 94% of fully supervised AUROC on binary classification tasks with a single forward pass of a 22M parameter model, as opposed to 84% for a 27B LLM. Fine-tuning yields state-of-the-art results with high sample efficiency. Our experiments show that RT's zero-shot transfer harnesses task-table context, relational attention patterns and schema semantics. Overall, RT provides a practical path toward foundation models for relational data.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.06377) | **Categories:** cs.LG, cs.AI, cs.DB

---
