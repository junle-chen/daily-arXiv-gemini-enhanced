# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-11-18

## 目录

- [人工智能 (Artificial Intelligence) (1)](#cs-ai)
- [计算语言学 (Computation and Language) (2)](#cs-cl)
- [cs.CR (1)](#cs-cr)
- [cs.DB (2)](#cs-db)
- [机器学习 (Machine Learning) (1)](#cs-lg)

## 人工智能 (Artificial Intelligence) [cs.AI]
### [1] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar, Kaustik Ranaware, Kamalasankari Subramaniakuppusamy, Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx提出了一种混合嵌入框架，通过学习到的注意力机制自适应地结合双曲空间、复数空间和欧几里得空间，以更有效地建模知识图谱中多样化的关系类型。


<details>
  <summary>Details</summary>
Motivation: 现有的嵌入方法在建模大规模知识图谱中的多样化关系类型时面临局限性：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称性，双曲模型在对称关系上表现不佳。

Method: HyperComplEx通过关系特定的空间权重策略动态地为每种关系类型选择最佳几何空间，并使用多空间一致性损失来确保跨空间的一致预测。

Result: 在包含1K到10M篇论文的计算机科学研究知识图谱上，HyperComplEx始终优于TransE、RotatE、DistMult、ComplEx、SEPA和UltraE等最先进的基线模型。在10M论文数据集上，HyperComplEx实现了0.612的MRR，相对于最佳基线提高了4.8%，同时保持了高效的训练，每个三元组的推理时间为85毫秒。

Conclusion: HyperComplEx通过自适应地结合不同的几何空间，能够有效地建模大规模知识图谱中的多样化关系类型，并在性能上优于现有方法。

Abstract: 知识图谱已成为表示跨科学和企业领域复杂关系数据的基础结构。然而，现有的嵌入方法在建模大规模知识图谱中的多样化关系类型时面临着关键的局限性：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称性，双曲模型在对称关系上表现不佳。我们提出了HyperComplEx，这是一种混合嵌入框架，它通过学习到的注意力机制自适应地结合双曲空间、复数空间和欧几里得空间。一种关系特定的空间权重策略动态地为每种关系类型选择最佳几何空间，而多空间一致性损失确保了跨空间的一致预测。我们评估了HyperComplEx在计算机科学研究知识图谱上的性能，这些知识图谱的规模从1K篇论文（约25K个三元组）到10M篇论文（约45M个三元组）不等，结果表明，相对于包括TransE、RotatE、DistMult、ComplEx、SEPA和UltraE在内的最先进的基线模型，HyperComplEx始终表现出性能提升。在标准基准上的额外测试证实了HyperComplEx的结果明显高于所有基线模型。在包含10M篇论文的数据集上，HyperComplEx实现了0.612的MRR，相对于最佳基线模型实现了4.8%的相对提升，同时保持了高效的训练，每个三元组的推理时间为85毫秒。该模型通过自适应维度分配实现了近乎线性的图规模扩展。我们发布了我们的实现和数据集族，以促进可扩展知识图谱嵌入的可复现研究。

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.10842) | **Categories:** cs.AI, cs.DB, cs.LG, cs.SI

---


## 计算语言学 (Computation and Language) [cs.CL]
### [1] [Continual Learning of Domain Knowledge from Human Feedback in Text-to-SQL](https://arxiv.org/abs/2511.10674)
*Thomas Cook, Kelly Patel, Sivapriya Vellaichamy, Saba Rahimi, Zhen Zeng, Sumitra Ganesh*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Large Language Models (LLMs) can generate SQL queries from natural language questions but struggle with database-specific schemas and tacit domain knowledge. We introduce a framework for continual learning from human feedback in text-to-SQL, where a learning agent receives natural language feedback to refine queries and distills the revealed knowledge for reuse on future tasks. This distilled knowledge is stored in a structured memory, enabling the agent to improve execution accuracy over time. We design and evaluate multiple variations of a learning agent architecture that vary in how they capture and retrieve past experiences. Experiments on the BIRD benchmark Dev set show that memory-augmented agents, particularly the Procedural Agent, achieve significant accuracy gains and error reduction by leveraging human-in-the-loop feedback. Our results highlight the importance of transforming tacit human expertise into reusable knowledge, paving the way for more adaptive, domain-aware text-to-SQL systems that continually learn from a human-in-the-loop.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.10674) | **Categories:** cs.CL, cs.AI, cs.DB

---

### [2] [MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking](https://arxiv.org/abs/2511.10887)
*Nishant Mishra, Wilker Aziz, Iacer Calixto*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.10887) | **Categories:** cs.CL, cs.DB

---


## cs.CR [cs.CR]
### [1] [HetDAPAC: Leveraging Attribute Heterogeneity in Distributed Attribute-Based Private Access Control](https://arxiv.org/abs/2511.11549)
*Shreya Meel, Sennur Ulukus*

Main category: cs.CR

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Verifying user attributes to provide fine-grained access control to databases is fundamental to attribute-based authentication. Either a single (central) authority verifies all the attributes, or multiple independent authorities verify the attributes distributedly. In the central setup, the authority verifies all user attributes, and the user downloads only the authorized record. While this is communication efficient, it reveals all user attributes to the authority. A distributed setup prevents this privacy breach by letting each authority verify and learn only one attribute. Motivated by this, Jafarpisheh~et~al. introduced an information-theoretic formulation, called distributed attribute-based private access control (DAPAC). With $N$ non-colluding authorities (servers), $N$ attributes and $K$ possible values for each attribute, the DAPAC system lets each server learn only the single attribute value that it verifies, and is oblivious to the remaining $N-1$. The user retrieves its designated record, without learning anything about the remaining database records. The goal is to maximize the rate, i.e., the ratio of desired message size to total download size. However, not all attributes are sensitive, and DAPAC's privacy constraints can be too restrictive, negatively affecting the rate. To leverage the heterogeneous privacy requirements of user attributes, we propose heterogeneous (Het)DAPAC, a framework which off-loads verification of $N-D$ of the $N$ attributes to a central server, and retains DAPAC's architecture for the $D$ sensitive attributes. We first present a HetDAPAC scheme, which improves the rate from $\frac{1}{2K}$ to $\frac{1}{K+1}$, while sacrificing the privacy of a few non-sensitive attributes. Unlike DAPAC, our scheme entails a download imbalance across servers; we propose a second scheme achieving a balanced per-server download and a rate of $\frac{D+1}{2KD}$.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.11549) | **Categories:** cs.CR, cs.DB, cs.IT, eess.SP

---


## cs.DB [cs.DB]
### [1] [ResBench: A Comprehensive Framework for Evaluating Database Resilience](https://arxiv.org/abs/2511.11088)
*Puyun Hu, Wei Pan, Xun Jian, Zeqi Ma, Tianjie Li, Yang Shen, Chengzhi Han, Yudong Zhao, Zhanhuai Li*

Main category: cs.DB

TL;DR: ResBench是一个用于评估数据库在各种不利条件下弹性的基准测试框架。


<details>
  <summary>Details</summary>
Motivation: 现有的数据库基准测试主要关注理想运行环境下的性能，但现实世界中数据库可能面临许多不利事件，如何全面量化应对这些事件的能力仍然是一个开放的问题。

Method: ResBench通过清晰的分层解耦实现测试过程的自动化、标准化和可视化。它模拟不利事件并在正常事务处理期间注入这些事件，利用一个模块来收集多个指标以进行评估模型。

Result: ResBench从八个维度评估数据库的弹性：吞吐量、延迟、稳定性、抵抗性、恢复性、扰动周期、适应能力和指标偏差。所有结果都通过用户友好的图形界面呈现给用户。

Conclusion: 我们使用两种类型的不利数据集演示了ResBench的执行过程和结果解释。

Abstract: 现有的数据库基准测试主要关注理想运行环境下的性能。然而，在实际应用中，数据库很可能面临各种不利事件。如何从全面的角度量化数据库应对这些事件的能力仍然是一个尚未解决的问题。本文提出了数据库弹性的定义，用于描述数据库在面对逆境时的性能，并提出了ResBench，一个用于评估数据库弹性的基准测试。该框架通过清晰的分层解耦实现了测试过程的自动化、标准化和可视化。ResBench模拟不利事件，并在正常事务处理期间注入这些事件，利用一个模块来收集多个指标用于评估模型。我们从八个维度评估数据库的弹性：吞吐量、延迟、稳定性、抵抗性、恢复性、扰动周期、适应能力和指标偏差。所有结果都通过用户友好的图形界面呈现给用户。我们使用两种类型的不利数据集演示了ResBench的执行过程和结果解释。

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.11088) | **Categories:** cs.DB

---

### [2] [Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database](https://arxiv.org/abs/2511.11399)
*Rosario Napoli, Antonio Celesti, Massimo Villari, Maria Fazio*

Main category: cs.DB

TL;DR: 本文提出了一种新的GDB-GML架构，该架构集成了知识补全阶段，通过揭示隐藏知识来提升数据集的行为和指标。


<details>
  <summary>Details</summary>
Motivation: 当前的GDB-GML应用在分析数据时忽略了知识补全(KC)，导致在不完整或碎片化的数据集上工作，从而造成错误的解释。

Method: 本文提出了一种创新的架构，该架构将知识补全阶段集成到GDB-GML应用中，并引入了可扩展的传递关系，这些关系通过衰减函数对网络上的信息传播进行建模。

Result: 实验结果表明，该方法能够重塑拓扑结构和整体数据集动态，从而改进模型。

Conclusion: 该研究强调了新的GDB-GML架构对于产生更好的模型和释放基于图的数据分析的全部潜力的必要性。

Abstract: 近年来，图机器学习（GML）与图数据库（GDB）的结合变得越来越重要，因为它能够处理复杂的互连数据并使用图数据科学（GDS）应用机器学习技术。然而，当前的GDB-GML应用在分析数据的方式上存在一个关键的差距，特别是在知识图谱（KG）中的知识补全（KC）方面。具体来说，当前的架构忽略了KC，即使数据集实际上包含有价值的隐藏知识，它们仍然在不完整或碎片化的数据集上工作。当这些数据被用作GML模型的输入时，这种限制可能会导致错误的解释。 本文提出了一种创新的架构，该架构将KC阶段集成到GDB-GML应用中，展示了揭示隐藏知识如何深刻地影响数据集的行为和指标。为此，我们引入了可扩展的传递关系，这些关系是在网络上传播信息的链接，并通过衰减函数进行建模，从而允许确定性的知识在多个节点之间流动。 实验结果表明，我们的直觉从根本上重塑了拓扑结构和整体数据集的动态，强调了这种新的GDB-GML架构对于产生更好的模型和释放基于图的数据分析的全部潜力的必要性。

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.11399) | **Categories:** cs.DB, cs.IR

---


## 机器学习 (Machine Learning) [cs.LG]
### [1] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.10964) | **Categories:** cs.LG, cs.AI, cs.DB

---
