{"id": "2509.20499", "pdf": "https://arxiv.org/pdf/2509.20499", "abs": "https://arxiv.org/abs/2509.20499", "authors": ["Boqi Li", "Siyuan Li", "Weiyi Wang", "Anran Li", "Zhong Cao", "Henry X. Liu"], "title": "Boosting Zero-Shot VLN via Abstract Obstacle Map-Based Waypoint Prediction with TopoGraph-and-VisitInfo-Aware Prompting", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "With the rapid progress of foundation models and robotics, vision-language\nnavigation (VLN) has emerged as a key task for embodied agents with broad\npractical applications. We address VLN in continuous environments, a\nparticularly challenging setting where an agent must jointly interpret natural\nlanguage instructions, perceive its surroundings, and plan low-level actions.\nWe propose a zero-shot framework that integrates a simplified yet effective\nwaypoint predictor with a multimodal large language model (MLLM). The predictor\noperates on an abstract obstacle map, producing linearly reachable waypoints,\nwhich are incorporated into a dynamically updated topological graph with\nexplicit visitation records. The graph and visitation information are encoded\ninto the prompt, enabling reasoning over both spatial structure and exploration\nhistory to encourage exploration and equip MLLM with local path planning for\nerror correction. Extensive experiments on R2R-CE and RxR-CE show that our\nmethod achieves state-of-the-art zero-shot performance, with success rates of\n41% and 36%, respectively, outperforming prior state-of-the-art methods.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4efb\u52a1\uff0c\u5176\u4e2d\u5305\u542b\u8def\u5f84\u89c4\u5212\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8fdb\u884c\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u548c\u63a2\u7d22\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u9884\u6d4b\u8f68\u8ff9\uff0c\u4f46 waypoint prediction \u548c\u8def\u5f84\u89c4\u5212\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002", "keywords": ["Large Language Models", "Foundation Models", "VLN", "Waypoint Prediction", "Path Planning", "Multimodal Large Language Model"]}}
{"id": "2509.20709", "pdf": "https://arxiv.org/pdf/2509.20709", "abs": "https://arxiv.org/abs/2509.20709", "authors": ["Mani Amani", "Reza Akhavian"], "title": "Digital Twin-Guided Robot Path Planning: A Beta-Bernoulli Fusion with Large Language Model as a Sensor", "categories": ["cs.RO"], "comment": null, "summary": "Integrating natural language (NL) prompts into robotic mission planning has\nattracted significant interest in recent years. In the construction domain,\nBuilding Information Models (BIM) encapsulate rich NL descriptions of the\nenvironment. We present a novel framework that fuses NL directives with\nBIM-derived semantic maps via a Beta-Bernoulli Bayesian fusion by interpreting\nthe LLM as a sensor: each obstacle's design-time repulsive coefficient is\ntreated as a Beta(alpha, beta) random variable and LLM-returned danger scores\nare incorporated as pseudo-counts to update alpha and beta. The resulting\nposterior mean yields a continuous, context-aware repulsive gain that augments\na Euclidean-distance-based potential field for cost heuristics. By adjusting\ngains based on sentiment and context inferred from user prompts, our method\nguides robots along safer, more context-aware paths. This provides a\nnumerically stable method that can chain multiple natural commands and prompts\nfrom construction workers and foreman to enable planning while giving\nflexibility to be integrated in any learned or classical AI framework.\nSimulation results demonstrate that this Beta-Bernoulli fusion yields both\nqualitative and quantitative improvements in path robustness and validity.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant as it combines robot path planning (a form of trajectory prediction) with the use of a Large Language Model (LLM) to interpret natural language prompts and inform the path planning process. The LLM is used as a 'sensor' to assess danger scores and influence the robot's trajectory. The use of Beta-Bernoulli fusion further strengthens the connection to probabilistic trajectory planning.", "keywords": ["robot path planning", "trajectory prediction", "large language model", "LLM", "natural language", "Beta-Bernoulli fusion"]}}
{"id": "2509.20843", "pdf": "https://arxiv.org/pdf/2509.20843", "abs": "https://arxiv.org/abs/2509.20843", "authors": ["Ziang Luo", "Kangan Qian", "Jiahua Wang", "Yuechen Luo", "Jinyu Miao", "Zheng Fu", "Yunlong Wang", "Sicong Jiang", "Zilin Huang", "Yifei Hu", "Yuhao Yang", "Hao Ye", "Mengmeng Yang", "Xiaojian Dong", "Kun Jiang", "Diange Yang"], "title": "MTRDrive: Memory-Tool Synergistic Reasoning for Robust Autonomous Driving in Corner Cases", "categories": ["cs.RO"], "comment": "8 pages", "summary": "Vision-Language Models(VLMs) have demonstrated significant potential for\nend-to-end autonomous driving, yet a substantial gap remains between their\ncurrent capabilities and the reliability necessary for real-world deployment. A\ncritical challenge is their fragility, characterized by hallucinations and poor\ngeneralization in out-of-distribution (OOD) scenarios. To bridge this gap, we\nintroduce MTRDrive, a novel framework that integrates procedural driving\nexperiences with a dynamic toolkit to enhance generalization and proactive\ndecision-making.\n  MTRDrive addresses these limitations through a closed-loop system that\ncombines a memory-based experience retrieval mechanism with dynamic toolkits.\nThis synergy enables the model to interact more effectively with its\nenvironment, improving both reasoning and decision-making capabilities with the\nhelp of our memory-tool synergistic reasoning. Additionally, we introduce a new\nbenchmark based on complex Roadwork construction scenarios to rigorously\nevaluate zero-shot generalization.\n  Extensive experiments demonstrate the superior effectiveness of our approach.\nOn the public NAVSIM benchmark, our 3B-parameter MTRDrive model achieves an\nexceptional PDMS of 88.3 without chain-of-thought and sets a state-of-the-art\nperformance bar on high-level planning, with a driving metric score of 79.8\\%\nand a planning accuracy of 82.6\\%. Rigorous zero-shot evaluation on the new\nRoadwork-VLM benchmark shows a strong ability to reason robustly in unseen\nscenarios, achieving a driving metric score of 80.2\\%. These results highlight\nMTRDrive's potential to advance autonomous driving toward safer and more\nreliable systems.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u5e76\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6765\u63d0\u9ad8\u5176\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u81ea\u52a8\u9a7e\u9a76\u672c\u8d28\u4e0a\u6d89\u53ca\u5bf9\u8f66\u8f86\u8f68\u8ff9\u7684\u9884\u6d4b\u548c\u89c4\u5212\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\uff083B\u53c2\u6570\uff09\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u5173\u952e\u8bcd\u8868\u660e\u5176\u91cd\u70b9\u662f\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u51b3\u7b56\u80fd\u529b\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u76ee\u6807\u4e00\u81f4\u3002", "keywords": ["Vision-Language Models", "autonomous driving", "generalization", "decision-making", "high-level planning"]}}
{"id": "2509.20501", "pdf": "https://arxiv.org/pdf/2509.20501", "abs": "https://arxiv.org/abs/2509.20501", "authors": ["Kishor Datta Gupta", "Mohd Ariful Haque", "Marufa Kamal", "Ahmed Rafi Hasan", "Md. Mahfuzur Rahman", "Roy George"], "title": "Beyond Visual Similarity: Rule-Guided Multimodal Clustering with explicit domain rules", "categories": ["cs.LG", "cs.CV"], "comment": "12 pages, 9 figures", "summary": "Traditional clustering techniques often rely solely on similarity in the\ninput data, limiting their ability to capture structural or semantic\nconstraints that are critical in many domains. We introduce the Domain Aware\nRule Triggered Variational Autoencoder (DARTVAE), a rule guided multimodal\nclustering framework that incorporates domain specific constraints directly\ninto the representation learning process. DARTVAE extends the VAE architecture\nby embedding explicit rules, semantic representations, and data driven features\ninto a unified latent space, while enforcing constraint compliance through rule\nconsistency and violation penalties in the loss function. Unlike conventional\nclustering methods that rely only on visual similarity or apply rules as post\nhoc filters, DARTVAE treats rules as first class learning signals. The rules\nare generated by LLMs, structured into knowledge graphs, and enforced through a\nloss function combining reconstruction, KL divergence, consistency, and\nviolation penalties. Experiments on aircraft and automotive datasets\ndemonstrate that rule guided clustering produces more operationally meaningful\nand interpretable clusters for example, isolating UAVs, unifying stealth\naircraft, or separating SUVs from sedans while improving traditional clustering\nmetrics. However, the framework faces challenges: LLM generated rules may\nhallucinate or conflict, excessive rules risk overfitting, and scaling to\ncomplex domains increases computational and consistency difficulties. By\ncombining rule encodings with learned representations, DARTVAE achieves more\nmeaningful and consistent clustering outcomes than purely data driven models,\nhighlighting the utility of constraint guided multimodal clustering for\ncomplex, knowledge intensive settings.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper incorporates LLMs for rule generation and applies these rules in a clustering framework. While the abstract mentions aircraft and automotive datasets, it doesn't explicitly state that it's used for trajectory prediction. However, the use of LLMs and application to vehicle-related data suggests a potential connection to trajectory prediction scenarios (e.g., predicting vehicle types based on trajectory patterns and LLM-derived rules).", "keywords": ["LLMs", "Large Language Models", "clustering", "automotive datasets", "aircraft datasets", "knowledge graphs"]}}
{"id": "2509.20623", "pdf": "https://arxiv.org/pdf/2509.20623", "abs": "https://arxiv.org/abs/2509.20623", "authors": ["Satyajeet Das", "Darren Chiu", "Zhehui Huang", "Lars Lindemann", "Gaurav S. Sukhatme"], "title": "Latent Activation Editing: Inference-Time Refinement of Learned Policies for Safer Multirobot Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Reinforcement learning has enabled significant progress in complex domains\nsuch as coordinating and navigating multiple quadrotors. However, even\nwell-trained policies remain vulnerable to collisions in obstacle-rich\nenvironments. Addressing these infrequent but critical safety failures through\nretraining or fine-tuning is costly and risks degrading previously learned\nskills. Inspired by activation steering in large language models and latent\nediting in computer vision, we introduce a framework for inference-time Latent\nActivation Editing (LAE) that refines the behavior of pre-trained policies\nwithout modifying their weights or architecture. The framework operates in two\nstages: (i) an online classifier monitors intermediate activations to detect\nstates associated with undesired behaviors, and (ii) an activation editing\nmodule that selectively modifies flagged activations to shift the policy\ntowards safer regimes. In this work, we focus on improving safety in\nmulti-quadrotor navigation. We hypothesize that amplifying a policy's internal\nperception of risk can induce safer behaviors. We instantiate this idea through\na latent collision world model trained to predict future pre-collision\nactivations, thereby prompting earlier and more cautious avoidance responses.\nExtensive simulations and real-world Crazyflie experiments demonstrate that LAE\nachieves statistically significant reduction in collisions (nearly 90% fewer\ncumulative collisions compared to the unedited baseline) and substantially\nincreases the fraction of collision-free trajectories, while preserving task\ncompletion. More broadly, our results establish LAE as a lightweight paradigm,\nfeasible on resource-constrained hardware, for post-deployment refinement of\nlearned robot policies.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses multi-robot navigation and uses a technique inspired by activation steering in large language models (LLMs). While the core focus is on reinforcement learning and robot navigation, the mention of activation steering from LLMs and the context of trajectory refinement for safer navigation suggest a moderate relevance to the specified topics.", "keywords": ["navigation", "multi-robot", "activation steering", "large language models", "trajectory", "collision avoidance"]}}
{"id": "2509.20754", "pdf": "https://arxiv.org/pdf/2509.20754", "abs": "https://arxiv.org/abs/2509.20754", "authors": ["Yufan Mao", "Hanjing Ye", "Wenlong Dong", "Chengjie Zhang", "Hong Zhang"], "title": "Meta-Memory: Retrieving and Integrating Semantic-Spatial Memories for Robot Spatial Reasoning", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Navigating complex environments requires robots to effectively store\nobservations as memories and leverage them to answer human queries about\nspatial locations, which is a critical yet underexplored research challenge.\nWhile prior work has made progress in constructing robotic memory, few have\naddressed the principled mechanisms needed for efficient memory retrieval and\nintegration. To bridge this gap, we propose Meta-Memory, a large language model\n(LLM)-driven agent that constructs a high-density memory representation of the\nenvironment. The key innovation of Meta-Memory lies in its capacity to retrieve\nand integrate relevant memories through joint reasoning over semantic and\nspatial modalities in response to natural language location queries, thereby\nempowering robots with robust and accurate spatial reasoning capabilities. To\nevaluate its performance, we introduce SpaceLocQA, a large-scale dataset\nencompassing diverse real-world spatial question-answering scenarios.\nExperimental results show that Meta-Memory significantly outperforms\nstate-of-the-art methods on both the SpaceLocQA and the public NaVQA\nbenchmarks. Furthermore, we successfully deployed Meta-Memory on real-world\nrobotic platforms, demonstrating its practical utility in complex environments.\nProject page: https://itsbaymax.github.io/meta-memory.github.io/ .", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses robot spatial reasoning using a large language model (LLM) to construct and utilize a memory representation of the environment. While it doesn't directly address trajectory prediction, the spatial reasoning aspect and the use of LLMs make it somewhat relevant. The robot's navigation could potentially involve trajectory prediction as a downstream task.", "keywords": ["large language model", "LLM", "spatial reasoning", "robot", "navigation"]}}
{"id": "2509.20715", "pdf": "https://arxiv.org/pdf/2509.20715", "abs": "https://arxiv.org/abs/2509.20715", "authors": ["Ruixu Zhang", "Yuran Wang", "Xinyi Hu", "Chaoyu Mai", "Wenxuan Liu", "Danni Xu", "Xian Zhong", "Zheng Wang"], "title": "Beyond the Individual: Introducing Group Intention Forecasting with SHOT Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Intention recognition has traditionally focused on individual intentions,\noverlooking the complexities of collective intentions in group settings. To\naddress this limitation, we introduce the concept of group intention, which\nrepresents shared goals emerging through the actions of multiple individuals,\nand Group Intention Forecasting (GIF), a novel task that forecasts when group\nintentions will occur by analyzing individual actions and interactions before\nthe collective goal becomes apparent. To investigate GIF in a specific\nscenario, we propose SHOT, the first large-scale dataset for GIF, consisting of\n1,979 basketball video clips captured from 5 camera views and annotated with 6\ntypes of individual attributes. SHOT is designed with 3 key characteristics:\nmulti-individual information, multi-view adaptability, and multi-level\nintention, making it well-suited for studying emerging group intentions.\nFurthermore, we introduce GIFT (Group Intention ForecasTer), a framework that\nextracts fine-grained individual features and models evolving group dynamics to\nforecast intention emergence. Experimental results confirm the effectiveness of\nSHOT and GIFT, establishing a strong foundation for future research in group\nintention forecasting. The dataset is available at\nhttps://xinyi-hu.github.io/SHOT_DATASET.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on forecasting group intentions based on individual actions, which is related to trajectory prediction, especially in scenarios involving multiple agents and their interactions. While it doesn't explicitly mention large language models, the task of forecasting intentions based on observed behavior could potentially benefit from the reasoning and predictive capabilities of LLMs. The core focus is on intention forecasting which is similar to action prediction in trajectory forecasting.", "keywords": ["intention forecasting", "group intention", "action prediction", "multi-agent", "trajectory prediction (implied)"]}}
{"id": "2509.20705", "pdf": "https://arxiv.org/pdf/2509.20705", "abs": "https://arxiv.org/abs/2509.20705", "authors": ["Reza Akhavian", "Mani Amani", "Johannes Mootz", "Robert Ashe", "Behrad Beheshti"], "title": "Building Information Models to Robot-Ready Site Digital Twins (BIM2RDT): An Agentic AI Safety-First Framework", "categories": ["cs.RO"], "comment": null, "summary": "The adoption of cyber-physical systems and jobsite intelligence that connects\ndesign models, real-time site sensing, and autonomous field operations can\ndramatically enhance digital management in the construction industry. This\npaper introduces BIM2RDT (Building Information Models to Robot-Ready Site\nDigital Twins), an agentic artificial intelligence (AI) framework designed to\ntransform static Building Information Modeling (BIM) into dynamic, robot-ready\ndigital twins (DTs) that prioritize safety during execution. The framework\nbridges the gap between pre-existing BIM data and real-time site conditions by\nintegrating three key data streams: geometric and semantic information from BIM\nmodels, activity data from IoT sensor networks, and visual-spatial data\ncollected by robots during site traversal. The methodology introduces\nSemantic-Gravity ICP (SG-ICP), a point cloud registration algorithm that\nleverages large language model (LLM) reasoning. Unlike traditional methods,\nSG-ICP utilizes an LLM to infer object-specific, plausible orientation priors\nbased on BIM semantics, improving alignment accuracy by avoiding convergence on\nlocal minima. This creates a feedback loop where robot-collected data updates\nthe DT, which in turn optimizes paths for missions. The framework employs YOLOE\nobject detection and Shi-Tomasi corner detection to identify and track\nconstruction elements while using BIM geometry as a priori maps. The framework\nalso integrates real-time Hand-Arm Vibration (HAV) monitoring, mapping\nsensor-detected safety events to the digital twin using IFC standards for\nintervention. Experiments demonstrate SG-ICP's superiority over standard ICP,\nachieving RMSE reductions of 64.3%--88.3% in alignment across scenarios with\noccluded features, ensuring plausible orientations. HAV integration triggers\nwarnings upon exceeding exposure limits, enhancing compliance with ISO 5349-1.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper utilizes LLMs to enhance point cloud registration for creating robot-ready digital twins. While the core focus isn't directly trajectory prediction, the use of robot path optimization based on the digital twin and the involvement of LLMs for reasoning about object orientations connect it to the broader themes of robot movement and large language models. The link to trajectory prediction is indirect through robot navigation and path planning.", "keywords": ["large language model (LLM)", "robot", "digital twin", "path optimization"]}}
{"id": "2509.20739", "pdf": "https://arxiv.org/pdf/2509.20739", "abs": "https://arxiv.org/abs/2509.20739", "authors": ["Guoyang Zhao", "Yudong Li", "Weiqing Qi", "Kai Zhang", "Bonan Liu", "Kai Chen", "Haoang Li", "Jun Ma"], "title": "SLAM-Free Visual Navigation with Hierarchical Vision-Language Perception and Coarse-to-Fine Semantic Topological Planning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Conventional SLAM pipelines for legged robot navigation are fragile under\nrapid motion, calibration demands, and sensor drift, while offering limited\nsemantic reasoning for task-driven exploration. To deal with these issues, we\npropose a vision-only, SLAM-free navigation framework that replaces dense\ngeometry with semantic reasoning and lightweight topological representations. A\nhierarchical vision-language perception module fuses scene-level context with\nobject-level cues for robust semantic inference. And a semantic-probabilistic\ntopological map supports coarse-to-fine planning: LLM-based global reasoning\nfor subgoal selection and vision-based local planning for obstacle avoidance.\nIntegrated with reinforcement-learning locomotion controllers, the framework is\ndeployable across diverse legged robot platforms. Experiments in simulation and\nreal-world settings demonstrate consistent improvements in semantic accuracy,\nplanning quality, and navigation success, while ablation studies further\nshowcase the necessity of both hierarchical perception and fine local planning.\nThis work introduces a new paradigm for SLAM-free, vision-language-driven\nnavigation, shifting robotic exploration from geometry-centric mapping to\nsemantics-driven decision making.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on visual navigation for legged robots, using a vision-language perception module and LLM-based global reasoning for planning. While it doesn't directly focus on trajectory prediction as its main goal, the planning aspect, particularly the use of an LLM for subgoal selection, touches upon related concepts. The system needs to plan a path, which implicitly involves predicting a future trajectory. The use of LLMs is a significant factor in determining relevance. However, the focus is more on navigation and planning than explicit trajectory prediction.", "keywords": ["Large Language Models", "LLM-based global reasoning", "planning", "navigation"]}}
{"id": "2509.21134", "pdf": "https://arxiv.org/pdf/2509.21134", "abs": "https://arxiv.org/abs/2509.21134", "authors": ["Yiwen Zhang", "Ziang Chen", "Fanqi Kong", "Yizhe Huang", "Xue Feng"], "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective", "categories": ["cs.AI", "cs.MA"], "comment": "22 pages, 14 figures", "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528LLM\u8fdb\u884c\u6218\u7565\u51b3\u7b56\uff0c\u5e76\u63d0\u51fa\u4e86ToMPO\u7b97\u6cd5\u4f18\u5316LLM\u7684\u51b3\u7b56\u80fd\u529b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u6218\u7565\u51b3\u7b56\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u53ef\u80fd\u4e0e\u8def\u5f84\u89c4\u5212\u548c\u884c\u4e3a\u9884\u6d4b\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u8bba\u6587\u660e\u786e\u63d0\u5230\u4e86Large Language Models (LLMs)\u3002", "keywords": ["Large Language Models", "LLMs", "strategic decision making", "multi-agent"]}}
{"id": "2509.20938", "pdf": "https://arxiv.org/pdf/2509.20938", "abs": "https://arxiv.org/abs/2509.20938", "authors": ["Jianbo Zhao", "Taiyu Ban", "Xiangjie Li", "Xingtai Gui", "Hangning Zhou", "Lei Liu", "Hongwei Zhao", "Bin Li"], "title": "Autoregressive End-to-End Planning with Time-Invariant Spatial Alignment and Multi-Objective Policy Refinement", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "The inherent sequential modeling capabilities of autoregressive models make\nthem a formidable baseline for end-to-end planning in autonomous driving.\nNevertheless, their performance is constrained by a spatio-temporal\nmisalignment, as the planner must condition future actions on past sensory\ndata. This creates an inconsistent worldview, limiting the upper bound of\nperformance for an otherwise powerful approach. To address this, we propose a\nTime-Invariant Spatial Alignment (TISA) module that learns to project initial\nenvironmental features into a consistent ego-centric frame for each future time\nstep, effectively correcting the agent's worldview without explicit future\nscene prediction. In addition, we employ a kinematic action prediction head\n(i.e., acceleration and yaw rate) to ensure physically feasible trajectories.\nFinally, we introduce a multi-objective post-training stage using Direct\nPreference Optimization (DPO) to move beyond pure imitation. Our approach\nprovides targeted feedback on specific driving behaviors, offering a more\nfine-grained learning signal than the single, overall objective used in\nstandard DPO. Our model achieves a state-of-the-art 89.8 PDMS on the NAVSIM\ndataset among autoregressive models. The video document is available at\nhttps://tisa-dpo-e2e.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on end-to-end planning for autonomous driving using an autoregressive model. It addresses spatio-temporal misalignment and employs a multi-objective post-training stage using Direct Preference Optimization (DPO). While it doesn't explicitly use a large language model, it leverages an autoregressive model for planning, which has connections to sequence modeling techniques used in LLMs, and uses DPO, a technique used to align LLMs with human preferences. It is relevant to trajectory prediction because it focuses on planning trajectories for autonomous vehicles.", "keywords": ["autoregressive models", "end-to-end planning", "autonomous driving", "trajectory prediction", "Direct Preference Optimization (DPO)", "planning"]}}
{"id": "2509.20712", "pdf": "https://arxiv.org/pdf/2509.20712", "abs": "https://arxiv.org/abs/2509.20712", "authors": ["Zhenpeng Su", "Leiyu Pan", "Minxuan Lv", "Yuntao Li", "Wenping Hu", "Fuzheng Zhang", "Kun Gai", "Guorui Zhou"], "title": "CE-GPPO: Controlling Entropy via Gradient-Preserving Clipping Policy Optimization in Reinforcement Learning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}ontrolling \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u5904\u7406\u590d\u6742\u7684\u63a8\u7406\u4efb\u52a1\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["large language models", "LLMs", "reinforcement learning", "policy optimization"]}}
{"id": "2509.21027", "pdf": "https://arxiv.org/pdf/2509.21027", "abs": "https://arxiv.org/abs/2509.21027", "authors": ["Sibo Li", "Qianyue Hao", "Yu Shang", "Yong Li"], "title": "KeyWorld: Key Frame Reasoning Enables Effective and Efficient World Models", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robotic world models are a promising paradigm for forecasting future\nenvironment states, yet their inference speed and the physical plausibility of\ngenerated trajectories remain critical bottlenecks, limiting their real-world\napplications. This stems from the redundancy of the prevailing frame-to-frame\ngeneration approach, where the model conducts costly computation on similar\nframes, as well as neglecting the semantic importance of key transitions. To\naddress this inefficiency, we propose KeyWorld, a framework that improves\ntext-conditioned robotic world models by concentrating transformers computation\non a few semantic key frames while employing a lightweight convolutional model\nto fill the intermediate frames. Specifically, KeyWorld first identifies\nsignificant transitions by iteratively simplifying the robot's motion\ntrajectories, obtaining the ground truth key frames. Then, a DiT model is\ntrained to reason and generate these physically meaningful key frames from\ntextual task descriptions. Finally, a lightweight interpolator efficiently\nreconstructs the full video by inpainting all intermediate frames. Evaluations\non the LIBERO benchmark demonstrate that KeyWorld achieves a 5.68$\\times$\nacceleration compared to the frame-to-frame generation baseline, and focusing\non the motion-aware key frames further contributes to the physical validity of\nthe generated videos, especially on complex tasks. Our approach highlights a\npractical path toward deploying world models in real-time robotic control and\nother domains requiring both efficient and effective world models. Code is\nreleased at https://anonymous.4open.science/r/Keyworld-E43D.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4e16\u754c\u6a21\u578b\u7684\u6784\u5efa\uff0c\u5e76\u901a\u8fc7\u5173\u952e\u5e27\u63a8\u7406\u6765\u63d0\u9ad8\u6548\u7387\u548c\u6548\u679c\u3002\u867d\u7136\u6d89\u53ca\u751f\u6210\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u4e86DiT\u6a21\u578b\uff08\u4e00\u79cd\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ee5\u89c6\u4e3a\u4e00\u79cd\u5927\u6a21\u578b\uff09\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u6784\u5efa\u548c\u5173\u952e\u5e27\u63d0\u53d6\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u76f4\u63a5\u5173\u8054\u6027\u76f8\u5bf9\u8f83\u4f4e\uff0c\u5927\u6a21\u578b\u7684\u4f7f\u7528\u4e5f\u53ea\u662f\u4f5c\u4e3a\u5176\u4e2d\u7684\u4e00\u4e2a\u6a21\u5757\u3002", "keywords": ["world models", "trajectory", "DiT", "key frame reasoning", "robotic motion"]}}
