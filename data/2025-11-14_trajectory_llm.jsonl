{"id": "2511.08939", "pdf": "https://arxiv.org/pdf/2511.08939", "abs": "https://arxiv.org/abs/2511.08939", "authors": ["Yingtong Dou", "Zhimeng Jiang", "Tianyi Zhang", "Mingzhi Hu", "Zhichao Xu", "Shubham Jain", "Uday Singh Saini", "Xiran Fan", "Jiarui Sun", "Menghai Pan", "Junpeng Wang", "Xin Dai", "Liang Wang", "Chin-Chia Michael Yeh", "Yujie Fan", "Vineeth Rakesh", "Huiyuan Chen", "Mangesh Bendre", "Zhongfang Zhuang", "Xiaoting Li", "Prince Aboagye", "Vivian Lai", "Minghua Xu", "Hao Yang", "Yiwei Cai", "Mahashweta Das", "Yuzhong Chen"], "title": "TransactionGPT", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report", "summary": "We present TransactionGPT (TGPT), a foundation model for consumer transaction data within one of world's largest payment networks. TGPT is designed to understand and generate transaction trajectories while simultaneously supporting a variety of downstream prediction and classification tasks. We introduce a novel 3D-Transformer architecture specifically tailored for capturing the complex dynamics in payment transaction data. This architecture incorporates design innovations that enhance modality fusion and computational efficiency, while seamlessly enabling joint optimization with downstream objectives. Trained on billion-scale real-world transactions, TGPT significantly improves downstream classification performance against a competitive production model and exhibits advantages over baselines in generating future transactions. We conduct extensive empirical evaluations utilizing a diverse collection of company transaction datasets spanning multiple downstream tasks, thereby enabling a thorough assessment of TGPT's effectiveness and efficiency in comparison to established methodologies. Furthermore, we examine the incorporation of LLM-derived embeddings within TGPT and benchmark its performance against fine-tuned LLMs, demonstrating that TGPT achieves superior predictive accuracy as well as faster training and inference. We anticipate that the architectural innovations and practical guidelines from this work will advance foundation models for transaction-like data and catalyze future research in this emerging field.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper presents TransactionGPT, a foundation model for consumer transaction data that aims to understand and generate transaction trajectories. It leverages a novel 3D-Transformer architecture and incorporates LLM-derived embeddings. This combines both trajectory prediction (generating transaction trajectories) and large language models (LLM-derived embeddings) aspects.", "keywords": ["trajectory prediction", "large language models", "foundation model", "transaction trajectories", "LLMs", "3D-Transformer"]}}
{"id": "2511.09025", "pdf": "https://arxiv.org/pdf/2511.09025", "abs": "https://arxiv.org/abs/2511.09025", "authors": ["Tianao Xiang", "Mingjian Zhi", "Yuanguo Bi", "Lin Cai", "Yuhao Chen"], "title": "FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have impressive data fusion and reasoning capabilities for autonomous driving (AD). However, training LLMs for AD faces significant challenges including high computation transmission costs, and privacy concerns associated with sensitive driving data. Federated Learning (FL) is promising for enabling autonomous vehicles (AVs) to collaboratively train models without sharing raw data. We present Federated LLM-based Autonomous Driving (FLAD), an FL framework that leverages distributed multimodal sensory data across AVs in heterogeneous environment. FLAD has three key innovations: (1) a cloud-edge-vehicle collaborative architecture that reduces communication delay and preserving data privacy; (2) an intelligent parallelized collaborative training with a communication scheduling mechanism that optimizes training efficiency, leveraging end-devices otherwise having insufficient resources for model training; and (3) a knowledge distillation method that personalizes LLM according to heterogeneous edge data. In addition, we prototype FLAD in a testbed with NVIDIA Jetsons, overcoming practical implementation challenges including CPU/GPU memory sharing in resource-constrained devices, dynamic model partitions, and fault-tolerant training.Extensive experimental evaluation demonstrates that FLAD achieves superior end-to-end AD performance while efficiently utilizing distributed vehicular resources, opening up new possibilities for future collaborative AD model training and knowledge sharing.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant because it explicitly focuses on using Large Language Models (LLMs) for autonomous driving, which inherently involves trajectory prediction. It also addresses the challenges of training LLMs in a federated learning setting, making it relevant to both trajectory prediction (through autonomous driving) and large language models.", "keywords": ["Large Language Models", "LLMs", "Autonomous Driving", "Federated Learning", "Vehicle", "Trajectory Prediction", "Autonomous Vehicles"]}}
{"id": "2511.08640", "pdf": "https://arxiv.org/pdf/2511.08640", "abs": "https://arxiv.org/abs/2511.08640", "authors": ["Xingcheng Liu", "Bin Rao", "Yanchen Guan", "Chengyue Wang", "Haicheng Liao", "Jiaxun Zhang", "Chengyu Lin", "Meixin Zhu", "Zhenning Li"], "title": "Predict and Resist: Long-Term Accident Anticipation under Sensor Noise", "categories": ["cs.CV", "cs.AI"], "comment": "accepted by the Fortieth AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u4e8b\u6545\u9884\u6d4b\uff0c\u5176\u4e2d\u6d89\u53ca\u5230\u957f\u671f\u9884\u6d4b\u548c\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7actor-critic\u6a21\u578b\u8fdb\u884c\u51b3\u7b56\u3002Diffusion\u6a21\u578b\u4e5f\u88ab\u7528\u4e8e\u53bb\u566a\uff0c\u53ef\u4ee5\u89c6\u4e3a\u4e00\u79cd\u751f\u6210\u6a21\u578b\uff0c\u4e0e\u5927\u6a21\u578b\u6709\u95f4\u63a5\u8054\u7cfb\u3002", "keywords": ["accident anticipation", "trajectory prediction", "actor-critic", "diffusion model", "autonomous driving"]}}
{"id": "2511.08912", "pdf": "https://arxiv.org/pdf/2511.08912", "abs": "https://arxiv.org/abs/2511.08912", "authors": ["Jinyu Zhang", "Lijun Han", "Feng Jian", "Lingxi Zhang", "Hesheng Wang"], "title": "A Shared Control Framework for Mobile Robots with Planning-Level Intention Prediction", "categories": ["cs.RO"], "comment": null, "summary": "In mobile robot shared control, effectively understanding human motion intention is critical for seamless human-robot collaboration. This paper presents a novel shared control framework featuring planning-level intention prediction. A path replanning algorithm is designed to adjust the robot's desired trajectory according to inferred human intentions. To represent future motion intentions, we introduce the concept of an intention domain, which serves as a constraint for path replanning. The intention-domain prediction and path replanning problems are jointly formulated as a Markov Decision Process and solved through deep reinforcement learning. In addition, a Voronoi-based human trajectory generation algorithm is developed, allowing the model to be trained entirely in simulation without human participation or demonstration data. Extensive simulations and real-world user studies demonstrate that the proposed method significantly reduces operator workload and enhances safety, without compromising task efficiency compared with existing assistive teleoperation approaches.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on intention prediction for mobile robots, which is closely related to trajectory prediction. It uses deep reinforcement learning to solve a Markov Decision Process related to intention domain prediction and path replanning. While it doesn't directly use large language models, the intention prediction aspect and the use of deep learning for trajectory-related tasks make it relevant.", "keywords": ["trajectory prediction", "intention prediction", "mobile robot", "path replanning", "deep reinforcement learning"]}}
{"id": "2511.09013", "pdf": "https://arxiv.org/pdf/2511.09013", "abs": "https://arxiv.org/abs/2511.09013", "authors": ["Ziyi Song", "Chen Xia", "Chenbing Wang", "Haibao Yu", "Sheng Zhou", "Zhisheng Niu"], "title": "UniMM-V2X: MoE-Enhanced Multi-Level Fusion for End-to-End Cooperative Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Autonomous driving holds transformative potential but remains fundamentally constrained by the limited perception and isolated decision-making with standalone intelligence. While recent multi-agent approaches introduce cooperation, they often focus merely on perception-level tasks, overlooking the alignment with downstream planning and control, or fall short in leveraging the full capacity of the recent emerging end-to-end autonomous driving. In this paper, we present UniMM-V2X, a novel end-to-end multi-agent framework that enables hierarchical cooperation across perception, prediction, and planning. At the core of our framework is a multi-level fusion strategy that unifies perception and prediction cooperation, allowing agents to share queries and reason cooperatively for consistent and safe decision-making. To adapt to diverse downstream tasks and further enhance the quality of multi-level fusion, we incorporate a Mixture-of-Experts (MoE) architecture to dynamically enhance the BEV representations. We further extend MoE into the decoder to better capture diverse motion patterns. Extensive experiments on the DAIR-V2X dataset demonstrate our approach achieves state-of-the-art (SOTA) performance with a 39.7% improvement in perception accuracy, a 7.2% reduction in prediction error, and a 33.2% improvement in planning performance compared with UniV2X, showcasing the strength of our MoE-enhanced multi-level cooperative paradigm.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper is relevant because it focuses on end-to-end autonomous driving, which includes trajectory prediction. The abstract mentions \"prediction error\" and incorporates a Mixture-of-Experts (MoE) architecture, which can be seen as a component related to large models, albeit not a Large Language Model directly. The multi-agent aspect also connects to the prediction of multiple interacting agents' trajectories.", "keywords": ["trajectory prediction", "autonomous driving", "multi-agent", "Mixture-of-Experts", "prediction error", "planning"]}}
{"id": "2511.09044", "pdf": "https://arxiv.org/pdf/2511.09044", "abs": "https://arxiv.org/abs/2511.09044", "authors": ["Yousef Emami", "Radha Reddy", "Azadeh Pourkabirian", "Miguel Gutierrez Gaitan"], "title": "Advancing Autonomous Emergency Response Systems: A Generative AI Perspective", "categories": ["cs.AI"], "comment": "8 pages, 3 figures, 2 tables", "summary": "Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper discusses autonomous vehicles, which inherently involve trajectory prediction. It also explicitly mentions Large Language Models (LLMs) and their application in emergency response systems. While the primary focus isn't solely on trajectory prediction, the intersection of AV navigation and LLM assistance makes it relevant.", "keywords": ["Autonomous Vehicles", "Large Language Model", "LLM", "Reinforcement Learning", "Diffusion Model"]}}
{"id": "2511.09057", "pdf": "https://arxiv.org/pdf/2511.09057", "abs": "https://arxiv.org/abs/2511.09057", "authors": ["PAN Team", "Jiannan Xiang", "Yi Gu", "Zihan Liu", "Zeyu Feng", "Qiyue Gao", "Yiyan Hu", "Benhao Huang", "Guangyi Liu", "Yichi Yang", "Kun Zhou", "Davit Abrahamyan", "Arif Ahmad", "Ganesh Bannur", "Junrong Chen", "Kimi Chen", "Mingkai Deng", "Ruobing Han", "Xinqi Huang", "Haoqiang Kang", "Zheqi Li", "Enze Ma", "Hector Ren", "Yashowardhan Shinde", "Rohan Shingre", "Ramsundar Tanikella", "Kaiming Tao", "Dequan Yang", "Xinle Yu", "Cong Zeng", "Binglin Zhou", "Hector Liu", "Zhiting Hu", "Eric P. Xing"], "title": "PAN: A World Model for General, Interactable, and Long-Horizon World Simulation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper describes PAN, a world model that uses a large language model (LLM) for predicting future world states through video simulation conditioned on history and natural language actions. While the paper doesn't explicitly focus on trajectory prediction, the concept of predicting future world states and long-horizon forecasting are closely related to trajectory prediction. The use of LLMs is a significant aspect, making it relevant to both trajectory prediction (in a broader sense of predicting future states) and large language models.", "keywords": ["world model", "large language model", "LLM", "video simulation", "long-horizon forecasting", "prediction", "action-conditioned"]}}
{"id": "2511.08615", "pdf": "https://arxiv.org/pdf/2511.08615", "abs": "https://arxiv.org/abs/2511.08615", "authors": ["Kosta Dakic", "Kanchana Thilakarathna", "Rodrigo N. Calheiros", "Teng Joon Lim"], "title": "A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking", "categories": ["cs.CV", "cs.IT", "cs.LG", "cs.RO", "eess.IV"], "comment": "Introduction of the MATRIX Dataset, featuring synchronized footage from eight drones in an urban environment with comprehensive annotations for detection and tracking, available at https://github.com/KostaDakic/MATRIX/tree/main", "summary": "Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\\sim$90\\% detection and tracking accuracy, as well as successfully tracks $\\sim$80\\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on pedestrian detection and tracking using a multi-drone system. While it doesn't explicitly mention trajectory prediction or large language models, the tracking aspect is closely related to trajectory estimation. The use of deep learning also suggests a potential for future integration with large models, although that is not explicitly addressed in the paper. The dataset and framework could be used for trajectory prediction tasks.", "keywords": ["pedestrian tracking", "deep learning", "multi-view", "trajectory"]}}
{"id": "2511.08633", "pdf": "https://arxiv.org/pdf/2511.08633", "abs": "https://arxiv.org/abs/2511.08633", "authors": ["Assaf Singer", "Noam Rotstein", "Amir Mann", "Ron Kimmel", "Or Litany"], "title": "Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.MM"], "comment": null, "summary": "Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on motion-controlled video generation using diffusion models. While it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting the future path of an agent), the concept of \"motion control\" and generating videos based on \"crude reference animations\" has some overlap with trajectory representation and manipulation. The use of diffusion models also places it within the realm of large, generative AI models, but it's not explicitly using LLMs. Therefore, the relevance is moderate.", "keywords": ["motion control", "video generation", "diffusion models", "animation"]}}
{"id": "2511.08892", "pdf": "https://arxiv.org/pdf/2511.08892", "abs": "https://arxiv.org/abs/2511.08892", "authors": ["Weihao Tan", "Xiangyang Li", "Yunhao Fang", "Heyuan Yao", "Shi Yan", "Hao Luo", "Tenglong Ao", "Huihui Li", "Hongbin Ren", "Bairen Yi", "Yujia Qin", "Bo An", "Libin Liu", "Guang Shi"], "title": "Lumine: An Open Recipe for Building Generalist Agents in 3D Open Worlds", "categories": ["cs.AI"], "comment": null, "summary": "We introduce Lumine, the first open recipe for developing generalist agents capable of completing hours-long complex missions in real time within challenging 3D open-world environments. Lumine adopts a human-like interaction paradigm that unifies perception, reasoning, and action in an end-to-end manner, powered by a vision-language model. It processes raw pixels at 5 Hz to produce precise 30 Hz keyboard-mouse actions and adaptively invokes reasoning only when necessary. Trained in Genshin Impact, Lumine successfully completes the entire five-hour Mondstadt main storyline on par with human-level efficiency and follows natural language instructions to perform a broad spectrum of tasks in both 3D open-world exploration and 2D GUI manipulation across collection, combat, puzzle-solving, and NPC interaction. In addition to its in-domain performance, Lumine demonstrates strong zero-shot cross-game generalization. Without any fine-tuning, it accomplishes 100-minute missions in Wuthering Waves and the full five-hour first chapter of Honkai: Star Rail. These promising results highlight Lumine's effectiveness across distinct worlds and interaction dynamics, marking a concrete step toward generalist agents in open-ended environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on building generalist agents in 3D open worlds using a vision-language model. While it doesn't explicitly mention trajectory prediction, the agent's ability to navigate and interact within the environment implicitly involves predicting and planning its movements. The use of a vision-language model connects it to the domain of large language models. The connection to trajectory prediction is weaker, but present.", "keywords": ["vision-language model", "generalist agents", "3D open-world environments", "reasoning", "action"]}}
{"id": "2511.08865", "pdf": "https://arxiv.org/pdf/2511.08865", "abs": "https://arxiv.org/abs/2511.08865", "authors": ["Cong Tai", "Hansheng Wu", "Haixu Long", "Zhengbin Long", "Zhaoyu Zheng", "Haodong Xiang", "Tao Shen"], "title": "MirrorLimb: Implementing hand pose acquisition and robot teleoperation based on RealMirror", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "In this work, we present a PICO-based robot remote operating framework that enables low-cost, real-time acquisition of hand motion and pose data, outperforming mainstream visual tracking and motion capture solutions in terms of cost-effectiveness. The framework is natively compatible with the RealMirror ecosystem, offering ready-to-use functionality for stable and precise robotic trajectory recording within the Isaac simulation environment, thereby facilitating the construction of Vision-Language-Action (VLA) datasets. Additionally, the system supports real-time teleoperation of a variety of end-effector-equipped robots, including dexterous hands and robotic grippers. This work aims to lower the technical barriers in the study of upper-limb robotic manipulation, thereby accelerating advancements in VLA-related research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses robot teleoperation and trajectory recording, which relates to trajectory prediction. It also mentions Vision-Language-Action (VLA) datasets, suggesting a potential connection to large language models, although not explicitly. The core focus is on hand pose acquisition and robot control, with a weaker link to LLMs.", "keywords": ["trajectory recording", "Vision-Language-Action (VLA)"]}}
{"id": "2511.08947", "pdf": "https://arxiv.org/pdf/2511.08947", "abs": "https://arxiv.org/abs/2511.08947", "authors": ["Xiaohan Zhang", "Tian Gao", "Mingyue Cheng", "Bokai Pan", "Ze Guo", "Yaguo Liu", "Xiaoyu Tao"], "title": "AlphaCast: A Human Wisdom-LLM Intelligence Co-Reasoning Framework for Interactive Time Series Forecasting", "categories": ["cs.AI"], "comment": null, "summary": "Time series forecasting plays a critical role in high-stakes domains such as energy, healthcare, and climate. Although recent advances have improved accuracy, most approaches still treat forecasting as a static one-time mapping task, lacking the interaction, reasoning, and adaptability of human experts. This gap limits their usefulness in complex real-world environments. To address this, we propose AlphaCast, a human wisdom-large language model (LLM) intelligence co-reasoning framework that redefines forecasting as an interactive process. The key idea is to enable step-by-step collaboration between human wisdom and LLM intelligence to jointly prepare, generate, and verify forecasts. The framework consists of two stages: (1) automated prediction preparation, where AlphaCast builds a multi-source cognitive foundation comprising a feature set that captures key statistics and time patterns, a domain knowledge base distilled from corpora and historical series, a contextual repository that stores rich information for each time window, and a case base that retrieves optimal strategies via pattern clustering and matching; and (2) generative reasoning and reflective optimization, where AlphaCast integrates statistical temporal features, prior knowledge, contextual information, and forecasting strategies, triggering a meta-reasoning loop for continuous self-correction and strategy refinement. Extensive experiments on short- and long-term datasets show that AlphaCast consistently outperforms state-of-the-art baselines in predictive accuracy. Code is available at this repository: https://github.com/SkyeGT/AlphaCast_Official .", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f85\u52a9\u63a8\u7406\u3002\u867d\u7136\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u4f46\u8bba\u6587\u5e76\u672a\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u3002\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Model", "LLM", "Time series forecasting"]}}
{"id": "2511.08942", "pdf": "https://arxiv.org/pdf/2511.08942", "abs": "https://arxiv.org/abs/2511.08942", "authors": ["Mobin Habibpour", "Fatemeh Afghah"], "title": "Think, Remember, Navigate: Zero-Shot Object-Goal Navigation with VLM-Powered Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "While Vision-Language Models (VLMs) are set to transform robotic navigation, existing methods often underutilize their reasoning capabilities. To unlock the full potential of VLMs in robotics, we shift their role from passive observers to active strategists in the navigation process. Our framework outsources high-level planning to a VLM, which leverages its contextual understanding to guide a frontier-based exploration agent. This intelligent guidance is achieved through a trio of techniques: structured chain-of-thought prompting that elicits logical, step-by-step reasoning; dynamic inclusion of the agent's recent action history to prevent getting stuck in loops; and a novel capability that enables the VLM to interpret top-down obstacle maps alongside first-person views, thereby enhancing spatial awareness. When tested on challenging benchmarks like HM3D, Gibson, and MP3D, this method produces exceptionally direct and logical trajectories, marking a substantial improvement in navigation efficiency over existing approaches and charting a path toward more capable embodied agents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses a Vision-Language Model (VLM) for robotic navigation, which involves planning and generating trajectories. While the primary focus isn't trajectory prediction in the traditional sense (like predicting the future trajectory of a pedestrian), it does involve generating a trajectory for the robot to follow. The VLM aspect is clear. Therefore, it has moderate relevance.", "keywords": ["Vision-Language Models", "VLMs", "robotic navigation", "trajectory", "reasoning", "exploration agent"]}}
{"id": "2511.09020", "pdf": "https://arxiv.org/pdf/2511.09020", "abs": "https://arxiv.org/abs/2511.09020", "authors": ["Mingyang Yu", "Haorui Yang", "Kangning An", "Xinjian Wei", "Xiaoxuan Xu", "Jing Xu"], "title": "A Quantum Tunneling and Bio-Phototactic Driven Enhanced Dwarf Mongoose Optimizer for UAV Trajectory Planning and Engineering Problem", "categories": ["cs.RO", "math.OC"], "comment": null, "summary": "With the widespread adoption of unmanned aerial vehicles (UAV), effective path planning has become increasingly important. Although traditional search methods have been extensively applied, metaheuristic algorithms have gained popularity due to their efficiency and problem-specific heuristics. However, challenges such as premature convergence and lack of solution diversity still hinder their performance in complex scenarios. To address these issues, this paper proposes an Enhanced Multi-Strategy Dwarf Mongoose Optimization (EDMO) algorithm, tailored for three-dimensional UAV trajectory planning in dynamic and obstacle-rich environments. EDMO integrates three novel strategies: (1) a Dynamic Quantum Tunneling Optimization Strategy (DQTOS) to enable particles to probabilistically escape local optima; (2) a Bio-phototactic Dynamic Focusing Search Strategy (BDFSS) inspired by microbial phototaxis for adaptive local refinement; and (3) an Orthogonal Lens Opposition-Based Learning (OLOBL) strategy to enhance global exploration through structured dimensional recombination. EDMO is benchmarked on 39 standard test functions from CEC2017 and CEC2020, outperforming 14 advanced algorithms in convergence speed, robustness, and optimization accuracy. Furthermore, real-world validations on UAV three-dimensional path planning and three engineering design tasks confirm its practical applicability and effectiveness in field robotics missions requiring intelligent, adaptive, and time-efficient planning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\uff08UAV\uff09\u8f68\u8ff9\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u4e3b\u9898\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u5e76\u4e14\u63d0\u5230\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8def\u5f84\u89c4\u5212\u548c\u667a\u80fd\u89c4\u5212\uff0c\u8fd9\u4e9b\u90fd\u53ef\u80fd\u4e0e\u672a\u6765\u5927\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u76f8\u5173\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "UAV\u8f68\u8ff9\u89c4\u5212", "\u8def\u5f84\u89c4\u5212", "\u65e0\u4eba\u673a", "\u667a\u80fd\u89c4\u5212"]}}
{"id": "2511.09032", "pdf": "https://arxiv.org/pdf/2511.09032", "abs": "https://arxiv.org/abs/2511.09032", "authors": ["Dingji Wang", "You Lu", "Bihuan Chen", "Shuo Hao", "Haowen Jiang", "Yifan Tian", "Xin Peng"], "title": "Argus: Resilience-Oriented Safety Assurance Framework for End-to-End ADSs", "categories": ["cs.AI", "cs.RO", "cs.SE"], "comment": "The paper has been accepted by the 40th IEEE/ACM International Conference on Automated Software Engineering, ASE 2025", "summary": "End-to-end autonomous driving systems (ADSs), with their strong capabilities in environmental perception and generalizable driving decisions, are attracting growing attention from both academia and industry. However, once deployed on public roads, ADSs are inevitably exposed to diverse driving hazards that may compromise safety and degrade system performance. This raises a strong demand for resilience of ADSs, particularly the capability to continuously monitor driving hazards and adaptively respond to potential safety violations, which is crucial for maintaining robust driving behaviors in complex driving scenarios.\n  To bridge this gap, we propose a runtime resilience-oriented framework, Argus, to mitigate the driving hazards, thus preventing potential safety violations and improving the driving performance of an ADS. Argus continuously monitors the trajectories generated by the ADS for potential hazards and, whenever the EGO vehicle is deemed unsafe, seamlessly takes control through a hazard mitigator. We integrate Argus with three state-of-the-art end-to-end ADSs, i.e., TCP, UniAD and VAD. Our evaluation has demonstrated that Argus effectively and efficiently enhances the resilience of ADSs, improving the driving score of the ADS by up to 150.30% on average, and preventing up to 64.38% of the violations, with little additional time overhead.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63d0\u5230\u4e86ADS\u751f\u6210\u7684\u8f68\u8ff9\u7684\u76d1\u63a7\u548c\u6f5c\u5728\u5371\u9669\u7684\u7f13\u89e3\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u8f66\u8f86\u884c\u4e3a\u5b89\u5168\u5bc6\u5207\u76f8\u5173\u3002\u5173\u952e\u8bcd\u5982\u201cADS\u201d\u3001\u201ctrajectories\u201d\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u800c\u201cend-to-end\u201d\u548c\u96c6\u6210\u7684\u6a21\u578b\uff08TCP, UniAD, VAD\uff09\u5219\u53ef\u80fd\u6697\u793a\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\u3002", "keywords": ["ADS", "trajectories", "end-to-end autonomous driving systems", "driving hazards", "safety", "UniAD", "VAD"]}}
{"id": "2511.09275", "pdf": "https://arxiv.org/pdf/2511.09275", "abs": "https://arxiv.org/abs/2511.09275", "authors": ["Minlan Shao", "Zijian Zhang", "Yili Wang", "Yiwei Dai", "Xu Shen", "Xin Wang"], "title": "HyperD: Hybrid Periodicity Decoupling Framework for Traffic Forecasting", "categories": ["cs.AI"], "comment": null, "summary": "Accurate traffic forecasting plays a vital role in intelligent transportation systems, enabling applications such as congestion control, route planning, and urban mobility optimization.However, traffic forecasting remains challenging due to two key factors: (1) complex spatial dependencies arising from dynamic interactions between road segments and traffic sensors across the network, and (2) the coexistence of multi-scale periodic patterns (e.g., daily and weekly periodic patterns driven by human routines) with irregular fluctuations caused by unpredictable events (e.g., accidents, weather, or construction). To tackle these challenges, we propose HyperD (Hybrid Periodic Decoupling), a novel framework that decouples traffic data into periodic and residual components. The periodic component is handled by the Hybrid Periodic Representation Module, which extracts fine-grained daily and weekly patterns using learnable periodic embeddings and spatial-temporal attention. The residual component, which captures non-periodic, high-frequency fluctuations, is modeled by the Frequency-Aware Residual Representation Module, leveraging complex-valued MLP in frequency domain. To enforce semantic separation between the two components, we further introduce a Dual-View Alignment Loss, which aligns low-frequency information with the periodic branch and high-frequency information with the residual branch. Extensive experiments on four real-world traffic datasets demonstrate that HyperD achieves state-of-the-art prediction accuracy, while offering superior robustness under disturbances and improved computational efficiency compared to existing methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u9884\u6d4b\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u4e2a\u5b50\u9886\u57df\uff08\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff09\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u4efb\u52a1\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u5e76\u4f7f\u7528\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5e38\u7528\u7684\u65b9\u6cd5\u3002", "keywords": ["traffic forecasting", "trajectory prediction", "route planning", "spatial-temporal attention", "time series prediction"]}}
{"id": "2511.09331", "pdf": "https://arxiv.org/pdf/2511.09331", "abs": "https://arxiv.org/abs/2511.09331", "authors": ["Stepan Dergachev", "Artem Pshenitsyn", "Aleksandr Panov", "Alexey Skrynnik", "Konstantin Yakovlev"], "title": "CoRL-MPPI: Enhancing MPPI With Learnable Behaviours For Efficient And Provably-Safe Multi-Robot Collision Avoidance", "categories": ["cs.RO", "cs.MA"], "comment": "The manuscript includes 9 pages, 4 figures, and 1 table", "summary": "Decentralized collision avoidance remains a core challenge for scalable multi-robot systems. One of the promising approaches to tackle this problem is Model Predictive Path Integral (MPPI) -- a framework that is naturally suited to handle any robot motion model and provides strong theoretical guarantees. Still, in practice MPPI-based controller may provide suboptimal trajectories as its performance relies heavily on uninformed random sampling. In this work, we introduce CoRL-MPPI, a novel fusion of Cooperative Reinforcement Learning and MPPI to address this limitation. We train an action policy (approximated as deep neural network) in simulation that learns local cooperative collision avoidance behaviors. This learned policy is then embedded into the MPPI framework to guide its sampling distribution, biasing it towards more intelligent and cooperative actions. Notably, CoRL-MPPI preserves all the theoretical guarantees of regular MPPI. We evaluate our approach in dense, dynamic simulation environments against state-of-the-art baselines, including ORCA, BVC, and a multi-agent MPPI implementation. Our results demonstrate that CoRL-MPPI significantly improves navigation efficiency (measured by success rate and makespan) and safety, enabling agile and robust multi-robot navigation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u673a\u5668\u4eba\u907f\u969c\u7684\u8def\u5f84\u89c4\u5212\uff0c\u4f7f\u7528\u4e86Model Predictive Path Integral (MPPI)\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u63d0\u5230\u4e86Cooperative Reinforcement Learning\uff0c\u5e76\u4e14\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002MPPI\u672c\u8eab\u4e5f\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "path planning", "collision avoidance", "Model Predictive Path Integral", "MPPI", "Reinforcement Learning", "deep neural network"]}}
{"id": "2511.08884", "pdf": "https://arxiv.org/pdf/2511.08884", "abs": "https://arxiv.org/abs/2511.08884", "authors": ["Oliver Wang", "Pengrui Quan", "Kang Yang", "Mani Srivastava"], "title": "Spectral Predictability as a Fast Reliability Indicator for Time Series Forecasting Model Selection", "categories": ["cs.LG"], "comment": null, "summary": "Practitioners deploying time series forecasting models face a dilemma: exhaustively validating dozens of models is computationally prohibitive, yet choosing the wrong model risks poor performance. We show that spectral predictability~$\u03a9$ -- a simple signal processing metric -- systematically stratifies model family performance, enabling fast model selection. We conduct controlled experiments in four different domains, then further expand our analysis to 51 models and 28 datasets from the GIFT-Eval benchmark. We find that large time series foundation models (TSFMs) systematically outperform lightweight task-trained baselines when $\u03a9$ is high, while their advantage vanishes as $\u03a9$ drops. Computing $\u03a9$ takes seconds per dataset, enabling practitioners to quickly assess whether their data suits TSFM approaches or whether simpler, cheaper models suffice. We demonstrate that $\u03a9$ stratifies model performance predictably, offering a practical first-pass filter that reduces validation costs while highlighting the need for models that excel on genuinely difficult (low-$\u03a9$) problems rather than merely optimizing easy ones.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses time series forecasting model selection using spectral predictability. While it doesn't directly address trajectory prediction, it does mention and evaluate large time series foundation models (TSFMs), indicating some relevance to large models used in time series analysis. It doesn't explicitly link these models to trajectory prediction, but the general methodology might be applicable.", "keywords": ["time series forecasting", "large time series foundation models", "TSFMs", "foundation models"]}}
{"id": "2511.09515", "pdf": "https://arxiv.org/pdf/2511.09515", "abs": "https://arxiv.org/abs/2511.09515", "authors": ["Fangqi Zhu", "Zhengyang Yan", "Zicong Hong", "Quanxin Shou", "Xiao Ma", "Song Guo"], "title": "WMPO: World Model-based Policy Optimization for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI"], "comment": "project website: https://wm-po.github.io", "summary": "Vision-Language-Action (VLA) models have shown strong potential for general-purpose robotic manipulation, but their reliance on expert demonstrations limits their ability to learn from failures and perform self-corrections. Reinforcement learning (RL) addresses these through self-improving interactions with the physical environment, but suffers from high sample complexity on real robots. We introduce World-Model-based Policy Optimization (WMPO), a principled framework for on-policy VLA RL without interacting with the real environment. In contrast to widely used latent world models, WMPO focuses on pixel-based predictions that align the \"imagined\" trajectories with the VLA features pretrained with web-scale images. Crucially, WMPO enables the policy to perform on-policy GRPO that provides stronger performance than the often-used off-policy methods. Extensive experiments in both simulation and real-robot settings demonstrate that WMPO (i) substantially improves sample efficiency, (ii) achieves stronger overall performance, (iii) exhibits emergent behaviors such as self-correction, and (iv) demonstrates robust generalization and lifelong learning capabilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action models, which often involve predicting future states or actions based on visual and linguistic inputs. While not directly trajectory prediction, the 'imagined trajectories' mentioned in the abstract suggest a form of predictive modeling. The mention of 'VLA features pretrained with web-scale images' indicates the use of large pre-trained models, although not strictly language models. Therefore, there is some relevance to both trajectory prediction and large models.", "keywords": ["Vision-Language-Action models", "VLA", "reinforcement learning", "world models", "pixel-based predictions", "imagined trajectories", "pretrained with web-scale images"]}}
{"id": "2511.08888", "pdf": "https://arxiv.org/pdf/2511.08888", "abs": "https://arxiv.org/abs/2511.08888", "authors": ["Christopher Cheong", "Gary Davis", "Seongjin Choi"], "title": "Weaver: Kronecker Product Approximations of Spatiotemporal Attention for Traffic Network Forecasting", "categories": ["cs.LG"], "comment": null, "summary": "Spatiotemporal forecasting on transportation networks is a complex task that requires understanding how traffic nodes interact within a dynamic, evolving system dictated by traffic flow dynamics and social behavioral patterns. The importance of transportation networks and ITS for modern mobility and commerce necessitates forecasting models that are not only accurate but also interpretable, efficient, and robust under structural or temporal perturbations. Recent approaches, particularly Transformer-based architectures, have improved predictive performance but often at the cost of high computational overhead and diminished architectural interpretability. In this work, we introduce Weaver, a novel attention-based model that applies Kronecker product approximations (KPA) to decompose the PN X PN spatiotemporal attention of O(P^2N^2) complexity into local P X P temporal and N X N spatial attention maps. This Kronecker attention map enables our Parallel-Kronecker Matrix-Vector product (P2-KMV) for efficient spatiotemporal message passing with O(P^2N + N^2P) complexity. To capture real-world traffic dynamics, we address the importance of negative edges in modeling traffic behavior by introducing Valence Attention using the continuous Tanimoto coefficient (CTC), which provides properties conducive to precise latent graph generation and training stability. To fully utilize the model's learning capacity, we introduce the Traffic Phase Dictionary for self-conditioning. Evaluations on PEMS-BAY and METR-LA show that Weaver achieves competitive performance across model categories while training more efficiently.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u7f51\u7edc\u9884\u6d4b\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u7279\u522b\u662f\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u662f\u4f7f\u7528\u4e86Transformer\u67b6\u6784\uff0c\u800cTransformer\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "traffic network forecasting", "spatiotemporal forecasting", "Transformer", "traffic flow dynamics"]}}
{"id": "2511.09516", "pdf": "https://arxiv.org/pdf/2511.09516", "abs": "https://arxiv.org/abs/2511.09516", "authors": ["Runhao Li", "Wenkai Guo", "Zhenyu Wu", "Changyuan Wang", "Haoyuan Deng", "Zhenyu Weng", "Yap-Peng Tan", "Ziwei Wang"], "title": "MAP-VLA: Memory-Augmented Prompting for Vision-Language-Action Model in Robotic Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Pre-trained Vision-Language-Action (VLA) models have achieved remarkable success in improving robustness and generalization for end-to-end robotic manipulation. However, these models struggle with long-horizon tasks due to their lack of memory and reliance solely on immediate sensory inputs. To address this limitation, we propose Memory-Augmented Prompting for Vision-Language-Action model (MAP-VLA), a novel framework that empowers pre-trained VLA models with demonstration-derived memory prompts to augment action generation for long-horizon robotic manipulation tasks. To achieve this, MAP-VLA first constructs a memory library from historical demonstrations, where each memory unit captures information about a specific stage of a task. These memory units are implemented as learnable soft prompts optimized through prompt tuning. Then, during real-time task execution, MAP-VLA retrieves relevant memory through trajectory similarity matching and dynamically integrates it into the VLA model for augmented action generation. Importantly, this prompt tuning and retrieval augmentation approach operates as a plug-and-play module for a frozen VLA model, offering a lightweight and flexible solution to improve task performance. Experimental results show that MAP-VLA delivers up to 7.0% absolute performance gains in the simulation benchmark and 25.0% on real robot evaluations for long-horizon tasks, surpassing the current state-of-the-art methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robotic manipulation and uses a Vision-Language-Action (VLA) model augmented with memory prompts. While it doesn't explicitly deal with trajectory prediction in the traditional sense (e.g., predicting future trajectories of pedestrians or vehicles), the 'action generation' component and the use of 'trajectory similarity matching' suggest a connection to predicting the robot's future actions/movements, which can be viewed as a form of trajectory prediction in the action space. It involves a 'Vision-Language-Action model', which can be considered related to large models, but is not a Large Language Model in the typical sense.", "keywords": ["Vision-Language-Action model", "VLA", "robotic manipulation", "action generation", "trajectory similarity matching"]}}
{"id": "2511.08616", "pdf": "https://arxiv.org/pdf/2511.08616", "abs": "https://arxiv.org/abs/2511.08616", "authors": ["Kelvin J. L. Koa", "Jan Chen", "Yunshan Ma", "Huanhuan Zheng", "Tat-Seng Chua"], "title": "Reasoning on Time-Series for Financial Technical Analysis", "categories": ["q-fin.ST", "cs.AI", "cs.LG", "q-fin.CP"], "comment": "ICAIF 2025 Workshop (Best Paper)", "summary": "While Large Language Models have been used to produce interpretable stock forecasts, they mainly focus on analyzing textual reports but not historical price data, also known as Technical Analysis. This task is challenging as it switches between domains: the stock price inputs and outputs lie in the time-series domain, while the reasoning step should be in natural language. In this work, we introduce Verbal Technical Analysis (VTA), a novel framework that combine verbal and latent reasoning to produce stock time-series forecasts that are both accurate and interpretable. To reason over time-series, we convert stock price data into textual annotations and optimize the reasoning trace using an inverse Mean Squared Error (MSE) reward objective. To produce time-series outputs from textual reasoning, we condition the outputs of a time-series backbone model on the reasoning-based attributes. Experiments on stock datasets across U.S., Chinese, and European markets show that VTA achieves state-of-the-art forecasting accuracy, while the reasoning traces also perform well on evaluation by industry experts.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4f46\u91cd\u70b9\u662f\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\uff0c\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6d89\u53ca\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u8fd0\u52a8\u7269\u4f53\u8def\u5f84\u89c4\u5212\u7b49\u4e3b\u9898\u5173\u8054\u8f83\u5f31\u3002\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5206\u6790\u548c\u9884\u6d4b\u3002", "keywords": ["Large Language Models", "time-series", "forecasting", "reasoning"]}}
