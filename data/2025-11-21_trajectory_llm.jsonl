{"id": "2511.14977", "pdf": "https://arxiv.org/pdf/2511.14977", "abs": "https://arxiv.org/abs/2511.14977", "authors": ["Xiangyu Li", "Zhaomiao Guo"], "title": "SVBRD-LLM: Self-Verifying Behavioral Rule Discovery for Autonomous Vehicle Identification", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "As more autonomous vehicles operate on public roads, understanding real-world behavior of autonomous vehicles is critical to analyzing traffic safety, making policies, and public acceptance. This paper proposes SVBRD-LLM, a framework that automatically discovers, verifies, and applies interpretable behavioral rules from real traffic videos through zero-shot prompt engineering. The framework extracts vehicle trajectories using YOLOv8 and ByteTrack, computes kinematic features, and employs GPT-5 zero-shot prompting to compare autonomous and human-driven vehicles, generating 35 structured behavioral rule hypotheses. These rules are tested on a validation set, iteratively refined based on failure cases to filter spurious correlations, and compiled into a high-confidence rule library. The framework is evaluated on an independent test set for speed change prediction, lane change prediction, and autonomous vehicle identification tasks. Experiments on over 1500 hours of real traffic videos show that the framework achieves 90.0% accuracy and 93.3% F1-score in autonomous vehicle identification. The discovered rules clearly reveal distinctive characteristics of autonomous vehicles in speed control smoothness, lane change conservativeness, and acceleration stability, with each rule accompanied by semantic description, applicable context, and validation confidence.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant because it combines trajectory data (extracted from vehicle trajectories) with a Large Language Model (GPT-5) to discover and verify behavioral rules for autonomous vehicle identification. It directly addresses both trajectory prediction (lane change prediction, speed change prediction) and large language models.", "keywords": ["trajectory prediction", "autonomous vehicles", "Large Language Models", "LLM", "GPT-5", "behavioral rule discovery", "lane change prediction", "speed change prediction"]}}
{"id": "2511.13326", "pdf": "https://arxiv.org/pdf/2511.13326", "abs": "https://arxiv.org/abs/2511.13326", "authors": ["Siyao Zhao", "Hao Ma", "Zhiqiang Pu", "Jingjing Huang", "Yi Pan", "Shijie Wang", "Zhi Ming"], "title": "TacEleven: generative tactic discovery for football open play", "categories": ["stat.AP", "cs.AI"], "comment": null, "summary": "Creating offensive advantages during open play is fundamental to football success. However, due to the highly dynamic and long-sequence nature of open play, the potential tactic space grows exponentially as the sequence progresses, making automated tactic discovery extremely challenging. To address this, we propose TacEleven, a generative framework for football open-play tactic discovery developed in close collaboration with domain experts from AJ Auxerre, designed to assist coaches and analysts in tactical decision-making. TacEleven consists of two core components: a language-controlled tactical generator that produces diverse tactical proposals, and a multimodal large language model-based tactical critic that selects the optimal proposal aligned with a high-level stylistic tactical instruction. The two components enables rapid exploration of tactical proposals and discovery of alternative open-play offensive tactics. We evaluate TacEleven across three tasks with progressive tactical complexity: counterfactual exploration, single-step discovery, and multi-step discovery, through both quantitative metrics and a questionnaire-based qualitative assessment. The results show that the TacEleven-discovered tactics exhibit strong realism and tactical creativity, with 52.50% of the multi-step tactical alternatives rated adoptable in real-world elite football scenarios, highlighting the framework's ability to rapidly generate numerous high-quality tactics for complex long-sequence open-play situations. TacEleven demonstrates the potential of creatively leveraging domain data and generative models to advance tactical analysis in sports.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on tactic discovery in football, which involves predicting player movements and actions. It also explicitly uses a multimodal large language model for tactical criticism. Therefore, it has strong relevance to both trajectory prediction (in the context of player movement) and large language models.", "keywords": ["trajectory prediction", "large language models", "generative models", "tactic discovery", "open play", "action prediction"]}}
{"id": "2511.15248", "pdf": "https://arxiv.org/pdf/2511.15248", "abs": "https://arxiv.org/abs/2511.15248", "authors": ["Kai Yang", "Xin Xu", "Yangkun Chen", "Weijie Liu", "Jiafei Lyu", "Zichuan Lin", "Deheng Ye", "Saiyong Yang"], "title": "EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on the long-term training of Large Language Models (LLMs) and uses reinforcement learning techniques to stabilize entropy during training. While it doesn't directly deal with trajectory prediction, its core focus on LLMs and the use of control mechanisms might be relevant to future research combining LLMs and trajectory prediction. The connection is indirect but present.", "keywords": ["Large Language Models", "LLMs", "reinforcement learning", "entropy", "training"]}}
{"id": "2511.14910", "pdf": "https://arxiv.org/pdf/2511.14910", "abs": "https://arxiv.org/abs/2511.14910", "authors": ["Yassine Ibork", "Myounggyu Won", "Lokesh Das"], "title": "Z-Merge: Multi-Agent Reinforcement Learning for On-Ramp Merging with Zone-Specific V2X Traffic Information", "categories": ["cs.RO"], "comment": null, "summary": "Ramp merging is a critical and challenging task for autonomous vehicles (AVs), particularly in mixed traffic environments with human-driven vehicles (HVs). Existing approaches typically rely on either lane-changing or inter-vehicle gap creation strategies based solely on local or neighboring information, often leading to suboptimal performance in terms of safety and traffic efficiency. In this paper, we present a V2X (vehicle-to-everything communication)-assisted Multiagent Reinforcement Learning (MARL) framework for on-ramp merging that effectively coordinates the complex interplay between lane-changing and inter-vehicle gap adaptation strategies by utilizing zone-specific global information available from a roadside unit (RSU). The merging control problem is formulated as a Multiagent Partially Observable Markov Decision Process (MA-POMDP), where agents leverage both local and global observations through V2X communication. To support both discrete and continuous control decisions, we design a hybrid action space and adopt a parameterized deep Q-learning approach. Extensive simulations, integrating the SUMO traffic simulator and the MOSAIC V2X simulator, demonstrate that our framework significantly improves merging success rate, traffic efficiency, and road safety across diverse traffic scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-agent reinforcement learning for on-ramp merging of autonomous vehicles. While it doesn't explicitly mention Large Language Models, it deals with a specific type of trajectory prediction and control in a multi-agent environment. The connection to trajectory prediction is through the autonomous vehicle's path planning and decision-making process during merging.", "keywords": ["trajectory prediction", "autonomous vehicles", "reinforcement learning", "multi-agent reinforcement learning", "V2X", "on-ramp merging"]}}
{"id": "2511.14994", "pdf": "https://arxiv.org/pdf/2511.14994", "abs": "https://arxiv.org/abs/2511.14994", "authors": ["Yue Yu", "Xiaobo Zheng", "Shaoming He"], "title": "Communication-Aware Asynchronous Distributed Trajectory Optimization for UAV Swarm", "categories": ["cs.RO"], "comment": null, "summary": "Distributed optimization offers a promising paradigm for trajectory planning in Unmanned Aerial Vehicle (UAV) swarms, yet its deployment in communication-constrained environments remains challenging due to unreliable links and limited data exchange. This paper addresses this issue via a two-tier architecture explicitly designed for operation under communication constraints. We develop a Communication-Aware Asynchronous Distributed Trajectory Optimization (CA-ADTO) framework that integrates Parameterized Differential Dynamic Programming (PDDP) for local trajectory optimization of individual UAVs with an asynchronous Alternating Direction Method of Multipliers (async-ADMM) for swarm-level coordination. The proposed architecture enables fully distributed optimization while substantially reducing communication overhead, making it suitable for real-world scenarios in which reliable connectivity cannot be guaranteed. The method is particularly effective in handling nonlinear dynamics and spatio-temporal coupling under communication constraints.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory optimization for UAV swarms, a specific type of trajectory prediction. While it doesn't directly involve large language models, the core topic of trajectory optimization places it within the broader scope of trajectory prediction. The keywords highlight the trajectory planning and optimization aspects.", "keywords": ["trajectory optimization", "trajectory planning", "UAV", "distributed optimization"]}}
{"id": "2511.14887", "pdf": "https://arxiv.org/pdf/2511.14887", "abs": "https://arxiv.org/abs/2511.14887", "authors": ["Nathan M. Roberts", "Xiaosong Du"], "title": "Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone", "categories": ["cs.LG"], "comment": "Conference version with 12 pages and 2 figures", "summary": "The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\\times10^6$ time steps, representing 25% of the $19.79\\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory design for eVTOL drones, which falls under trajectory prediction/planning. It utilizes a transformer to guide deep reinforcement learning, but does not explicitly involve large language models. The transformer is used as an architecture within the DRL framework, not as a large language model in the typical sense.", "keywords": ["trajectory design", "eVTOL", "deep reinforcement learning", "transformer", "optimal control"]}}
{"id": "2511.15055", "pdf": "https://arxiv.org/pdf/2511.15055", "abs": "https://arxiv.org/abs/2511.15055", "authors": ["Jian-Ting Guo", "Yu-Cheng Chen", "Ping-Chun Hsieh", "Kuo-Hao Ho", "Po-Wei Huang", "Ti-Rong Wu", "I-Chen Wu"], "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "Accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on learning human-like behavior in reinforcement learning agents by optimizing trajectories to align with human demonstrations. While it doesn't directly involve Large Language Models, it does heavily focus on trajectory optimization and learning from human trajectories, making it relevant to the trajectory prediction aspect. The connection to trajectory prediction lies in the goal of generating trajectories similar to human trajectories.", "keywords": ["trajectory optimization", "human-like behavior", "reinforcement learning", "trajectory similarity", "action quantization"]}}
{"id": "2511.15069", "pdf": "https://arxiv.org/pdf/2511.15069", "abs": "https://arxiv.org/abs/2511.15069", "authors": ["Haoyong Wu", "Yongmei Liu"], "title": "ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses Large Language Models (LLMs) for reasoning about actions. While it doesn't explicitly mention trajectory prediction, reasoning about actions is a component within trajectory prediction. Therefore, there's a moderate relevance.", "keywords": ["Large Language Models", "LLMs", "reasoning about actions"]}}
{"id": "2511.14945", "pdf": "https://arxiv.org/pdf/2511.14945", "abs": "https://arxiv.org/abs/2511.14945", "authors": ["Fan Yang", "Quanting Xie", "Atsunori Moteki", "Shoichi Masui", "Shan Jiang", "Yonatan Bisk", "Graham Neubig"], "title": "Unsupervised Discovery of Long-Term Spatiotemporal Periodic Workflows in Human Activities", "categories": ["cs.CV"], "comment": "accepted to WACV 2026", "summary": "Periodic human activities with implicit workflows are common in manufacturing, sports, and daily life. While short-term periodic activities -- characterized by simple structures and high-contrast patterns -- have been widely studied, long-term periodic workflows with low-contrast patterns remain largely underexplored. To bridge this gap, we introduce the first benchmark comprising 580 multimodal human activity sequences featuring long-term periodic workflows. The benchmark supports three evaluation tasks aligned with real-world applications: unsupervised periodic workflow detection, task completion tracking, and procedural anomaly detection. We also propose a lightweight, training-free baseline for modeling diverse periodic workflow patterns. Experiments show that: (i) our benchmark presents significant challenges to both unsupervised periodic detection methods and zero-shot approaches based on powerful large language models (LLMs); (ii) our baseline outperforms competing methods by a substantial margin in all evaluation tasks; and (iii) in real-world applications, our baseline demonstrates deployment advantages on par with traditional supervised workflow detection approaches, eliminating the need for annotation and retraining. Our project page is https://sites.google.com/view/periodicworkflow.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on unsupervised discovery of long-term spatiotemporal periodic workflows in human activities. While it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting the future trajectory of a moving object), it deals with sequential patterns in human activities which are related. The abstract also mentions the use of large language models (LLMs) for zero-shot approaches, increasing the relevance.", "keywords": ["human activity sequences", "periodic workflows", "large language models", "LLMs", "spatiotemporal"]}}
{"id": "2511.15239", "pdf": "https://arxiv.org/pdf/2511.15239", "abs": "https://arxiv.org/abs/2511.15239", "authors": ["Tomoki Nakao", "Kazumi Kasaura", "Tadashi Kozuno"], "title": "Symmetry-Breaking in Multi-Agent Navigation: Winding Number-Aware MPC with a Learned Topological Strategy", "categories": ["cs.RO", "cs.MA"], "comment": "11 pages, 5 figures", "summary": "We address the fundamental challenge of resolving symmetry-induced deadlocks in distributed multi-agent navigation by proposing a new hierarchical navigation method. When multiple agents interact, it is inherently difficult for them to autonomously break the symmetry of deciding how to pass each other. To tackle this problem, we introduce an approach that quantifies cooperative symmetry-breaking strategies using a topological invariant called the winding number, and learns the strategies themselves through reinforcement learning. Our method features a hierarchical policy consisting of a learning-based Planner, which plans topological cooperative strategies, and a model-based Controller, which executes them. Through reinforcement learning, the Planner learns to produce two types of parameters for the Controller: one is the topological cooperative strategy represented by winding numbers, and the other is a set of dynamic weights that determine which agent interaction to prioritize in dense scenarios where multiple agents cross simultaneously. The Controller then generates collision-free and efficient motions based on the strategy and weights provided by the Planner. This hierarchical structure combines the flexible decision-making ability of learning-based methods with the reliability of model-based approaches. Simulation and real-world robot experiments demonstrate that our method outperforms existing baselines, particularly in dense environments, by efficiently avoiding collisions and deadlocks while achieving superior navigation performance. The code for the experiments is available at https://github.com/omron-sinicx/WNumMPC.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on multi-agent navigation and uses reinforcement learning to learn cooperative strategies. While it doesn't directly use or discuss large language models, the problem of multi-agent navigation is related to trajectory prediction and path planning. The use of reinforcement learning implies a learned model, although not a large language model.", "keywords": ["multi-agent navigation", "reinforcement learning", "trajectory prediction", "path planning"]}}
{"id": "2511.15279", "pdf": "https://arxiv.org/pdf/2511.15279", "abs": "https://arxiv.org/abs/2511.15279", "authors": ["Jiashu Yang", "Yifan Han", "Yucheng Xie", "Ning Guo", "Wenzhao Lian"], "title": "Look, Zoom, Understand: The Robotic Eyeball for Embodied Perception", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "In embodied AI perception systems, visual perception should be active: the goal is not to passively process static images, but to actively acquire more informative data within pixel and spatial budget constraints. Existing vision models and fixed RGB-D camera systems fundamentally fail to reconcile wide-area coverage with fine-grained detail acquisition, severely limiting their efficacy in open-world robotic applications. To address this issue, we propose EyeVLA, a robotic eyeball for active visual perception that can take proactive actions based on instructions, enabling clear observation of fine-grained target objects and detailed information across a wide spatial extent. EyeVLA discretizes action behaviors into action tokens and integrates them with vision-language models (VLMs) that possess strong open-world understanding capabilities, enabling joint modeling of vision, language, and actions within a single autoregressive sequence. By using the 2D bounding box coordinates to guide the reasoning chain and applying reinforcement learning to refine the viewpoint selection policy, we transfer the open-world scene understanding capability of the VLM to a vision language action (VLA) policy using only minimal real-world data. Experiments show that our system efficiently performs instructed scenes in real-world environments and actively acquires more accurate visual information through instruction-driven actions of rotation and zoom, thereby achieving strong environmental perception capabilities. EyeVLA introduces a novel robotic vision system that leverages detailed and spatially rich, large-scale embodied data, and actively acquires highly informative visual observations for downstream embodied tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on active visual perception for embodied AI, using a robotic eyeball (EyeVLA). It leverages Vision-Language Models (VLMs) for understanding and action planning. While not directly addressing trajectory prediction, the active perception and action planning aspects, combined with the use of VLMs, suggest a moderate relevance. The system learns a policy to control the viewpoint of the robotic eyeball, which can be seen as a form of trajectory planning in the space of camera poses. However, it's not explicitly framed as trajectory prediction.", "keywords": ["Vision-Language Models", "VLMs", "embodied AI", "action planning", "reinforcement learning"]}}
{"id": "2511.15284", "pdf": "https://arxiv.org/pdf/2511.15284", "abs": "https://arxiv.org/abs/2511.15284", "authors": ["Jonas De Maeyer", "Hossein Yarahmadi", "Moharram Challenger"], "title": "Path Planning through Multi-Agent Reinforcement Learning in Dynamic Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Path planning in dynamic environments is a fundamental challenge in intelligent transportation and robotics, where obstacles and conditions change over time, introducing uncertainty and requiring continuous adaptation. While existing approaches often assume complete environmental unpredictability or rely on global planners, these assumptions limit scalability and practical deployment in real-world settings. In this paper, we propose a scalable, region-aware reinforcement learning (RL) framework for path planning in dynamic environments. Our method builds on the observation that environmental changes, although dynamic, are often localized within bounded regions. To exploit this, we introduce a hierarchical decomposition of the environment and deploy distributed RL agents that adapt to changes locally. We further propose a retraining mechanism based on sub-environment success rates to determine when policy updates are necessary. Two training paradigms are explored: single-agent Q-learning and multi-agent federated Q-learning, where local Q-tables are aggregated periodically to accelerate the learning process. Unlike prior work, we evaluate our methods in more realistic settings, where multiple simultaneous obstacle changes and increasing difficulty levels are present. Results show that the federated variants consistently outperform their single-agent counterparts and closely approach the performance of A* Oracle while maintaining shorter adaptation times and robust scalability. Although initial training remains time-consuming in large environments, our decentralized framework eliminates the need for a global planner and lays the groundwork for future improvements using deep RL and flexible environment decomposition.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on path planning in dynamic environments using reinforcement learning. While it doesn't directly involve Large Language Models, path planning is related to trajectory prediction. The mention of 'dynamic environments' and 'obstacle changes' further suggests a connection to trajectory prediction, albeit a somewhat indirect one. The use of Reinforcement Learning is also a common technique for trajectory prediction. The paper does not mention LLMs.", "keywords": ["path planning", "dynamic environments", "reinforcement learning", "trajectory prediction"]}}
{"id": "2511.15202", "pdf": "https://arxiv.org/pdf/2511.15202", "abs": "https://arxiv.org/abs/2511.15202", "authors": ["Yinsheng Wang", "Tario G You", "L\u00e9onard Boussioux", "Shan Liu"], "title": "SOLID: a Framework of Synergizing Optimization and LLMs for Intelligent Decision-Making", "categories": ["cs.AI"], "comment": "NeurIPS 2025 WORKSHOP ML*OR Workshop: Mathematical Foundations and Operational Integration of Machine Learning for Uncertainty-Aware Decision-Making", "summary": "This paper introduces SOLID (Synergizing Optimization and Large Language Models for Intelligent Decision-Making), a novel framework that integrates mathematical optimization with the contextual capabilities of large language models (LLMs). SOLID facilitates iterative collaboration between optimization and LLMs agents through dual prices and deviation penalties. This interaction improves the quality of the decisions while maintaining modularity and data privacy. The framework retains theoretical convergence guarantees under convexity assumptions, providing insight into the design of LLMs prompt. To evaluate SOLID, we applied it to a stock portfolio investment case with historical prices and financial news as inputs. Empirical results demonstrate convergence under various scenarios and indicate improved annualized returns compared to a baseline optimizer-only method, validating the synergy of the two agents. SOLID offers a promising framework for advancing automated and intelligent decision-making across diverse domains.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on synergizing optimization and Large Language Models (LLMs) for intelligent decision-making. While it doesn't directly address trajectory prediction, the framework could potentially be applied to trajectory prediction tasks by using LLMs to provide contextual information or guide the optimization process. The connection is not explicit but plausible, hence a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "optimization", "intelligent decision-making"]}}
{"id": "2511.15414", "pdf": "https://arxiv.org/pdf/2511.15414", "abs": "https://arxiv.org/abs/2511.15414", "authors": ["Mingyang Feng", "Shaoyuan Li", "Xiang Yin"], "title": "RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to IROS 2025", "summary": "We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \\emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528Transformer\u7f51\u7edc\u6765\u6539\u8fdbRRT*\u7b97\u6cd5\uff0c\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\u3002\u867d\u7136\u6d89\u53caTransformer\u6a21\u578b\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u8fd0\u52a8\u89c4\u5212\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5173\u7cfb\u4e0d\u5927\u3002Transformer\u5728\u8fd9\u91cc\u4e3b\u8981\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u800c\u975e\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["motion planning", "RRT*", "Transformer", "sampling-based algorithm"]}}
{"id": "2511.02505", "pdf": "https://arxiv.org/pdf/2511.02505", "abs": "https://arxiv.org/abs/2511.02505", "authors": ["Yaosen Chen", "Wei Wang", "Tianheng Zheng", "Xuming Wen", "Han Yang", "Yanru Zhang"], "title": "ESA: Energy-Based Shot Assembly Optimization for Automatic Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Shot assembly is a crucial step in film production and video editing, involving the sequencing and arrangement of shots to construct a narrative, convey information, or evoke emotions. Traditionally, this process has been manually executed by experienced editors. While current intelligent video editing technologies can handle some automated video editing tasks, they often fail to capture the creator's unique artistic expression in shot assembly. To address this challenge, we propose an energy-based optimization method for video shot assembly. Specifically, we first perform visual-semantic matching between the script generated by a large language model and a video library to obtain subsets of candidate shots aligned with the script semantics. Next, we segment and label the shots from reference videos, extracting attributes such as shot size, camera motion, and semantics. We then employ energy-based models to learn from these attributes, scoring candidate shot sequences based on their alignment with reference styles. Finally, we achieve shot assembly optimization by combining multiple syntax rules, producing videos that align with the assembly style of the reference videos. Our method not only automates the arrangement and combination of independent shots according to specific logic, narrative requirements, or artistic styles but also learns the assembly style of reference videos, creating a coherent visual sequence or holistic visual expression. With our system, even users with no prior video editing experience can create visually compelling videos. Project page: https://sobeymil.github.io/esa.com", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u7f16\u8f91\u4e2d\u7684\u955c\u5934\u7ec4\u88c5\u4f18\u5316\u95ee\u9898\u3002\u867d\u7136\u5176\u6838\u5fc3\u662f\u89c6\u9891\u7f16\u8f91\uff0c\u4f46\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u811a\u672c\uff0c\u5e76\u5229\u7528\u89c6\u89c9-\u8bed\u4e49\u5339\u914d\u5c06\u811a\u672c\u4e0e\u89c6\u9891\u5e93\u4e2d\u7684\u5019\u9009\u955c\u5934\u5bf9\u9f50\u3002\u56e0\u6b64\uff0c\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\u3002", "keywords": ["large language model", "LLM"]}}
{"id": "2511.15065", "pdf": "https://arxiv.org/pdf/2511.15065", "abs": "https://arxiv.org/abs/2511.15065", "authors": ["Cheng Yang", "Haiyuan Wan", "Yiran Peng", "Xin Cheng", "Zhaoyang Yu", "Jiayi Zhang", "Junchi Yu", "Xinlei Yu", "Xiawu Zheng", "Dongzhan Zhou", "Chenglin Wu"], "title": "Reasoning via Video: The First Evaluation of Video Models' Reasoning Abilities through Maze-Solving Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Models have achieved remarkable success in high-fidelity video generation with coherent motion dynamics. Analogous to the development from text generation to text-based reasoning in language modeling, the development of video models motivates us to ask: Can video models reason via video generation? Compared with the discrete text corpus, video grounds reasoning in explicit spatial layouts and temporal continuity, which serves as an ideal substrate for spatial reasoning. In this work, we explore the reasoning via video paradigm and introduce VR-Bench -- a comprehensive benchmark designed to systematically evaluate video models' reasoning capabilities. Grounded in maze-solving tasks that inherently require spatial planning and multi-step reasoning, VR-Bench contains 7,920 procedurally generated videos across five maze types and diverse visual styles. Our empirical analysis demonstrates that SFT can efficiently elicit the reasoning ability of video model. Video models exhibit stronger spatial perception during reasoning, outperforming leading VLMs and generalizing well across diverse scenarios, tasks, and levels of complexity. We further discover a test-time scaling effect, where diverse sampling during inference improves reasoning reliability by 10--20%. These findings highlight the unique potential and scalability of reasoning via video for spatial reasoning tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper explores reasoning via video generation, specifically focusing on maze-solving tasks which inherently involve spatial planning. While it doesn't directly address trajectory prediction, the spatial reasoning and motion dynamics aspects are relevant. It also mentions VLMs (Video Language Models) and scaling effects, connecting to large models. The connection to trajectory prediction is weaker but present due to the spatial reasoning component.", "keywords": ["Video Models", "reasoning", "spatial reasoning", "VLMs", "maze-solving", "motion dynamics"]}}
{"id": "2511.15208", "pdf": "https://arxiv.org/pdf/2511.15208", "abs": "https://arxiv.org/abs/2511.15208", "authors": ["Ranfei Chen", "Ming Chen", "Kaifei Wang"], "title": "Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones", "categories": ["cs.LG"], "comment": null, "summary": "Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured \"zones of confusion\": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses Diffusion Large Language Models (dLLMs) and uses trajectory-based Reinforcement Learning (RL) methods. It focuses on analyzing trajectories within dLLMs to improve reasoning accuracy. While it involves trajectory analysis within the context of large language models, it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting the future path of an object). However, the use of 'trajectory' and large language models warrants a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "Diffusion Large Language Models", "dLLMs", "trajectory", "reinforcement learning", "reasoning"]}}
{"id": "2511.15077", "pdf": "https://arxiv.org/pdf/2511.15077", "abs": "https://arxiv.org/abs/2511.15077", "authors": ["Shengjing Tian", "Yinan Han", "Xiantong Zhao", "Xuehu Liu", "Qi Lang"], "title": "MambaTrack3D: A State Space Model Framework for LiDAR-Based Object Tracking under High Temporal Variation", "categories": ["cs.CV"], "comment": "This work has been submitted to a journal for possible publication", "summary": "Dynamic outdoor environments with high temporal variation (HTV) pose significant challenges for 3D single object tracking in LiDAR point clouds. Existing memory-based trackers often suffer from quadratic computational complexity, temporal redundancy, and insufficient exploitation of geometric priors. To address these issues, we propose MambaTrack3D, a novel HTV-oriented tracking framework built upon the state space model Mamba. Specifically, we design a Mamba-based Inter-frame Propagation (MIP) module that replaces conventional single-frame feature extraction with efficient inter-frame propagation, achieving near-linear complexity while explicitly modeling spatial relations across historical frames. Furthermore, a Grouped Feature Enhancement Module (GFEM) is introduced to separate foreground and background semantics at the channel level, thereby mitigating temporal redundancy in the memory bank. Extensive experiments on KITTI-HTV and nuScenes-HTV benchmarks demonstrate that MambaTrack3D consistently outperforms both HTV-oriented and normal-scenario trackers, achieving improvements of up to 6.5 success and 9.5 precision over HVTrack under moderate temporal gaps. On the standard KITTI dataset, MambaTrack3D remains highly competitive with state-of-the-art normal-scenario trackers, confirming its strong generalization ability. Overall, MambaTrack3D achieves a superior accuracy-efficiency trade-off, delivering robust performance across both specialized HTV and conventional tracking scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 3D object tracking using LiDAR point clouds and proposes a novel tracking framework (MambaTrack3D) built upon the state space model Mamba. While it doesn't directly use Large Language Models, Mamba is a state-space model architecture that has gained attention as a potential alternative to Transformers in some sequence modeling tasks. Therefore, there is some connection to the broader field of large models, though the primary focus is on object tracking. The paper also addresses temporal variation which is related to trajectory prediction.", "keywords": ["object tracking", "LiDAR", "state space model", "Mamba", "temporal variation"]}}
{"id": "2511.14876", "pdf": "https://arxiv.org/pdf/2511.14876", "abs": "https://arxiv.org/abs/2511.14876", "authors": ["Henry Wong", "Clement Fung", "Weiran Lin", "Karen Li", "Stanley Chen", "Lujo Bauer"], "title": "Attacking Autonomous Driving Agents with Adversarial Machine Learning: A Holistic Evaluation with the CARLA Leaderboard", "categories": ["cs.CR", "cs.CV", "cs.LG", "cs.RO"], "comment": "12 pages", "summary": "To autonomously control vehicles, driving agents use outputs from a combination of machine-learning (ML) models, controller logic, and custom modules. Although numerous prior works have shown that adversarial examples can mislead ML models used in autonomous driving contexts, it remains unclear if these attacks are effective at producing harmful driving actions for various agents, environments, and scenarios.\n  To assess the risk of adversarial examples to autonomous driving, we evaluate attacks against a variety of driving agents, rather than against ML models in isolation. To support this evaluation, we leverage CARLA, an urban driving simulator, to create and evaluate adversarial examples. We create adversarial patches designed to stop or steer driving agents, stream them into the CARLA simulator at runtime, and evaluate them against agents from the CARLA Leaderboard, a public repository of best-performing autonomous driving agents from an annual research competition. Unlike prior work, we evaluate attacks against autonomous driving systems without creating or modifying any driving-agent code and against all parts of the agent included with the ML model.\n  We perform a case-study investigation of two attack strategies against three open-source driving agents from the CARLA Leaderboard across multiple driving scenarios, lighting conditions, and locations. Interestingly, we show that, although some attacks can successfully mislead ML models into predicting erroneous stopping or steering commands, some driving agents use modules, such as PID control or GPS-based rules, that can overrule attacker-manipulated predictions from ML models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4ee3\u7406\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u5176\u4e2d\u81ea\u52a8\u9a7e\u9a76\u4ee3\u7406\u4f9d\u8d56\u4e8e\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u51b3\u7b56\uff0c\u5305\u62ec\u8def\u5f84\u89c4\u5212\u548c\u63a7\u5236\u3002\u867d\u7136\u8bba\u6587\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u6d89\u53ca\u4e86\u8f68\u8ff9\u9884\u6d4b\uff08\u901a\u8fc7steering commands\uff09\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\u3002\u5bf9\u6297\u653b\u51fb\u7684\u76ee\u6807\u662f\u5f71\u54cd\u8fd9\u4e9b\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u4ece\u800c\u6539\u53d8\u8f66\u8f86\u7684\u884c\u9a76\u8f68\u8ff9\u3002", "keywords": ["autonomous driving agents", "machine learning", "adversarial examples", "steering commands", "driving scenarios", "trajectory prediction"]}}
{"id": "2511.15645", "pdf": "https://arxiv.org/pdf/2511.15645", "abs": "https://arxiv.org/abs/2511.15645", "authors": ["Shanshan Zhang"], "title": "MambaIO: Global-Coordinate Inertial Odometry for Pedestrians via Multi-Scale Frequency-Decoupled Modeling", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Inertial Odometry (IO) enables real-time localization using only acceleration and angular velocity measurements from an Inertial Measurement Unit (IMU), making it a promising solution for localization in consumer-grade applications. Traditionally, IMU measurements in IO have been processed under two coordinate system paradigms: the body coordinate frame and the global coordinate frame, with the latter being widely adopted. However, recent studies in drone scenarios have demonstrated that the body frame can significantly improve localization accuracy, prompting a re-evaluation of the suitability of the global frame for pedestrian IO. To address this issue, this paper systematically evaluates the effectiveness of the global coordinate frame in pedestrian IO through theoretical analysis, qualitative inspection, and quantitative experiments. Building upon these findings, we further propose MambaIO, which decomposes IMU measurements into high-frequency and low-frequency components using a Laplacian pyramid. The low-frequency component is processed by a Mamba architecture to extract implicit contextual motion cues, while the high-frequency component is handled by a convolutional structure to capture fine-grained local motion details. Experiments on multiple public datasets show that MambaIO substantially reduces localization error and achieves state-of-the-art (SOTA) performance. To the best of our knowledge, this is the first application of the Mamba architecture to the inertial odometry task.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Inertial Odometry for pedestrians, which is related to trajectory estimation and localization. While it doesn't directly use Large Language Models, it does employ the Mamba architecture, which is a state-space model often considered as an alternative to Transformers in some contexts and related to the broader field of large models. Therefore, there's a moderate relevance.", "keywords": ["Inertial Odometry", "pedestrian", "Mamba architecture", "trajectory estimation", "localization"]}}
{"id": "2511.15669", "pdf": "https://arxiv.org/pdf/2511.15669", "abs": "https://arxiv.org/abs/2511.15669", "authors": ["Cheng Yin", "Yankai Lin", "Wang Xu", "Sikyuen Tam", "Xiangrui Zeng", "Zhiyuan Liu", "Zhouping Yin"], "title": "DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "16 pages, 6 figures, conference", "summary": "Enabling Vision-Language-Action (VLA) models to \"think before acting\" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Vision-Language-Action models, which are related to robotics and potentially trajectory planning. It utilizes a large language model and reinforcement learning for reasoning and action. While not directly focused on trajectory prediction, the action component could involve trajectory generation or control. The connection to large models is clear through the use of SFT and RL.", "keywords": ["Large Language Models", "Vision-Language-Action Models", "Reinforcement Learning", "robot actions", "reasoning"]}}
{"id": "2511.15179", "pdf": "https://arxiv.org/pdf/2511.15179", "abs": "https://arxiv.org/abs/2511.15179", "authors": ["Kyotaro Tokoro", "Hiromu Taketsugu", "Norimichi Ukita"], "title": "MMCM: Multimodality-aware Metric using Clustering-based Modes for Probabilistic Human Motion Prediction", "categories": ["cs.CV"], "comment": "Accepted to WACV2026", "summary": "This paper proposes a novel metric for Human Motion Prediction (HMP). Since a single past sequence can lead to multiple possible futures, a probabilistic HMP method predicts such multiple motions. While a single motion predicted by a deterministic method is evaluated only with the difference from its ground truth motion, multiple predicted motions should also be evaluated based on their distribution. For this evaluation, this paper focuses on the following two criteria. \\textbf{(a) Coverage}: motions should be distributed among multiple motion modes to cover diverse possibilities. \\textbf{(b) Validity}: motions should be kinematically valid as future motions observable from a given past motion. However, existing metrics simply appreciate widely distributed motions even if these motions are observed in a single mode and kinematically invalid. To resolve these disadvantages, this paper proposes a Multimodality-aware Metric using Clustering-based Modes (MMCM). For (a) coverage, MMCM divides a motion space into several clusters, each of which is regarded as a mode. These modes are used to explicitly evaluate whether predicted motions are distributed among multiple modes. For (b) validity, MMCM identifies valid modes by collecting possible future motions from a motion dataset. Our experiments validate that our clustering yields sensible mode definitions and that MMCM accurately scores multimodal predictions. Code: https://github.com/placerkyo/MMCM", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on human motion prediction, which is a subfield of trajectory prediction. However, it doesn't mention or utilize large language models. The paper introduces a new metric for evaluating probabilistic human motion prediction methods.", "keywords": ["Human Motion Prediction", "motion prediction", "trajectory prediction", "probabilistic prediction"]}}
