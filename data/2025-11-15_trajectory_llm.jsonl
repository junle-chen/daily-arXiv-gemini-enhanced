{"id": "2511.10411", "pdf": "https://arxiv.org/pdf/2511.10411", "abs": "https://arxiv.org/abs/2511.10411", "authors": ["Benjamin Stoler", "Jonathan Francis", "Jean Oh"], "title": "LongComp: Long-Tail Compositional Zero-Shot Generalization for Robust Trajectory Prediction", "categories": ["cs.RO"], "comment": "8 pages, 3 figures", "summary": "Methods for trajectory prediction in Autonomous Driving must contend with rare, safety-critical scenarios that make reliance on real-world data collection alone infeasible. To assess robustness under such conditions, we propose new long-tail evaluation settings that repartition datasets to create challenging out-of-distribution (OOD) test sets. We first introduce a safety-informed scenario factorization framework, which disentangles scenarios into discrete ego and social contexts. Building on analogies to compositional zero-shot image-labeling in Computer Vision, we then hold out novel context combinations to construct challenging closed-world and open-world settings. This process induces OOD performance gaps in future motion prediction of 5.0% and 14.7% in closed-world and open-world settings, respectively, relative to in-distribution performance for a state-of-the-art baseline. To improve generalization, we extend task-modular gating networks to operate within trajectory prediction models, and develop an auxiliary, difficulty-prediction head to refine internal representations. Our strategies jointly reduce the OOD performance gaps to 2.8% and 11.5% in the two settings, respectively, while still improving in-distribution performance.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on trajectory prediction for autonomous driving, specifically addressing robustness and generalization in challenging out-of-distribution scenarios. While it doesn't directly use large language models, the problem it addresses is highly relevant to trajectory prediction. The mention of 'compositional zero-shot image-labeling in Computer Vision' hints at a potential connection to techniques sometimes used in conjunction with large models, though this isn't explicitly stated. The techniques used, such as task-modular gating networks, are not directly related to LLMs but are relevant to improving trajectory prediction models.", "keywords": ["trajectory prediction", "autonomous driving", "out-of-distribution", "generalization", "motion prediction", "long-tail"]}}
{"id": "2511.10203", "pdf": "https://arxiv.org/pdf/2511.10203", "abs": "https://arxiv.org/abs/2511.10203", "authors": ["Stephane Da Silva Martins", "Emanuel Aldea", "Sylvie Le H\u00e9garat-Mascle"], "title": "VISTA: A Vision and Intent-Aware Social Attention Framework for Multi-Agent Trajectory Prediction", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "Paper accepted at WACV 2026", "summary": "Multi-agent trajectory prediction is crucial for autonomous systems operating in dense, interactive environments. Existing methods often fail to jointly capture agents' long-term goals and their fine-grained social interactions, which leads to unrealistic multi-agent futures. We propose VISTA, a recursive goal-conditioned transformer for multi-agent trajectory forecasting. VISTA combines (i) a cross-attention fusion module that integrates long-horizon intent with past motion, (ii) a social-token attention mechanism for flexible interaction modeling across agents, and (iii) pairwise attention maps that make social influence patterns interpretable at inference time. Our model turns single-agent goal-conditioned prediction into a coherent multi-agent forecasting framework. Beyond standard displacement metrics, we evaluate trajectory collision rates as a measure of joint realism. On the high-density MADRAS benchmark and on SDD, VISTA achieves state-of-the-art accuracy and substantially fewer collisions. On MADRAS, it reduces the average collision rate of strong baselines from 2.14 to 0.03 percent, and on SDD it attains zero collisions while improving ADE, FDE, and minFDE. These results show that VISTA generates socially compliant, goal-aware, and interpretable trajectories, making it promising for safety-critical autonomous systems.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u660e\u786e\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u4f7f\u7528transformer\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u4e14transformer\u662f\u5f88\u591a\u5927\u6a21\u578b\u7684\u57fa\u7840\u67b6\u6784\u3002\u56e0\u6b64\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "multi-agent trajectory prediction", "transformer", "social attention", "goal-conditioned prediction"]}}
{"id": "2511.10418", "pdf": "https://arxiv.org/pdf/2511.10418", "abs": "https://arxiv.org/abs/2511.10418", "authors": ["Yaqiao Zhu", "Hongkai Wen", "Mark Birkin", "Man Luo"], "title": "CityVerse: A Unified Data Platform for Multi-Task Urban Computing with Large Language Models", "categories": ["cs.DB"], "comment": null, "summary": "Large Language Models (LLMs) show remarkable potential for urban computing, from spatial reasoning to predictive analytics. However, evaluating LLMs across diverse urban tasks faces two critical challenges: lack of unified platforms for consistent multi-source data access and fragmented task definitions that hinder fair comparison. To address these challenges, we present CityVerse, the first unified platform integrating multi-source urban data, capability-based task taxonomy, and dynamic simulation for systematic LLM evaluation in urban contexts. CityVerse provides: 1) coordinate-based Data APIs unifying ten categories of urban data-including spatial features, temporal dynamics, demographics, and multi-modal imagery-with over 38 million curated records; 2) Task APIs organizing 43 urban computing tasks into a four-level cognitive hierarchy: Perception, Spatial Understanding, Reasoning and Prediction, and Decision and Interaction, enabling standardized evaluation across capability levels; 3) an interactive visualization frontend supporting real-time data retrieval, multi-layer display, and simulation replay for intuitive exploration and validation. We validate the platform's effectiveness through evaluations on mainstream LLMs across representative tasks, demonstrating its capability to support reproducible and systematic assessment. CityVerse provides a reusable foundation for advancing LLMs and multi-task approaches in the urban computing domain.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86Large Language Models (LLMs) \u5728\u57ce\u5e02\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4e14\u6458\u8981\u4e2d\u63d0\u5230\u201cReasoning and Prediction\u201d\u5c5e\u4e8e\u4efb\u52a1API\u7684\u4e00\u90e8\u5206\uff0c\u867d\u7136\u6ca1\u6709\u660e\u786e\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662fPrediction\u53ef\u4ee5\u6db5\u76d6\u8f68\u8ff9\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "Prediction", "urban computing", "predictive analytics"]}}
{"id": "2511.09735", "pdf": "https://arxiv.org/pdf/2511.09735", "abs": "https://arxiv.org/abs/2511.09735", "authors": ["Ahmed Alia", "Mohcine Chraibi", "Armin Seyfried"], "title": "Social LSTM with Dynamic Occupancy Modeling for Realistic Pedestrian Trajectory Prediction", "categories": ["cs.CV", "cs.AI"], "comment": "19 pages, 9 figures, 4 tables", "summary": "In dynamic and crowded environments, realistic pedestrian trajectory prediction remains a challenging task due to the complex nature of human motion and the mutual influences among individuals. Deep learning models have recently achieved promising results by implicitly learning such patterns from 2D trajectory data. However, most approaches treat pedestrians as point entities, ignoring the physical space that each person occupies. To address these limitations, this paper proposes a novel deep learning model that enhances the Social LSTM with a new Dynamic Occupied Space loss function. This loss function guides Social LSTM in learning to avoid realistic collisions without increasing displacement error across different crowd densities, ranging from low to high, in both homogeneous and heterogeneous density settings. Such a function achieves this by combining the average displacement error with a new collision penalty that is sensitive to scene density and individual spatial occupancy. For efficient training and evaluation, five datasets were generated from real pedestrian trajectories recorded during the Festival of Lights in Lyon 2022. Four datasets represent homogeneous crowd conditions -- low, medium, high, and very high density -- while the fifth corresponds to a heterogeneous density distribution. The experimental findings indicate that the proposed model not only lowers collision rates but also enhances displacement prediction accuracy in each dataset. Specifically, the model achieves up to a 31% reduction in the collision rate and reduces the average displacement error and the final displacement error by 5% and 6%, respectively, on average across all datasets compared to the baseline. Moreover, the proposed model consistently outperforms several state-of-the-art deep learning models across most test sets.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86 Social LSTM \u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u635f\u5931\u51fd\u6570\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7684\u771f\u5b9e\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u5185\u5bb9\u662f\u8f68\u8ff9\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "pedestrian trajectory prediction", "Social LSTM", "deep learning", "collision avoidance"]}}
{"id": "2511.10110", "pdf": "https://arxiv.org/pdf/2511.10110", "abs": "https://arxiv.org/abs/2511.10110", "authors": ["Kamil Dreczkowski", "Pietro Vitiello", "Vitalis Vosylius", "Edward Johns"], "title": "Learning a Thousand Tasks in a Day", "categories": ["cs.RO"], "comment": "This is the author's version of the work. It is posted here by permission of the AAAS for personal use, not for redistribution. The definitive version was published in Science Robotics on 12 November 2025, DOI: https://www.science.org/doi/10.1126/scirobotics.adv7594. Link to project website: https://www.robot-learning.uk/learning-1000-tasks", "summary": "Humans are remarkably efficient at learning tasks from demonstrations, but today's imitation learning methods for robot manipulation often require hundreds or thousands of demonstrations per task. We investigate two fundamental priors for improving learning efficiency: decomposing manipulation trajectories into sequential alignment and interaction phases, and retrieval-based generalisation. Through 3,450 real-world rollouts, we systematically study this decomposition. We compare different design choices for the alignment and interaction phases, and examine generalisation and scaling trends relative to today's dominant paradigm of behavioural cloning with a single-phase monolithic policy. In the few-demonstrations-per-task regime (<10 demonstrations), decomposition achieves an order of magnitude improvement in data efficiency over single-phase learning, with retrieval consistently outperforming behavioural cloning for both alignment and interaction. Building on these insights, we develop Multi-Task Trajectory Transfer (MT3), an imitation learning method based on decomposition and retrieval. MT3 learns everyday manipulation tasks from as little as a single demonstration each, whilst also generalising to novel object instances. This efficiency enables us to teach a robot 1,000 distinct everyday tasks in under 24 hours of human demonstrator time. Through 2,200 additional real-world rollouts, we reveal MT3's capabilities and limitations across different task families. Videos of our experiments can be found on at https://www.robot-learning.uk/learning-1000-tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper describes a method for imitation learning that enables a robot to learn manipulation tasks from a few demonstrations. While it doesn't explicitly use large language models, it focuses on trajectory transfer and learning from demonstrations, which are related to trajectory prediction and could potentially be combined with LLMs in future work. The paper focuses on learning manipulation trajectories, aligning with the concept of trajectory prediction, although not in the typical sense of predicting future states given past states.", "keywords": ["imitation learning", "trajectory transfer", "manipulation trajectories", "robot learning", "demonstration"]}}
{"id": "2511.10403", "pdf": "https://arxiv.org/pdf/2511.10403", "abs": "https://arxiv.org/abs/2511.10403", "authors": ["Mingxing Peng", "Ruoyu Yao", "Xusen Guo", "Jun Ma"], "title": "nuPlan-R: A Closed-Loop Planning Benchmark for Autonomous Driving via Reactive Multi-Agent Simulation", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 3 figures", "summary": "Recent advances in closed-loop planning benchmarks have significantly improved the evaluation of autonomous vehicles. However, existing benchmarks still rely on rule-based reactive agents such as the Intelligent Driver Model (IDM), which lack behavioral diversity and fail to capture realistic human interactions, leading to oversimplified traffic dynamics. To address these limitations, we present nuPlan-R, a new reactive closed-loop planning benchmark that integrates learning-based reactive multi-agent simulation into the nuPlan framework. Our benchmark replaces the rule-based IDM agents with noise-decoupled diffusion-based reactive agents and introduces an interaction-aware agent selection mechanism to ensure both realism and computational efficiency. Furthermore, we extend the benchmark with two additional metrics to enable a more comprehensive assessment of planning performance. Extensive experiments demonstrate that our reactive agent model produces more realistic, diverse, and human-like traffic behaviors, leading to a benchmark environment that better reflects real-world interactive driving. We further reimplement a collection of rule-based, learning-based, and hybrid planning approaches within our nuPlan-R benchmark, providing a clearer reflection of planner performance in complex interactive scenarios and better highlighting the advantages of learning-based planners in handling complex and dynamic scenarios. These results establish nuPlan-R as a new standard for fair, reactive, and realistic closed-loop planning evaluation. We will open-source the code for the new benchmark.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on closed-loop planning for autonomous driving and uses learning-based reactive multi-agent simulation. While it doesn't directly use large language models, it does involve learning-based agents and addresses the trajectory prediction of multiple agents in a driving scenario, making it somewhat relevant to trajectory prediction. The mention of diffusion models also hints at a connection to generative models, which are sometimes related to large models.", "keywords": ["autonomous driving", "closed-loop planning", "reactive multi-agent simulation", "trajectory prediction", "diffusion models"]}}
{"id": "2511.10598", "pdf": "https://arxiv.org/pdf/2511.10598", "abs": "https://arxiv.org/abs/2511.10598", "authors": ["Raghav Adhikari", "Sachet Khatiwada", "Suman Poudel"], "title": "Optimizing the flight path for a scouting Uncrewed Aerial Vehicle", "categories": ["cs.RO", "eess.SY"], "comment": "This paper was prepared as an end of semester project for ME8710: Engineering Optimization, Clemson University. Consists of 7 pages and 8 figures", "summary": "Post-disaster situations pose unique navigation challenges. One of those challenges is the unstructured nature of the environment, which makes it hard to layout paths for rescue vehicles. We propose the use of Uncrewed Aerial Vehicle (UAV) in such scenario to perform reconnaissance across the environment. To accomplish this, we propose an optimization-based approach to plan a path for the UAV at optimal height where the sensors of the UAV can cover the most area and collect data with minimum uncertainty.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u4f46\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u8def\u5f84\u89c4\u5212\u672c\u8eab\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u5e94\u7528\uff0c\u4f46\u8be5\u8bba\u6587\u66f4\u504f\u5411\u4e8e\u4f18\u5316\u7b97\u6cd5\u548c\u65e0\u4eba\u673a\u63a7\u5236\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u6838\u5fc3\u7814\u7a76\u65b9\u5411\uff08\u5982\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u3001\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u7b49\uff09\u7565\u6709\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u8bc4\u5206\u4e3a0.6\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8def\u5f84\u89c4\u5212", "\u65e0\u4eba\u673a", "UAV"]}}
{"id": "2511.09737", "pdf": "https://arxiv.org/pdf/2511.09737", "abs": "https://arxiv.org/abs/2511.09737", "authors": ["Bram Grooten", "Patrick MacAlpine", "Kaushik Subramanian", "Peter Stone", "Peter R. Wurman"], "title": "Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted as an oral at AAAI 2026. For code and videos, please see https://github.com/bramgrooten/sparc", "summary": "Generalization to unseen environments is a significant challenge in the field of robotics and control. In this work, we focus on contextual reinforcement learning, where agents act within environments with varying contexts, such as self-driving cars or quadrupedal robots that need to operate in different terrains or weather conditions than they were trained for. We tackle the critical task of generalizing to out-of-distribution (OOD) settings, without access to explicit context information at test time. Recent work has addressed this problem by training a context encoder and a history adaptation module in separate stages. While promising, this two-phase approach is cumbersome to implement and train. We simplify the methodology and introduce SPARC: single-phase adaptation for robust control. We test SPARC on varying contexts within the high-fidelity racing simulator Gran Turismo 7 and wind-perturbed MuJoCo environments, and find that it achieves reliable and robust OOD generalization.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on out-of-distribution generalization in robotics and control, specifically in the context of self-driving cars. While it doesn't explicitly mention trajectory prediction or large language models, the application to self-driving cars implies a connection to trajectory prediction, as path planning and vehicle control are inherently linked to predicting future states. The use of reinforcement learning also suggests a potential, albeit indirect, connection to learning patterns from data, which is a common theme in large language models, though not explicitly used here. The connection to trajectory prediction is stronger than the connection to large language models.", "keywords": ["self-driving cars", "reinforcement learning", "out-of-distribution generalization", "robust control"]}}
{"id": "2511.09780", "pdf": "https://arxiv.org/pdf/2511.09780", "abs": "https://arxiv.org/abs/2511.09780", "authors": ["Nikolay Blagoev", "O\u011fuzhan Ersoy", "Lydia Yiyu Chen"], "title": "Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO", "categories": ["cs.LG"], "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on attacks and defenses in decentralized Group Relative Policy Optimization (GRPO) used for post-training Large Language Models (LLMs). While it directly involves Large Language Models, it does not touch upon trajectory prediction. The relevance stems solely from its connection to LLMs.", "keywords": ["Large Language Models", "LLMs", "Group Relative Policy Optimization", "GRPO", "post-training", "adversarial attack"]}}
{"id": "2511.10586", "pdf": "https://arxiv.org/pdf/2511.10586", "abs": "https://arxiv.org/abs/2511.10586", "authors": ["Omid Mirzaeedodangeh", "Eliot Shekhtman", "Nikolai Matni", "Lars Lindemann"], "title": "Safe Planning in Interactive Environments via Iterative Policy Updates and Adversarially Robust Conformal Prediction", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "Safe planning of an autonomous agent in interactive environments -- such as the control of a self-driving vehicle among pedestrians and human-controlled vehicles -- poses a major challenge as the behavior of the environment is unknown and reactive to the behavior of the autonomous agent. This coupling gives rise to interaction-driven distribution shifts where the autonomous agent's control policy may change the environment's behavior, thereby invalidating safety guarantees in existing work. Indeed, recent works have used conformal prediction (CP) to generate distribution-free safety guarantees using observed data of the environment. However, CP's assumption on data exchangeability is violated in interactive settings due to a circular dependency where a control policy update changes the environment's behavior, and vice versa. To address this gap, we propose an iterative framework that robustly maintains safety guarantees across policy updates by quantifying the potential impact of a planned policy update on the environment's behavior. We realize this via adversarially robust CP where we perform a regular CP step in each episode using observed data under the current policy, but then transfer safety guarantees across policy updates by analytically adjusting the CP result to account for distribution shifts. This adjustment is performed based on a policy-to-trajectory sensitivity analysis, resulting in a safe, episodic open-loop planner. We further conduct a contraction analysis of the system providing conditions under which both the CP results and the policy updates are guaranteed to converge. We empirically demonstrate these safety and convergence guarantees on a two-dimensional car-pedestrian case study. To the best of our knowledge, these are the first results that provide valid safety guarantees in such interactive settings.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on safe planning in interactive environments, specifically addressing the challenges of autonomous agents (like self-driving vehicles) interacting with pedestrians and other vehicles. This involves predicting the behavior of the environment, making it relevant to trajectory prediction. While it doesn't explicitly mention or utilize large language models, the core problem of predicting interactive behavior is fundamental to trajectory prediction. The use of conformal prediction for safety guarantees further strengthens its connection to prediction tasks, albeit without involving LLMs. The keywords suggest a focus on trajectory prediction and planning in dynamic environments.", "keywords": ["trajectory prediction", "safe planning", "interactive environments", "autonomous vehicles", "conformal prediction", "policy updates"]}}
{"id": "2511.09820", "pdf": "https://arxiv.org/pdf/2511.09820", "abs": "https://arxiv.org/abs/2511.09820", "authors": ["Jeongho Min", "Dongyoung Kim", "Jaehyup Lee"], "title": "From Street to Orbit: Training-Free Cross-View Retrieval via Location Semantics and LLM Guidance", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to WACV 2026, 10pages, 4 figures", "summary": "Cross-view image retrieval, particularly street-to-satellite matching, is a critical task for applications such as autonomous navigation, urban planning, and localization in GPS-denied environments. However, existing approaches often require supervised training on curated datasets and rely on panoramic or UAV-based images, which limits real-world deployment. In this paper, we present a simple yet effective cross-view image retrieval framework that leverages a pretrained vision encoder and a large language model (LLM), requiring no additional training. Given a monocular street-view image, our method extracts geographic cues through web-based image search and LLM-based location inference, generates a satellite query via geocoding API, and retrieves matching tiles using a pretrained vision encoder (e.g., DINOv2) with PCA-based whitening feature refinement. Despite using no ground-truth supervision or finetuning, our proposed method outperforms prior learning-based approaches on the benchmark dataset under zero-shot settings. Moreover, our pipeline enables automatic construction of semantically aligned street-to-satellite datasets, which is offering a scalable and cost-efficient alternative to manual annotation. All source codes will be made publicly available at https://jeonghomin.github.io/street2orbit.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u8fdb\u884c\u8de8\u89c6\u89d2\u56fe\u50cf\u68c0\u7d22\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u5b9a\u4f4d\u548c\u5bfc\u822a\u76f8\u5173\u3002\u867d\u7136\u8bba\u6587\u672c\u8eab\u4e0d\u76f4\u63a5\u9884\u6d4b\u8f68\u8ff9\uff0c\u4f46\u5b83\u5229\u7528LLM\u8f85\u52a9\u7684\u5730\u7406\u4fe1\u606f\u5904\u7406\uff0c\u53ef\u4ee5\u4e3a\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u4fe1\u606f\u3002", "keywords": ["Large Language Models", "LLM", "autonomous navigation", "localization"]}}
{"id": "2511.09883", "pdf": "https://arxiv.org/pdf/2511.09883", "abs": "https://arxiv.org/abs/2511.09883", "authors": ["Liheng Zhang", "Jin Wang", "Hui Li", "Bingfeng Zhang", "Weifeng Liu"], "title": "HCC-3D: Hierarchical Compensatory Compression for 98% 3D Token Reduction in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "3D understanding has drawn significant attention recently, leveraging Vision-Language Models (VLMs) to enable multi-modal reasoning between point cloud and text data. Current 3D-VLMs directly embed the 3D point clouds into 3D tokens, following large 2D-VLMs with powerful reasoning capabilities. However, this framework has a great computational cost limiting its application, where we identify that the bottleneck lies in processing all 3D tokens in the Large Language Model (LLM) part. This raises the question: how can we reduce the computational overhead introduced by 3D tokens while preserving the integrity of their essential information? To address this question, we introduce Hierarchical Compensatory Compression (HCC-3D) to efficiently compress 3D tokens while maintaining critical detail retention. Specifically, we first propose a global structure compression (GSC), in which we design global queries to compress all 3D tokens into a few key tokens while keeping overall structural information. Then, to compensate for the information loss in GSC, we further propose an adaptive detail mining (ADM) module that selectively recompresses salient but under-attended features through complementary scoring. Extensive experiments demonstrate that HCC-3D not only achieves extreme compression ratios (approximately 98%) compared to previous 3D-VLMs, but also achieves new state-of-the-art performance, showing the great improvements on both efficiency and performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce83D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u538b\u7f29\u548c\u6548\u7387\u63d0\u5347\uff0c\u63d0\u5230\u4e86Large Language Model (LLM)\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u5c5e\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9886\u57df\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language Models", "Large Language Model", "3D understanding"]}}
