{"id": "2507.02029", "pdf": "https://arxiv.org/pdf/2507.02029", "abs": "https://arxiv.org/abs/2507.02029", "authors": ["BAAI RoboBrain Team", "Mingyu Cao", "Huajie Tan", "Yuheng Ji", "Minglan Lin", "Zhiyu Li", "Zhou Cao", "Pengwei Wang", "Enshen Zhou", "Yi Han", "Yingbo Tang", "Xiangqi Xu", "Wei Guo", "Yaoxu Lyu", "Yijie Xu", "Jiayu Shi", "Cheng Chi", "Mengdi Zhao", "Xiaoshuai Hao", "Shanyu Rong", "Zhengliang Cai", "Bolun Zhang", "Shuyi Zhang", "Huaihai Lyu", "Mengfei Du", "Lingfeng Zhang", "Xi Feng", "Xiaodan Liu", "Yance Jiao", "Chenrui He", "Mengsi Lyu", "Zhuo Chen", "Yulong Ao", "Xue Sun", "Zheqi He", "Jingshu Zheng", "Xi Yang", "Donghai Shi", "Kunchang Xie", "Bochao Zhang", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "RoboBrain 2.0 Technical Report", "categories": ["cs.RO"], "comment": null, "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86RoboBrain 2.0\uff0c\u4e00\u4e2a\u5177\u8eab\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u65e8\u5728\u7edf\u4e00\u7269\u7406\u73af\u5883\u4e2d\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u89c4\u5212\u3002\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\u201ctrajectory forecasting\u201d\uff0c\u8868\u660e\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u540c\u65f6\uff0c\u8be5\u8bba\u6587\u4e5f\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0832B model\uff09\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u76f8\u5173\u3002", "keywords": ["trajectory forecasting", "foundation models", "large language models", "embodied AI", "vision-language model"]}}
{"id": "2507.02406", "pdf": "https://arxiv.org/pdf/2507.02406", "abs": "https://arxiv.org/abs/2507.02406", "authors": ["Caio Azevedo", "Lina Achaji", "Stefano Sabatini", "Nicola Poerio", "Grzegorz Bartyzel", "Sascha Hornauer", "Fabien Moutarde"], "title": "Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization", "categories": ["cs.LG"], "comment": "Accepted for publication at ITSC 2025", "summary": "Trajectory prediction is an essential step in the pipeline of an autonomous\nvehicle. Inaccurate or inconsistent predictions regarding the movement of\nagents in its surroundings lead to poorly planned maneuvers and potentially\ndangerous situations for the end-user. Current state-of-the-art\ndeep-learning-based trajectory prediction models can achieve excellent accuracy\non public datasets. However, when used in more complex, interactive scenarios,\nthey often fail to capture important interdependencies between agents, leading\nto inconsistent predictions among agents in the traffic scene. Inspired by the\nefficacy of incorporating human preference into large language models, this\nwork fine-tunes trajectory prediction models in multi-agent settings using\npreference optimization. By taking as input automatically calculated preference\nrankings among predicted futures in the fine-tuning process, our\nexperiments--using state-of-the-art models on three separate datasets--show\nthat we are able to significantly improve scene consistency while minimally\nsacrificing trajectory prediction accuracy and without adding any excess\ncomputational requirements at inference time.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper explicitly addresses vehicle trajectory prediction and draws inspiration from human preference incorporation in large language models, using preference optimization to improve consistency. It directly connects trajectory prediction with concepts borrowed from the LLM field.", "keywords": ["trajectory prediction", "vehicle trajectory prediction", "preference optimization", "large language models", "multi-agent settings", "scene consistency"]}}
{"id": "2507.02190", "pdf": "https://arxiv.org/pdf/2507.02190", "abs": "https://arxiv.org/abs/2507.02190", "authors": ["Max Argus", "Jelena Bratulic", "Houman Masnavi", "Maxim Velikanov", "Nick Heppert", "Abhinav Valada", "Thomas Brox"], "title": "cVLA: Towards Efficient Camera-Space VLAs", "categories": ["cs.RO", "cs.LG"], "comment": "20 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models offer a compelling framework for tackling\ncomplex robotic manipulation tasks, but they are often expensive to train. In\nthis paper, we propose a novel VLA approach that leverages the competitive\nperformance of Vision Language Models (VLMs) on 2D images to directly infer\nrobot end-effector poses in image frame coordinates. Unlike prior VLA models\nthat output low-level controls, our model predicts trajectory waypoints, making\nit both more efficient to train and robot embodiment agnostic. Despite its\nlightweight design, our next-token prediction architecture effectively learns\nmeaningful and executable robot trajectories. We further explore the\nunderutilized potential of incorporating depth images, inference-time\ntechniques such as decoding strategies, and demonstration-conditioned action\ngeneration. Our model is trained on a simulated dataset and exhibits strong\nsim-to-real transfer capabilities. We evaluate our approach using a combination\nof simulated and real data, demonstrating its effectiveness on a real robotic\nsystem.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper explores Vision-Language-Action (VLA) models for robotic manipulation, predicting trajectory waypoints using Vision Language Models (VLMs). While it doesn't explicitly use Large Language Models (LLMs) in the traditional sense, it leverages VLMs for action prediction, which is related to trajectory prediction. The use of VLMs and the prediction of trajectory waypoints contribute to the relevance.", "keywords": ["Vision-Language-Action models", "VLA", "Vision Language Models", "VLMs", "trajectory waypoints", "action prediction"]}}
{"id": "2507.02074", "pdf": "https://arxiv.org/pdf/2507.02074", "abs": "https://arxiv.org/abs/2507.02074", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on crash detection in video using Large Language Models. While it doesn't directly address trajectory prediction, it leverages LLMs for video analysis, which can be related to understanding object movement and potential future states. The use of LLMs is a strong indicator of relevance, although the application is not directly trajectory prediction.", "keywords": ["Large Language Models", "LLMs", "vision-language models", "foundation models", "video understanding"]}}
{"id": "2507.01982", "pdf": "https://arxiv.org/pdf/2507.01982", "abs": "https://arxiv.org/abs/2507.01982", "authors": ["Siqing Long", "Xiangzhi Huang", "Jiemin Xie", "Ming Cai"], "title": "DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": "39 pages, 14 figures", "summary": "Accurate traffic demand forecasting enables transportation management\ndepartments to allocate resources more effectively, thereby improving their\nutilization efficiency. However, complex spatiotemporal relationships in\ntraffic systems continue to limit the performance of demand forecasting models.\nTo improve the accuracy of spatiotemporal traffic demand prediction, we propose\na new graph convolutional network structure called DKGCM. Specifically, we\nfirst consider the spatial flow distribution of different traffic nodes and\npropose a novel temporal similarity-based clustering graph convolution method,\nDK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering\nto group traffic nodes and more effectively capture spatial dependencies. On\nthe temporal scale, we integrate the Fast Fourier Transform (FFT) within the\nbidirectional Mamba deep learning framework to capture temporal dependencies in\ntraffic demand. To further optimize model training, we incorporate the GRPO\nreinforcement learning strategy to enhance the loss function feedback\nmechanism. Extensive experiments demonstrate that our model outperforms several\nadvanced methods and achieves strong results on three public datasets.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on traffic flow prediction, which is related to trajectory prediction. It uses a novel graph convolutional network and a bidirectional Mamba mechanism. While it doesn't explicitly use large language models, the Mamba architecture is a state-space model that has gained popularity in the LLM community. The paper's emphasis on spatiotemporal prediction and the use of Mamba contribute to its relevance, although the absence of LLMs lowers the score.", "keywords": ["traffic flow prediction", "spatio-temporal prediction", "Mamba", "graph convolutional network", "trajectory prediction"]}}
{"id": "2507.02250", "pdf": "https://arxiv.org/pdf/2507.02250", "abs": "https://arxiv.org/abs/2507.02250", "authors": ["Jiangxia Chen", "Tongyuan Huang", "Ke Song"], "title": "FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic occupancy prediction plays a pivotal role in autonomous driving.\nHowever, inherent limitations of fewframe images and redundancy in 3D space\ncompromise prediction accuracy for occluded and distant scenes. Existing\nmethods enhance performance by fusing historical frame data, which need\nadditional data and significant computational resources. To address these\nissues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement\noccupancy network with flow matching selective state space model for few-frame\n3D occupancy prediction. Firstly, to generate missing features, we designed a\nfeature refinement module based on a flow matching model, which is called Flow\nMatching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and\nPlane Selective SSM (PS3M), we selectively filter TPV features to reduce the\nimpact of air voxels on non-air voxels, thereby enhancing the overall\nefficiency of the model and prediction capability for distant scenes. Finally,\nwe design the Mask Training (MT) method to enhance the robustness of FMOcc and\naddress the issue of sensor data loss. Experimental results on the\nOcc3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing\nstate-of-theart methods. Our FMOcc with two frame input achieves notable scores\nof 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on\nOpenOcc with 5.4 G inference memory and 330ms inference time.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 3D occupancy prediction for autonomous driving, which is related to trajectory prediction. It uses flow matching and a selective state space model to improve prediction accuracy. While it doesn't directly use large language models, the state space model aspect has connections to sequence modeling and could potentially be relevant in the future. The core focus is on improving occupancy prediction, a key component for predicting future trajectories in autonomous driving.", "keywords": ["3D occupancy prediction", "autonomous driving", "flow matching", "state space model", "trajectory prediction"]}}
{"id": "2507.02085", "pdf": "https://arxiv.org/pdf/2507.02085", "abs": "https://arxiv.org/abs/2507.02085", "authors": ["Wanjia Zhao", "Jiaqi Han", "Siyi Gu", "Mingjian Jiang", "James Zou", "Stefano Ermon"], "title": "GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Geometric diffusion models have shown remarkable success in molecular\ndynamics and structure generation. However, efficiently fine-tuning them for\ndownstream tasks with varying geometric controls remains underexplored. In this\nwork, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables\nflexible and parameter-efficient fine-tuning for controlled generative tasks\nwithout modifying the original model architecture. GeoAda introduces a\nstructured adapter design: control signals are first encoded through coupling\noperators, then processed by a trainable copy of selected pretrained model\nlayers, and finally projected back via decoupling operators followed by an\nequivariant zero-initialized convolution. By fine-tuning only these lightweight\nadapter modules, GeoAda preserves the model's geometric consistency while\nmitigating overfitting and catastrophic forgetting. We theoretically prove that\nthe proposed adapters maintain SE(3)-equivariance, ensuring that the geometric\ninductive biases of the pretrained diffusion model remain intact during\nadaptation. We demonstrate the wide applicability of GeoAda across diverse\ngeometric control types, including frame control, global control, subgraph\ncontrol, and a broad range of application domains such as particle dynamics,\nmolecular dynamics, human motion prediction, and molecule generation. Empirical\nresults show that GeoAda achieves state-of-the-art fine-tuning performance\nwhile preserving original task accuracy, whereas other baselines experience\nsignificant performance degradation due to overfitting and catastrophic\nforgetting.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u5e76\u5e94\u7528\u4e8e\u5305\u62ec\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u5728\u5185\u7684\u591a\u4e2a\u9886\u57df\u3002\u867d\u7136\u63d0\u5230\u4e86\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\uff08\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\uff09\uff0c\u4f46\u8bba\u6587\u7684\u6838\u5fc3\u5728\u4e8e\u51e0\u4f55\u6269\u6563\u6a21\u578b\u548cSE(3)-equivariant\u9002\u914d\u5668\uff0c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u9002\u4e2d\u3002", "keywords": ["human motion prediction", "geometric diffusion models", "SE(3)-equivariant adapters"]}}
{"id": "2507.02700", "pdf": "https://arxiv.org/pdf/2507.02700", "abs": "https://arxiv.org/abs/2507.02700", "authors": ["M\u00e1t\u00e9 B. Vizi", "D\u00e9nes T\u00e1k\u00e1cs", "G\u00e1bor St\u00e9p\u00e1n", "G\u00e1bor Orosz"], "title": "Integrating path-planning and control for robotic unicycles", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This article focuses on integrating path-planning and control with\nspecializing on the unique needs of robotic unicycles. A unicycle design is\npresented which is capable of accelerating/breaking and carrying out a variety\nof maneuvers. The proposed path-planning method segments the path into straight\nand curved path sections dedicated for accelerating/breaking and turning\nmaneuvers, respectively. The curvature profiles of the curved sections are\noptimized while considering the control performance and the slipping limits of\nthe wheel. The performance of the proposed integrated approach is demonstrated\nvia numerical simulations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on path planning for robotic unicycles, which is related to trajectory prediction. However, it does not mention large language models or any related concepts. The path planning aspect contributes to the relevance, but the absence of any LLM connection lowers the score.", "keywords": ["path-planning", "trajectory prediction"]}}
{"id": "2507.02761", "pdf": "https://arxiv.org/pdf/2507.02761", "abs": "https://arxiv.org/abs/2507.02761", "authors": ["Long Xu", "Choilam Wong", "Mengke Zhang", "Junxiao Lin", "Fei Gao"], "title": "Trajectory Optimization for Differential Drive Mobile Manipulators via Topological Paths Search and Arc Length-Yaw Parameterization", "categories": ["cs.RO"], "comment": "Technical Report", "summary": "We present an efficient hierarchical motion planning pipeline for\ndifferential drive mobile manipulators. Our approach first searches for\nmultiple collisionfree and topologically distinct paths for the mobile base to\nextract the space in which optimal solutions may exist. Further sampling and\noptimization are then conducted in parallel to explore feasible whole-body\ntrajectories. For trajectory optimization, we employ polynomial trajectories\nand arc length-yaw parameterization, enabling efficient handling of the\nnonholonomic dynamics while ensuring optimality.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u6d89\u53ca\u8f68\u8ff9\u89c4\u5212\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u3002\u4f46\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e0d\u9ad8\u3002", "keywords": ["trajectory optimization", "motion planning", "trajectory"]}}
{"id": "2507.02747", "pdf": "https://arxiv.org/pdf/2507.02747", "abs": "https://arxiv.org/abs/2507.02747", "authors": ["Jiawei He", "Danshi Li", "Xinqiang Yu", "Zekun Qi", "Wenyao Zhang", "Jiayi Chen", "Zhaoxiang Zhang", "Zhizheng Zhang", "Li Yi", "He Wang"], "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "As large models gain traction, vision-language-action (VLA) systems are\nenabling robots to tackle increasingly complex tasks. However, limited by the\ndifficulty of data collection, progress has mainly focused on controlling\nsimple gripper end-effectors. There is little research on functional grasping\nwith large models for human-like dexterous hands. In this paper, we introduce\nDexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction\naligned with language instructions using single-view RGBD input. To accomplish\nthis, we generate a dataset of 170 million dexterous grasp poses mapped to\nsemantic parts across 174,000 objects in simulation, paired with detailed\npart-level captions. This large-scale dataset, named DexGraspNet 3.0, is used\nto train a VLM and flow-matching-based pose head capable of producing\ninstruction-aligned grasp poses for tabletop objects. To assess DexVLG's\nperformance, we create benchmarks in physics-based simulations and conduct\nreal-world experiments. Extensive testing demonstrates DexVLG's strong\nzero-shot generalization capabilities-achieving over 76% zero-shot execution\nsuccess rate and state-of-the-art part-grasp accuracy in simulation-and\nsuccessful part-aligned grasps on physical objects in real-world scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using large vision-language models for dexterous grasp pose prediction. While it doesn't directly address trajectory prediction, the grasp pose prediction could be considered a related task, and it explicitly uses large models. Therefore, it has a moderate relevance.", "keywords": ["large models", "vision-language model", "grasp pose prediction"]}}
{"id": "2507.02771", "pdf": "https://arxiv.org/pdf/2507.02771", "abs": "https://arxiv.org/abs/2507.02771", "authors": ["Melanie Segado", "Felipe Parodi", "Jordan K. Matelsky", "Michael L. Platt", "Eva B. Dyer", "Konrad P. Kording"], "title": "Grounding Intelligence in Movement", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "9 pages, 2 figures", "summary": "Recent advances in machine learning have dramatically improved our ability to\nmodel language, vision, and other high-dimensional data, yet they continue to\nstruggle with one of the most fundamental aspects of biological systems:\nmovement. Across neuroscience, medicine, robotics, and ethology, movement is\nessential for interpreting behavior, predicting intent, and enabling\ninteraction. Despite its core significance in our intelligence, movement is\noften treated as an afterthought rather than as a rich and structured modality\nin its own right. This reflects a deeper fragmentation in how movement data is\ncollected and modeled, often constrained by task-specific goals and\ndomain-specific assumptions. But movement is not domain-bound. It reflects\nshared physical constraints, conserved morphological structures, and purposeful\ndynamics that cut across species and settings. We argue that movement should be\ntreated as a primary modeling target for AI. It is inherently structured and\ngrounded in embodiment and physics. This structure, often allowing for compact,\nlower-dimensional representations (e.g., pose), makes it more interpretable and\ncomputationally tractable to model than raw, high-dimensional sensory inputs.\nDeveloping models that can learn from and generalize across diverse movement\ndata will not only advance core capabilities in generative modeling and\ncontrol, but also create a shared foundation for understanding behavior across\nbiological and artificial systems. Movement is not just an outcome, it is a\nwindow into how intelligent systems engage with the world.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses the importance of movement as a core aspect of intelligence and argues for treating it as a primary modeling target for AI. While it doesn't explicitly mention trajectory prediction or large language models, it touches upon areas relevant to movement modeling and generative modeling, which are related to trajectory prediction. The grounding of intelligence in movement also suggests a connection to understanding behavior and predicting intent, further strengthening the relevance. However, the absence of direct mentions of trajectory prediction or LLMs lowers the score.", "keywords": ["movement", "generative modeling", "behavior", "prediction", "AI", "grounded"]}}
{"id": "2507.02479", "pdf": "https://arxiv.org/pdf/2507.02479", "abs": "https://arxiv.org/abs/2507.02479", "authors": ["Teng Fu", "Yuwen Chen", "Zhuofan Chen", "Mengyang Zhao", "Bin Li", "Xiangyang Xue"], "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-object tracking is a classic field in computer vision. Among them,\npedestrian tracking has extremely high application value and has become the\nmost popular research category. Existing methods mainly use motion or\nappearance information for tracking, which is often difficult in complex\nscenarios. For the motion information, mutual occlusions between objects often\nprevent updating of the motion state; for the appearance information,\nnon-robust results are often obtained due to reasons such as only partial\nvisibility of the object or blurred images. Although learning how to perform\ntracking in these situations from the annotated data is the simplest solution,\nthe existing MOT dataset fails to satisfy this solution. Existing methods\nmainly have two drawbacks: relatively simple scene composition and\nnon-realistic scenarios. Although some of the video sequences in existing\ndataset do not have the above-mentioned drawbacks, the number is far from\nadequate for research purposes. To this end, we propose a difficult large-scale\ndataset for multi-pedestrian tracking, shot mainly from the first-person view\nand all from real-life complex scenarios. We name it ``CrowdTrack'' because\nthere are numerous objects in most of the sequences. Our dataset consists of 33\nvideos, containing a total of 5,185 trajectories. Each object is annotated with\na complete bounding box and a unique object ID. The dataset will provide a\nplatform to facilitate the development of algorithms that remain effective in\ncomplex situations. We analyzed the dataset comprehensively and tested multiple\nSOTA models on our dataset. Besides, we analyzed the performance of the\nfoundation models on our dataset. The dataset and project code is released at:\nhttps://github.com/loseevaya/CrowdTrack .", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on multi-pedestrian tracking, which is related to trajectory prediction. While the abstract mentions analyzing the performance of \"foundation models\" on the dataset, it doesn't explicitly connect them to the tracking methodology itself. Therefore, the relevance is moderate.", "keywords": ["pedestrian tracking", "multi-object tracking", "trajectory", "foundation models"]}}
