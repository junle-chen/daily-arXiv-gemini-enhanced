{"id": "2512.04441", "pdf": "https://arxiv.org/pdf/2512.04441", "abs": "https://arxiv.org/abs/2512.04441", "authors": ["Bin Suna", "Yaoguang Caob", "Yan Wanga", "Rui Wanga", "Jiachen Shanga", "Xiejie Fenga", "Jiayi Lu", "Jia Shi", "Shichun Yang", "Xiaoyu Yane", "Ziying Song"], "title": "MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of \"context simulation - candidate generation - multi-objective trade-off\". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned \"what-if\" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper is highly relevant as it combines trajectory prediction (trajectory generation, planning) with a vision-language model (VLM), which is a type of large language model. The paper explicitly mentions trajectory planning and uses a VLM for multi-objective evaluation of trajectories.", "keywords": ["trajectory prediction", "trajectory planning", "large language model", "vision-language model", "autonomous driving", "world models"]}}
{"id": "2512.04221", "pdf": "https://arxiv.org/pdf/2512.04221", "abs": "https://arxiv.org/abs/2512.04221", "authors": ["Xiangyu Bai", "He Liang", "Bishoy Galoaa", "Utsav Nandi", "Shayda Moezzi", "Yuhang He", "Sarah Ostadabbas"], "title": "MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u3002\u5b83\u4f7f\u7528\u591a\u667a\u80fd\u4f53LLM\u6765\u751f\u6210\u89c6\u9891\uff0c\u5e76\u4e14\u63d0\u5230\u4e86\u7269\u4f53\u8f68\u8ff9\uff0c\u4ee5\u53ca\u7269\u7406\u6a21\u62df\u5668\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u751f\u6210\uff0c\u4f46\u5176\u6838\u5fc3\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u548cLLM\u7684\u4f7f\u7528\u3002", "keywords": ["trajectory prediction", "Large Language Models", "LLMs", "motion", "multi-agent", "text-to-video", "physics", "motion-reasoning"]}}
{"id": "2512.04381", "pdf": "https://arxiv.org/pdf/2512.04381", "abs": "https://arxiv.org/abs/2512.04381", "authors": ["Chengyang He", "Ge Sun", "Yue Bai", "Junkai Lu", "Jiadong Zhao", "Guillaume Sartoretti"], "title": "FALCON: Actively Decoupled Visuomotor Policies for Loco-Manipulation with Foundation-Model-Based Coordination", "categories": ["cs.RO"], "comment": null, "summary": "We present FoundAtion-model-guided decoupled LoCO-maNipulation visuomotor policies (FALCON), a framework for loco-manipulation that combines modular diffusion policies with a vision-language foundation model as the coordinator. Our approach explicitly decouples locomotion and manipulation into two specialized visuomotor policies, allowing each subsystem to rely on its own observations. This mitigates the performance degradation that arise when a single policy is forced to fuse heterogeneous, potentially mismatched observations from locomotion and manipulation. Our key innovation lies in restoring coordination between these two independent policies through a vision-language foundation model, which encodes global observations and language instructions into a shared latent embedding conditioning both diffusion policies. On top of this backbone, we introduce a phase-progress head that uses textual descriptions of task stages to infer discrete phase and continuous progress estimates without manual phase labels. To further structure the latent space, we incorporate a coordination-aware contrastive loss that explicitly encodes cross-subsystem compatibility between arm and base actions. We evaluate FALCON on two challenging loco-manipulation tasks requiring navigation, precise end-effector placement, and tight base-arm coordination. Results show that it surpasses centralized and decentralized baselines while exhibiting improved robustness and generalization to out-of-distribution scenarios.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528Foundation Model (\u5927\u6a21\u578b) \u4f5c\u4e3a\u534f\u8c03\u5668\uff0c\u7528\u4e8eloco-manipulation\u4efb\u52a1\u3002 \u867d\u7136\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4f46\u6d89\u53ca\u5bfc\u822a\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\u3002\u8bba\u6587\u660e\u786e\u63d0\u53cavision-language foundation model\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002", "keywords": ["Foundation Model", "vision-language foundation model", "loco-manipulation", "navigation"]}}
{"id": "2512.04459", "pdf": "https://arxiv.org/pdf/2512.04459", "abs": "https://arxiv.org/abs/2512.04459", "authors": ["Yingzi Ma", "Yulong Cao", "Wenhao Ding", "Shuibai Zhang", "Yan Wang", "Boris Ivanovic", "Ming Jiang", "Marco Pavone", "Chaowei Xiao"], "title": "dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant as it focuses on using a diffusion-based vision-language model (a type of large model) for end-to-end driving, which inherently involves trajectory planning and prediction. The abstract mentions reasoning and planning, and specifically evaluates the system's performance on behavior-trajectory consistency. The use of nuScenes and WOD-E2E datasets further indicates relevance to autonomous driving and trajectory prediction.", "keywords": ["vision-language models", "VLMs", "diffusion", "autonomous driving", "end-to-end driving", "trajectory", "planning", "reasoning", "nuScenes", "WOD-E2E"]}}
{"id": "2512.04532", "pdf": "https://arxiv.org/pdf/2512.04532", "abs": "https://arxiv.org/abs/2512.04532", "authors": ["Yu-Wei Zhan", "Xin Wang", "Hong Chen", "Tongtong Feng", "Wei Feng", "Ren Wang", "Guangyao Li", "Qing Li", "Wenwu Zhu"], "title": "PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant because it combines Video Large Language Models (Video LLMs) with physical motion modeling. While not directly focused on trajectory prediction as its primary goal, it incorporates dynamic representations of motion using Neural ODEs, which is a technique that could be useful for trajectory prediction. The use of LLMs is also explicitly mentioned and is a central component of the framework.", "keywords": ["Video Large Language Models", "Video LLMs", "motion modeling", "Neural Ordinary Differential Equation", "Neural ODE", "physical dynamics"]}}
{"id": "2512.04513", "pdf": "https://arxiv.org/pdf/2512.04513", "abs": "https://arxiv.org/abs/2512.04513", "authors": ["Yu-Wei Zhan", "Xin Wang", "Pengzhe Mao", "Tongtong Feng", "Ren Wang", "Wenwu Zhu"], "title": "BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models", "categories": ["cs.AI"], "comment": null, "summary": "Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u6784\u5efa\u80fd\u591f\u7406\u89e3\u73af\u5883\u52a8\u6001\u5e76\u6267\u884c\u52a8\u4f5c\u7684\u5177\u8eab\u667a\u80fd\u4f53\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5728\u76ee\u6807\u5bfc\u5411\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u9762\u6709\u4e00\u5b9a\u5173\u8054\u3002\u540c\u65f6\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u548c\u4e16\u754c\u6a21\u578b\uff08WMs\uff09\uff0c\u5e76\u63a2\u7d22\u4e86\u4e24\u8005\u4e4b\u95f4\u7684\u53cc\u5411\u8026\u5408\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e3b\u9898\u9ad8\u5ea6\u76f8\u5173\u3002\u867d\u7136\u4fa7\u91cd\u70b9\u662f\u5177\u8eab\u667a\u80fd\uff0c\u4f46\u5176\u6d89\u53ca\u7684\u9884\u6d4b\u548c\u63a7\u5236\uff0c\u4ee5\u53ca\u5bf9\u73af\u5883\u52a8\u6001\u7684\u5efa\u6a21\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u4ea4\u53c9\u3002", "keywords": ["Multimodal Large Language Models (MLLMs)", "World Models (WMs)", "embodied agents", "dynamic prediction", "Task-Aware"]}}
{"id": "2512.04213", "pdf": "https://arxiv.org/pdf/2512.04213", "abs": "https://arxiv.org/abs/2512.04213", "authors": ["Bishoy Galoaa", "Xiangyu Bai", "Shayda Moezzi", "Utsav Nandi", "Sai Siddhartha Vivek Dhir Rangoju", "Somaieh Amraee", "Sarah Ostadabbas"], "title": "Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86Transformer\u67b6\u6784\uff0cTransformer\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7840\uff0c\u5e76\u4e14\u8bba\u6587\u7814\u7a76\u7684\u662f\u591a\u76f8\u673a\u70b9\u8ddf\u8e2a\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u660e\u786e\u5730\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u3002", "keywords": ["transformer", "point tracking", "multi-camera"]}}
{"id": "2512.04279", "pdf": "https://arxiv.org/pdf/2512.04279", "abs": "https://arxiv.org/abs/2512.04279", "authors": ["Feeza Khan Khanzada", "Jaerock Kwon"], "title": "Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies", "categories": ["cs.RO"], "comment": null, "summary": "We study how to exploit dense simulator-defined rewards in vision-based autonomous driving without inheriting their misalignment with deployment metrics. In realistic simulators such as CARLA, privileged state (e.g., lane geometry, infractions, time-to-collision) can be converted into dense rewards that stabilize and accelerate model-based reinforcement learning, but policies trained directly on these signals often overfit and fail to generalize when evaluated on sparse objectives such as route completion and collision-free overtaking. We propose reward-privileged world model distillation, a two-stage framework in which a teacher DreamerV3-style agent is first trained with a dense privileged reward, and only its latent dynamics are distilled into a student trained solely on sparse task rewards. Teacher and student share the same observation space (semantic bird's-eye-view images); privileged information enters only through the teacher's reward, and the student does not imitate the teacher's actions or value estimates. Instead, the student's world model is regularized to match the teacher's latent dynamics while its policy is learned from scratch on sparse success/failure signals. In CARLA lane-following and overtaking benchmarks, sparse-reward students outperform both dense-reward teachers and sparse-from-scratch baselines. On unseen lane-following routes, reward-privileged distillation improves success by about 23 percent relative to the dense teacher while maintaining comparable or better safety. On overtaking, students retain near-perfect performance on training routes and achieve up to a 27x improvement in success on unseen routes, with improved lane keeping. These results show that dense rewards can be leveraged to learn richer dynamics models while keeping the deployed policy optimized strictly for sparse, deployment-aligned objectives.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autonomous driving using reinforcement learning and world models. While it doesn't explicitly mention large language models, the concept of distilling knowledge from a teacher model to a student model is related to knowledge distillation techniques sometimes used with LLMs. The core focus is on driving, which inherently involves trajectory prediction and control.", "keywords": ["autonomous driving", "reinforcement learning", "world model", "trajectory prediction", "knowledge distillation"]}}
{"id": "2512.04282", "pdf": "https://arxiv.org/pdf/2512.04282", "abs": "https://arxiv.org/abs/2512.04282", "authors": ["Tasmiah Haque", "Srinjoy Das"], "title": "Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u8fd0\u52a8\u8f68\u8ff9\u7684\u9884\u6d4b\uff0c\u7279\u522b\u662f\u89c6\u9891\u8fd0\u52a8\u4f20\u9012\u4e2d\u7684\u672a\u6765\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86GRU-Normalizing Flow\u7b49\u5e8f\u5217\u6a21\u578b\uff0c\u5e76\u7740\u91cd\u5f3a\u8c03\u4e86\u9884\u6d4b\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5173\u8054\u8f83\u5f31\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8fd0\u52a8\u8f68\u8ff9", "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b", "GRU", "Normalizing Flows", "\u89c6\u9891\u8fd0\u52a8\u4f20\u9012", "\u52a8\u4f5c\u9884\u6d4b"]}}
{"id": "2512.04315", "pdf": "https://arxiv.org/pdf/2512.04315", "abs": "https://arxiv.org/abs/2512.04315", "authors": ["Yonghan Lee", "Tsung-Wei Huang", "Shiv Gehlot", "Jaehoon Choi", "Guan-Ming Su", "Dinesh Manocha"], "title": "SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on 4D Gaussian Splatting for dynamic scene reconstruction from unsynchronized videos. While it involves tracking motion and generating 3D trajectories, it doesn't directly use or discuss Large Language Models. The connection to trajectory prediction is present through the reconstruction of 3D trajectories, but the primary focus is on synchronization and reconstruction rather than prediction.", "keywords": ["trajectory", "motion", "4D Gaussian Splatting", "motion-spline"]}}
{"id": "2512.05094", "pdf": "https://arxiv.org/pdf/2512.05094", "abs": "https://arxiv.org/abs/2512.05094", "authors": ["James Ni", "Zekai Wang", "Wei Lin", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik", "Roei Herzig"], "title": "From Generated Human Videos to Physically Plausible Robot Trajectories", "categories": ["cs.RO", "cs.CV"], "comment": "For project website, see https://genmimic.github.io", "summary": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u5c06\u751f\u6210\u7684\u89c6\u9891\u4e2d\u7684\u4eba\u7c7b\u884c\u4e3a\u8f6c\u5316\u4e3a\u7269\u7406\u4e0a\u53ef\u884c\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u3002\u867d\u7136\u6d89\u53ca\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u67d0\u79cd\u5f62\u5f0f\u7684\u5927\u6a21\u578b\uff09\uff0c\u5e76\u4e14\u6700\u7ec8\u76ee\u6807\u662f\u751f\u6210\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u4f46\u5176\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u673a\u5668\u4eba\u63a7\u5236\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u800c\u975e\u76f4\u63a5\u7814\u7a76\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u6a21\u578b\u672c\u8eab\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["robot trajectories", "video generation models", "reinforcement learning", "humanoid", "robot control"]}}
{"id": "2512.05107", "pdf": "https://arxiv.org/pdf/2512.05107", "abs": "https://arxiv.org/abs/2512.05107", "authors": ["Feng Xu", "Guangyao Zhai", "Xin Kong", "Tingzhong Fu", "Daniel F. N. Gordon", "Xueli An", "Benjamin Busam"], "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models", "categories": ["cs.RO"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8Vision-Language-Action (VLA)\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5fae\u8c03\u3002\u867d\u7136\u52a8\u4f5c\u5e8f\u5217\u53ef\u4ee5\u88ab\u89c6\u4e3a\u8f68\u8ff9\uff0c\u4f46\u5176\u6838\u5fc3\u91cd\u70b9\u5728\u4e8e\u52a8\u4f5c\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\u3002 \u7136\u800c\uff0c\u5b83\u786e\u5b9e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e14\u52a8\u4f5c\u7684\u89c4\u5212\u53ef\u4ee5\u88ab\u89c6\u4e3a\u5e7f\u4e49\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "keywords": ["Large Language Models", "Vision-Language-Action Models", "Reinforcement Learning", "Action Trajectories", "Trajectory-wise Preference Optimization", "Proximal Policy Optimization"]}}
{"id": "2512.04476", "pdf": "https://arxiv.org/pdf/2512.04476", "abs": "https://arxiv.org/abs/2512.04476", "authors": ["Zehao Fan", "Zhenyu Liu", "Yunzhen Liu", "Yayue Hou", "Hadjer Benmeziane", "Kaoutar El Maghraoui", "Liu Liu"], "title": "Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on optimizing Mixture-of-Experts (MoE) models for large language model inference, particularly addressing memory constraints and improving throughput using CXL-attached near-data processing. While it doesn't directly address trajectory prediction, it deals with large language models, which is one of the two key areas of interest. Therefore, there is a moderate level of relevance.", "keywords": ["large language models", "Mixture-of-Experts", "MoE", "inference", "GPU"]}}
{"id": "2512.04797", "pdf": "https://arxiv.org/pdf/2512.04797", "abs": "https://arxiv.org/abs/2512.04797", "authors": ["SIMA team", "Adrian Bolton", "Alexander Lerchner", "Alexandra Cordell", "Alexandre Moufarek", "Andrew Bolt", "Andrew Lampinen", "Anna Mitenkova", "Arne Olav Hallingstad", "Bojan Vujatovic", "Bonnie Li", "Cong Lu", "Daan Wierstra", "Daniel P. Sawyer", "Daniel Slater", "David Reichert", "Davide Vercelli", "Demis Hassabis", "Drew A. Hudson", "Duncan Williams", "Ed Hirst", "Fabio Pardo", "Felix Hill", "Frederic Besse", "Hannah Openshaw", "Harris Chan", "Hubert Soyer", "Jane X. Wang", "Jeff Clune", "John Agapiou", "John Reid", "Joseph Marino", "Junkyung Kim", "Karol Gregor", "Kaustubh Sridhar", "Kay McKinney", "Laura Kampis", "Lei M. Zhang", "Loic Matthey", "Luyu Wang", "Maria Abi Raad", "Maria Loks-Thompson", "Martin Engelcke", "Matija Kecman", "Matthew Jackson", "Maxime Gazeau", "Ollie Purkiss", "Oscar Knagg", "Peter Stys", "Piermaria Mendolicchio", "Raia Hadsell", "Rosemary Ke", "Ryan Faulkner", "Sarah Chakera", "Satinder Singh Baveja", "Shane Legg", "Sheleem Kashem", "Tayfun Terzi", "Thomas Keck", "Tim Harley", "Tim Scholtes", "Tyson Roberts", "Volodymyr Mnih", "Yulan Liu", "Zhengdong Wang", "Zoubin Ghahramani"], "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5177\u8eab\u667a\u80fd\u4f53\u5728\u865a\u62df\u4e16\u754c\u4e2d\u7684\u884c\u4e3a\uff0c\u4f7f\u7528\u4e86Gemini\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u57fa\u7840\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u667a\u80fd\u4f53\u5728\u865a\u62df\u4e16\u754c\u4e2d\u7684\u884c\u52a8\u5fc5\u7136\u5305\u542b\u8def\u5f84\u89c4\u5212\u548c\u884c\u4e3a\u9884\u6d4b\u7b49\u5143\u7d20\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\u3002\u540c\u65f6\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["Large Language Models", "foundation models", "embodied agent", "virtual worlds", "Gemini"]}}
{"id": "2512.04425", "pdf": "https://arxiv.org/pdf/2512.04425", "abs": "https://arxiv.org/abs/2512.04425", "authors": ["Manar Alnaasan", "Md Selim Sarowar", "Sungho Kim"], "title": "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes a Large Language Model (LLM) to provide explainable gait analysis for Parkinson's disease. While the core task is gait recognition, which is related to movement patterns and can be considered tangentially related to trajectory prediction, the primary focus is on disease detection and explainability using LLMs. Therefore, it has moderate relevance.", "keywords": ["Large Language Models", "LLM", "gait analysis"]}}
{"id": "2512.04952", "pdf": "https://arxiv.org/pdf/2512.04952", "abs": "https://arxiv.org/abs/2512.04952", "authors": ["Yicheng Liu", "Shiduo Zhang", "Zibin Dong", "Baijun Ye", "Tianyuan Yuan", "Xiaopeng Yu", "Linqi Yin", "Chenhao Lu", "Junhao Shi", "Luca Jiang-Tao Yu", "Liangtao Zheng", "Tao Jiang", "Jingjing Gong", "Xipeng Qiu", "Hang Zhao"], "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on vision-language-action (VLA) models for robotic manipulation. While it doesn't explicitly mention trajectory prediction, the 'action modeling' aspect is closely related to predicting future actions, which can be interpreted as a form of trajectory prediction. The use of 'Autoregressive vision-language-action' models connects it to large models. However, the primary focus is on efficient action tokenization and robotic manipulation rather than general trajectory prediction or leveraging large language models directly for trajectory prediction.", "keywords": ["vision-language-action models", "autoregressive models", "action modeling", "robot learning"]}}
{"id": "2512.05106", "pdf": "https://arxiv.org/pdf/2512.05106", "abs": "https://arxiv.org/abs/2512.05106", "authors": ["Yu Zeng", "Charles Ochoa", "Mingyuan Zhou", "Vishal M. Patel", "Vitor Guizilini", "Rowan McAllister"], "title": "NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.RO"], "comment": null, "summary": "Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion \u03c6-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. \u03c6-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, \u03c6-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, \u03c6-PD improves CARLA-to-Waymo planner performance by 50\\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \\href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528Diffusion\u6a21\u578b\u8fdb\u884c\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\uff0c\u5e76\u7279\u522b\u5f3a\u8c03\u4fdd\u6301\u7a7a\u95f4\u7ed3\u6784\u3002\u867d\u7136\u5b83\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u5728\u9a7e\u9a76\u89c4\u5212\u7684sim-to-real\u589e\u5f3a\u65b9\u9762\u5e94\u7528\u4e86\u8be5\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728CARLA\u6a21\u62df\u5668\u4e0a\u63d0\u9ad8\u4e86planner\u7684\u6027\u80fd\u3002\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u5e94\u7528\u65b9\u9762\u3002\u4f46\u662f\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\u6216\u8005\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e0d\u662f\u975e\u5e38\u9ad8\u3002", "keywords": ["diffusion models", "sim-to-real", "driving planners", "CARLA simulator"]}}
{"id": "2512.04601", "pdf": "https://arxiv.org/pdf/2512.04601", "abs": "https://arxiv.org/abs/2512.04601", "authors": ["Joey Hong", "Kang Liu", "Zhan Ling", "Jiecao Chen", "Sergey Levine"], "title": "Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space", "categories": ["cs.LG", "cs.CL"], "comment": "22 pages, 4 figures", "summary": "Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on training LLM agents using actor-critic methods with natural language critics. While it doesn't directly address trajectory prediction, the concept of an agent interacting with an environment over long horizons and optimizing its actions has some overlap with trajectory planning and decision-making. The primary focus is on LLMs and their application in complex tasks, but the underlying reinforcement learning framework could potentially be applied to trajectory prediction problems in the future. Therefore, the relevance is moderate.", "keywords": ["Large Language Model", "LLM agents", "Actor-Critic", "Policy Gradient", "Natural Language"]}}
{"id": "2512.04487", "pdf": "https://arxiv.org/pdf/2512.04487", "abs": "https://arxiv.org/abs/2512.04487", "authors": ["Eunjong Lee", "Eunhee Kim", "Sanghoon Hong", "Eunho Jung", "Jihoon Kim"], "title": "Controllable Long-term Motion Generation with Extended Joint Targets", "categories": ["cs.CV"], "comment": "WACV 2026", "summary": "Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on controllable long-term motion generation, which is related to trajectory prediction. While it uses a Transformer-based architecture, it doesn't explicitly mention or leverage Large Language Models. The 'motion generation' aspect connects to trajectory prediction, especially considering it involves long-term sequences and control.", "keywords": ["motion generation", "long-term", "Transformer", "motion control"]}}
{"id": "2512.04499", "pdf": "https://arxiv.org/pdf/2512.04499", "abs": "https://arxiv.org/abs/2512.04499", "authors": ["Yuduo Jin", "Brandon Haworth"], "title": "Back to Basics: Motion Representation Matters for Human Motion Generation Using Diffusion Model", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Diffusion models have emerged as a widely utilized and successful methodology in human motion synthesis. Task-oriented diffusion models have significantly advanced action-to-motion, text-to-motion, and audio-to-motion applications. In this paper, we investigate fundamental questions regarding motion representations and loss functions in a controlled study, and we enumerate the impacts of various decisions in the workflow of the generative motion diffusion model. To answer these questions, we conduct empirical studies based on a proxy motion diffusion model (MDM). We apply v loss as the prediction objective on MDM (vMDM), where v is the weighted sum of motion data and noise. We aim to enhance the understanding of latent data distributions and provide a foundation for improving the state of conditional motion diffusion models. First, we evaluate the six common motion representations in the literature and compare their performance in terms of quality and diversity metrics. Second, we compare the training time under various configurations to shed light on how to speed up the training process of motion diffusion models. Finally, we also conduct evaluation analysis on a large motion dataset. The results of our experiments indicate clear performance differences across motion representations in diverse datasets. Our results also demonstrate the impacts of distinct configurations on model training and suggest the importance and effectiveness of these decisions on the outcomes of motion diffusion models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on human motion generation using diffusion models, which can be considered related to trajectory prediction. While it doesn't directly involve Large Language Models, the use of diffusion models for motion synthesis connects to the broader theme of generative models for sequential data, which is relevant to both trajectory prediction and the capabilities being explored with LLMs for generating various types of sequential outputs. The paper mentions action-to-motion, text-to-motion, and audio-to-motion, indicating a conditional generation aspect that is often a key component in trajectory prediction.", "keywords": ["motion generation", "diffusion model", "motion representation", "action-to-motion", "text-to-motion", "motion synthesis"]}}
{"id": "2512.04871", "pdf": "https://arxiv.org/pdf/2512.04871", "abs": "https://arxiv.org/abs/2512.04871", "authors": ["Junjie Fan", "Hongye Zhao", "Linduo Wei", "Jiayu Rao", "Guijia Li", "Jiaxin Yuan", "Wenqi Xu", "Yong Qi"], "title": "STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Large Language Models (LLMs) for time series forecasting. While not directly focused on trajectory prediction, time series forecasting shares similarities with trajectory prediction, especially in modeling temporal dependencies. The paper uses LLMs to model time series data, which aligns with the use of large models. Therefore, it has a moderate relevance.", "keywords": ["Large Language Models", "LLMs", "time series forecasting", "forecasting", "semantic abstraction", "foundation models"]}}
