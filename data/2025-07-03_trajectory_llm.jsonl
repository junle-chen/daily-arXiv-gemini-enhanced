{"id": "2507.00319", "pdf": "https://arxiv.org/pdf/2507.00319", "abs": "https://arxiv.org/abs/2507.00319", "authors": ["Tanmay Vilas Samak", "Chinmay Vilas Samak", "Bing Li", "Venkat Krovi"], "title": "When Digital Twins Meet Large Language Models: Realistic, Interactive, and Editable Simulation for Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Simulation frameworks have been key enablers for the development and\nvalidation of autonomous driving systems. However, existing methods struggle to\ncomprehensively address the autonomy-oriented requirements of balancing: (i)\ndynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant\nscenario orchestration, and (iv) real-time performance. To address these\nlimitations, we present a unified framework for creating and curating\nhigh-fidelity digital twins to accelerate advancements in autonomous driving\nresearch. Our framework leverages a mix of physics-based and data-driven\ntechniques for developing and simulating digital twins of autonomous vehicles\nand their operating environments. It is capable of reconstructing real-world\nscenes and assets (real2sim) with geometric and photorealistic accuracy and\ninfusing them with various physical properties to enable real-time dynamical\nsimulation of the ensuing driving scenarios. Additionally, it also incorporates\na large language model (LLM) interface to flexibly edit the driving scenarios\nonline via natural language prompts. We analyze the presented framework in\nterms of its fidelity, performance, and serviceability. Results indicate that\nour framework can reconstruct 3D scenes and assets with up to 97% structural\nsimilarity, while maintaining frame rates above 60 Hz. We also demonstrate that\nit can handle natural language prompts to generate diverse driving scenarios\nwith up to 95% repeatability and 85% generalizability.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper discusses a simulation framework for autonomous driving that incorporates both digital twins and a large language model (LLM) interface. While it doesn't directly focus on trajectory prediction algorithms, the context of autonomous driving inherently involves trajectory planning and prediction for simulated vehicles. The use of LLMs for scenario editing further strengthens the relevance.", "keywords": ["Large Language Models", "LLM", "autonomous driving", "digital twins", "simulation", "driving scenarios"]}}
{"id": "2507.01016", "pdf": "https://arxiv.org/pdf/2507.01016", "abs": "https://arxiv.org/abs/2507.01016", "authors": ["Yating Wang", "Haoyi Zhu", "Mingyu Liu", "Jiange Yang", "Hao-Shu Fang", "Tong He"], "title": "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "In this paper, we introduce an innovative vector quantization based action\ntokenizer built upon the largest-scale action trajectory dataset to date,\nleveraging over 100 times more data than previous approaches. This extensive\ndataset enables our tokenizer to capture rich spatiotemporal dynamics,\nresulting in a model that not only accelerates inference but also generates\nsmoother and more coherent action outputs. Once trained, the tokenizer can be\nseamlessly adapted to a wide range of downstream tasks in a zero-shot manner,\nfrom short-horizon reactive behaviors to long-horizon planning. A key finding\nof our work is that the domain gap between synthetic and real action\ntrajectories is marginal, allowing us to effectively utilize a vast amount of\nsynthetic data during training without compromising real-world performance. To\nvalidate our approach, we conducted extensive experiments in both simulated\nenvironments and on real robotic platforms. The results demonstrate that as the\nvolume of synthetic trajectory data increases, the performance of our tokenizer\non downstream tasks improves significantly-most notably, achieving up to a 30%\nhigher success rate on two real-world tasks in long-horizon scenarios. These\nfindings highlight the potential of our action tokenizer as a robust and\nscalable solution for real-time embodied intelligence systems, paving the way\nfor more efficient and reliable robotic control in diverse application\ndomains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant because it focuses on improving Vision-Language-Action models using a vector-quantized action tokenizer trained on a large-scale action trajectory dataset. While it doesn't explicitly mention Large Language Models, it addresses action trajectory generation and utilizes a large dataset, making it closely related to trajectory prediction and implicitly connected to the scaling aspects often associated with large models. The use of 'large-scale' and the focus on 'action trajectories' are key indicators of relevance.", "keywords": ["action trajectory", "trajectory prediction", "large-scale", "action tokenizer", "spatiotemporal dynamics", "long-horizon planning"]}}
{"id": "2507.00603", "pdf": "https://arxiv.org/pdf/2507.00603", "abs": "https://arxiv.org/abs/2507.00603", "authors": ["Yupeng Zheng", "Pengxuan Yang", "Zebin Xing", "Qichao Zhang", "Yuhang Zheng", "Yinfeng Gao", "Pengfei Li", "Teng Zhang", "Zhongpu Xia", "Peng Jia", "Dongbin Zhao"], "title": "World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model", "categories": ["cs.CV"], "comment": "ICCV 2025, first version", "summary": "End-to-end autonomous driving directly generates planning trajectories from\nraw sensor data, yet it typically relies on costly perception supervision to\nextract scene information. A critical research challenge arises: constructing\nan informative driving world model to enable perception annotation-free,\nend-to-end planning via self-supervised learning. In this paper, we present\nWorld4Drive, an end-to-end autonomous driving framework that employs vision\nfoundation models to build latent world models for generating and evaluating\nmulti-modal planning trajectories. Specifically, World4Drive first extracts\nscene features, including driving intention and world latent representations\nenriched with spatial-semantic priors provided by vision foundation models. It\nthen generates multi-modal planning trajectories based on current scene\nfeatures and driving intentions and predicts multiple intention-driven future\nstates within the latent space. Finally, it introduces a world model selector\nmodule to evaluate and select the best trajectory. We achieve perception\nannotation-free, end-to-end planning through self-supervised alignment between\nactual future observations and predicted observations reconstructed from the\nlatent space. World4Drive achieves state-of-the-art performance without manual\nperception annotations on both the open-loop nuScenes and closed-loop NavSim\nbenchmarks, demonstrating an 18.1\\% relative reduction in L2 error, 46.7% lower\ncollision rate, and 3.75 faster training convergence. Codes will be accessed at\nhttps://github.com/ucaszyp/World4Drive.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u8868\u660e\u5176\u4e0e\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\uff0c\u800c\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u6b64\u5916\uff0c\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u4f7f\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08vision foundation models\uff09\u6765\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528LLM\uff0c\u4f46\u4f7f\u7528\u4e86\u89c6\u89c9\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\uff0c\u4e0e\u5927\u6a21\u578b\u7684\u6982\u5ff5\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "autonomous driving", "vision foundation models", "world model", "planning trajectories"]}}
{"id": "2507.00990", "pdf": "https://arxiv.org/pdf/2507.00990", "abs": "https://arxiv.org/abs/2507.00990", "authors": ["Shivansh Patel", "Shraddhaa Mohan", "Hanlin Mai", "Unnat Jain", "Svetlana Lazebnik", "Yunzhu Li"], "title": "Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project Page: https://rigvid-robot.github.io/", "summary": "This work introduces Robots Imitating Generated Videos (RIGVid), a system\nthat enables robots to perform complex manipulation tasks--such as pouring,\nwiping, and mixing--purely by imitating AI-generated videos, without requiring\nany physical demonstrations or robot-specific training. Given a language\ncommand and an initial scene image, a video diffusion model generates potential\ndemonstration videos, and a vision-language model (VLM) automatically filters\nout results that do not follow the command. A 6D pose tracker then extracts\nobject trajectories from the video, and the trajectories are retargeted to the\nrobot in an embodiment-agnostic fashion. Through extensive real-world\nevaluations, we show that filtered generated videos are as effective as real\ndemonstrations, and that performance improves with generation quality. We also\nshow that relying on generated videos outperforms more compact alternatives\nsuch as keypoint prediction using VLMs, and that strong 6D pose tracking\noutperforms other ways to extract trajectories, such as dense feature point\ntracking. These findings suggest that videos produced by a state-of-the-art\noff-the-shelf model can offer an effective source of supervision for robotic\nmanipulation.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u89c6\u9891\uff0c\u5e76\u4ece\u4e2d\u63d0\u53d6\u7269\u4f53\u8f68\u8ff9\uff0c\u7136\u540e\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u867d\u7136\u4e3b\u8981\u4fa7\u91cd\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u4f46\u6d89\u53ca\u5230\u4e86\u8f68\u8ff9\u63d0\u53d6\u548c\u4f7f\u7528\u5927\u578b\u6a21\u578b\uff08\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory", "video diffusion model", "vision-language model", "VLM"]}}
{"id": "2507.00036", "pdf": "https://arxiv.org/pdf/2507.00036", "abs": "https://arxiv.org/abs/2507.00036", "authors": ["Rohan Putatunda", "Sanjay Purushotham", "Ratnaksha Lele", "Vandana P. Janeja"], "title": "IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting", "categories": ["cs.LG", "physics.ao-ph"], "comment": "16 pages, 4 figures", "summary": "Drifting icebergs in the polar oceans play a key role in the Earth's climate\nsystem, impacting freshwater fluxes into the ocean and regional ecosystems\nwhile also posing a challenge to polar navigation. However, accurately\nforecasting iceberg trajectories remains a formidable challenge, primarily due\nto the scarcity of spatiotemporal data and the complex, nonlinear nature of\niceberg motion, which is also impacted by environmental variables. The iceberg\nmotion is influenced by multiple dynamic environmental factors, creating a\nhighly variable system that makes trajectory identification complex. These\nlimitations hinder the ability of deep learning models to effectively capture\nthe underlying dynamics and provide reliable predictive outcomes. To address\nthese challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep\nlearning model that combines an analytical formulation of iceberg drift\nphysics, with an augmented residual learning model. The model learns the\npattern of mismatch between the analytical solution and ground-truth\nobservations, which is combined with a rotate-augmented spectral neural network\nthat captures both global and local patterns from the data to forecast future\niceberg drift positions. We compare IDRIFTNET model performance with\nstate-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings\ndemonstrate that IDRIFTNET outperforms other models by achieving a lower Final\nDisplacement Error (FDE) and Average Displacement Error (ADE) across a variety\nof time points. These results highlight IDRIFTNET's effectiveness in capturing\nthe complex, nonlinear drift of icebergs for forecasting iceberg trajectories\nunder limited data and dynamic environmental conditions.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u51b0\u5c71\u6f02\u79fb\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u4f7f\u7528\u4e86\u6df1\u5ea6\u5b66\u4e60\uff0c\u4f46\u91cd\u70b9\u5728\u4e8e\u7269\u7406\u9a71\u52a8\u7684\u5efa\u6a21\u548c\u65f6\u7a7a\u6570\u636e\u5206\u6790\uff0c\u4e0e\u901a\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5173\u7cfb\u4e0d\u5927\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4f46\u5e76\u975e\u5b8c\u5168\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "deep learning", "spatiotemporal data", "iceberg drift forecasting", "physics-driven deep learning"]}}
{"id": "2507.00236", "pdf": "https://arxiv.org/pdf/2507.00236", "abs": "https://arxiv.org/abs/2507.00236", "authors": ["Chinmay Vilas Samak", "Tanmay Vilas Samak", "Bing Li", "Venkat Krovi"], "title": "Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "Simulation-based design, optimization, and validation of autonomous driving\nalgorithms have proven to be crucial for their iterative improvement over the\nyears. Nevertheless, the ultimate measure of effectiveness is their successful\ntransition from simulation to reality (sim2real). However, existing sim2real\ntransfer methods struggle to comprehensively address the autonomy-oriented\nrequirements of balancing: (i) conditioned domain adaptation, (ii) robust\nperformance with limited examples, (iii) modularity in handling multiple domain\nrepresentations, and (iv) real-time performance. To alleviate these pain\npoints, we present a unified framework for learning cross-domain adaptive\nrepresentations for sim2real transferable autonomous driving algorithms using\nconditional latent diffusion models. Our framework offers options to leverage:\n(i) alternate foundation models, (ii) a few-shot fine-tuning pipeline, and\n(iii) textual as well as image prompts for mapping across given source and\ntarget domains. It is also capable of generating diverse high-quality samples\nwhen diffusing across parameter spaces such as times of day, weather\nconditions, seasons, and operational design domains. We systematically analyze\nthe presented framework and report our findings in the form of critical\nquantitative metrics and ablation studies, as well as insightful qualitative\nexamples and remarks. Additionally, we demonstrate the serviceability of the\nproposed approach in bridging the sim2real gap for end-to-end autonomous\ndriving using a behavioral cloning case study. Our experiments indicate that\nthe proposed framework is capable of bridging the perceptual sim2real gap by\nover 40%. We hope that our approach underscores the potential of generative\ndiffusion models in sim2real transfer, offering a pathway toward more robust\nand adaptive autonomous driving.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses sim2real transfer for autonomous driving using diffusion models, which can be considered a type of generative model. While it doesn't explicitly focus on trajectory prediction, autonomous driving inherently involves trajectory planning and prediction. The mention of 'foundation models' also links it to the broader category of large models. However, the primary focus is on domain adaptation and sim2real transfer rather than the core aspects of trajectory prediction or the architecture/training of large language models.", "keywords": ["autonomous driving", "sim2real", "diffusion models", "foundation models"]}}
{"id": "2507.00079", "pdf": "https://arxiv.org/pdf/2507.00079", "abs": "https://arxiv.org/abs/2507.00079", "authors": ["Ethan Smyth", "Alessandro Suglia"], "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems", "categories": ["cs.AI", "cs.LG"], "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/", "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses the use of Large Language Models (LLMs) for open-ended learning in a Minecraft environment. While it doesn't directly focus on trajectory prediction, the ability to interpret spatial environments and build structures could be considered a form of path planning or navigation, making it somewhat relevant. The core focus is on LLMs and their ability to interact with visual information, which aligns with the broader theme of large models.", "keywords": ["Large Language Models", "LLMs", "multi-modal", "spatial environments", "path planning (implied)", "navigation (implied)"]}}
{"id": "2507.00816", "pdf": "https://arxiv.org/pdf/2507.00816", "abs": "https://arxiv.org/abs/2507.00816", "authors": ["Mengyun Wang", "Bo Wang", "Yifeng Niu", "Chang Wang"], "title": "PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Accurate dynamics modeling is essential for quadrotors to achieve precise\ntrajectory tracking in various applications. Traditional physical\nknowledge-driven modeling methods face substantial limitations in unknown\nenvironments characterized by variable payloads, wind disturbances, and\nexternal perturbations. On the other hand, data-driven modeling methods suffer\nfrom poor generalization when handling out-of-distribution (OoD) data,\nrestricting their effectiveness in unknown scenarios. To address these\nchallenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),\nwhich combines knowledge-driven and data-driven modeling methods by embedding\nphysical constraints directly into the training process for robust quadrotor\ndynamics learning. Specifically, PI-WAN employs a Temporal Convolutional\nNetwork (TCN) architecture that efficiently captures temporal dependencies from\nhistorical flight data, while a physics-informed loss function applies physical\nprinciples to improve model generalization and robustness across previously\nunseen conditions. By incorporating real-time prediction results into a model\npredictive control (MPC) framework, we achieve improvements in closed-loop\ntracking performance. Comprehensive simulations and real-world flight\nexperiments demonstrate that our approach outperforms baseline methods in terms\nof prediction accuracy, tracking precision, and robustness to unknown\nenvironments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on quadrotor dynamics prediction using a physics-informed neural network. While it involves trajectory prediction (quadrotor dynamics prediction implies predicting future states/trajectory), it does not involve Large Language Models. The relevance comes from the 'trajectory prediction' aspect, specifically in the context of quadrotor control and dynamics modeling.", "keywords": ["trajectory prediction", "dynamics prediction", "quadrotor", "model predictive control"]}}
{"id": "2507.00028", "pdf": "https://arxiv.org/pdf/2507.00028", "abs": "https://arxiv.org/abs/2507.00028", "authors": ["Lihuan Li", "Hao Xue", "Shuang Ao", "Yang Song", "Flora Salim"], "title": "HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The representation of urban trajectory data plays a critical role in\neffectively analyzing spatial movement patterns. Despite considerable progress,\nthe challenge of designing trajectory representations that can capture diverse\nand complementary information remains an open research problem. Existing\nmethods struggle in incorporating trajectory fine-grained details and\nhigh-level summary in a single model, limiting their ability to attend to both\nlong-term dependencies while preserving local nuances. To address this, we\npropose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint\nEmbedding Predictive Architecture), a unified framework for learning\nmulti-scale urban trajectory representations across semantic abstraction\nlevels. HiT-JEPA adopts a three-layer hierarchy that progressively captures\npoint-level fine-grained details, intermediate patterns, and high-level\ntrajectory abstractions, enabling the model to integrate both local dynamics\nand global semantics in one coherent structure. Extensive experiments on\nmultiple real-world datasets for trajectory similarity computation show that\nHiT-JEPA's hierarchical design yields richer, multi-scale representations. Code\nis available at: https://anonymous.4open.science/r/HiT-JEPA.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u6570\u636e\u7684\u8868\u5f81\u5b66\u4e60\uff0c\u7279\u522b\u662f\u57ce\u5e02\u8f68\u8ff9\u7684\u76f8\u4f3c\u6027\u8ba1\u7b97\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u5b66\u4e60\u66f4\u6709\u6548\u7684\u8f68\u8ff9\u8868\u5f81\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u4f7f\u7528\u4e86\u5206\u5c42\u7ed3\u6784\u6765\u6355\u6349\u4e0d\u540c\u5c3a\u5ea6\u7684\u8f68\u8ff9\u4fe1\u606f\uff0c\u8fd9\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u4e5f\u5f88\u5e38\u89c1\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory embedding", "trajectory representation", "trajectory similarity computation", "urban trajectory", "multi-scale"]}}
{"id": "2507.00029", "pdf": "https://arxiv.org/pdf/2507.00029", "abs": "https://arxiv.org/abs/2507.00029", "authors": ["Wenbing Li", "Zikai Song", "Hang Zhou", "Yunyao Zhang", "Junqing Yu", "Wei Yang"], "title": "LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts\n(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit\nprevailing limitations: they either swap entire attention/feed-forward layers\nfor switch experts or bolt on parallel expert branches, diluting parameter\nefficiency and task fidelity. We propose the LoRA-Mixer, a modular and\nlightweight MoE framework that integrates LoRA experts. Our core innovation\nlies in replacing the projection matrices of the attention module's\ninput/output linear layers with dynamically routed, task-specific LoRA experts.\nThis design ensures seamless compatibility with diverse foundation models,\nincluding transformers and state space models (SSMs), by leveraging their\ninherent linear projection structures. The framework supports two operational\nparadigms: (1) joint optimization of LoRA experts and routing mechanisms via a\nnovel hard-soft routing strategy, or (2) direct deployment of pre-trained,\nfrozen LoRA modules sourced from external repositories. To enable robust router\ntraining with limited data while ensuring stable routing decisions and\nmaximizing expert reuse, we introduce an adaptive Specialization Balance Loss\n(SBL) that jointly optimizes expert balance and task-specific alignment.\nExtensive experiments on seven benchmark datasets, including MedQA, CoLA,\nSST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of\nLoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer\nachieves significant improvements of 7.61%, 4.88%, and 3.08% over the base\nmodels, respectively. Compared with state-of-the-art methods, LoRA-Mixer\nachieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,\nusing only 48% of the parameters, demonstrating its efficiency and strong\nperformance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on improving the efficiency and performance of Large Language Models (LLMs) using LoRA and Mixture-of-Experts (MoE). While it doesn't directly address trajectory prediction, the techniques developed could potentially be applied to trajectory prediction tasks that utilize LLMs or similar architectures. The connection is indirect but present.", "keywords": ["Large Language Models", "LLMs", "LoRA", "Mixture-of-Experts", "MoE", "foundation models"]}}
{"id": "2507.00031", "pdf": "https://arxiv.org/pdf/2507.00031", "abs": "https://arxiv.org/abs/2507.00031", "authors": ["Chuan Li", "Jiang You", "Hassine Moungla", "Vincent Gauthier", "Miguel Nunez-del-Prado", "Hugo Alatrista-Salas"], "title": "Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru", "categories": ["cs.LG"], "comment": null, "summary": "Accurate modeling of human mobility is critical for understanding epidemic\nspread and deploying timely interventions. In this work, we leverage a\nlarge-scale spatio-temporal dataset collected from Peru's national Digital\nContact Tracing (DCT) application during the COVID-19 pandemic to forecast\nmobility flows across urban regions. A key challenge lies in the spatial\nsparsity of hourly mobility counts across hexagonal grid cells, which limits\nthe predictive power of conventional time series models. To address this, we\npropose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)\ntechnique that augments each cell's features with aggregated signals from its\nimmediate H3 neighbors. We evaluate this strategy on three forecasting\nbackbones: NLinear, PatchTST, and K-U-Net, under various historical input\nlengths. Experimental results show that SPN consistently improves forecasting\nperformance, achieving up to 9.85 percent reduction in test MSE. Our findings\ndemonstrate that spatial smoothing of sparse mobility signals provides a simple\nyet effective path toward robust spatio-temporal forecasting during public\nhealth crises.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on spatio-temporal forecasting of human mobility, which is related to trajectory prediction. It uses time series models to predict mobility flows. While it doesn't directly involve large language models, the core problem of predicting movement patterns connects to the broader field of trajectory forecasting.", "keywords": ["spatio-temporal forecasting", "human mobility", "mobility flows", "time series models", "trajectory prediction"]}}
{"id": "2507.00078", "pdf": "https://arxiv.org/pdf/2507.00078", "abs": "https://arxiv.org/abs/2507.00078", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "title": "The language of time: a language model perspective on time-series foundation models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on time series foundation models and draws parallels to large language models. While it doesn't directly address trajectory prediction, the underlying time series analysis techniques could potentially be applicable. The strong connection to large language models increases the relevance.", "keywords": ["large language models", "foundation models", "time series", "representation learning"]}}
{"id": "2507.00085", "pdf": "https://arxiv.org/pdf/2507.00085", "abs": "https://arxiv.org/abs/2507.00085", "authors": ["Ruiyuan Jiang", "Dongyao Jia", "Eng Gee Lim", "Pengfei Fan", "Yuli Zhang", "Shangbo Wang"], "title": "A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate traffic prediction is essential for Intelligent Transportation\nSystems (ITS), yet current methods struggle with the inherent complexity and\nnon-linearity of traffic dynamics, making it difficult to integrate spatial and\ntemporal characteristics. Furthermore, existing approaches use static\ntechniques to address non-stationary and anomalous historical data, which\nlimits adaptability and undermines data smoothing. To overcome these\nchallenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative\nframework for network-level traffic speed prediction. GFEN introduces a novel\ntopological spatiotemporal graph fusion technique that meticulously extracts\nand merges spatial and temporal correlations from both data distribution and\nnetwork topology using trainable methods, enabling the modeling of multi-scale\nspatiotemporal features. Additionally, GFEN employs a hybrid methodology\ncombining a k-th order difference-based mathematical framework with an\nattention-based deep learning structure to adaptively smooth historical\nobservations and dynamically mitigate data anomalies and non-stationarity.\nExtensive experiments demonstrate that GFEN surpasses state-of-the-art methods\nby approximately 6.3% in prediction accuracy and exhibits convergence rates\nnearly twice as fast as recent hybrid models, confirming its superior\nperformance and potential to significantly enhance traffic prediction system\nefficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on traffic speed prediction, which is related to trajectory prediction (specifically vehicle trajectory prediction). However, it does not mention or utilize Large Language Models. The method involves graph networks and spatiotemporal data fusion, which are relevant techniques but not directly related to LLMs. Therefore, the relevance score is moderate.", "keywords": ["traffic prediction", "spatiotemporal", "graph network", "trajectory prediction"]}}
{"id": "2507.00525", "pdf": "https://arxiv.org/pdf/2507.00525", "abs": "https://arxiv.org/abs/2507.00525", "authors": ["Djamahl Etchegaray", "Yuxia Fu", "Zi Huang", "Yadan Luo"], "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Interpretable communication is essential for safe and trustworthy autonomous\ndriving, yet current vision-language models (VLMs) often operate under\nidealized assumptions and struggle to capture user intent in real-world\nscenarios. Existing driving-oriented VQA datasets are limited to full-scene\ndescriptions or waypoint prediction, preventing the assessment of whether VLMs\ncan respond to localized user-driven queries. We introduce Box-QAymo, a\nbox-referring dataset and benchmark designed to both evaluate and finetune VLMs\non spatial and temporal reasoning over user-specified objects. Users express\nintent by drawing bounding boxes, offering a fast and intuitive interface for\nfocused queries in complex scenes. Specifically, we propose a hierarchical\nevaluation protocol that begins with binary sanity-check questions to assess\nbasic model capacities, and progresses to (1) attribute prediction for\nbox-referred objects, (2) motion understanding of target instances, and (3)\nspatiotemporal motion reasoning over inter-object dynamics across frames. To\nsupport this, we crowd-sourced fine-grained object classes and visual\nattributes that reflect the complexity drivers encounter, and extract object\ntrajectories to construct temporally grounded QA pairs. Rigorous quality\ncontrol through negative sampling, temporal consistency checks, and\ndifficulty-aware balancing guarantee dataset robustness and diversity. Our\ncomprehensive evaluation reveals significant limitations in current VLMs when\nqueried about perception questions, highlighting the gap in achieving\nreal-world performance. This work provides a foundation for developing more\nrobust and interpretable autonomous driving systems that can communicate\neffectively with users under real-world conditions. Project page and dataset\nare available at https://djamahl99.github.io/qaymo-pages/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on a VQA dataset for autonomous driving, which involves understanding object motion and spatiotemporal reasoning based on user queries. While it doesn't directly use large language models for trajectory prediction, it evaluates VLMs on tasks related to motion understanding and object trajectories, making it relevant to the broader theme of trajectory prediction in autonomous driving and the use of VLMs. It does not explicitly combine trajectory prediction with large language models, but the evaluation of VLMs on tasks that require understanding trajectories provides some relevance.", "keywords": ["autonomous driving", "vision-language models", "motion understanding", "object trajectories", "spatiotemporal reasoning"]}}
{"id": "2507.00593", "pdf": "https://arxiv.org/pdf/2507.00593", "abs": "https://arxiv.org/abs/2507.00593", "authors": ["Fernando Alonso-Fernandez", "Talha Hanif Butt", "Prayag Tiwari"], "title": "Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods", "categories": ["cs.CV"], "comment": "Under review at ESWA", "summary": "Safe overtaking manoeuvres in trucks are vital for preventing accidents and\nensuring efficient traffic flow. Accurate prediction of such manoeuvres is\nessential for Advanced Driver Assistance Systems (ADAS) to make timely and\ninformed decisions. In this study, we focus on overtake detection using\nController Area Network (CAN) bus data collected from five in-service trucks\nprovided by the Volvo Group. We evaluate three common classifiers for vehicle\nmanoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and\nSupport Vector Machines (SVM), and analyse how different preprocessing\nconfigurations affect performance. We find that variability in traffic\nconditions strongly influences the signal patterns, particularly in the\nno-overtake class, affecting classification performance if training data lacks\nadequate diversity. Since the data were collected under unconstrained,\nreal-world conditions, class diversity cannot be guaranteed a priori. However,\ntraining with data from multiple vehicles improves generalisation and reduces\ncondition-specific bias. Our pertruck analysis also reveals that classification\naccuracy, especially for overtakes, depends on the amount of training data per\nvehicle. To address this, we apply a score-level fusion strategy, which yields\nthe best per-truck performance across most cases. Overall, we achieve an\naccuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True\nPositive Rate). This research has been part of the BIG FUN project, which\nexplores how Artificial Intelligence can be applied to logged vehicle data to\nunderstand and predict driver behaviour, particularly in relation to Camera\nMonitor Systems (CMS), being introduced as digital replacements for traditional\nexterior mirrors.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u8d85\u8f66\u884c\u4e3a\u7684\u68c0\u6d4b\uff0c\u5c5e\u4e8e\u8f66\u8f86\u884c\u4e3a\u9884\u6d4b\u7684\u8303\u7574\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u4e2a\u5b50\u95ee\u9898\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08ANN, RF, SVM\uff09\u8fdb\u884c\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002BIG FUN\u9879\u76ee\u63a2\u7d22AI\u5728\u8f66\u8f86\u6570\u636e\u4e2d\u7684\u5e94\u7528\uff0c\u4e5f\u6697\u793a\u4e86\u53ef\u80fd\u5b58\u5728\u66f4\u590d\u6742\u7684\u6a21\u578b\u3002", "keywords": ["overtake detection", "vehicle manoeuvre detection", "Artificial Neural Networks", "Random Forest", "Support Vector Machines", "driver behaviour", "trajectory prediction"]}}
