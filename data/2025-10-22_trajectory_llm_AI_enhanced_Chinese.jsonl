{"id": "2510.16767", "pdf": "https://arxiv.org/pdf/2510.16767", "abs": "https://arxiv.org/abs/2510.16767", "authors": ["Jia Li", "Guoxiang Zhao"], "title": "T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic", "categories": ["cs.RO"], "comment": null, "summary": "Translating natural language instructions into executable motion plans is a\nfundamental challenge in robotics. Traditional approaches are typically\nconstrained by their reliance on domain-specific expertise to customize\nplanners, and often struggle with spatio-temporal couplings that usually lead\nto infeasible motions or discrepancies between task planning and motion\nexecution. Despite the proficiency of Large Language Models (LLMs) in\nhigh-level semantic reasoning, hallucination could result in infeasible motion\nplans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic\nmotion planning framework that self-corrects it output with formal methods. The\nframework decomposes spatio-temporal task constraints via three cascaded\nmodules, each of which stimulates an LLM to generate candidate trajectory\nsequences and examines their feasibility via a Signal Temporal Logic (STL)\nverifier until one that satisfies complex spatial, temporal, and logical\nconstraints is found.Experiments across different scenarios show that T3\nPlanner significantly outperforms the baselines. The required reasoning can be\ndistilled into a lightweight Qwen3-4B model that enables efficient deployment.\nAll supplementary materials are accessible at\nhttps://github.com/leeejia/T3_Planner.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u53ca\u4e86\u4f7f\u7528LLM\u8fdb\u884c\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u6d89\u53ca\u8f68\u8ff9\u751f\u6210\u548c\u65f6\u95f4\u903b\u8f91\u7ea6\u675f\uff0c\u5c06\u5927\u6a21\u578b\u548c\u8f68\u8ff9\u89c4\u5212\u7d27\u5bc6\u7ed3\u5408\u3002\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86LLM\u751f\u6210\u8f68\u8ff9\u5e8f\u5217\uff0c\u5e76\u4f7f\u7528STL\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\uff0c\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u90fd\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "Large Language Models (LLMs)", "robotic motion planning", "motion plans", "temporal logic", "Signal Temporal Logic (STL)"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.17191", "pdf": "https://arxiv.org/pdf/2510.17191", "abs": "https://arxiv.org/abs/2510.17191", "authors": ["Peiru Zheng", "Yun Zhao", "Zhan Gong", "Hong Zhu", "Shaohua Wu"], "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "6 pages, 2 figures, 2 tables", "summary": "End-to-end autonomous driving has emerged as a promising paradigm for\nachieving robust and intelligent driving policies. However, existing end-to-end\nmethods still face significant challenges, such as suboptimal decision-making\nin complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring\nFusion), a novel framework that enhances end-to-end planning by leveraging the\ncognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory\nfusion techniques. We utilize the conventional scorers and the novel\nVLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative\naggregation and a powerful VLM-based fusioner for qualitative, context-aware\ndecision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End\nDriving Challenge, our SimpleVSF framework demonstrates state-of-the-art\nperformance, achieving a superior balance between safety, comfort, and\nefficiency.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6765\u6539\u8fdb\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u8bba\u6587\u63d0\u51fa\u7684SimpleVSF\u6846\u67b6\u5229\u7528VLMs\u7684\u8ba4\u77e5\u80fd\u529b\u548c\u8f68\u8ff9\u878d\u5408\u6280\u672f\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u51b3\u7b56\u80fd\u529b\u3002\u5173\u952e\u8bcd\u5305\u62ec\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u8868\u660e\u8be5\u8bba\u6587\u4e0e\u8fd9\u4e24\u4e2a\u9886\u57df\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "vision-language models", "VLMs", "end-to-end autonomous driving", "autonomous driving"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.17301", "pdf": "https://arxiv.org/pdf/2510.17301", "abs": "https://arxiv.org/abs/2510.17301", "authors": ["Panos Kalnis. Shuo Shang", "Christian S. Jensen"], "title": "Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models", "categories": ["cs.DB", "cs.AI"], "comment": "5 pages", "summary": "Spatio-temporal data captures complex dynamics across both space and time,\nyet traditional visualizations are complex, require domain expertise and often\nfail to resonate with broader audiences. Here, we propose MapMuse, a\nstorytelling-based framework for interpreting spatio-temporal datasets,\ntransforming them into compelling, narrative-driven experiences. We utilize\nlarge language models and employ retrieval augmented generation (RAG) and\nagent-based techniques to generate comprehensive stories. Drawing on principles\ncommon in cinematic storytelling, we emphasize clarity, emotional connection,\nand audience-centric design. As a case study, we analyze a dataset of taxi\ntrajectories. Two perspectives are presented: a captivating story based on a\nheat map that visualizes millions of taxi trip endpoints to uncover urban\nmobility patterns; and a detailed narrative following a single long taxi\njourney, enriched with city landmarks and temporal shifts. By portraying\nlocations as characters and movement as plot, we argue that data storytelling\ndrives insight, engagement, and action from spatio-temporal information. The\ncase study illustrates how MapMuse can bridge the gap between data complexity\nand human understanding. The aim of this short paper is to provide a glimpse to\nthe potential of the cinematic storytelling technique as an effective\ncommunication tool for spatio-temporal data, as well as to describe open\nproblems and opportunities for future research.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper demonstrates a strong relevance to both trajectory prediction and large language models. It explicitly mentions using LLMs for cinematic storytelling based on spatio-temporal data, specifically taxi trajectories. The use of RAG and agent-based techniques further strengthens the connection to LLMs. The analysis of taxi trajectories directly relates to trajectory prediction.", "keywords": ["Large Language Models", "Spatio-temporal data", "taxi trajectories", "trajectory prediction", "RAG", "Retrieval Augmented Generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.17148", "pdf": "https://arxiv.org/pdf/2510.17148", "abs": "https://arxiv.org/abs/2510.17148", "authors": ["Yu Gao", "Yiru Wang", "Anqing Jiang", "Heng Yuwen", "Wang Shuo", "Sun Hao", "Wang Jijun"], "title": "DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Conventional end-to-end (E2E) driving models are effective at generating\nphysically plausible trajectories, but often fail to generalize to long-tail\nscenarios due to the lack of essential world knowledge to understand and reason\nabout surrounding environments. In contrast, Vision-Language-Action (VLA)\nmodels leverage world knowledge to handle challenging cases, but their limited\n3D reasoning capability can lead to physically infeasible actions. In this work\nwe introduce DiffVLA++, an enhanced autonomous driving framework that\nexplicitly bridges cognitive reasoning and E2E planning through metric-guided\nalignment. First, we build a VLA module directly generating semantically\ngrounded driving trajectories. Second, we design an E2E module with a dense\ntrajectory vocabulary that ensures physical feasibility. Third, and most\ncritically, we introduce a metric-guided trajectory scorer that guides and\naligns the outputs of the VLA and E2E modules, thereby integrating their\ncomplementary strengths. The experiment on the ICCV 2025 Autonomous Grand\nChallenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant as it combines elements of both trajectory prediction and large language models. It utilizes Vision-Language-Action (VLA) models, which are a type of large model, to generate driving trajectories, and then refines these trajectories using an end-to-end driving module. The core idea is to bridge the cognitive reasoning of VLAs with the physical feasibility of E2E driving, making it relevant to both trajectory prediction and large models.", "keywords": ["trajectory prediction", "large language models", "VLA models", "autonomous driving", "end-to-end driving", "driving trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.16701", "pdf": "https://arxiv.org/pdf/2510.16701", "abs": "https://arxiv.org/abs/2510.16701", "authors": ["Ni Zhang", "Zhiguang Cao", "Jianan Zhou", "Cong Zhang", "Yew-Soon Ong"], "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems", "categories": ["cs.AI"], "comment": null, "summary": "Complex vehicle routing problems (VRPs) remain a fundamental challenge,\ndemanding substantial expert effort for intent interpretation and algorithm\ndesign. While large language models (LLMs) offer a promising path toward\nautomation, current approaches still rely on external intervention, which\nrestrict autonomy and often lead to execution errors and low solution\nfeasibility. To address these challenges, we propose an Agentic Framework with\nLLMs (AFL) for solving complex vehicle routing problems, achieving full\nautomation from problem instance to solution. AFL directly extracts knowledge\nfrom raw inputs and enables self-contained code generation without handcrafted\nmodules or external solvers. To improve trustworthiness, AFL decomposes the\noverall pipeline into three manageable subtasks and employs four specialized\nagents whose coordinated interactions enforce cross-functional consistency and\nlogical soundness. Extensive experiments on 60 complex VRPs, ranging from\nstandard benchmarks to practical variants, validate the effectiveness and\ngenerality of our framework, showing comparable performance against\nmeticulously designed algorithms. Notably, it substantially outperforms\nexisting LLM-based baselines in both code reliability and solution feasibility,\nachieving rates close to 100% on the evaluated benchmarks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper directly combines Large Language Models (LLMs) with a vehicle routing problem, which can be seen as a form of trajectory optimization. While not strictly trajectory *prediction*, routing shares similarities and the use of LLMs is central. The paper focuses on using LLMs to solve a complex routing problem, thus linking both areas of interest, although trajectory prediction is not its primary focus.", "keywords": ["Large Language Models", "LLMs", "vehicle routing problems", "routing"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.17525", "pdf": "https://arxiv.org/pdf/2510.17525", "abs": "https://arxiv.org/abs/2510.17525", "authors": ["Simon Schaefer", "Helen Oleynikova", "Sandra Hirche", "Stefan Leutenegger"], "title": "HumanMPC - Safe and Efficient MAV Navigation among Humans", "categories": ["cs.RO"], "comment": null, "summary": "Safe and efficient robotic navigation among humans is essential for\nintegrating robots into everyday environments. Most existing approaches focus\non simplified 2D crowd navigation and fail to account for the full complexity\nof human body dynamics beyond root motion. We present HumanMPC, a Model\nPredictive Control (MPC) framework for 3D Micro Air Vehicle (MAV) navigation\namong humans that combines theoretical safety guarantees with data-driven\nmodels for realistic human motion forecasting. Our approach introduces a novel\ntwist to reachability-based safety formulation that constrains only the initial\ncontrol input for safety while modeling its effects over the entire planning\nhorizon, enabling safe yet efficient navigation. We validate HumanMPC in both\nsimulated experiments using real human trajectories and in the real-world,\ndemonstrating its effectiveness across tasks ranging from goal-directed\nnavigation to visual servoing for human tracking. While we apply our method to\nMAVs in this work, it is generic and can be adapted by other platforms. Our\nresults show that the method ensures safety without excessive conservatism and\noutperforms baseline approaches in both efficiency and reliability.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5b89\u5168\u548c\u9ad8\u6548\u7684\u65e0\u4eba\u673a\u5bfc\u822a\uff0c\u7279\u522b\u662f\u5728\u4eba\u7fa4\u4e2d\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201chuman motion forecasting\u201d\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u6a21\u578b\uff0c\u4f46\u662f\u4f7f\u7528\u4e86\u6570\u636e\u9a71\u52a8\u6a21\u578b\u8fdb\u884c\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\uff0c\u6697\u793a\u4e86\u53ef\u80fd\u4f7f\u7528\u4e86\u67d0\u79cd\u5f62\u5f0f\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4f46\u4e0e\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5173\u8054\u4e0d\u5927\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u504f\u4e0a\u3002", "keywords": ["trajectory prediction", "human motion forecasting", "model predictive control", "navigation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.15963", "pdf": "https://arxiv.org/pdf/2510.15963", "abs": "https://arxiv.org/abs/2510.15963", "authors": ["Jiani Huang", "Amish Sethi", "Matthew Kuo", "Mayank Keoliya", "Neelay Velingker", "JungHo Jung", "Ser-Nam Lim", "Ziyang Li", "Mayur Naik"], "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted as a Spotlight Paper at NeurIPS 2025", "summary": "Multi-modal large language models (MLLMs) are making rapid progress toward\ngeneral-purpose embodied agents. However, current training pipelines primarily\nrely on high-level vision-sound-text pairs and lack fine-grained, structured\nalignment between pixel-level visual content and textual semantics. To overcome\nthis challenge, we propose ESCA, a new framework for contextualizing embodied\nagents through structured spatial-temporal understanding. At its core is\nSGClip, a novel CLIP-based, open-domain, and promptable model for generating\nscene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic\nlearning pipeline, which harnesses model-driven self-supervision from\nvideo-caption pairs and structured reasoning, thereby eliminating the need for\nhuman-labeled scene graph annotations. We demonstrate that SGClip supports both\nprompt-based inference and task-specific fine-tuning, excelling in scene graph\ngeneration and action localization benchmarks. ESCA with SGClip consistently\nimproves both open-source and commercial MLLMs, achieving state-of-the-art\nperformance across two embodied environments. Notably, it significantly reduces\nagent perception errors and enables open-source models to surpass proprietary\nbaselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on improving embodied agents using scene graph generation and multi-modal large language models (MLLMs). While it doesn't directly address trajectory prediction, the context of embodied agents and action localization suggests a potential connection. The use of MLLMs is a strong indicator of relevance to the large language model aspect.", "keywords": ["Multi-modal large language models", "MLLMs", "embodied agents", "action localization", "scene graph generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.16308", "pdf": "https://arxiv.org/pdf/2510.16308", "abs": "https://arxiv.org/abs/2510.16308", "authors": ["Chi Zhang", "Xian Huang", "Wei Dong"], "title": "SPOT: Sensing-augmented Trajectory Planning via Obstacle Threat Modeling", "categories": ["cs.RO"], "comment": null, "summary": "UAVs equipped with a single depth camera encounter significant challenges in\ndynamic obstacle avoidance due to limited field of view and inevitable blind\nspots. While active vision strategies that steer onboard cameras have been\nproposed to expand sensing coverage, most existing methods separate motion\nplanning from sensing considerations, resulting in less effective and delayed\nobstacle response. To address this limitation, we introduce SPOT\n(Sensing-augmented Planning via Obstacle Threat modeling), a unified planning\nframework for observation-aware trajectory planning that explicitly\nincorporates sensing objectives into motion optimization. At the core of our\nmethod is a Gaussian Process-based obstacle belief map, which establishes a\nunified probabilistic representation of both recognized (previously observed)\nand potential obstacles. This belief is further processed through a\ncollision-aware inference mechanism that transforms spatial uncertainty and\ntrajectory proximity into a time-varying observation urgency map. By\nintegrating urgency values within the current field of view, we define\ndifferentiable objectives that enable real-time, observation-aware trajectory\nplanning with computation times under 10 ms. Simulation and real-world\nexperiments in dynamic, cluttered, and occluded environments show that our\nmethod detects potential dynamic obstacles 2.8 seconds earlier than baseline\napproaches, increasing dynamic obstacle visibility by over 500\\%, and enabling\nsafe navigation through cluttered, occluded environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u969c\u788d\u7269\u5a01\u80c1\u5efa\u6a21\u6765\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u89c4\u5212\uff0c\u4f46\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory planning", "obstacle avoidance", "UAV", "motion planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.16500", "pdf": "https://arxiv.org/pdf/2510.16500", "abs": "https://arxiv.org/abs/2510.16500", "authors": ["Chen Min", "Jilin Mei", "Heng Zhai", "Shuai Wang", "Tong Sun", "Fanjie Kong", "Haoyang Li", "Fangyuan Mao", "Fuyang Liu", "Shuo Wang", "Yiming Nie", "Qi Zhu", "Liang Xiao", "Dawei Zhao", "Yu Hu"], "title": "Advancing Off-Road Autonomous Driving: The Large-Scale ORAD-3D Dataset and Comprehensive Benchmarks", "categories": ["cs.RO"], "comment": "Off-road robotics", "summary": "A major bottleneck in off-road autonomous driving research lies in the\nscarcity of large-scale, high-quality datasets and benchmarks. To bridge this\ngap, we present ORAD-3D, which, to the best of our knowledge, is the largest\ndataset specifically curated for off-road autonomous driving. ORAD-3D covers a\nwide spectrum of terrains, including woodlands, farmlands, grasslands,\nriversides, gravel roads, cement roads, and rural areas, while capturing\ndiverse environmental variations across weather conditions (sunny, rainy,\nfoggy, and snowy) and illumination levels (bright daylight, daytime, twilight,\nand nighttime). Building upon this dataset, we establish a comprehensive suite\nof benchmark evaluations spanning five fundamental tasks: 2D free-space\ndetection, 3D occupancy prediction, rough GPS-guided path planning,\nvision-language model-driven autonomous driving, and world model for off-road\nenvironments. Together, the dataset and benchmarks provide a unified and robust\nresource for advancing perception and planning in challenging off-road\nscenarios. The dataset and code will be made publicly available at\nhttps://github.com/chaytonmin/ORAD-3D.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper introduces a dataset and benchmarks for off-road autonomous driving. While the primary focus is on dataset creation and benchmarks, the inclusion of 'rough GPS-guided path planning' and 'vision-language model-driven autonomous driving' suggests a connection to trajectory prediction and large language models, respectively. The connection is not direct, but the presence of these elements warrants a moderate relevance score.", "keywords": ["path planning", "vision-language model", "autonomous driving"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.16004", "pdf": "https://arxiv.org/pdf/2510.16004", "abs": "https://arxiv.org/abs/2510.16004", "authors": ["Andreas Radler", "Vincent Seyfried", "Stefan Pirker", "Johannes Brandstetter", "Thomas Lichtenegger"], "title": "PAINT: Parallel-in-time Neural Twins for Dynamical System Reconstruction", "categories": ["cs.AI", "physics.flu-dyn"], "comment": "22 pages, 16 figures", "summary": "Neural surrogates have shown great potential in simulating dynamical systems,\nwhile offering real-time capabilities. We envision Neural Twins as a\nprogression of neural surrogates, aiming to create digital replicas of real\nsystems. A neural twin consumes measurements at test time to update its state,\nthereby enabling context-specific decision-making. A critical property of\nneural twins is their ability to remain on-trajectory, i.e., to stay close to\nthe true system state over time. We introduce Parallel-in-time Neural Twins\n(PAINT), an architecture-agnostic family of methods for modeling dynamical\nsystems from measurements. PAINT trains a generative neural network to model\nthe distribution of states parallel over time. At test time, states are\npredicted from measurements in a sliding window fashion. Our theoretical\nanalysis shows that PAINT is on-trajectory, whereas autoregressive models\ngenerally are not. Empirically, we evaluate our method on a challenging\ntwo-dimensional turbulent fluid dynamics problem. The results demonstrate that\nPAINT stays on-trajectory and predicts system states from sparse measurements\nwith high fidelity. These findings underscore PAINT's potential for developing\nneural twins that stay on-trajectory, enabling more accurate state estimation\nand decision-making.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on modeling dynamical systems using neural networks, specifically introducing Parallel-in-time Neural Twins (PAINT). While it doesn't directly mention trajectory prediction in the context of agents or vehicles, the core idea of predicting system states over time is relevant to trajectory prediction. It does not mention or use large language models.", "keywords": ["dynamical systems", "neural surrogates", "state estimation", "on-trajectory", "neural networks", "prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.16518", "pdf": "https://arxiv.org/pdf/2510.16518", "abs": "https://arxiv.org/abs/2510.16518", "authors": ["Jes\u00fas Ortega-Peimbert", "Finn Lukas Busch", "Timon Homberger", "Quantao Yang", "Olov Andersson"], "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Advances in open-vocabulary semantic mapping and object navigation have\nenabled robots to perform an informed search of their environment for an\narbitrary object. However, such zero-shot object navigation is typically\ndesigned for simple queries with an object name like \"television\" or \"blue\nrug\". Here, we consider more complex free-text queries with spatial\nrelationships, such as \"find the remote on the table\" while still leveraging\nrobustness of a semantic map. We present DIV-Nav, a real-time navigation system\nthat efficiently addresses this problem through a series of relaxations: i)\nDecomposing natural language instructions with complex spatial constraints into\nsimpler object-level queries on a semantic map, ii) computing the Intersection\nof individual semantic belief maps to identify regions where all objects\nco-exist, and iii) Validating the discovered objects against the original,\ncomplex spatial constrains via a LVLM. We further investigate how to adapt the\nfrontier exploration objectives of online semantic mapping to such spatial\nsearch queries to more effectively guide the search process. We validate our\nsystem through extensive experiments on the MultiON benchmark and real-world\ndeployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More\ndetails and videos are available at https://anonsub42.github.io/reponame/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on object navigation using spatial relationships and leverages a Large Vision-Language Model (LVLM) for validating object arrangements. While the primary focus is not trajectory prediction in the traditional sense, the navigation aspect and the use of a large model contribute to its relevance. The connection to trajectory prediction is weaker, as the paper emphasizes finding objects based on spatial relationships rather than predicting future trajectories. However, the use of LLMs in navigation makes it relevant.", "keywords": ["navigation", "Large Language Models", "LVLM", "spatial relationships", "semantic mapping"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIV-Nav\u7684\u5b9e\u65f6\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u89e3\u590d\u6742\u7a7a\u95f4\u7ea6\u675f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u5229\u7528\u8bed\u4e49\u5730\u56fe\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u81ea\u7531\u6587\u672c\u67e5\u8be2\u7684\u6709\u6548\u5bf9\u8c61\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u5bf9\u8c61\u5bfc\u822a\u901a\u5e38\u53ea\u9002\u7528\u4e8e\u7b80\u5355\u7684\u5bf9\u8c61\u540d\u79f0\u67e5\u8be2\uff0c\u800c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5e26\u6709\u7a7a\u95f4\u5173\u7cfb\u7684\u590d\u6742\u81ea\u7531\u6587\u672c\u67e5\u8be2\u7684\u5bf9\u8c61\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u5b9e\u73b0\uff1a1) \u5c06\u5e26\u6709\u590d\u6742\u7a7a\u95f4\u7ea6\u675f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u8bed\u4e49\u5730\u56fe\u4e0a\u66f4\u7b80\u5355\u7684\u5bf9\u8c61\u7ea7\u522b\u67e5\u8be2\uff1b2) \u8ba1\u7b97\u5404\u4e2a\u8bed\u4e49\u7f6e\u4fe1\u56fe\u7684\u4ea4\u96c6\uff0c\u4ee5\u8bc6\u522b\u6240\u6709\u5bf9\u8c61\u5171\u5b58\u7684\u533a\u57df\uff1b3) \u901a\u8fc7\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9a8c\u8bc1\u53d1\u73b0\u7684\u5bf9\u8c61\u662f\u5426\u7b26\u5408\u539f\u59cb\u7684\u590d\u6742\u7a7a\u95f4\u7ea6\u675f\u3002", "result": "\u5728MultiON\u57fa\u51c6\u6d4b\u8bd5\u548cBoston Dynamics Spot\u673a\u5668\u4eba\u4e0a\u7684\u771f\u5b9e\u90e8\u7f72\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "DIV-Nav\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5e26\u6709\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u7684\u81ea\u7531\u6587\u672c\u67e5\u8be2\uff0c\u5e76\u6307\u5bfc\u673a\u5668\u4eba\u8fdb\u884c\u5bf9\u8c61\u5bfc\u822a\u3002", "summary_zh": "\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5730\u56fe\u548c\u7269\u4f53\u5bfc\u822a\u7684\u8fdb\u6b65\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u5bf9\u5176\u73af\u5883\u8fdb\u884c\u77e5\u60c5\u7684\u641c\u7d22\uff0c\u4ee5\u5bfb\u627e\u4efb\u610f\u7269\u4f53\u3002\u7136\u800c\uff0c\u8fd9\u79cd\u96f6\u6837\u672c\u7269\u4f53\u5bfc\u822a\u901a\u5e38\u662f\u4e3a\u7b80\u5355\u7684\u67e5\u8be2\u800c\u8bbe\u8ba1\u7684\uff0c\u4f8b\u5982\u201c\u7535\u89c6\u201d\u6216\u201c\u84dd\u8272\u5730\u6bef\u201d\u8fd9\u6837\u7684\u7269\u4f53\u540d\u79f0\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u8003\u8651\u5177\u6709\u7a7a\u95f4\u5173\u7cfb\u7684\u66f4\u590d\u6742\u7684\u81ea\u7531\u6587\u672c\u67e5\u8be2\uff0c\u4f8b\u5982\u201c\u5728\u684c\u5b50\u4e0a\u627e\u5230\u9065\u63a7\u5668\u201d\uff0c\u540c\u65f6\u4ecd\u7136\u5229\u7528\u8bed\u4e49\u5730\u56fe\u7684\u9c81\u68d2\u6027\u3002\u6211\u4eec\u63d0\u51faDIV-Nav\uff0c\u8fd9\u662f\u4e00\u4e2a\u5b9e\u65f6\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u677e\u5f1b\u6709\u6548\u5730\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff1ai) \u5c06\u5177\u6709\u590d\u6742\u7a7a\u95f4\u7ea6\u675f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5206\u89e3\u4e3a\u8bed\u4e49\u5730\u56fe\u4e0a\u66f4\u7b80\u5355\u7684\u5bf9\u8c61\u7ea7\u522b\u67e5\u8be2\uff0cii) \u8ba1\u7b97\u5404\u4e2a\u8bed\u4e49\u7f6e\u4fe1\u56fe\u7684\u4ea4\u96c6\uff0c\u4ee5\u8bc6\u522b\u6240\u6709\u5bf9\u8c61\u5171\u5b58\u7684\u533a\u57df\uff0ciii) \u901a\u8fc7LVLM\u9a8c\u8bc1\u53d1\u73b0\u7684\u5bf9\u8c61\u662f\u5426\u7b26\u5408\u539f\u59cb\u7684\u590d\u6742\u7a7a\u95f4\u7ea6\u675f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u7814\u7a76\u4e86\u5982\u4f55\u8c03\u6574\u5728\u7ebf\u8bed\u4e49\u5730\u56fe\u7684\u524d\u6cbf\u63a2\u7d22\u76ee\u6807\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u6307\u5bfc\u641c\u7d22\u8fc7\u7a0b\uff0c\u4ee5\u6ee1\u8db3\u8fd9\u79cd\u7a7a\u95f4\u641c\u7d22\u67e5\u8be2\u3002\u6211\u4eec\u901a\u8fc7\u5728MultiON\u57fa\u51c6\u6d4b\u8bd5\u548c\u5728Jetson Orin AGX\u4e0a\u4f7f\u7528Boston Dynamics Spot\u673a\u5668\u4eba\u7684\u771f\u5b9e\u90e8\u7f72\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u6765\u9a8c\u8bc1\u6211\u4eec\u7684\u7cfb\u7edf\u3002\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u548c\u89c6\u9891\u8bf7\u8bbf\u95eehttps://anonsub42.github.io/reponame/\u3002"}}
{"id": "2510.16905", "pdf": "https://arxiv.org/pdf/2510.16905", "abs": "https://arxiv.org/abs/2510.16905", "authors": ["Yukang Cao", "Rahul Moorthy", "O. Goktug Poyrazoglu", "Volkan Isler"], "title": "C-Free-Uniform: A Map-Conditioned Trajectory Sampler for Model Predictive Path Integral Control", "categories": ["cs.RO"], "comment": "Submitted to the 2026 IEEE International Conference on Robotics and\n  Automation (ICRA). 8 pages, 4 figures", "summary": "Trajectory sampling is a key component of sampling-based control mechanisms.\nTrajectory samplers rely on control input samplers, which generate control\ninputs u from a distribution p(u | x) where x is the current state. We\nintroduce the notion of Free Configuration Space Uniformity (C-Free-Uniform for\nshort) which has two key features: (i) it generates a control input\ndistribution so as to uniformly sample the free configuration space, and (ii)\nin contrast to previously introduced trajectory sampling mechanisms where the\ndistribution p(u | x) is independent of the environment, C-Free-Uniform is\nexplicitly conditioned on the current local map. Next, we integrate this\nsampler into a new Model Predictive Path Integral (MPPI) Controller, CFU-MPPI.\nExperiments show that CFU-MPPI outperforms existing methods in terms of success\nrate in challenging navigation tasks in cluttered polygonal environments while\nrequiring a much smaller sampling budget.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory sampling and control mechanisms, specifically for path planning in cluttered environments. While it doesn't directly involve Large Language Models, it's highly relevant to trajectory prediction and path planning, which are subfields of trajectory prediction. The core is about improving trajectory sampling for control, not using LLMs.", "keywords": ["trajectory sampling", "trajectory prediction", "path planning", "Model Predictive Path Integral Control", "navigation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.17111", "pdf": "https://arxiv.org/pdf/2510.17111", "abs": "https://arxiv.org/abs/2510.17111", "authors": ["Weifan Guan", "Qinghao Hu", "Aosheng Li", "Jian Cheng"], "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models extend vision-language models to embodied\ncontrol by mapping natural-language instructions and visual observations to\nrobot actions. Despite their capabilities, VLA systems face significant\nchallenges due to their massive computational and memory demands, which\nconflict with the constraints of edge platforms such as on-board mobile\nmanipulators that require real-time performance. Addressing this tension has\nbecome a central focus of recent research. In light of the growing efforts\ntoward more efficient and scalable VLA systems, this survey provides a\nsystematic review of approaches for improving VLA efficiency, with an emphasis\non reducing latency, memory footprint, and training and inference costs. We\ncategorize existing solutions into four dimensions: model architecture,\nperception feature, action generation, and training/inference strategies,\nsummarizing representative techniques within each category. Finally, we discuss\nfuture trends and open challenges, highlighting directions for advancing\nefficient embodied intelligence.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Vision-Language-Action models, which are related to Large Language Models and embodied control. While it doesn't directly address trajectory prediction, the action generation aspect and the context of embodied manipulation suggest a potential connection to predicting future actions in a physical environment. The use of Vision-Language models implies the use of large language models as a component.", "keywords": ["Vision-Language Models", "Large Language Models", "Action Generation", "Embodied Manipulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.17261", "pdf": "https://arxiv.org/pdf/2510.17261", "abs": "https://arxiv.org/abs/2510.17261", "authors": ["Fernando Salanova", "Jes\u00fas Roche", "Cristian Mahuela", "Eduardo Montijano"], "title": "High-Level Multi-Robot Trajectory Planning And Spurious Behavior Detection", "categories": ["cs.RO", "cs.LG"], "comment": "6 pages,3 figures, Iberian Robotics Conference 2025", "summary": "The reliable execution of high-level missions in multi-robot systems with\nheterogeneous agents, requires robust methods for detecting spurious behaviors.\nIn this paper, we address the challenge of identifying spurious executions of\nplans specified as a Linear Temporal Logic (LTL) formula, as incorrect task\nsequences, violations of spatial constraints, timing inconsis- tencies, or\ndeviations from intended mission semantics. To tackle this, we introduce a\nstructured data generation framework based on the Nets-within-Nets (NWN)\nparadigm, which coordinates robot actions with LTL-derived global mission\nspecifications. We further propose a Transformer-based anomaly detection\npipeline that classifies robot trajectories as normal or anomalous. Experi-\nmental evaluations show that our method achieves high accuracy (91.3%) in\nidentifying execution inefficiencies, and demonstrates robust detection\ncapabilities for core mission violations (88.3%) and constraint-based adaptive\nanomalies (66.8%). An ablation experiment of the embedding and architecture was\ncarried out, obtaining successful results where our novel proposition performs\nbetter than simpler representations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u591a\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u4f7f\u7528Transformer\u8fdb\u884c\u5f02\u5e38\u68c0\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46Transformer\u67b6\u6784\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\uff0c\u4e14\u8bba\u6587\u4e3b\u9898\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory planning", "multi-robot", "Transformer", "anomaly detection", "robot trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.16444", "pdf": "https://arxiv.org/pdf/2510.16444", "abs": "https://arxiv.org/abs/2510.16444", "authors": ["Kunyu Peng", "Di Wen", "Jia Fu", "Jiamin Wu", "Kailun Yang", "Junwei Zheng", "Ruiping Liu", "Yufan Chen", "Yuqian Fu", "Danda Pani Paudel", "Luc Van Gool", "Rainer Stiefelhagen"], "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba", "categories": ["cs.CV", "cs.MM", "cs.RO", "eess.IV"], "comment": "Extended version of ECCV 2024 paper arXiv:2407.01872. The dataset and\n  code are released at https://github.com/KPeng9510/refAVA2", "summary": "Referring Atomic Video Action Recognition (RAVAR) aims to recognize\nfine-grained, atomic-level actions of a specific person of interest conditioned\non natural language descriptions. Distinct from conventional action recognition\nand detection tasks, RAVAR emphasizes precise language-guided action\nunderstanding, which is particularly critical for interactive human action\nanalysis in complex multi-person scenarios. In this work, we extend our\npreviously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million\nframes and >75.1k annotated persons in total. We benchmark this dataset using\nbaselines from multiple related domains, including atomic action localization,\nvideo question answering, and text-video retrieval, as well as our earlier\nmodel, RefAtomNet. Although RefAtomNet surpasses other baselines by\nincorporating agent attention to highlight salient features, its ability to\nalign and retrieve cross-modal information remains limited, leading to\nsuboptimal performance in localizing the target person and predicting\nfine-grained actions. To overcome the aforementioned limitations, we introduce\nRefAtomNet++, a novel framework that advances cross-modal token aggregation\nthrough a multi-hierarchical semantic-aligned cross-attention mechanism\ncombined with multi-trajectory Mamba modeling at the partial-keyword,\nscene-attribute, and holistic-sentence levels. In particular, scanning\ntrajectories are constructed by dynamically selecting the nearest visual\nspatial tokens at each timestep for both partial-keyword and scene-attribute\nlevels. Moreover, we design a multi-hierarchical semantic-aligned\ncross-attention strategy, enabling more effective aggregation of spatial and\ntemporal tokens across different semantic hierarchies. Experiments show that\nRefAtomNet++ establishes new state-of-the-art results. The dataset and code are\nreleased at https://github.com/KPeng9510/refAVA2.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on action recognition using a multi-trajectory Mamba model and semantic retrieval. While it involves trajectories and action prediction, it doesn't directly leverage large language models in a significant way. The \"multi-trajectory Mamba\" aspect relates to trajectory modeling, but the overall focus is on video action recognition and cross-modal understanding rather than trajectory prediction as a primary goal. The connection to large language models is implicit through the natural language descriptions, but not a core component.", "keywords": ["trajectory", "action prediction", "Mamba", "cross-modal"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.16053", "pdf": "https://arxiv.org/pdf/2510.16053", "abs": "https://arxiv.org/abs/2510.16053", "authors": ["Chenyang Yu", "Xinpeng Xie", "Yan Huang", "Chenxi Qiu"], "title": "FUSE-Traffic: Fusion of Unstructured and Structured Data for Event-aware Traffic Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate traffic forecasting is a core technology for building Intelligent\nTransportation Systems (ITS), enabling better urban resource allocation and\nimproved travel experiences. With growing urbanization, traffic congestion has\nintensified, highlighting the need for reliable and responsive forecasting\nmodels. In recent years, deep learning, particularly Graph Neural Networks\n(GNNs), has emerged as the mainstream paradigm in traffic forecasting. GNNs can\neffectively capture complex spatial dependencies in road network topology and\ndynamic temporal evolution patterns in traffic flow data. Foundational models\nsuch as STGCN and GraphWaveNet, along with more recent developments including\nSTWave and D2STGNN, have achieved impressive performance on standard traffic\ndatasets. These approaches incorporate sophisticated graph convolutional\nstructures and temporal modeling mechanisms, demonstrating particular\neffectiveness in capturing and forecasting traffic patterns characterized by\nperiodic regularities. To address this challenge, researchers have explored\nvarious ways to incorporate event information. Early attempts primarily relied\non manually engineered event features. For instance, some approaches introduced\nmanually defined incident effect scores or constructed specific subgraphs for\ndifferent event-induced traffic conditions. While these methods somewhat\nenhance responsiveness to specific events, their core drawback lies in a heavy\nreliance on domain experts' prior knowledge, making generalization to diverse\nand complex unknown events difficult, and low-dimensional manual features often\nlead to the loss of rich semantic details.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7b49\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff08\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff09\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u63d0\u5230\u4e86\u5229\u7528\u4e8b\u4ef6\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\uff0c\u4e8b\u4ef6\u4fe1\u606f\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u6587\u672c\u4fe1\u606f\uff0c\u5982\u679c\u672a\u6765\u7ed3\u5408LLM\u8fdb\u884c\u4e8b\u4ef6\u7406\u89e3\u548c\u7279\u5f81\u63d0\u53d6\uff0c\u5219\u76f8\u5173\u6027\u4f1a\u66f4\u9ad8\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["traffic forecasting", "trajectory prediction", "Graph Neural Networks", "GNNs", "Intelligent Transportation Systems", "ITS"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
