{"id": "2506.14144", "pdf": "https://arxiv.org/pdf/2506.14144", "abs": "https://arxiv.org/abs/2506.14144", "authors": ["Juho Bai", "Inwook Shim"], "title": "SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate prediction of pedestrian trajectories is essential for applications in robotics and surveillance systems. While existing approaches primarily focus on social interactions between pedestrians, they often overlook the rich environmental context that significantly shapes human movement patterns. In this paper, we propose SceneAware, a novel framework that explicitly incorporates scene understanding to enhance trajectory prediction accuracy. Our method leverages a Vision Transformer~(ViT) scene encoder to process environmental context from static scene images, while Multi-modal Large Language Models~(MLLMs) generate binary walkability masks that distinguish between accessible and restricted areas during training. We combine a Transformer-based trajectory encoder with the ViT-based scene encoder, capturing both temporal dynamics and spatial constraints. The framework integrates collision penalty mechanisms that discourage predicted trajectories from violating physical boundaries, ensuring physically plausible predictions. SceneAware is implemented in both deterministic and stochastic variants. Comprehensive experiments on the ETH/UCY benchmark datasets show that our approach outperforms state-of-the-art methods, with more than 50\\% improvement over previous models. Our analysis based on different trajectory categories shows that the model performs consistently well across various types of pedestrian movement. This highlights the importance of using explicit scene information and shows that our scene-aware approach is both effective and reliable in generating accurate and physically plausible predictions. Code is available at: https://github.com/juho127/SceneAware.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u7684\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u8868\u660e\u5b83\u4e0e\u8f68\u8ff9\u9884\u6d4b\uff08\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u76f8\u5173\u3002\u5b83\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u53ef\u884c\u8d70\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u5c06\u4e24\u4e2a\u9886\u57df\u7ed3\u5408\u8d77\u6765\u3002", "keywords": ["trajectory prediction", "pedestrian trajectory prediction", "Large Language Models", "LLMs", "scene understanding", "walkability"]}}
{"id": "2506.14299", "pdf": "https://arxiv.org/pdf/2506.14299", "abs": "https://arxiv.org/abs/2506.14299", "authors": ["Fanzhi Zeng", "Siqi Wang", "Chuzhao Zhu", "Li Li"], "title": "ADRD: LLM-Driven Autonomous Driving Based on Rule-based Decision Systems", "categories": ["cs.AI"], "comment": null, "summary": "How to construct an interpretable autonomous driving decision-making system has become a focal point in academic research. In this study, we propose a novel approach that leverages large language models (LLMs) to generate executable, rule-based decision systems to address this challenge. Specifically, harnessing the strong reasoning and programming capabilities of LLMs, we introduce the ADRD(LLM-Driven Autonomous Driving Based on Rule-based Decision Systems) framework, which integrates three core modules: the Information Module, the Agents Module, and the Testing Module. The framework operates by first aggregating contextual driving scenario information through the Information Module, then utilizing the Agents Module to generate rule-based driving tactics. These tactics are iteratively refined through continuous interaction with the Testing Module. Extensive experimental evaluations demonstrate that ADRD exhibits superior performance in autonomous driving decision tasks. Compared to traditional reinforcement learning approaches and the most advanced LLM-based methods, ADRD shows significant advantages in terms of interpretability, response speed, and driving performance. These results highlight the framework's ability to achieve comprehensive and accurate understanding of complex driving scenarios, and underscore the promising future of transparent, rule-based decision systems that are easily modifiable and broadly applicable. To the best of our knowledge, this is the first work that integrates large language models with rule-based systems for autonomous driving decision-making, and our findings validate its potential for real-world deployment.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper explicitly uses Large Language Models (LLMs) to generate rule-based decision systems for autonomous driving. While it doesn't directly focus on trajectory *prediction*, it is highly relevant because autonomous driving inherently involves trajectory planning and decision-making, and the paper leverages LLMs for this purpose. The abstract mentions 'driving tactics' which are directly related to trajectory generation and modification.", "keywords": ["Large Language Models", "LLMs", "autonomous driving", "decision-making", "driving tactics", "rule-based decision systems"]}}
{"id": "2506.14009", "pdf": "https://arxiv.org/pdf/2506.14009", "abs": "https://arxiv.org/abs/2506.14009", "authors": ["Qianzhong Chen", "Naixiang Gao", "Suning Huang", "JunEn Low", "Timothy Chen", "Jiankai Sun", "Mac Schwager"], "title": "GRaD-Nav++: Vision-Language Model Enabled Visual Drone Navigation with Gaussian Radiance Fields and Differentiable Dynamics", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous drones capable of interpreting and executing high-level language instructions in unstructured environments remain a long-standing goal. Yet existing approaches are constrained by their dependence on hand-crafted skills, extensive parameter tuning, or computationally intensive models unsuitable for onboard use. We introduce GRaD-Nav++, a lightweight Vision-Language-Action (VLA) framework that runs fully onboard and follows natural-language commands in real time. Our policy is trained in a photorealistic 3D Gaussian Splatting (3DGS) simulator via Differentiable Reinforcement Learning (DiffRL), enabling efficient learning of low-level control from visual and linguistic inputs. At its core is a Mixture-of-Experts (MoE) action head, which adaptively routes computation to improve generalization while mitigating forgetting. In multi-task generalization experiments, GRaD-Nav++ achieves a success rate of 83% on trained tasks and 75% on unseen tasks in simulation. When deployed on real hardware, it attains 67% success on trained tasks and 50% on unseen ones. In multi-environment adaptation experiments, GRaD-Nav++ achieves an average success rate of 81% across diverse simulated environments and 67% across varied real-world settings. These results establish a new benchmark for fully onboard Vision-Language-Action (VLA) flight and demonstrate that compact, efficient models can enable reliable, language-guided navigation without relying on external infrastructure.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u65e0\u4eba\u673a\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u4f7f\u7528\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLA\uff09\u6765\u63a7\u5236\u65e0\u4eba\u673a\u5728\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u65e0\u4eba\u673a\u5bfc\u822a\u672c\u8d28\u4e0a\u5305\u542b\u8def\u5f84\u89c4\u5212\u548c\u8fd0\u52a8\u8f68\u8ff9\u7684\u751f\u6210\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8868\u660e\u4e0e\u5927\u6a21\u578b\u9886\u57df\u76f8\u5173\u3002", "keywords": ["Vision-Language Model", "navigation", "drones", "Reinforcement Learning", "action"]}}
{"id": "2506.14233", "pdf": "https://arxiv.org/pdf/2506.14233", "abs": "https://arxiv.org/abs/2506.14233", "authors": ["Amirreza Payandeh", "Anuj Pokhrel", "Daeun Song", "Marcos Zampieri", "Xuesu Xiao"], "title": "Narrate2Nav: Real-Time Visual Navigation with Implicit Language Reasoning in Human-Centric Environments", "categories": ["cs.RO"], "comment": null, "summary": "Large Vision-Language Models (VLMs) have demonstrated potential in enhancing mobile robot navigation in human-centric environments by understanding contextual cues, human intentions, and social dynamics while exhibiting reasoning capabilities. However, their computational complexity and limited sensitivity to continuous numerical data impede real-time performance and precise motion control. To this end, we propose Narrate2Nav, a novel real-time vision-action model that leverages a novel self-supervised learning framework based on the Barlow Twins redundancy reduction loss to embed implicit natural language reasoning, social cues, and human intentions within a visual encoder-enabling reasoning in the model's latent space rather than token space. The model combines RGB inputs, motion commands, and textual signals of scene context during training to bridge from robot observations to low-level motion commands for short-horizon point-goal navigation during deployment. Extensive evaluation of Narrate2Nav across various challenging scenarios in both offline unseen dataset and real-world experiments demonstrates an overall improvement of 52.94 percent and 41.67 percent, respectively, over the next best baseline. Additionally, qualitative comparative analysis of Narrate2Nav's visual encoder attention map against four other baselines demonstrates enhanced attention to navigation-critical scene elements, underscoring its effectiveness in human-centric navigation tasks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper discusses using Vision-Language Models (VLMs) for robot navigation in human-centric environments. While it doesn't directly focus on trajectory prediction, navigation implies predicting future states and paths. The use of VLMs connects it to large language models, and the focus on human-centric environments suggests consideration of human movement patterns, which is related to trajectory prediction.", "keywords": ["Large Vision-Language Models", "VLMs", "navigation", "human-centric environments", "motion commands"]}}
{"id": "2506.14589", "pdf": "https://arxiv.org/pdf/2506.14589", "abs": "https://arxiv.org/abs/2506.14589", "authors": ["Ren Xin", "Hongji Liu", "Xiaodong Mei", "Wenru Liu", "Maosheng Ye", "Zhili Chen", "Jun Ma"], "title": "NetRoller: Interfacing General and Specialized Models for End-to-End Autonomous Driving", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Integrating General Models (GMs) such as Large Language Models (LLMs), with Specialized Models (SMs) in autonomous driving tasks presents a promising approach to mitigating challenges in data diversity and model capacity of existing specialized driving models. However, this integration leads to problems of asynchronous systems, which arise from the distinct characteristics inherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an adapter that incorporates a set of novel mechanisms to facilitate the seamless integration of GMs and specialized driving models. Specifically, our mechanisms for interfacing the asynchronous GMs and SMs are organized into three key stages. NetRoller first harvests semantically rich and computationally efficient representations from the reasoning processes of LLMs using an early stopping mechanism, which preserves critical insights on driving context while maintaining low overhead. It then applies learnable query embeddings, nonsensical embeddings, and positional layer embeddings to facilitate robust and efficient cross-modality translation. At last, it employs computationally efficient Query Shift and Feature Shift mechanisms to enhance the performance of SMs through few-epoch fine-tuning. Based on the mechanisms formalized in these three stages, NetRoller enables specialized driving models to operate at their native frequencies while maintaining situational awareness of the GM. Experiments conducted on the nuScenes dataset demonstrate that integrating GM through NetRoller significantly improves human similarity and safety in planning tasks, and it also achieves noticeable precision improvements in detection and mapping tasks for end-to-end autonomous driving. The code and models are available at https://github.com/Rex-sys-hk/NetRoller .", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper discusses integrating Large Language Models (LLMs) with specialized models for autonomous driving, which implicitly involves trajectory planning and decision-making. While not explicitly focusing on trajectory prediction as its primary contribution, the application to autonomous driving planning tasks and the mention of improving 'human similarity and safety in planning tasks' suggests relevance to trajectory prediction. It combines LLMs with specialized models, thus bridging both topics.", "keywords": ["Large Language Models", "LLMs", "autonomous driving", "planning tasks", "trajectory planning"]}}
{"id": "2506.14570", "pdf": "https://arxiv.org/pdf/2506.14570", "abs": "https://arxiv.org/abs/2506.14570", "authors": ["Mohammad Hashemi", "Andreas Zufle"], "title": "From Points to Places: Towards Human Mobility-Driven Spatiotemporal Foundation Models via Understanding Places", "categories": ["cs.AI"], "comment": null, "summary": "Capturing human mobility is essential for modeling how people interact with and move through physical spaces, reflecting social behavior, access to resources, and dynamic spatial patterns. To support scalable and transferable analysis across diverse geographies and contexts, there is a need for a generalizable foundation model for spatiotemporal data. While foundation models have transformed language and vision, they remain limited in handling the unique challenges posed by the spatial, temporal, and semantic complexity of mobility data. This vision paper advocates for a new class of spatial foundation models that integrate geolocation semantics with human mobility across multiple scales. Central to our vision is a shift from modeling discrete points of interest to understanding places: dynamic, context-rich regions shaped by human behavior and mobility that may comprise many places of interest. We identify key gaps in adaptability, scalability, and multi-granular reasoning, and propose research directions focused on modeling places and enabling efficient learning. Our goal is to guide the development of scalable, context-aware models for next-generation geospatial intelligence. These models unlock powerful applications ranging from personalized place discovery and logistics optimization to urban planning, ultimately enabling smarter and more responsive spatial decision-making.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper proposes a spatiotemporal foundation model for human mobility, which is relevant to both trajectory prediction (human mobility) and large language models (foundation models). While it doesn't directly use LLMs for trajectory prediction, it aims to build a foundation model that could potentially be used for such tasks in the future. The focus on understanding 'places' and their relationship to human mobility is also relevant to understanding and predicting trajectories.", "keywords": ["foundation models", "human mobility", "spatiotemporal data", "geospatial intelligence", "trajectory prediction (implied)"]}}
{"id": "2506.13915", "pdf": "https://arxiv.org/pdf/2506.13915", "abs": "https://arxiv.org/abs/2506.13915", "authors": ["Katherine Mao", "Hongzhan Yu", "Ruipeng Zhang", "Igor Spasojevic", "M Ani Hsieh", "Sicun Gao", "Vijay Kumar"], "title": "Sequence Modeling for Time-Optimal Quadrotor Trajectory Optimization with Sampling-based Robustness Analysis", "categories": ["cs.RO"], "comment": null, "summary": "Time-optimal trajectories drive quadrotors to their dynamic limits, but computing such trajectories involves solving non-convex problems via iterative nonlinear optimization, making them prohibitively costly for real-time applications. In this work, we investigate learning-based models that imitate a model-based time-optimal trajectory planner to accelerate trajectory generation. Given a dataset of collision-free geometric paths, we show that modeling architectures can effectively learn the patterns underlying time-optimal trajectories. We introduce a quantitative framework to analyze local analytic properties of the learned models, and link them to the Backward Reachable Tube of the geometric tracking controller. To enhance robustness, we propose a data augmentation scheme that applies random perturbations to the input paths. Compared to classical planners, our method achieves substantial speedups, and we validate its real-time feasibility on a hardware quadrotor platform. Experiments demonstrate that the learned models generalize to previously unseen path lengths. The code for our approach can be found here: https://github.com/maokat12/lbTOPPQuad", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory optimization for quadrotors using learning-based models to imitate a model-based planner. While it involves trajectory planning and optimization, it doesn't explicitly use or discuss large language models. The 'learning-based models' are likely neural networks, but not necessarily LLMs. The focus is more on time-optimal trajectory generation and robustness analysis.", "keywords": ["trajectory optimization", "quadrotor", "trajectory planning", "learning-based models"]}}
{"id": "2506.13922", "pdf": "https://arxiv.org/pdf/2506.13922", "abs": "https://arxiv.org/abs/2506.13922", "authors": ["Maximilian Du", "Shuran Song"], "title": "DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance", "categories": ["cs.RO"], "comment": "9 pages main, 21 pages with appendix and citations. 9 figures. Submitted to Neurips 2025", "summary": "Deploying large, complex policies in the real world requires the ability to steer them to fit the needs of a situation. Most common steering approaches, like goal-conditioning, require training the robot policy with a distribution of test-time objectives in mind. To overcome this limitation, we present DynaGuide, a steering method for diffusion policies using guidance from an external dynamics model during the diffusion denoising process. DynaGuide separates the dynamics model from the base policy, which gives it multiple advantages, including the ability to steer towards multiple objectives, enhance underrepresented base policy behaviors, and maintain robustness on low-quality objectives. The separate guidance signal also allows DynaGuide to work with off-the-shelf pretrained diffusion policies. We demonstrate the performance and features of DynaGuide against other steering approaches in a series of simulated and real experiments, showing an average steering success of 70% on a set of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x when steered with low-quality objectives. We also successfully steer an off-the-shelf real robot policy to express preference for particular objects and even create novel behavior. Videos and more can be found on the project website: https://dynaguide.github.io", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses steering diffusion policies using guidance from an external dynamics model. While it mentions 'policies' and indirectly relates to robot behavior and potentially trajectory generation, the core focus is on steering and dynamics models within a reinforcement learning context. It doesn't explicitly mention or heavily rely on Large Language Models. The connection to trajectory prediction is present but not central.", "keywords": ["diffusion policies", "dynamics model", "steering", "robot policy", "behavior"]}}
{"id": "2506.13953", "pdf": "https://arxiv.org/pdf/2506.13953", "abs": "https://arxiv.org/abs/2506.13953", "authors": ["Caio C. G. Ribeiro", "Leonardo R. D. Paes", "Douglas G. Macharet"], "title": "Socially-aware Object Transportation by a Mobile Manipulator in Static Planar Environments with Obstacles", "categories": ["cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and Human Interactive Communication (ROMAN)", "summary": "Socially-aware robotic navigation is essential in environments where humans and robots coexist, ensuring both safety and comfort. However, most existing approaches have been primarily developed for mobile robots, leaving a significant gap in research that addresses the unique challenges posed by mobile manipulators. In this paper, we tackle the challenge of navigating a robotic mobile manipulator, carrying a non-negligible load, within a static human-populated environment while adhering to social norms. Our goal is to develop a method that enables the robot to simultaneously manipulate an object and navigate between locations in a socially-aware manner. We propose an approach based on the Risk-RRT* framework that enables the coordinated actuation of both the mobile base and manipulator. This approach ensures collision-free navigation while adhering to human social preferences. We compared our approach in a simulated environment to socially-aware mobile-only methods applied to a mobile manipulator. The results highlight the necessity for mobile manipulator-specific techniques, with our method outperforming mobile-only approaches. Our method enabled the robot to navigate, transport an object, avoid collisions, and minimize social discomfort effectively.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u5728\u9759\u6001\u5e73\u9762\u73af\u5883\u4e2d\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u793e\u4ea4\u611f\u77e5\u7269\u4f53\u8fd0\u8f93\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u79fb\u52a8\u7269\u4f53\u7684\u8def\u5f84\u89c4\u5212\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u793e\u4ea4\u611f\u77e5\u53ef\u80fd\u6697\u793a\u4e86\u4f7f\u7528\u6a21\u578b\u6765\u7406\u89e3\u4eba\u7c7b\u884c\u4e3a\u3002\u603b\u4f53\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory prediction", "mobile manipulator", "socially-aware navigation", "path planning"]}}
{"id": "2506.14096", "pdf": "https://arxiv.org/pdf/2506.14096", "abs": "https://arxiv.org/abs/2506.14096", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses the use of Large Language Models (LLMs) for image segmentation in the context of Intelligent Transportation Systems (ITS). While it doesn't directly address trajectory prediction, ITS often involves understanding the movement of vehicles and pedestrians, which is closely related. The paper focuses on scene understanding, which is a prerequisite for accurate trajectory prediction. The use of LLMs is a key aspect.", "keywords": ["Large Language Models", "LLMs", "Intelligent Transportation Systems", "ITS", "image segmentation", "autonomous driving"]}}
{"id": "2506.14130", "pdf": "https://arxiv.org/pdf/2506.14130", "abs": "https://arxiv.org/abs/2506.14130", "authors": ["Chunyu Cao", "Jintao Cheng", "Zeyu Chen", "Linfan Zhan", "Rui Fan", "Zhijian He", "Xiaoyu Tang"], "title": "KDMOS:Knowledge Distillation for Motion Segmentation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Motion Object Segmentation (MOS) is crucial for autonomous driving, as it enhances localization, path planning, map construction, scene flow estimation, and future state prediction. While existing methods achieve strong performance, balancing accuracy and real-time inference remains a challenge. To address this, we propose a logits-based knowledge distillation framework for MOS, aiming to improve accuracy while maintaining real-time efficiency. Specifically, we adopt a Bird's Eye View (BEV) projection-based model as the student and a non-projection model as the teacher. To handle the severe imbalance between moving and non-moving classes, we decouple them and apply tailored distillation strategies, allowing the teacher model to better learn key motion-related features. This approach significantly reduces false positives and false negatives. Additionally, we introduce dynamic upsampling, optimize the network architecture, and achieve a 7.69% reduction in parameter count, mitigating overfitting. Our method achieves a notable IoU of 78.8% on the hidden test set of the SemanticKITTI-MOS dataset and delivers competitive results on the Apollo dataset. The KDMOS implementation is available at https://github.com/SCNU-RISLAB/KDMOS.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on motion segmentation, which is closely related to future state prediction, a component of trajectory prediction. While it doesn't directly use or discuss large language models, the application of motion segmentation to autonomous driving scenarios (path planning) increases its relevance to the broader context of trajectory prediction. The paper mentions 'future state prediction' directly.", "keywords": ["motion segmentation", "autonomous driving", "path planning", "future state prediction"]}}
{"id": "2506.14100", "pdf": "https://arxiv.org/pdf/2506.14100", "abs": "https://arxiv.org/abs/2506.14100", "authors": ["Yupeng Zhou", "Can Cui", "Juntong Peng", "Zichong Yang", "Juanwu Lu", "Jitesh H Panchal", "Bin Yao", "Ziran Wang"], "title": "A Hierarchical Test Platform for Vision Language Model (VLM)-Integrated Real-World Autonomous Driving", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated notable promise in autonomous driving by offering the potential for multimodal reasoning through pretraining on extensive image-text pairs. However, adapting these models from broad web-scale data to the safety-critical context of driving presents a significant challenge, commonly referred to as domain shift. Existing simulation-based and dataset-driven evaluation methods, although valuable, often fail to capture the full complexity of real-world scenarios and cannot easily accommodate repeatable closed-loop testing with flexible scenario manipulation. In this paper, we introduce a hierarchical real-world test platform specifically designed to evaluate VLM-integrated autonomous driving systems. Our approach includes a modular, low-latency on-vehicle middleware that allows seamless incorporation of various VLMs, a clearly separated perception-planning-control architecture that can accommodate both VLM-based and conventional modules, and a configurable suite of real-world testing scenarios on a closed track that facilitates controlled yet authentic evaluations. We demonstrate the effectiveness of the proposed platform`s testing and evaluation ability with a case study involving a VLM-enabled autonomous vehicle, highlighting how our test framework supports robust experimentation under diverse conditions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on evaluating Vision-Language Models (VLMs) in autonomous driving. While it doesn't directly address trajectory prediction, it explores the application of large models (VLMs) in a domain that often involves trajectory planning and control. The connection to trajectory prediction is indirect, as the VLM could potentially influence or be integrated with trajectory prediction modules within the autonomous driving system. Therefore, the relevance score is moderately high.", "keywords": ["Vision-Language Models", "VLMs", "autonomous driving", "large models"]}}
{"id": "2506.14198", "pdf": "https://arxiv.org/pdf/2506.14198", "abs": "https://arxiv.org/abs/2506.14198", "authors": ["Jeremy A. Collins", "Lor\u00e1nd Cheng", "Kunal Aneja", "Albert Wilcox", "Benjamin Joffe", "Animesh Garg"], "title": "AMPLIFY: Actionless Motion Priors for Robot Learning from Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Action-labeled data for robotics is scarce and expensive, limiting the generalization of learned policies. In contrast, vast amounts of action-free video data are readily available, but translating these observations into effective policies remains a challenge. We introduce AMPLIFY, a novel framework that leverages large-scale video data by encoding visual dynamics into compact, discrete motion tokens derived from keypoint trajectories. Our modular approach separates visual motion prediction from action inference, decoupling the challenges of learning what motion defines a task from how robots can perform it. We train a forward dynamics model on abundant action-free videos and an inverse dynamics model on a limited set of action-labeled examples, allowing for independent scaling. Extensive evaluations demonstrate that the learned dynamics are both accurate, achieving up to 3.7x better MSE and over 2.5x better pixel prediction accuracy compared to prior approaches, and broadly useful. In downstream policy learning, our dynamics predictions enable a 1.2-2.2x improvement in low-data regimes, a 1.4x average improvement by learning from action-free human videos, and the first generalization to LIBERO tasks from zero in-distribution action data. Beyond robotic control, we find the dynamics learned by AMPLIFY to be a versatile latent world model, enhancing video prediction quality. Our results present a novel paradigm leveraging heterogeneous data sources to build efficient, generalizable world models. More information can be found at https://amplify-robotics.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5229\u7528\u65e0\u52a8\u4f5c\u6807\u7b7e\u7684\u89c6\u9891\u6570\u636e\u8fdb\u884c\u8fd0\u52a8\u9884\u6d4b\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff08keypoint trajectories, visual motion prediction\uff09\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5229\u7528\u4e86\u8fd0\u52a8\u9884\u6d4b\u8fdb\u884c\u673a\u5668\u4eba\u63a7\u5236\u3002", "keywords": ["motion prediction", "keypoint trajectories", "visual dynamics", "robot learning", "world model"]}}
{"id": "2506.14070", "pdf": "https://arxiv.org/pdf/2506.14070", "abs": "https://arxiv.org/abs/2506.14070", "authors": ["Xinglei Wang", "Tao Cheng", "Stephen Law", "Zichao Zeng", "Ilya Ilyankou", "Junyuan Liu", "Lu Yin", "Weiming Huang", "Natchapon Jongwiriyanurak"], "title": "Into the Unknown: Applying Inductive Spatial-Semantic Location Embeddings for Predicting Individuals' Mobility Beyond Visited Places", "categories": ["cs.AI"], "comment": "10 pages, 5 figures", "summary": "Predicting individuals' next locations is a core task in human mobility modelling, with wide-ranging implications for urban planning, transportation, public policy and personalised mobility services. Traditional approaches largely depend on location embeddings learned from historical mobility patterns, limiting their ability to encode explicit spatial information, integrate rich urban semantic context, and accommodate previously unseen locations. To address these challenges, we explore the application of CaLLiPer -- a multimodal representation learning framework that fuses spatial coordinates and semantic features of points of interest through contrastive learning -- for location embedding in individual mobility prediction. CaLLiPer's embeddings are spatially explicit, semantically enriched, and inductive by design, enabling robust prediction performance even in scenarios involving emerging locations. Through extensive experiments on four public mobility datasets under both conventional and inductive settings, we demonstrate that CaLLiPer consistently outperforms strong baselines, particularly excelling in inductive scenarios. Our findings highlight the potential of multimodal, inductive location embeddings to advance the capabilities of human mobility prediction systems. We also release the code and data (https://github.com/xlwang233/Into-the-Unknown) to foster reproducibility and future research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on predicting human mobility using location embeddings, which falls under the umbrella of trajectory prediction. While it doesn't directly involve large language models, the use of multimodal representation learning and contrastive learning for encoding spatial and semantic information is a related technique that could potentially be combined with LLMs in future work. The paper focuses on 'mobility prediction' and 'location embeddings' which are related to trajectory prediction.", "keywords": ["trajectory prediction", "human mobility prediction", "location embeddings", "spatial-semantic location embeddings", "mobility modeling"]}}
{"id": "2506.14271", "pdf": "https://arxiv.org/pdf/2506.14271", "abs": "https://arxiv.org/abs/2506.14271", "authors": ["Weiming Zhang", "Dingwen Xiao", "Aobotao Dai", "Yexin Liu", "Tianbo Pan", "Shiqi Wen", "Lei Chen", "Lin Wang"], "title": "Leader360V: The Large-scale, Real-world 360 Video Dataset for Multi-task Learning in Diverse Environment", "categories": ["cs.CV"], "comment": "23 pages, 16 figures", "summary": "360 video captures the complete surrounding scenes with the ultra-large field of view of 360X180. This makes 360 scene understanding tasks, eg, segmentation and tracking, crucial for appications, such as autonomous driving, robotics. With the recent emergence of foundation models, the community is, however, impeded by the lack of large-scale, labelled real-world datasets. This is caused by the inherent spherical properties, eg, severe distortion in polar regions, and content discontinuities, rendering the annotation costly yet complex. This paper introduces Leader360V, the first large-scale, labeled real-world 360 video datasets for instance segmentation and tracking. Our datasets enjoy high scene diversity, ranging from indoor and urban settings to natural and dynamic outdoor scenes. To automate annotation, we design an automatic labeling pipeline, which subtly coordinates pre-trained 2D segmentors and large language models to facilitate the labeling. The pipeline operates in three novel stages. Specifically, in the Initial Annotation Phase, we introduce a Semantic- and Distortion-aware Refinement module, which combines object mask proposals from multiple 2D segmentors with LLM-verified semantic labels. These are then converted into mask prompts to guide SAM2 in generating distortion-aware masks for subsequent frames. In the Auto-Refine Annotation Phase, missing or incomplete regions are corrected either by applying the SDR again or resolving the discontinuities near the horizontal borders. The Manual Revision Phase finally incorporates LLMs and human annotators to further refine and validate the annotations. Extensive user studies and evaluations demonstrate the effectiveness of our labeling pipeline. Meanwhile, experiments confirm that Leader360V significantly enhances model performance for 360 video segmentation and tracking, paving the way for more scalable 360 scene understanding.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8360\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5b9e\u4f8b\u5206\u5272\u548c\u8ddf\u8e2a\u3002\u867d\u7136\u63d0\u5230\u4e86\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u7b49\u5e94\u7528\u573a\u666f\uff0c\u8fd9\u4e9b\u573a\u666f\u901a\u5e38\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4f46\u8bba\u6587\u672c\u8eab\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u3002\u4e0d\u8fc7\uff0c\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u8f85\u52a9\u6807\u6ce8\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "foundation models", "tracking", "autonomous driving", "robotics"]}}
{"id": "2506.14305", "pdf": "https://arxiv.org/pdf/2506.14305", "abs": "https://arxiv.org/abs/2506.14305", "authors": ["Zhirui Sun", "Xingrong Diao", "Yao Wang", "Bi-Ke Zhu", "Jiankun Wang"], "title": "Socially Aware Robot Crowd Navigation via Online Uncertainty-Driven Risk Adaptation", "categories": ["cs.RO"], "comment": null, "summary": "Navigation in human-robot shared crowded environments remains challenging, as robots are expected to move efficiently while respecting human motion conventions. However, many existing approaches emphasize safety or efficiency while overlooking social awareness. This article proposes Learning-Risk Model Predictive Control (LR-MPC), a data-driven navigation algorithm that balances efficiency, safety, and social awareness. LR-MPC consists of two phases: an offline risk learning phase, where a Probabilistic Ensemble Neural Network (PENN) is trained using risk data from a heuristic MPC-based baseline (HR-MPC), and an online adaptive inference phase, where local waypoints are sampled and globally guided by a Multi-RRT planner. Each candidate waypoint is evaluated for risk by PENN, and predictions are filtered using epistemic and aleatoric uncertainty to ensure robust decision-making. The safest waypoint is selected as the MPC input for real-time navigation. Extensive experiments demonstrate that LR-MPC outperforms baseline methods in success rate and social awareness, enabling robots to navigate complex crowds with high adaptability and low disruption. A website about this work is available at https://sites.google.com/view/lr-mpc.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robot navigation in crowded environments, using a data-driven approach (Probabilistic Ensemble Neural Network - PENN) to predict risk and adapt robot behavior. While it involves predicting future states (risk associated with waypoints), which is related to trajectory prediction, and uses a neural network, it doesn't explicitly utilize or discuss large language models. The connection to trajectory prediction is present, but the absence of LLMs lowers the relevance.", "keywords": ["trajectory prediction", "navigation", "risk prediction", "neural network"]}}
{"id": "2506.14507", "pdf": "https://arxiv.org/pdf/2506.14507", "abs": "https://arxiv.org/abs/2506.14507", "authors": ["Nitesh Subedi", "Adam Haroon", "Shreyan Ganguly", "Samuel T. K. Tetteh", "Prajwal Koirala", "Cody Fleming", "Soumik Sarkar"], "title": "Can Pretrained Vision-Language Embeddings Alone Guide Robot Navigation?", "categories": ["cs.RO"], "comment": "6 figures, 2 tables, Accepted to Robotics: Science and Systems (RSS) 2025 Workshop on Robot Planning in the Era of Foundation Models (FM4RoboPlan)", "summary": "Foundation models have revolutionized robotics by providing rich semantic representations without task-specific training. While many approaches integrate pretrained vision-language models (VLMs) with specialized navigation architectures, the fundamental question remains: can these pretrained embeddings alone successfully guide navigation without additional fine-tuning or specialized modules? We present a minimalist framework that decouples this question by training a behavior cloning policy directly on frozen vision-language embeddings from demonstrations collected by a privileged expert. Our approach achieves a 74% success rate in navigation to language-specified targets, compared to 100% for the state-aware expert, though requiring 3.2 times more steps on average. This performance gap reveals that pretrained embeddings effectively support basic language grounding but struggle with long-horizon planning and spatial reasoning. By providing this empirical baseline, we highlight both the capabilities and limitations of using foundation models as drop-in representations for embodied tasks, offering critical insights for robotics researchers facing practical design tradeoffs between system complexity and performance in resource-constrained scenarios. Our code is available at https://github.com/oadamharoon/text2nav", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper explores the use of pre-trained vision-language models (VLMs) for robot navigation. While it doesn't explicitly mention trajectory prediction, navigation inherently involves predicting the robot's future trajectory. The use of VLMs connects it to the large language model theme. The paper investigates the capabilities and limitations of using foundation models for embodied tasks, which is relevant to both fields, although not directly focusing on trajectory prediction in its traditional sense.", "keywords": ["vision-language models", "VLMs", "foundation models", "robot navigation", "behavior cloning", "pretrained embeddings"]}}
{"id": "2506.14496", "pdf": "https://arxiv.org/pdf/2506.14496", "abs": "https://arxiv.org/abs/2506.14496", "authors": ["Muhammad Atta Ur Rahman", "Melanie Schranz"], "title": "LLM-Powered Swarms: A New Frontier or a Conceptual Stretch?", "categories": ["cs.AI"], "comment": "This is the author's version of a paper submitted to IEEE Intelligent Systems. 6 Tables, 3 Figures", "summary": "Swarm intelligence traditionally refers to systems of simple, decentralized agents whose local interactions lead to emergent, collective behavior. Recently, the term 'swarm' has been extended to describe AI systems like OpenAI's Swarm, where large language models (LLMs) act as collaborative agents. This paper contrasts traditional swarm algorithms with LLM-driven swarms exploring how decentralization, scalability, and emergence are redefined in modern artificial intelligence (AI). We implement and compare both paradigms using Boids and Ant Colony Optimization (ACO), evaluating latency, resource usage, and behavioral accuracy. The suitability of both cloud-based and local LLMs is assessed for the agent-based use in swarms. Although LLMs offer powerful reasoning and abstraction capabilities, they introduce new constraints in computation and coordination that challenge traditional notions of swarm design. This study highlights the opportunities and limitations of integrating LLMs into swarm systems and discusses the evolving definition of 'swarm' in modern AI research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on the application of Large Language Models (LLMs) in swarm systems. While it doesn't directly address trajectory prediction, the use of LLMs as agents within a swarm could potentially be applied to trajectory prediction tasks in the future. The connection is indirect, but present.", "keywords": ["Large Language Models", "LLMs", "swarm intelligence", "agent-based"]}}
{"id": "2506.14428", "pdf": "https://arxiv.org/pdf/2506.14428", "abs": "https://arxiv.org/abs/2506.14428", "authors": ["Ruihao Xi", "Xuekuan Wang", "Yongcheng Li", "Shuhua Li", "Zichen Wang", "Yiwei Wang", "Feng Wei", "Cairong Zhao"], "title": "Toward Rich Video Human-Motion2D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Generating realistic and controllable human motions, particularly those involving rich multi-character interactions, remains a significant challenge due to data scarcity and the complexities of modeling inter-personal dynamics. To address these limitations, we first introduce a new large-scale rich video human motion 2D dataset (Motion2D-Video-150K) comprising 150,000 video sequences. Motion2D-Video-150K features a balanced distribution of diverse single-character and, crucially, double-character interactive actions, each paired with detailed textual descriptions. Building upon this dataset, we propose a novel diffusion-based rich video human motion2D generation (RVHM2D) model. RVHM2D incorporates an enhanced textual conditioning mechanism utilizing either dual text encoders (CLIP-L/B) or T5-XXL with both global and local features. We devise a two-stage training strategy: the model is first trained with a standard diffusion objective, and then fine-tuned using reinforcement learning with an FID-based reward to further enhance motion realism and text alignment. Extensive experiments demonstrate that RVHM2D achieves leading performance on the Motion2D-Video-150K benchmark in generating both single and interactive double-character scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating human motion from video, which is related to action prediction and can be seen as a form of trajectory prediction in a 2D space. It also leverages large models like CLIP and T5-XXL for textual conditioning. However, it doesn't explicitly address trajectory prediction in the context of navigation or path planning, and the focus is more on motion generation than prediction. The connection to trajectory prediction is therefore somewhat indirect.", "keywords": ["human motion", "motion generation", "diffusion models", "CLIP", "T5-XXL", "large language models", "action prediction"]}}
{"id": "2506.14502", "pdf": "https://arxiv.org/pdf/2506.14502", "abs": "https://arxiv.org/abs/2506.14502", "authors": ["Xiao Wang", "Junru Yu", "Jun Huang", "Qiong Wu", "Ljubo Vacic", "Changyin Sun"], "title": "Toward Safety-First Human-Like Decision Making for Autonomous Vehicles in Time-Varying Traffic Flow", "categories": ["cs.AI"], "comment": null, "summary": "Despite the recent advancements in artificial intelligence technologies have shown great potential in improving transport efficiency and safety, autonomous vehicles(AVs) still face great challenge of driving in time-varying traffic flow, especially in dense and interactive situations. Meanwhile, human have free wills and usually do not make the same decisions even situate in the exactly same scenarios, leading to the data-driven methods suffer from poor migratability and high search cost problems, decreasing the efficiency and effectiveness of the behavior policy. In this research, we propose a safety-first human-like decision-making framework(SF-HLDM) for AVs to drive safely, comfortably, and social compatiblely in effiency. The framework integrates a hierarchical progressive framework, which combines a spatial-temporal attention (S-TA) mechanism for other road users' intention inference, a social compliance estimation module for behavior regulation, and a Deep Evolutionary Reinforcement Learning(DERL) model for expanding the search space efficiently and effectively to make avoidance of falling into the local optimal trap and reduce the risk of overfitting, thus make human-like decisions with interpretability and flexibility. The SF-HLDM framework enables autonomous driving AI agents dynamically adjusts decision parameters to maintain safety margins and adhering to contextually appropriate driving behaviors at the same time.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u7279\u522b\u662f\u5b89\u5168\u548c\u7c7b\u4eba\u884c\u4e3a\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u4ea4\u901a\u6d41\u4e2d\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u7684\u610f\u56fe\u63a8\u65ad\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u5229\u7528AI\u8fdb\u884c\u884c\u4e3a\u9884\u6d4b\u548c\u51b3\u7b56\u6709\u5173\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["autonomous vehicles", "decision making", "intention inference", "traffic flow", "behavior policy"]}}
{"id": "2506.14727", "pdf": "https://arxiv.org/pdf/2506.14727", "abs": "https://arxiv.org/abs/2506.14727", "authors": ["Huihan Liu", "Rutav Shah", "Shuijing Liu", "Jack Pittenger", "Mingyo Seo", "Yuchen Cui", "Yonatan Bisk", "Roberto Mart\u00edn-Mart\u00edn", "Yuke Zhu"], "title": "Casper: Inferring Diverse Intents for Assistive Teleoperation with Vision Language Models", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Assistive teleoperation, where control is shared between a human and a robot, enables efficient and intuitive human-robot collaboration in diverse and unstructured environments. A central challenge in real-world assistive teleoperation is for the robot to infer a wide range of human intentions from user control inputs and to assist users with correct actions. Existing methods are either confined to simple, predefined scenarios or restricted to task-specific data distributions at training, limiting their support for real-world assistance. We introduce Casper, an assistive teleoperation system that leverages commonsense knowledge embedded in pre-trained visual language models (VLMs) for real-time intent inference and flexible skill execution. Casper incorporates an open-world perception module for a generalized understanding of novel objects and scenes, a VLM-powered intent inference mechanism that leverages commonsense reasoning to interpret snippets of teleoperated user input, and a skill library that expands the scope of prior assistive teleoperation systems to support diverse, long-horizon mobile manipulation tasks. Extensive empirical evaluation, including human studies and system ablations, demonstrates that Casper improves task performance, reduces human cognitive load, and achieves higher user satisfaction than direct teleoperation and assistive teleoperation baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes Vision Language Models (VLMs) for intent inference in assistive teleoperation. While it doesn't explicitly focus on trajectory prediction, the inference of human intentions can be seen as a precursor to predicting future actions or trajectories. The use of VLMs connects it to the domain of Large Language Models. The connection to trajectory prediction is somewhat indirect but present.", "keywords": ["Vision Language Models", "VLMs", "intent inference", "teleoperation", "human-robot collaboration"]}}
{"id": "2506.14763", "pdf": "https://arxiv.org/pdf/2506.14763", "abs": "https://arxiv.org/abs/2506.14763", "authors": ["Chunru Lin", "Haotian Yuan", "Yian Wang", "Xiaowen Qiu", "Tsun-Hsuan Wang", "Minghao Guo", "Bohan Wang", "Yashraj Narang", "Dieter Fox", "Chuang Gan"], "title": "RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills", "categories": ["cs.RO"], "comment": null, "summary": "Endowing robots with tool design abilities is critical for enabling them to solve complex manipulation tasks that would otherwise be intractable. While recent generative frameworks can automatically synthesize task settings, such as 3D scenes and reward functions, they have not yet addressed the challenge of tool-use scenarios. Simply retrieving human-designed tools might not be ideal since many tools (e.g., a rolling pin) are difficult for robotic manipulators to handle. Furthermore, existing tool design approaches either rely on predefined templates with limited parameter tuning or apply generic 3D generation methods that are not optimized for tool creation. To address these limitations, we propose RobotSmith, an automated pipeline that leverages the implicit physical knowledge embedded in vision-language models (VLMs) alongside the more accurate physics provided by physics simulations to design and use tools for robotic manipulation. Our system (1) iteratively proposes tool designs using collaborative VLM agents, (2) generates low-level robot trajectories for tool use, and (3) jointly optimizes tool geometry and usage for task performance. We evaluate our approach across a wide range of manipulation tasks involving rigid, deformable, and fluid objects. Experiments show that our method consistently outperforms strong baselines in terms of both task success rate and overall performance. Notably, our approach achieves a 50.0\\% average success rate, significantly surpassing other baselines such as 3D generation (21.4%) and tool retrieval (11.1%). Finally, we deploy our system in real-world settings, demonstrating that the generated tools and their usage plans transfer effectively to physical execution, validating the practicality and generalization capabilities of our approach.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robotic tool design and manipulation, leveraging vision-language models (VLMs) to generate and optimize tool geometry and usage. While it doesn't directly address trajectory prediction as its primary focus, the generation of low-level robot trajectories for tool use connects to the broader concept of motion planning and prediction. The use of VLMs also makes it relevant to the large language model domain, although it's not the central theme.", "keywords": ["vision-language models", "VLMs", "robot trajectories", "robotic manipulation"]}}
{"id": "2506.14728", "pdf": "https://arxiv.org/pdf/2506.14728", "abs": "https://arxiv.org/abs/2506.14728", "authors": ["Jiahao Qiu", "Xinzhe Juan", "Yimin Wang", "Ling Yang", "Xuan Qi", "Tongcheng Zhang", "Jiacheng Guo", "Yifu Lu", "Zixin Yao", "Hongru Wang", "Shilong Liu", "Xun Jiang", "Liu Leqi", "Mengdi Wang"], "title": "AgentDistill: Training-Free Agent Distillation with Generalizable MCP Boxes", "categories": ["cs.AI"], "comment": "10 pages, 5 figures", "summary": "While knowledge distillation has become a mature field for compressing large language models (LLMs) into smaller ones by aligning their outputs or internal representations, the distillation of LLM-based agents, which involve planning, memory, and tool use, remains relatively underexplored. Existing agent distillation methods typically replay full teacher trajectories or imitate step-by-step teacher tool usage, but they often struggle to train student agents to dynamically plan and act in novel environments. We propose AgentDistill, a novel, training-free agent distillation framework that enables efficient and scalable knowledge transfer via direct reuse of Model-Context-Protocols (MCPs), which are structured and reusable task-solving modules autonomously generated by teacher agents. The reuse of these distilled MCPs enables student agents to generalize their capabilities across domains and solve new problems with minimal supervision or human intervention. Experiments on biomedical and mathematical benchmarks demonstrate that our distilled student agents, built on small language models, can achieve performance comparable to advanced systems using large LLMs such as OctoTools (GPT-4o), highlighting the effectiveness of our framework in building scalable and cost-efficient intelligent agents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on distilling knowledge from large language model-based agents to smaller agents. While it doesn't directly address trajectory prediction, the concept of agent behavior and planning can be loosely related. The paper prominently features large language models and the distillation process, making it moderately relevant.", "keywords": ["large language models", "LLMs", "agent distillation", "planning", "agent"]}}
{"id": "2506.14512", "pdf": "https://arxiv.org/pdf/2506.14512", "abs": "https://arxiv.org/abs/2506.14512", "authors": ["Zijian Song", "Xiaoxin Lin", "Qiuming Huang", "Guangrun Wang", "Liang Lin"], "title": "SIRI-Bench: Challenging VLMs' Spatial Intelligence through Complex Reasoning Tasks", "categories": ["cs.CV"], "comment": "16 pages, 9 figures", "summary": "Large Language Models (LLMs) are experiencing rapid advancements in complex reasoning, exhibiting remarkable generalization in mathematics and programming. In contrast, while spatial intelligence is fundamental for Vision-Language Models (VLMs) in real-world interaction, the systematic evaluation of their complex reasoning ability within spatial contexts remains underexplored. To bridge this gap, we introduce SIRI-Bench, a benchmark designed to evaluate VLMs' spatial intelligence through video-based reasoning tasks. SIRI-Bench comprises nearly 1K video-question-answer triplets, where each problem is embedded in a realistic 3D scene and captured by video. By carefully designing questions and corresponding 3D scenes, our benchmark ensures that solving the questions requires both spatial comprehension for extracting information and high-level reasoning for deriving solutions, making it a challenging benchmark for evaluating VLMs. To facilitate large-scale data synthesis, we develop an Automatic Scene Creation Engine. This engine, leveraging multiple specialized LLM agents, can generate realistic 3D scenes from abstract math problems, ensuring faithfulness to the original descriptions. Experimental results reveal that state-of-the-art VLMs struggle significantly on SIRI-Bench, underscoring the challenge of spatial reasoning. We hope that our study will bring researchers' attention to spatially grounded reasoning and advance VLMs in visual problem-solving.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on evaluating Vision-Language Models (VLMs) on spatial reasoning tasks using video data. While it doesn't directly address trajectory prediction, the spatial reasoning aspect and the use of LLMs for scene generation make it moderately relevant. The connection to trajectory prediction is that spatial reasoning is a component skill needed for trajectory prediction, although that connection is not explicitly made. The use of LLMs is central to the paper's methodology.", "keywords": ["Large Language Models", "VLMs", "spatial reasoning", "3D scene", "reasoning"]}}
{"id": "2506.13956", "pdf": "https://arxiv.org/pdf/2506.13956", "abs": "https://arxiv.org/abs/2506.13956", "authors": ["Shang-Chi Tsai", "Seiya Kawano", "Angel Garcia Contreras", "Koichiro Yoshino", "Yun-Nung Chen"], "title": "ASMR: Augmenting Life Scenario using Large Generative Models for Robotic Action Reflection", "categories": ["cs.CL", "cs.AI", "cs.RO"], "comment": "IWSDS 2024 Best Paper Award", "summary": "When designing robots to assist in everyday human activities, it is crucial to enhance user requests with visual cues from their surroundings for improved intent understanding. This process is defined as a multimodal classification task. However, gathering a large-scale dataset encompassing both visual and linguistic elements for model training is challenging and time-consuming. To address this issue, our paper introduces a novel framework focusing on data augmentation in robotic assistance scenarios, encompassing both dialogues and related environmental imagery. This approach involves leveraging a sophisticated large language model to simulate potential conversations and environmental contexts, followed by the use of a stable diffusion model to create images depicting these environments. The additionally generated data serves to refine the latest multimodal models, enabling them to more accurately determine appropriate actions in response to user interactions with the limited target data. Our experimental results, based on a dataset collected from real-world scenarios, demonstrate that our methodology significantly enhances the robot's action selection capabilities, achieving the state-of-the-art performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using large language models (LLMs) for data augmentation in robotic action selection. While it doesn't directly deal with trajectory prediction, it uses LLMs to understand user intent and generate environmental contexts to improve robot's action selection, which can be loosely related to future action prediction. The core is the application of LLMs in a robotics context, making it somewhat relevant.", "keywords": ["Large Language Models", "LLMs", "Robotic Action Reflection", "data augmentation", "multimodal models", "action selection"]}}
{"id": "2506.13782", "pdf": "https://arxiv.org/pdf/2506.13782", "abs": "https://arxiv.org/abs/2506.13782", "authors": ["Ke Wang", "Bo Pan", "Yingchaojie Feng", "Yuwei Wu", "Jieyi Chen", "Minfeng Zhu", "Wei Chen"], "title": "XGraphRAG: Interactive Visual Analysis for Graph-based Retrieval-Augmented Generation", "categories": ["cs.IR", "cs.AI"], "comment": "Accepted to IEEE Pacific Visualization Conference 2025", "summary": "Graph-based Retrieval-Augmented Generation (RAG) has shown great capability in enhancing Large Language Model (LLM)'s answer with an external knowledge base. Compared to traditional RAG, it introduces a graph as an intermediate representation to capture better structured relational knowledge in the corpus, elevating the precision and comprehensiveness of generation results. However, developers usually face challenges in analyzing the effectiveness of GraphRAG on their dataset due to GraphRAG's complex information processing pipeline and the overwhelming amount of LLM invocations involved during graph construction and query, which limits GraphRAG interpretability and accessibility. This research proposes a visual analysis framework that helps RAG developers identify critical recalls of GraphRAG and trace these recalls through the GraphRAG pipeline. Based on this framework, we develop XGraphRAG, a prototype system incorporating a set of interactive visualizations to facilitate users' analysis process, boosting failure cases collection and improvement opportunities identification. Our evaluation demonstrates the effectiveness and usability of our approach. Our work is open-sourced and available at https://github.com/Gk0Wk/XGraphRAG.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving Large Language Models (LLMs) through Graph-based Retrieval-Augmented Generation (RAG). While it doesn't directly address trajectory prediction, it is highly relevant to the use of LLMs, a key component of the query.", "keywords": ["Large Language Model", "LLM", "Retrieval-Augmented Generation", "RAG", "Graph"]}}
{"id": "2506.14762", "pdf": "https://arxiv.org/pdf/2506.14762", "abs": "https://arxiv.org/abs/2506.14762", "authors": ["Chengyuan Zhang", "Cathy Wu", "Lijun Sun"], "title": "Markov Regime-Switching Intelligent Driver Model for Interpretable Car-Following Behavior", "categories": ["stat.AP", "cs.LG", "cs.RO"], "comment": null, "summary": "Accurate and interpretable car-following models are essential for traffic simulation and autonomous vehicle development. However, classical models like the Intelligent Driver Model (IDM) are fundamentally limited by their parsimonious and single-regime structure. They fail to capture the multi-modal nature of human driving, where a single driving state (e.g., speed, relative speed, and gap) can elicit many different driver actions. This forces the model to average across distinct behaviors, reducing its fidelity and making its parameters difficult to interpret. To overcome this, we introduce a regime-switching framework that allows driving behavior to be governed by different IDM parameter sets, each corresponding to an interpretable behavioral mode. This design enables the model to dynamically switch between interpretable behavioral modes, rather than averaging across diverse driving contexts. We instantiate the framework using a Factorial Hidden Markov Model with IDM dynamics (FHMM-IDM), which explicitly separates intrinsic driving regimes (e.g., aggressive acceleration, steady-state following) from external traffic scenarios (e.g., free-flow, congestion, stop-and-go) through two independent latent Markov processes. Bayesian inference via Markov chain Monte Carlo (MCMC) is used to jointly estimate the regime-specific parameters, transition dynamics, and latent state trajectories. Experiments on the HighD dataset demonstrate that FHMM-IDM uncovers interpretable structure in human driving, effectively disentangling internal driver actions from contextual traffic conditions and revealing dynamic regime-switching patterns. This framework provides a tractable and principled solution to modeling context-dependent driving behavior under uncertainty, offering improvements in the fidelity of traffic simulations, the efficacy of safety analyses, and the development of more human-centric ADAS.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on car-following behavior modeling using a Markov Regime-Switching Intelligent Driver Model. While it doesn't directly involve Large Language Models, it is relevant to trajectory prediction as it deals with modeling and predicting vehicle movements. The focus is more on interpretable modeling than prediction accuracy, but it still falls within the realm of trajectory prediction.", "keywords": ["trajectory prediction", "car-following", "Intelligent Driver Model", "Markov Regime-Switching", "driving behavior"]}}
{"id": "2506.14391", "pdf": "https://arxiv.org/pdf/2506.14391", "abs": "https://arxiv.org/abs/2506.14391", "authors": ["Yaqiao Zhu", "Hongkai Wen", "Geyong Min", "Man Luo"], "title": "HiLight: A Hierarchical Reinforcement Learning Framework with Global Adversarial Guidance for Large-Scale Traffic Signal Control", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Efficient traffic signal control (TSC) is essential for mitigating urban congestion, yet existing reinforcement learning (RL) methods face challenges in scaling to large networks while maintaining global coordination. Centralized RL suffers from scalability issues, while decentralized approaches often lack unified objectives, resulting in limited network-level efficiency. In this paper, we propose HiLight, a hierarchical reinforcement learning framework with global adversarial guidance for large-scale TSC. HiLight consists of a high-level Meta-Policy, which partitions the traffic network into subregions and generates sub-goals using a Transformer-LSTM architecture, and a low-level Sub-Policy, which controls individual intersections with global awareness. To improve the alignment between global planning and local execution, we introduce an adversarial training mechanism, where the Meta-Policy generates challenging yet informative sub-goals, and the Sub-Policy learns to surpass these targets, leading to more effective coordination. We evaluate HiLight across both synthetic and real-world benchmarks, and additionally construct a large-scale Manhattan network with diverse traffic conditions, including peak transitions, adverse weather, and holiday surges. Experimental results show that HiLight exhibits significant advantages in large-scale scenarios and remains competitive across standard benchmarks of varying sizes.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff0c\u4f46\u4f7f\u7528\u4e86Transformer-LSTM\u67b6\u6784\uff08\u5c5e\u4e8e\u5927\u6a21\u578b\u76f8\u5173\u6280\u672f\uff09\u6765\u751f\u6210\u5b50\u76ee\u6807\uff0c\u5e76\u4e14\u6d89\u53ca\u4ea4\u901a\u7f51\u7edc\u4e2d\u7684\u8f66\u8f86\u8f68\u8ff9\u89c4\u5212\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["traffic signal control", "Transformer-LSTM", "reinforcement learning", "traffic network"]}}
