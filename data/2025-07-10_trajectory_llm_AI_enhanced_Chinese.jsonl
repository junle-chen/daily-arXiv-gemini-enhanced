{"id": "2507.05754", "pdf": "https://arxiv.org/pdf/2507.05754", "abs": "https://arxiv.org/abs/2507.05754", "authors": ["Yuhang Zhang", "Jiaqi Liu", "Chengkai Xu", "Peng Hang", "Jian Sun"], "title": "LeAD: The LLM Enhanced Planning System Converged with End-to-end Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "A principal barrier to large-scale deployment of urban autonomous driving\nsystems lies in the prevalence of complex scenarios and edge cases. Existing\nsystems fail to effectively interpret semantic information within traffic\ncontexts and discern intentions of other participants, consequently generating\ndecisions misaligned with skilled drivers' reasoning patterns. We present LeAD,\na dual-rate autonomous driving architecture integrating imitation\nlearning-based end-to-end (E2E) frameworks with large language model (LLM)\naugmentation. The high-frequency E2E subsystem maintains real-time\nperception-planning-control cycles, while the low-frequency LLM module enhances\nscenario comprehension through multi-modal perception fusion with HD maps and\nderives optimal decisions via chain-of-thought (CoT) reasoning when baseline\nplanners encounter capability limitations. Our experimental evaluation in the\nCARLA Simulator demonstrates LeAD's superior handling of unconventional\nscenarios, achieving 71 points on Leaderboard V1 benchmark, with a route\ncompletion of 93%.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper title and abstract explicitly mention the use of Large Language Models (LLMs) in the context of autonomous driving, which inherently involves trajectory prediction and planning. The abstract describes a system integrating LLMs to enhance scenario comprehension and decision-making in complex driving scenarios. The end-to-end autonomous driving system also implies trajectory planning and control.", "keywords": ["Large Language Models", "LLM", "Autonomous Driving", "planning", "decision-making", "trajectory prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05283", "pdf": "https://arxiv.org/pdf/2507.05283", "abs": "https://arxiv.org/abs/2507.05283", "authors": ["Yue Wang", "Miao Zhou", "Guijing Huang", "Rui Zhuo", "Chao Yi", "Zhenliang Ma"], "title": "Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Pre-timed traffic signal control, commonly used for operating signalized\nintersections and coordinated arterials, requires tedious manual work for\nsignaling plan creating and updating. When the time-of-day or day-of-week plans\nare utilized, one intersection is often associated with multiple plans, leading\nto further repetitive manual plan parameter inputting. To enable a\nuser-friendly traffic signal control plan management process, this study\nproposes Chat2SPaT, a method to convert users' semi-structured and ambiguous\ndescriptions on the signal control plan to exact signal phase and timing (SPaT)\nresults, which could further be transformed into structured stage-based or\nring-based plans to interact with intelligent transportation system (ITS)\nsoftware and traffic signal controllers. With curated prompts, Chat2SPaT first\nleverages large language models' (LLMs) capability of understanding users' plan\ndescriptions and reformulate the plan as a combination of phase sequence and\nphase attribute results in the json format. Based on LLM outputs, python\nscripts are designed to locate phases in a cycle, address nuances of traffic\nsignal control, and finally assemble the complete traffic signal control plan.\nWithin a chat, the pipeline can be utilized iteratively to conduct further plan\nediting. Experiments show that Chat2SPaT can generate plans with an accuracy of\nover 94% for both English and Chinese cases, using a test dataset with over 300\nplan descriptions. As the first benchmark for evaluating LLMs' capability of\nunderstanding traffic signal control plan descriptions, Chat2SPaT provides an\neasy-to-use plan management pipeline for traffic practitioners and researchers,\nserving as a potential new building block for a more accurate and versatile\napplication of LLMs in the field of ITS. The source codes, prompts and test\ndataset are openly accessible at https://github.com/yuewangits/Chat2SPaT.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Large Language Models (LLMs) to automate traffic signal control plan management. While it doesn't directly address trajectory prediction, it operates within the broader context of Intelligent Transportation Systems (ITS) and leverages LLMs to interpret and generate traffic control plans. The connection to trajectory prediction is indirect, as traffic signal control influences vehicle trajectories, but it's not the primary focus. The paper explicitly mentions and utilizes LLMs.", "keywords": ["Large Language Models", "LLMs", "traffic signal control", "Intelligent Transportation System", "ITS"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05458", "pdf": "https://arxiv.org/pdf/2507.05458", "abs": "https://arxiv.org/abs/2507.05458", "authors": ["Yi-Shiuan Tung", "Bradley Hayes", "Alessandro Roncone"], "title": "CRED: Counterfactual Reasoning and Environment Design for Active Preference Learning", "categories": ["cs.RO"], "comment": null, "summary": "For effective real-world deployment, robots should adapt to human\npreferences, such as balancing distance, time, and safety in delivery routing.\nActive preference learning (APL) learns human reward functions by presenting\ntrajectories for ranking. However, existing methods often struggle to explore\nthe full trajectory space and fail to identify informative queries,\nparticularly in long-horizon tasks. We propose CRED, a trajectory generation\nmethod for APL that improves reward estimation by jointly optimizing\nenvironment design and trajectory selection. CRED \"imagines\" new scenarios\nthrough environment design and uses counterfactual reasoning -- by sampling\nrewards from its current belief and asking \"What if this reward were the true\npreference?\" -- to generate a diverse and informative set of trajectories for\nranking. Experiments in GridWorld and real-world navigation using OpenStreetMap\ndata show that CRED improves reward learning and generalizes effectively across\ndifferent environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u8f68\u8ff9\u751f\u6210\u548c\u504f\u597d\u5b66\u4e60\uff0c\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176counterfactual reasoning\u7684\u601d\u60f3\u53ef\u4ee5\u501f\u9274\u5230\u5927\u6a21\u578b\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u4f8b\u5982\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u751f\u6210\u4e0d\u540c\u7684\u8f68\u8ff9\uff0c\u7136\u540e\u4f7f\u7528counterfactual reasoning\u9009\u62e9\u6700\u4f18\u8f68\u8ff9\u3002", "keywords": ["trajectory generation", "active preference learning", "robot navigation", "counterfactual reasoning", "reward learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05419", "pdf": "https://arxiv.org/pdf/2507.05419", "abs": "https://arxiv.org/abs/2507.05419", "authors": ["Aliasghar Khani", "Arianna Rampini", "Bruno Roy", "Larasika Nadela", "Noa Kaplan", "Evan Atherton", "Derek Cheung", "Jacky Bibliowicz"], "title": "Motion Generation: A Survey of Generative Approaches and Benchmarks", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Motion generation, the task of synthesizing realistic motion sequences from\nvarious conditioning inputs, has become a central problem in computer vision,\ncomputer graphics, and robotics, with applications ranging from animation and\nvirtual agents to human-robot interaction. As the field has rapidly progressed\nwith the introduction of diverse modeling paradigms including GANs,\nautoencoders, autoregressive models, and diffusion-based techniques, each\napproach brings its own advantages and limitations. This growing diversity has\ncreated a need for a comprehensive and structured review that specifically\nexamines recent developments from the perspective of the generative approach\nemployed.\n  In this survey, we provide an in-depth categorization of motion generation\nmethods based on their underlying generative strategies. Our main focus is on\npapers published in top-tier venues since 2023, reflecting the most recent\nadvancements in the field. In addition, we analyze architectural principles,\nconditioning mechanisms, and generation settings, and compile a detailed\noverview of the evaluation metrics and datasets used across the literature. Our\nobjective is to enable clearer comparisons and identify open challenges,\nthereby offering a timely and foundational reference for researchers and\npractitioners navigating the rapidly evolving landscape of motion generation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper surveys motion generation techniques, which is related to trajectory prediction. While it doesn't explicitly mention large language models, the generative approaches discussed (GANs, autoencoders, autoregressive models, diffusion-based techniques) are sometimes used in conjunction with or inspired by LLMs. The connection is not direct, but the topic overlaps with the generative aspect of trajectory prediction.", "keywords": ["motion generation", "generative approaches", "trajectory prediction", "autoregressive models", "GANs", "autoencoders", "diffusion-based techniques"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05463", "pdf": "https://arxiv.org/pdf/2507.05463", "abs": "https://arxiv.org/abs/2507.05463", "authors": ["Md Zahid Hasan", "Guillermo Basulto-Elias", "Jun Ha Chang", "Sahuna Hallmark", "Matthew Rizzo", "Anuj Sharma", "Soumik Sarkar"], "title": "Driving as a Diagnostic Tool: Scenario-based Cognitive Assessment in Older Drivers From Driving Video", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 8 figures", "summary": "We introduce scenario-based cognitive status identification in older drivers\nfrom Naturalistic driving videos and large vision models. In recent times,\ncognitive decline, including Alzheimer's disease (AD) and mild cognitive\nimpairment (MCI), is often underdiagnosed due to the time-consuming and costly\nnature of current diagnostic methods. By analyzing real-world driving behavior\ncaptured through in-vehicle systems, this research aims to extract \"digital\nfingerprints\" that correlate with functional decline and clinical features of\nMCI and AD. Moreover, modern large vision models can draw meaningful insights\nfrom everyday driving patterns of older patients to early detect cognitive\ndecline. We propose a framework that uses large vision models and naturalistic\ndriving videos to analyze driver behavior, classify cognitive status and\npredict disease progression. We leverage the strong relationship between\nreal-world driving behavior as an observation of the current cognitive status\nof the drivers where the vehicle can be utilized as a \"diagnostic tool\". Our\nmethod identifies early warning signs of functional impairment, contributing to\nproactive intervention strategies. This work enhances early detection and\nsupports the development of scalable, non-invasive monitoring systems to\nmitigate the growing societal and economic burden of cognitive decline in the\naging population.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses large vision models to analyze driving videos for cognitive assessment. While it doesn't directly focus on trajectory prediction, the driving behavior analysis and the use of large vision models suggest a moderate relevance. The analysis of driving patterns could potentially involve trajectory analysis as a component, and the use of large vision models aligns with the large language model theme, although it's vision-focused.", "keywords": ["large vision models", "driving behavior", "cognitive assessment"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05710", "pdf": "https://arxiv.org/pdf/2507.05710", "abs": "https://arxiv.org/abs/2507.05710", "authors": ["Hyeongchan Ham", "Heejin Ahn"], "title": "DRO-EDL-MPC: Evidential Deep Learning-Based Distributionally Robust Model Predictive Control for Safe Autonomous Driving", "categories": ["cs.RO"], "comment": "11 pages; Project page can be found at https://dro-edl-mpc.github.io", "summary": "Safety is a critical concern in motion planning for autonomous vehicles.\nModern autonomous vehicles rely on neural network-based perception, but making\ncontrol decisions based on these inference results poses significant safety\nrisks due to inherent uncertainties. To address this challenge, we present a\ndistributionally robust optimization (DRO) framework that accounts for both\naleatoric and epistemic perception uncertainties using evidential deep learning\n(EDL). Our approach introduces a novel ambiguity set formulation based on\nevidential distributions that dynamically adjusts the conservativeness\naccording to perception confidence levels. We integrate this uncertainty-aware\nconstraint into model predictive control (MPC), proposing the DRO-EDL-MPC\nalgorithm with computational tractability for autonomous driving applications.\nValidation in the CARLA simulator demonstrates that our approach maintains\nefficiency under high perception confidence while enforcing conservative\nconstraints under low confidence.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on safe autonomous driving using Model Predictive Control (MPC) and incorporates uncertainty from neural network-based perception using evidential deep learning (EDL). While it doesn't explicitly use Large Language Models, it addresses trajectory prediction in the context of autonomous driving and utilizes deep learning for perception. The connection to trajectory prediction is strong, while the connection to Large Language Models is weak but present through the use of deep learning.", "keywords": ["trajectory prediction", "autonomous driving", "model predictive control", "deep learning", "motion planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05748", "pdf": "https://arxiv.org/pdf/2507.05748", "abs": "https://arxiv.org/abs/2507.05748", "authors": ["Bei Zhou", "Zhouheng Li", "Lei Xie", "Hongye Su", "Johannes Betz"], "title": "A Learning-based Planning and Control Framework for Inertia Drift Vehicles", "categories": ["cs.RO"], "comment": null, "summary": "Inertia drift is a transitional maneuver between two sustained drift stages\nin opposite directions, which provides valuable insights for navigating\nconsecutive sharp corners for autonomous racing.However, this can be a\nchallenging scenario for the drift controller to handle rapid transitions\nbetween opposing sideslip angles while maintaining accurate path tracking.\nMoreover, accurate drift control depends on a high-fidelity vehicle model to\nderive drift equilibrium points and predict vehicle states, but this is often\ncompromised by the strongly coupled longitudinal-lateral drift dynamics and\nunpredictable environmental variations. To address these challenges, this paper\nproposes a learning-based planning and control framework utilizing Bayesian\noptimization (BO), which develops a planning logic to ensure a smooth\ntransition and minimal velocity loss between inertia and sustained drift\nphases. BO is further employed to learn a performance-driven control policy\nthat mitigates modeling errors for enhanced system performance. Simulation\nresults on an 8-shape reference path demonstrate that the proposed framework\ncan achieve smooth and stable inertia drift through sharp corners.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on planning and control for autonomous vehicles, specifically inertia drift maneuvers. While it doesn't directly involve large language models, it touches upon trajectory prediction and control aspects within the context of vehicle dynamics and path tracking. The use of Bayesian Optimization for learning a control policy also hints at a data-driven approach, although not explicitly related to LLMs.", "keywords": ["trajectory prediction", "planning", "control", "autonomous vehicles", "path tracking"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05507", "pdf": "https://arxiv.org/pdf/2507.05507", "abs": "https://arxiv.org/abs/2507.05507", "authors": ["Godwin Badu-Marfo", "Bilal Farooq"], "title": "Dynamic Campus Origin-Destination Mobility Prediction using Graph Convolutional Neural Network on WiFi Logs", "categories": ["cs.LG"], "comment": null, "summary": "We present an integrated graph-based neural networks architecture for\npredicting campus buildings occupancy and inter-buildings movement at dynamic\ntemporal resolution that learns traffic flow patterns from Wi-Fi logs combined\nwith the usage schedules within the buildings. The relative traffic flows are\ndirectly estimated from the WiFi data without assuming the occupant behaviour\nor preferences while maintaining individual privacy. We formulate the problem\nas a data-driven graph structure represented by a set of nodes (representing\nbuildings), connected through a route of edges or links using a novel Graph\nConvolution plus LSTM Neural Network (GCLSTM) which has shown remarkable\nsuccess in modelling complex patterns. We describe the formulation, model\nestimation, interpretability and examine the relative performance of our\nproposed model. We also present an illustrative architecture of the models and\napply on real-world WiFi logs collected at the Toronto Metropolitan University\ncampus. The results of the experiments show that the integrated GCLSTM models\nsignificantly outperform traditional pedestrian flow estimators like the Multi\nLayer Perceptron (MLP) and Linear Regression.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on predicting movement between buildings using a Graph Convolutional LSTM network. While it's related to trajectory prediction in a broader sense (predicting movement patterns), it doesn't explicitly address trajectory prediction of individual agents (pedestrians, vehicles, etc.) and there's no mention of Large Language Models.", "keywords": ["trajectory prediction", "graph convolutional neural network", "LSTM", "movement prediction", "traffic flow"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.06149", "pdf": "https://arxiv.org/pdf/2507.06149", "abs": "https://arxiv.org/abs/2507.06149", "authors": ["Charles Champagne Cossette", "Taylor Scott Clawson", "Andrew Feit"], "title": "Fast and Accurate Collision Probability Estimation for Autonomous Vehicles using Adaptive Sigma-Point Sampling", "categories": ["cs.RO", "cs.AI", "cs.CG"], "comment": "8 pages, 6 figures", "summary": "A novel algorithm is presented for the estimation of collision probabilities\nbetween dynamic objects with uncertain trajectories, where the trajectories are\ngiven as a sequence of poses with Gaussian distributions. We propose an\nadaptive sigma-point sampling scheme, which ultimately produces a fast, simple\nalgorithm capable of estimating the collision probability with a median error\nof 3.5%, and a median runtime of 0.21ms, when measured on an Intel Xeon Gold\n6226R Processor. Importantly, the algorithm explicitly accounts for the\ncollision probability's temporal dependence, which is often neglected in prior\nwork and otherwise leads to an overestimation of the collision probability.\nFinally, the method is tested on a diverse set of relevant real-world\nscenarios, consisting of 400 6-second snippets of autonomous vehicle logs,\nwhere the accuracy and latency is rigorously evaluated.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on collision probability estimation for autonomous vehicles, which is closely related to trajectory prediction. It uses probabilistic methods to predict the future paths of vehicles. However, it doesn't mention or utilize large language models. Therefore, while relevant to trajectory prediction, its connection to large language models is non-existent, resulting in a moderate relevance score.", "keywords": ["trajectory prediction", "autonomous vehicles", "collision probability", "dynamic objects"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.06172", "pdf": "https://arxiv.org/pdf/2507.06172", "abs": "https://arxiv.org/abs/2507.06172", "authors": ["Kangle Yuan", "Atar Babgei", "Luca Romanello", "Hai-Nguyen Nguyen", "Ronald Clark", "Mirko Kovac", "Sophie F. Armanini", "Basaran Bahadir Kocer"], "title": "Learning Agile Tensile Perching for Aerial Robots from Demonstrations", "categories": ["cs.RO"], "comment": "9 pages, 9 figures", "summary": "Perching on structures such as trees, beams, and ledges is essential for\nextending the endurance of aerial robots by enabling energy conservation in\nstandby or observation modes. A tethered tensile perching mechanism offers a\nsimple, adaptable solution that can be retrofitted to existing robots and\naccommodates a variety of structure sizes and shapes. However, tethered tensile\nperching introduces significant modelling challenges which require precise\nmanagement of aerial robot dynamics, including the cases of tether slack &\ntension, and momentum transfer. Achieving smooth wrapping and secure anchoring\nby targeting a specific tether segment adds further complexity. In this work,\nwe present a novel trajectory framework for tethered tensile perching,\nutilizing reinforcement learning (RL) through the Soft Actor-Critic from\nDemonstrations (SACfD) algorithm. By incorporating both optimal and suboptimal\ndemonstrations, our approach enhances training efficiency and responsiveness,\nachieving precise control over position and velocity. This framework enables\nthe aerial robot to accurately target specific tether segments, facilitating\nreliable wrapping and secure anchoring. We validate our framework through\nextensive simulation and real-world experiments, and demonstrate effectiveness\nin achieving agile and reliable trajectory generation for tensile perching.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory generation for aerial robots, specifically for tensile perching. While it doesn't directly involve large language models, it does heavily rely on trajectory prediction and control, employing reinforcement learning for trajectory optimization. The connection to 'trajectory prediction' is strong due to the trajectory generation aspect, although it's within a specific robotic application. There's no mention of large language models.", "keywords": ["trajectory generation", "aerial robots", "reinforcement learning", "trajectory optimization"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05938", "pdf": "https://arxiv.org/pdf/2507.05938", "abs": "https://arxiv.org/abs/2507.05938", "authors": ["Yucheng Sheng", "Jiacheng Wang", "Xingyu Zhou", "Le Liang", "Hao Ye", "Shi Jin", "Geoffrey Ye Li"], "title": "A Wireless Foundation Model for Multi-Task Prediction", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "With the growing complexity and dynamics of the mobile communication\nnetworks, accurately predicting key system parameters, such as channel state\ninformation (CSI), user location, and network traffic, has become essential for\na wide range of physical (PHY)-layer and medium access control (MAC)-layer\ntasks. Although traditional deep learning (DL)-based methods have been widely\napplied to such prediction tasks, they often struggle to generalize across\ndifferent scenarios and tasks. In response, we propose a unified foundation\nmodel for multi-task prediction in wireless networks that supports diverse\nprediction intervals. The proposed model enforces univariate decomposition to\nunify heterogeneous tasks, encodes granularity for interval awareness, and uses\na causal Transformer backbone for accurate predictions. Additionally, we\nintroduce a patch masking strategy during training to support arbitrary input\nlengths. After trained on large-scale datasets, the proposed foundation model\ndemonstrates strong generalization to unseen scenarios and achieves zero-shot\nperformance on new tasks that surpass traditional full-shot baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u65e0\u7ebf\u7f51\u7edc\u4e2d\u591a\u4efb\u52a1\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\u3002\u867d\u7136\u5b83\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u63d0\u5230\u4e86\u7528\u6237\u4f4d\u7f6e\u9884\u6d4b\uff0c\u8fd9\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u660e\u786e\u63d0\u5230\u4e86\u201cfoundation model\u201d\uff0c\u8868\u660e\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["foundation model", "multi-task prediction", "user location", "Transformer"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05678", "pdf": "https://arxiv.org/pdf/2507.05678", "abs": "https://arxiv.org/abs/2507.05678", "authors": ["Yisu Zhang", "Chenjie Cao", "Chaohui Yu", "Jianke Zhu"], "title": "LiON-LoRA: Rethinking LoRA Fusion to Unify Controllable Spatial and Temporal Generation for Video Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Video Diffusion Models (VDMs) have demonstrated remarkable capabilities in\nsynthesizing realistic videos by learning from large-scale data. Although\nvanilla Low-Rank Adaptation (LoRA) can learn specific spatial or temporal\nmovement to driven VDMs with constrained data, achieving precise control over\nboth camera trajectories and object motion remains challenging due to the\nunstable fusion and non-linear scalability. To address these issues, we propose\nLiON-LoRA, a novel framework that rethinks LoRA fusion through three core\nprinciples: Linear scalability, Orthogonality, and Norm consistency. First, we\nanalyze the orthogonality of LoRA features in shallow VDM layers, enabling\ndecoupled low-level controllability. Second, norm consistency is enforced\nacross layers to stabilize fusion during complex camera motion combinations.\nThird, a controllable token is integrated into the diffusion transformer (DiT)\nto linearly adjust motion amplitudes for both cameras and objects with a\nmodified self-attention mechanism to ensure decoupled control. Additionally, we\nextend LiON-LoRA to temporal generation by leveraging static-camera videos,\nunifying spatial and temporal controllability. Experiments demonstrate that\nLiON-LoRA outperforms state-of-the-art methods in trajectory control accuracy\nand motion strength adjustment, achieving superior generalization with minimal\ntraining data. Project Page: https://fuchengsu.github.io/lionlora.github.io/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u4e8e\u89c6\u9891\u6269\u6563\u6a21\u578b\uff08VDMs\uff09\u4e2d\u53ef\u63a7\u7684\u65f6\u7a7a\u751f\u6210\uff0c\u7279\u522b\u662f\u76f8\u673a\u8f68\u8ff9\u548c\u7269\u4f53\u8fd0\u52a8\u7684\u63a7\u5236\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u4f20\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f46\u5176\u6838\u5fc3\u76ee\u6807\u662f\u63a7\u5236\u89c6\u9891\u4e2d\u7269\u4f53\u7684\u8fd0\u52a8\u8f68\u8ff9\u548c\u76f8\u673a\u8f68\u8ff9\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u8bba\u6587\u4f7f\u7528\u4e86LoRA\uff0c\u8fd9\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e38\u7528\u4e8e\u5927\u578b\u6a21\u578b\uff0c\u4f46\u8bba\u6587\u672c\u8eab\u5e76\u6ca1\u6709\u660e\u786e\u4f7f\u7528\u6216\u6784\u5efa\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u8bc4\u5206\u4e3a0.6\u3002", "keywords": ["trajectory control", "motion", "video diffusion models", "LoRA"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.05822", "pdf": "https://arxiv.org/pdf/2507.05822", "abs": "https://arxiv.org/abs/2507.05822", "authors": ["L'ea Dubois", "Klaus Schmidt", "Chengyu Wang", "Ji-Hoon Park", "Lin Wang", "Santiago Munoz"], "title": "Video Event Reasoning and Prediction by Fusing World Knowledge from LLMs with Vision Foundation Models", "categories": ["cs.CV", "CS", "I.2.10"], "comment": "22 pages, 4 figures", "summary": "Current video understanding models excel at recognizing \"what\" is happening\nbut fall short in high-level cognitive tasks like causal reasoning and future\nprediction, a limitation rooted in their lack of commonsense world knowledge.\nTo bridge this cognitive gap, we propose a novel framework that synergistically\nfuses a powerful Vision Foundation Model (VFM) for deep visual perception with\na Large Language Model (LLM) serving as a knowledge-driven reasoning core. Our\nkey technical innovation is a sophisticated fusion module, inspired by the\nQ-Former architecture, which distills complex spatiotemporal and object-centric\nvisual features into a concise, language-aligned representation. This enables\nthe LLM to effectively ground its inferential processes in direct visual\nevidence. The model is trained via a two-stage strategy, beginning with\nlarge-scale alignment pre-training on video-text data, followed by targeted\ninstruction fine-tuning on a curated dataset designed to elicit advanced\nreasoning and prediction skills. Extensive experiments demonstrate that our\nmodel achieves state-of-the-art performance on multiple challenging benchmarks.\nNotably, it exhibits remarkable zero-shot generalization to unseen reasoning\ntasks, and our in-depth ablation studies validate the critical contribution of\neach architectural component. This work pushes the boundary of machine\nperception from simple recognition towards genuine cognitive understanding,\npaving the way for more intelligent and capable AI systems in robotics,\nhuman-computer interaction, and beyond.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on video event reasoning and prediction by fusing vision foundation models with large language models. While it mentions 'future prediction,' the primary focus is on reasoning about events in videos rather than predicting trajectories of objects. However, the use of LLMs and the 'prediction' aspect justify a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "Vision Foundation Model", "prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
