{"id": "2510.14702", "pdf": "https://arxiv.org/pdf/2510.14702", "abs": "https://arxiv.org/abs/2510.14702", "authors": ["Penglong Zhai", "Jie Li", "Fanyi Di", "Yue Liu", "Yifang Yuan", "Jie Huang", "Peng Wu", "Sicong Wang", "Mingyang Yin", "Tingting Hu", "Yao Xu", "Xin Li"], "title": "Cognitive-Aligned Spatio-Temporal Large Language Models For Next Point-of-Interest Prediction", "categories": ["cs.AI"], "comment": "12 pages, 5 figures", "summary": "The next point-of-interest (POI) recommendation task aims to predict the\nusers' immediate next destinations based on their preferences and historical\ncheck-ins, holding significant value in location-based services. Recently,\nlarge language models (LLMs) have shown great potential in recommender systems,\nwhich treat the next POI prediction in a generative manner. However, these\nLLMs, pretrained primarily on vast corpora of unstructured text, lack the\nnative understanding of structured geographical entities and sequential\nmobility patterns required for next POI prediction tasks. Moreover, in\nindustrial-scale POI prediction applications, incorporating world knowledge and\nalignment of human cognition, such as seasons, weather conditions, holidays,\nand users' profiles (such as habits, occupation, and preferences), can enhance\nthe user experience while improving recommendation performance. To address\nthese issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a\nframework employing natural language as an interface, allowing for the\nincorporation of world knowledge, spatio-temporal trajectory patterns,\nprofiles, and situational information. Specifically, CoAST mainly comprises of\n2 stages: (1) Recommendation Knowledge Acquisition through continued\npretraining on the enriched spatial-temporal trajectory data of the\ndesensitized users; (2) Cognitive Alignment to align cognitive judgments with\nhuman preferences using enriched training data through Supervised Fine-Tuning\n(SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline\nexperiments on various real-world datasets and online experiments deployed in\n\"Guess Where You Go\" of AMAP App homepage demonstrate the effectiveness of\nCoAST.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884cnext Point-of-Interest (POI) prediction\uff0c\u800cPOI prediction\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u3002\u8bba\u6587\u91cd\u70b9\u5728\u4e8e\u5982\u4f55\u4f7fLLM\u66f4\u597d\u5730\u7406\u89e3\u65f6\u7a7a\u4fe1\u606f\u548c\u4eba\u7c7b\u8ba4\u77e5\uff0c\u4ece\u800c\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "large language models", "LLMs", "next Point-of-Interest prediction", "POI prediction", "spatial-temporal trajectory data", "recommender systems"]}}
{"id": "2510.14615", "pdf": "https://arxiv.org/pdf/2510.14615", "abs": "https://arxiv.org/abs/2510.14615", "authors": ["Edward Sandra", "Lander Vanroye", "Dries Dirckx", "Ruben Cartuyvels", "Jan Swevers", "Wilm Decr\u00e9"], "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models", "categories": ["cs.RO"], "comment": "This paper has been submitted and has not yet been peer reviewed or\n  accepted for publication", "summary": "Classical methods in robot motion planning, such as sampling-based and\noptimization-based methods, often struggle with scalability towards\nhigher-dimensional state spaces and complex environments. Diffusion models,\nknown for their capability to learn complex, high-dimensional and multi-modal\ndata distributions, provide a promising alternative when applied to motion\nplanning problems and have already shown interesting results. However, most of\nthe current approaches train their model for a single environment, limiting\ntheir generalization to environments not seen during training. The techniques\nthat do train a model for multiple environments rely on a specific camera to\nprovide the model with the necessary environmental information and therefore\nalways require that sensor. To effectively adapt to diverse scenarios without\nthe need for retraining, this research proposes Context-Aware Motion Planning\nDiffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic\ndiffusion model, conditioned on sensor-agnostic contextual information. An\nattention mechanism, integrated in the well-known U-Net architecture,\nconditions the model on an arbitrary number of contextual parameters. CAMPD is\nevaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art\napproaches on real-world tasks, showing its ability to generalize to unseen\nenvironments and generate high-quality, multi-modal trajectories, at a fraction\nof the time required by existing methods.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\uff0c\u8fd9\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u5f62\u5f0f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u591a\u6a21\u6001\u8f68\u8ff9\uff0c\u5e76\u5f3a\u8c03\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u4e9b\u90fd\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u3002", "keywords": ["motion planning", "diffusion models", "trajectory", "multi-modal", "generalization"]}}
{"id": "2510.14827", "pdf": "https://arxiv.org/pdf/2510.14827", "abs": "https://arxiv.org/abs/2510.14827", "authors": ["Yufei Zhu", "Shih-Min Yang", "Andrey Rudenko", "Tomasz P. Kucner", "Achim J. Lilienthal", "Martin Magnusson"], "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping", "categories": ["cs.RO"], "comment": null, "summary": "Safe and efficient robot operation in complex human environments can benefit\nfrom good models of site-specific motion patterns. Maps of Dynamics (MoDs)\nprovide such models by encoding statistical motion patterns in a map, but\nexisting representations use discrete spatial sampling and typically require\ncostly offline construction. We propose a continuous spatio-temporal MoD\nrepresentation based on implicit neural functions that directly map coordinates\nto the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the\nneed for discretization and imputation for unevenly sampled regions, enabling\nsmooth generalization across both space and time. Evaluated on a large public\ndataset with long-term real-world people tracking data, our method achieves\nbetter accuracy of motion representation and smoother velocity distributions in\nsparse regions while still being computationally efficient, compared to\navailable baselines. The proposed approach demonstrates a powerful and\nefficient way of modeling complex human motion patterns.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u7a7a\u8fd0\u52a8\u6a21\u5f0f\u7684\u5efa\u6a21\uff0c\u7279\u522b\u662f\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u51fd\u6570\u6765\u8868\u793a\u8fd0\u52a8\u6a21\u5f0f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u7684\u5185\u5bb9\uff0c\u4f8b\u5982\u884c\u4eba\u8fd0\u52a8\u6a21\u5f0f\u7684\u5efa\u6a21\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["motion mapping", "spatio-temporal", "motion patterns", "trajectory prediction", "human motion"]}}
{"id": "2510.14893", "pdf": "https://arxiv.org/pdf/2510.14893", "abs": "https://arxiv.org/abs/2510.14893", "authors": ["Helene J. Levy", "Brett T. Lopez"], "title": "STITCHER: Constrained Trajectory Planning in Known Environments with Real-Time Motion Primitive Search", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Modern trajectory\nplanning techniques primarily use numerical optimization, as they enable the\nsystematic computation of high-quality, expressive trajectories that satisfy\nvarious constraints. However, stringent requirements on computation time and\nthe risk of numerical instability can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework called STITCHER that stitches short trajectory segments\ntogether with graph search to compute long-range, expressive, and near-optimal\ntrajectories in real-time. STITCHER outperforms modern optimization-based\nplanners through our innovative planning architecture and several algorithmic\ndevelopments that make real-time planning possible. Extensive simulation\ntesting is performed to analyze the algorithmic components that make up\nSTITCHER, along with a thorough comparison with two state-of-the-art\noptimization planners. Simulation tests show that safe trajectories can be\ncreated within a few milliseconds for paths that span the entirety of two 50 m\nx 50 m environments. Hardware tests with a custom quadrotor verify that\nSTITCHER can produce trackable paths in real-time while respecting nonconvex\nconstraints, such as limits on tilt angle and motor forces, which are otherwise\nhard to include in optimization-based planners.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u4f46\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u5173\u3002\u8bba\u6587\u8ba8\u8bba\u4e86\u5728\u5df2\u77e5\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u5b9e\u65f6\u8fd0\u52a8\u57fa\u5143\u641c\u7d22\u8fdb\u884c\u7ea6\u675f\u8f68\u8ff9\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u7814\u7a76\u3002", "keywords": ["trajectory planning", "real-time motion primitive search", "trajectory segments", "collision-free", "trajectory"]}}
{"id": "2510.14000", "pdf": "https://arxiv.org/pdf/2510.14000", "abs": "https://arxiv.org/abs/2510.14000", "authors": ["Mingyang Jiang", "Yueyuan Li", "Jiaru Zhang", "Songan Zhang", "Ming Yang"], "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking", "categories": ["cs.RO"], "comment": null, "summary": "The growing demand for parking has increased the need for automated parking\nplanning methods that can operate reliably in confined spaces. In restricted\nand complex environments, high-precision maneuvers are required to achieve a\nhigh success rate in planning, yet existing approaches often rely on explicit\naction modeling, which faces challenges when accurately modeling the optimal\naction distribution. In this paper, we propose DRIP, a diffusion-refined\nplanner anchored in reinforcement learning (RL) prior action distribution, in\nwhich an RL-pretrained policy provides prior action distributions to regularize\nthe diffusion training process. During the inference phase the denoising\nprocess refines these coarse priors into more precise action distributions. By\nsteering the denoising trajectory through the reinforcement learning prior\ndistribution during training, the diffusion model inherits a well-informed\ninitialization, resulting in more accurate action modeling, a higher planning\nsuccess rate, and reduced inference steps. We evaluate our approach across\nparking scenarios with varying degrees of spatial constraints. Experimental\nresults demonstrate that our method significantly improves planning performance\nin confined-space parking environments while maintaining strong generalization\nin common scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on automated parking planning, which is a type of trajectory prediction/planning for vehicles. It uses reinforcement learning and a diffusion model, but there's no mention of large language models. The connection to trajectory prediction is clear, but the absence of LLMs lowers the relevance score.", "keywords": ["trajectory prediction", "parking planning", "diffusion model", "reinforcement learning", "action planning"]}}
{"id": "2510.14063", "pdf": "https://arxiv.org/pdf/2510.14063", "abs": "https://arxiv.org/abs/2510.14063", "authors": ["Nan Li", "Jiming Ren", "Haris Miller", "Samuel Coogan", "Karen M. Feigh", "Ye Zhao"], "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming", "categories": ["cs.RO"], "comment": "16 pages, 11 figures, 4 tables", "summary": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing\nattention but remains challenging in terms of scalability, spatial reasoning,\nand adaptability in obstacle-rich environments. To address these challenges, we\npropose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for\nHeterogeneous Robot Teaming, which advances MATP by introducing a novel\nobstacle-aware strategy for task assignment. First, we develop an adaptive\nHalton sequence map, the first known application of Halton sampling with\nobstacle-aware adaptation in MATP, which adjusts sampling density based on\nobstacle distribution. Second, we propose a cluster-auction-selection framework\nthat integrates obstacle-aware clustering with weighted auctions and\nintra-cluster task selection. These mechanisms jointly enable effective\ncoordination among heterogeneous robots while maintaining scalability and\nnear-optimal allocation performance. In addition, our framework leverages an\nLLM to interpret human instructions and directly guide the planner in real\ntime. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in\ntask assignment quality, scalability, adaptability to dynamic changes, and\noverall execution performance compared to state-of-the-art MATP baselines. A\nproject website is available at https://llm-oath.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-agent task assignment and planning for heterogeneous robots, which is related to trajectory prediction in the context of robot navigation and path planning. The use of an LLM to interpret human instructions and guide the planner further increases the relevance, indicating a connection between large language models and robot control/planning, although trajectory prediction is not explicitly mentioned as the primary focus.", "keywords": ["LLM", "task assignment", "planning", "robot", "obstacle-aware"]}}
{"id": "2510.14293", "pdf": "https://arxiv.org/pdf/2510.14293", "abs": "https://arxiv.org/abs/2510.14293", "authors": ["Yushi Du", "Yixuan Li", "Baoxiong Jia", "Yutang Lin", "Pei Zhou", "Wei Liang", "Yanchao Yang", "Siyuan Huang"], "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Human-humanoid collaboration shows significant promise for applications in\nhealthcare, domestic assistance, and manufacturing. While compliant robot-human\ncollaboration has been extensively developed for robotic arms, enabling\ncompliant human-humanoid collaboration remains largely unexplored due to\nhumanoids' complex whole-body dynamics. In this paper, we propose a\nproprioception-only reinforcement learning approach, COLA, that combines leader\nand follower behaviors within a single policy. The model is trained in a\nclosed-loop environment with dynamic object interactions to predict object\nmotion patterns and human intentions implicitly, enabling compliant\ncollaboration to maintain load balance through coordinated trajectory planning.\nWe evaluate our approach through comprehensive simulator and real-world\nexperiments on collaborative carrying tasks, demonstrating the effectiveness,\ngeneralization, and robustness of our model across various terrains and\nobjects. Simulation experiments demonstrate that our model reduces human effort\nby 24.7%. compared to baseline approaches while maintaining object stability.\nReal-world experiments validate robust collaborative carrying across different\nobject types (boxes, desks, stretchers, etc.) and movement patterns\n(straight-line, turning, slope climbing). Human user studies with 23\nparticipants confirm an average improvement of 27.4% compared to baseline\nmodels. Our method enables compliant human-humanoid collaborative carrying\nwithout requiring external sensors or complex interaction models, offering a\npractical solution for real-world deployment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on human-humanoid collaboration for object carrying, which involves trajectory planning and prediction of object motion and human intentions. While it doesn't explicitly mention large language models, the implicit prediction of human intentions and object motion patterns can be seen as a form of trajectory prediction. The use of reinforcement learning to achieve coordinated trajectory planning also contributes to the relevance. However, the absence of any mention of LLMs lowers the overall score.", "keywords": ["trajectory planning", "object motion patterns", "human intentions", "reinforcement learning", "prediction"]}}
{"id": "2510.14300", "pdf": "https://arxiv.org/pdf/2510.14300", "abs": "https://arxiv.org/abs/2510.14300", "authors": ["Weijie Shen", "Yitian Liu", "Yuhao Wu", "Zhixuan Liang", "Sijia Gu", "Dehui Wang", "Tian Nian", "Lei Xu", "Yusen Qin", "Jiangmiao Pang", "Xinping Guan", "Xiaokang Yang", "Yao Mu"], "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models are experiencing rapid development and\ndemonstrating promising capabilities in robotic manipulation tasks. However,\nscaling up VLA models presents several critical challenges: (1) Training new\nVLA models from scratch demands substantial computational resources and\nextensive datasets. Given the current scarcity of robot data, it becomes\nparticularly valuable to fully leverage well-pretrained VLA model weights\nduring the scaling process. (2) Real-time control requires carefully balancing\nmodel capacity with computational efficiency. To address these challenges, We\npropose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits\npretrained weights from dense VLA models, and scales up the action expert by\nsubstituting the feedforward layers into sparsely activated MoE layers. AdaMoE\nemploys a decoupling technique that decouples expert selection from expert\nweighting through an independent scale adapter working alongside the\ntraditional router. This enables experts to be selected based on task relevance\nwhile contributing with independently controlled weights, allowing\ncollaborative expert utilization rather than winner-takes-all dynamics. Our\napproach demonstrates that expertise need not monopolize. Instead, through\ncollaborative expert utilization, we can achieve superior performance while\nmaintaining computational efficiency. AdaMoE consistently outperforms the\nbaseline model across key benchmarks, delivering performance gains of 1.8% on\nLIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement\nin real-world experiments validates its practical effectiveness for robotic\nmanipulation tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8Vision-Language-Action (VLA)\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2aMoE\u67b6\u6784\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u901a\u5e38\u6d89\u53ca\u5bf9\u7269\u4f53\u8fd0\u52a8\u8f68\u8ff9\u7684\u9884\u6d4b\u548c\u63a7\u5236\u3002\u6b64\u5916\uff0c\u8bba\u6587\u63d0\u5230\u4e86VLA\u6a21\u578b\uff0c\u5c5e\u4e8e\u5927\u6a21\u578b\u8303\u7574\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language-Action models", "VLA models", "Mixture-of-Experts", "MoE", "robot manipulation", "foundation models"]}}
{"id": "2510.14627", "pdf": "https://arxiv.org/pdf/2510.14627", "abs": "https://arxiv.org/abs/2510.14627", "authors": ["Yao Zhong", "Hanzhi Chen", "Simon Schaefer", "Anran Zhang", "Stefan Leutenegger"], "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Robots are expected to serve as intelligent assistants, helping humans with\neveryday household organization. A central challenge in this setting is the\ntask of object placement, which requires reasoning about both semantic\npreferences (e.g., common-sense object relations) and geometric feasibility\n(e.g., collision avoidance). We present GOPLA, a hierarchical framework that\nlearns generalizable object placement from augmented human demonstrations. A\nmulti-modal large language model translates human instructions and visual\ninputs into structured plans that specify pairwise object relationships. These\nplans are then converted into 3D affordance maps with geometric common sense by\na spatial mapper, while a diffusion-based planner generates placement poses\nguided by test-time costs, considering multi-plan distributions and collision\navoidance. To overcome data scarcity, we introduce a scalable pipeline that\nexpands human placement demonstrations into diverse synthetic training data.\nExtensive experiments show that our approach improves placement success rates\nby 30.04 percentage points over the runner-up, evaluated on positioning\naccuracy and physical plausibility, demonstrating strong generalization across\na wide range of real-world robotic placement scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a large language model to translate human instructions into structured plans for object placement. While not directly about trajectory prediction, the \"spatial mapper\" and \"diffusion-based planner\" components, along with the consideration of geometric feasibility and collision avoidance, suggest a tangential relationship. The use of a large language model increases the relevance.", "keywords": ["large language model", "object placement", "diffusion-based planner", "collision avoidance"]}}
{"id": "2510.14319", "pdf": "https://arxiv.org/pdf/2510.14319", "abs": "https://arxiv.org/abs/2510.14319", "authors": ["Xu Shen", "Qi Zhang", "Song Wang", "Zhen Tan", "Xinyu Zhao", "Laura Yao", "Vaishnav Tadiparthi", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi Pari", "Kwonjoon Lee", "Tianlong Chen"], "title": "Metacognitive Self-Correction for Multi-Agent System via Prototype-Guided Next-Execution Reconstruction", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Model based multi-agent systems (MAS) excel at collaborative\nproblem solving but remain brittle to cascading errors: a single faulty step\ncan propagate across agents and disrupt the trajectory. In this paper, we\npresent MASC, a metacognitive framework that endows MAS with real-time,\nunsupervised, step-level error detection and self-correction. MASC rethinks\ndetection as history-conditioned anomaly scoring via two complementary designs:\n(1) Next-Execution Reconstruction, which predicts the embedding of the next\nstep from the query and interaction history to capture causal consistency, and\n(2) Prototype-Guided Enhancement, which learns a prototype prior over\nnormal-step embeddings and uses it to stabilize reconstruction and anomaly\nscoring under sparse context (e.g., early steps). When an anomaly step is\nflagged, MASC triggers a correction agent to revise the acting agent's output\nbefore information flows downstream. On the Who&When benchmark, MASC\nconsistently outperforms all baselines, improving step-level error detection by\nup to 8.47% AUC-ROC ; When plugged into diverse MAS frameworks, it delivers\nconsistent end-to-end gains across architectures, confirming that our\nmetacognitive monitoring and targeted correction can mitigate error propagation\nwith minimal overhead.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses multi-agent systems and uses large language models. While it focuses on correcting errors in the execution trajectory of these systems, it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting the future path of a moving object). The 'trajectory' refers to the sequence of actions in a multi-agent system.", "keywords": ["Large Language Model", "multi-agent systems", "execution trajectory"]}}
{"id": "2510.14677", "pdf": "https://arxiv.org/pdf/2510.14677", "abs": "https://arxiv.org/abs/2510.14677", "authors": ["Steffen Hagedorn", "Luka Donkov", "Aron Distelzweig", "Alexandru P. Condurache"], "title": "When Planners Meet Reality: How Learned, Reactive Traffic Agents Shift nuPlan Benchmarks", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Planner evaluation in closed-loop simulation often uses rule-based traffic\nagents, whose simplistic and passive behavior can hide planner deficiencies and\nbias rankings. Widely used IDM agents simply follow a lead vehicle and cannot\nreact to vehicles in adjacent lanes, hindering tests of complex interaction\ncapabilities. We address this issue by integrating the state-of-the-art learned\ntraffic agent model SMART into nuPlan. Thus, we are the first to evaluate\nplanners under more realistic conditions and quantify how conclusions shift\nwhen narrowing the sim-to-real gap. Our analysis covers 14 recent planners and\nestablished baselines and shows that IDM-based simulation overestimates\nplanning performance: nearly all scores deteriorate. In contrast, many planners\ninteract better than previously assumed and even improve in multi-lane,\ninteraction-heavy scenarios like lane changes or turns. Methods trained in\nclosed-loop demonstrate the best and most stable driving performance. However,\nwhen reaching their limits in augmented edge-case scenarios, all learned\nplanners degrade abruptly, whereas rule-based planners maintain reasonable\nbasic behavior. Based on our results, we suggest SMART-reactive simulation as a\nnew standard closed-loop benchmark in nuPlan and release the SMART agents as a\ndrop-in alternative to IDM at https://github.com/shgd95/InteractiveClosedLoop.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c4\u5212\u5668\u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u4ea4\u901a\u667a\u80fd\u4f53\u6a21\u578bSMART\u6765\u6a21\u62df\u66f4\u771f\u5b9e\u7684\u4ea4\u901a\u573a\u666f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u5230\u4e86\u5b66\u4e60\u5230\u7684\u4ea4\u901a\u667a\u80fd\u4f53\u6a21\u578b\uff0c\u5e76\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u8f66\u8f86\u884c\u4e3a\u5efa\u6a21\u76f8\u5173\uff0c\u56e0\u6b64\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["traffic agents", "learned traffic agent model", "planners", "closed-loop simulation", "planning performance"]}}
{"id": "2510.14952", "pdf": "https://arxiv.org/pdf/2510.14952", "abs": "https://arxiv.org/abs/2510.14952", "authors": ["Zhe Li", "Cheng Chi", "Yangyang Wei", "Boan Zhu", "Yibo Peng", "Tao Huang", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang", "Chang Xu"], "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Natural language offers a natural interface for humanoid robots, but existing\nlanguage-guided humanoid locomotion pipelines remain cumbersome and unreliable.\nThey typically decode human motion, retarget it to robot morphology, and then\ntrack it with a physics-based controller. However, this multi-stage process is\nprone to cumulative errors, introduces high latency, and yields weak coupling\nbetween semantics and control. These limitations call for a more direct pathway\nfrom language to action, one that eliminates fragile intermediate stages.\nTherefore, we present RoboGhost, a retargeting-free framework that directly\nconditions humanoid policies on language-grounded motion latents. By bypassing\nexplicit motion decoding and retargeting, RoboGhost enables a diffusion-based\npolicy to denoise executable actions directly from noise, preserving semantic\nintent and supporting fast, reactive control. A hybrid causal\ntransformer-diffusion motion generator further ensures long-horizon consistency\nwhile maintaining stability and diversity, yielding rich latent representations\nfor precise humanoid behavior. Extensive experiments demonstrate that RoboGhost\nsubstantially reduces deployment latency, improves success rates and tracking\naccuracy, and produces smooth, semantically aligned locomotion on real\nhumanoids. Beyond text, the framework naturally extends to other modalities\nsuch as images, audio, and music, providing a general foundation for\nvision-language-action humanoid systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u8bed\u8a00\u6765\u5f15\u5bfc\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u6d89\u53ca\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\uff0c\u4f46\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\u6216\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u3002\u8bba\u6587\u91cd\u70b9\u5728\u4e8e\u7ed5\u8fc7\u4e2d\u95f4\u6b65\u9aa4\uff0c\u76f4\u63a5\u4ece\u8bed\u8a00\u5230\u52a8\u4f5c\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u8fd0\u52a8\u3002\u867d\u7136\u4e0e\u8fd0\u52a8\u76f8\u5173\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u5173\u8054\u6027\u8f83\u5f31\uff0c\u4e0e\u5927\u6a21\u578b\u7684\u5173\u8054\u4e3b\u8981\u4f53\u73b0\u5728\u4f7f\u7528\u4e86transformer\u3002", "keywords": ["language", "humanoid locomotion", "diffusion", "transformer", "motion control"]}}
{"id": "2509.26255", "pdf": "https://arxiv.org/pdf/2509.26255", "abs": "https://arxiv.org/abs/2509.26255", "authors": ["Yichao Liang", "Dat Nguyen", "Cambridge Yang", "Tianyang Li", "Joshua B. Tenenbaum", "Carl Edward Rasmussen", "Adrian Weller", "Zenna Tavares", "Tom Silver", "Kevin Ellis"], "title": "ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot Planning", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "41 pages. The last two authors contributed equally in co-advising", "summary": "Long-horizon embodied planning is challenging because the world does not only\nchange through an agent's actions: exogenous processes (e.g., water heating,\ndominoes cascading) unfold concurrently with the agent's actions. We propose a\nframework for abstract world models that jointly learns (i) symbolic state\nrepresentations and (ii) causal processes for both endogenous actions and\nexogenous mechanisms. Each causal process models the time course of a\nstochastic cause-effect relation. We learn these world models from limited data\nvia variational Bayesian inference combined with LLM proposals. Across five\nsimulated tabletop robotics environments, the learned models enable fast\nplanning that generalizes to held-out tasks with more objects and more complex\ngoals, outperforming a range of baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper describes a framework for learning abstract world models for robot planning. It uses LLMs for generating proposals in the learning process. While the overall topic is robot planning, which can involve trajectory prediction, the main focus is on learning causal processes and symbolic representations. The use of LLMs is a significant factor for relevance, but the connection to trajectory prediction is not explicit.", "keywords": ["LLM", "world models", "robot planning", "causal processes"]}}
{"id": "2510.14828", "pdf": "https://arxiv.org/pdf/2510.14828", "abs": "https://arxiv.org/abs/2510.14828", "authors": ["Jinrui Liu", "Bingyan Nie", "Boyu Li", "Yaran Chen", "Yuze Wang", "Shunsen He", "Haoran Li"], "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Improving the reasoning capabilities of embodied agents is crucial for robots\nto complete complex human instructions in long-view manipulation tasks\nsuccessfully. Despite the success of large language models and vision language\nmodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continue\nfacing challenges in performing long-horizon manipulation tasks in complex\nreal-world environments, owing to their restricted common sense and reasoning\ncapabilities. Considering that aligning general-purpose vision language models\nto robotic planning tasks via supervised fine-tuning suffers from poor\ngeneralization and insufficient physical understanding, we propose RoboGPT-R1,\na two-stage fine-tuning framework for embodied planning. In this framework,\nsupervised training acquires foundational knowledge through expert sequences,\nfollowed by RL to address the model's shortcomings in visual-spatial\nunderstanding and reasoning. To achieve physical understanding and action\nsequence consistency in multi-step reasoning tasks, we design a rule-based\nreward function that simultaneously considers long-horizon performance and\naction constraint in the environment. The reasoning model, trained on\nQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,\nby 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the\nEmbodiedBench benchmark.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on robot planning and uses a large language model (Qwen2.5-VL-3B) for reasoning. While not directly addressing trajectory prediction, the planning aspect can involve generating sequences of actions that implicitly define a trajectory. The use of large language models is a key aspect. However, the primary focus is on manipulation tasks rather than explicit trajectory prediction.", "keywords": ["large language models", "LLMs", "robot planning", "reasoning", "Qwen2.5-VL-3B", "embodied agents"]}}
