{"id": "2511.04464", "pdf": "https://arxiv.org/pdf/2511.04464", "abs": "https://arxiv.org/abs/2511.04464", "authors": ["Carnot Braun", "Rafael O. Jarczewski", "Gabriel U. Talasso", "Leandro A. Villas", "Allan M. de Souza"], "title": "Beyond Shortest Path: Agentic Vehicular Routing with Semantic Context", "categories": ["cs.AI"], "comment": null, "summary": "Traditional vehicle routing systems efficiently optimize singular metrics\nlike time or distance, and when considering multiple metrics, they need more\nprocesses to optimize . However, they lack the capability to interpret and\nintegrate the complex, semantic, and dynamic contexts of human drivers, such as\nmulti-step tasks, situational constraints, or urgent needs. This paper\nintroduces and evaluates PAVe (Personalized Agentic Vehicular Routing), a\nhybrid agentic assistant designed to augment classical pathfinding algorithms\nwith contextual reasoning. Our approach employs a Large Language Model (LLM)\nagent that operates on a candidate set of routes generated by a multi-objective\n(time, CO2) Dijkstra algorithm. The agent evaluates these options against\nuser-provided tasks, preferences, and avoidance rules by leveraging a\npre-processed geospatial cache of urban Points of Interest (POIs). In a\nbenchmark of realistic urban scenarios, PAVe successfully used complex user\nintent into appropriate route modifications, achieving over 88% accuracy in its\ninitial route selections with a local model. We conclude that combining\nclassical routing algorithms with an LLM-based semantic reasoning layer is a\nrobust and effective approach for creating personalized, adaptive, and scalable\nsolutions for urban mobility optimization.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper directly combines vehicular routing (a form of trajectory prediction) with a Large Language Model (LLM) to create a personalized routing assistant. It leverages LLMs for semantic reasoning to augment classical pathfinding algorithms, making it highly relevant to both trajectory prediction and large language models.", "keywords": ["vehicular routing", "trajectory prediction", "Large Language Model", "LLM", "pathfinding", "semantic reasoning", "urban mobility"]}}
{"id": "2511.04032", "pdf": "https://arxiv.org/pdf/2511.04032", "abs": "https://arxiv.org/abs/2511.04032", "authors": ["Divya Pathak", "Harshit Kumar", "Anuska Roy", "Felix George", "Mudit Verma", "Pratibha Moogi"], "title": "Detecting Silent Failures in Multi-Agentic AI Trajectories", "categories": ["cs.AI"], "comment": null, "summary": "Multi-Agentic AI systems, powered by large language models (LLMs), are\ninherently non-deterministic and prone to silent failures such as drift,\ncycles, and missing details in outputs, which are difficult to detect. We\nintroduce the task of anomaly detection in agentic trajectories to identify\nthese failures and present a dataset curation pipeline that captures user\nbehavior, agent non-determinism, and LLM variation. Using this pipeline, we\ncurate and label two benchmark datasets comprising \\textbf{4,275 and 894}\ntrajectories from Multi-Agentic AI systems. Benchmarking anomaly detection\nmethods on these datasets, we show that supervised (XGBoost) and\nsemi-supervised (SVDD) approaches perform comparably, achieving accuracies up\nto 98% and 96%, respectively. This work provides the first systematic study of\nanomaly detection in Multi-Agentic AI systems, offering datasets, benchmarks,\nand insights to guide future research.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper focuses on detecting failures in multi-agentic AI systems powered by large language models. The concept of \"trajectories\" in the title and abstract, while not directly referring to movement trajectories, implies a sequence of states or actions taken by the AI agents. The paper explicitly mentions large language models (LLMs) and multi-agentic AI systems, making it relevant to both trajectory-like sequences and large language models. The core task of anomaly detection in \"agentic trajectories\" further strengthens the relevance.", "keywords": ["large language models", "LLMs", "trajectories", "multi-agentic AI systems", "anomaly detection"]}}
{"id": "2511.04664", "pdf": "https://arxiv.org/pdf/2511.04664", "abs": "https://arxiv.org/abs/2511.04664", "authors": ["Phat Nguyen", "Erfan Aasi", "Shiva Sreeram", "Guy Rosman", "Andrew Silva", "Sertac Karaman", "Daniela Rus"], "title": "SAFe-Copilot: Unified Shared Autonomy Framework", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous driving systems remain brittle in rare, ambiguous, and\nout-of-distribution scenarios, where human driver succeed through contextual\nreasoning. Shared autonomy has emerged as a promising approach to mitigate such\nfailures by incorporating human input when autonomy is uncertain. However, most\nexisting methods restrict arbitration to low-level trajectories, which\nrepresent only geometric paths and therefore fail to preserve the underlying\ndriving intent. We propose a unified shared autonomy framework that integrates\nhuman input and autonomous planners at a higher level of abstraction. Our\nmethod leverages Vision Language Models (VLMs) to infer driver intent from\nmulti-modal cues -- such as driver actions and environmental context -- and to\nsynthesize coherent strategies that mediate between human and autonomous\ncontrol. We first study the framework in a mock-human setting, where it\nachieves perfect recall alongside high accuracy and precision. A human-subject\nsurvey further shows strong alignment, with participants agreeing with\narbitration outcomes in 92% of cases. Finally, evaluation on the Bench2Drive\nbenchmark demonstrates a substantial reduction in collision rate and\nimprovement in overall performance compared to pure autonomy. Arbitration at\nthe level of semantic, language-based representations emerges as a design\nprinciple for shared autonomy, enabling systems to exercise common-sense\nreasoning and maintain continuity with human intent.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8fdb\u884c\u9a7e\u9a76\u610f\u56fe\u63a8\u65ad\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5171\u4eab\u81ea\u4e3b\u9a7e\u9a76\u7cfb\u7edf\uff0c\u4ee5\u5728\u9ad8\u5c42\u6b21\u4e0a\u6574\u5408\u4eba\u7c7b\u8f93\u5165\u548c\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u3002\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u56e0\u4e3a\u5176\u76ee\u6807\u662f\u9884\u6d4b\u9a7e\u9a76\u884c\u4e3a\uff0c\u5e76\u4e0e\u5927\u6a21\u578b\u76f8\u5173\uff0c\u56e0\u4e3a\u5b83\u4f7f\u7528\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u3002", "keywords": ["trajectory prediction", "large language models", "vision language models", "autonomous driving", "shared autonomy", "driver intent"]}}
{"id": "2511.04375", "pdf": "https://arxiv.org/pdf/2511.04375", "abs": "https://arxiv.org/abs/2511.04375", "authors": ["Anna M\u00e9sz\u00e1ros", "Javier Alonso-Mora", "Jens Kober"], "title": "Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories", "categories": ["cs.RO"], "comment": null, "summary": "Effectively capturing the joint distribution of all agents in a scene is\nrelevant for predicting the true evolution of the scene and in turn providing\nmore accurate information to the decision processes of autonomous vehicles.\nWhile new models have been developed for this purpose in recent years, it\nremains unclear how to best represent the joint distributions particularly from\nthe perspective of the interactions between agents. Thus far there is no clear\nconsensus on how best to represent interactions between agents; whether they\nshould be learned implicitly from data by neural networks, or explicitly\nmodeled using the spatial and temporal relations that are more grounded in\nhuman decision-making. This paper aims to study various means of describing\ninteractions within the same network structure and their effect on the final\nlearned joint distributions. Our findings show that more often than not, simply\nallowing a network to establish interactive connections between agents based on\ndata has a detrimental effect on performance. Instead, having well defined\ninteractions (such as which agent of an agent pair passes first at an\nintersection) can often bring about a clear boost in performance.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u573a\u666f\u4e2dagent\u4e4b\u95f4\u7684\u4ea4\u4e92\u8868\u793a\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u4ea4\u4e92\u8868\u793a\u65b9\u5f0f\u5bf9\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u5185\u5bb9\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\uff0c\u4e0e\u81ea\u52a8\u9a7e\u9a76\u7b49\u5e94\u7528\u5bc6\u5207\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "human trajectories", "scene-level distributions", "agent interaction", "autonomous vehicles"]}}
{"id": "2511.04155", "pdf": "https://arxiv.org/pdf/2511.04155", "abs": "https://arxiv.org/abs/2511.04155", "authors": ["Olav Finne Praesteng Larsen", "Massimiliano Ruocco", "Michail Spitieris", "Abdulmajid Murad", "Martina Ragosta"], "title": "Learning to Land Anywhere: Transferable Generative Models for Aircraft Trajectories", "categories": ["cs.LG", "I.2.6; I.5.1"], "comment": null, "summary": "Access to trajectory data is a key requirement for developing and validating\nAir Traffic Management (ATM) solutions, yet many secondary and regional\nairports face severe data scarcity. This limits the applicability of machine\nlearning methods and the ability to perform large-scale simulations or\n\"what-if\" analyses. In this paper, we investigate whether generative models\ntrained on data-rich airports can be efficiently adapted to data-scarce\nairports using transfer learning. We adapt state-of-the-art diffusion- and\nflow-matching-based architectures to the aviation domain and evaluate their\ntransferability between Zurich (source) and Dublin (target) landing trajectory\ndatasets. Models are pretrained on Zurich and fine-tuned on Dublin with varying\namounts of local data, ranging from 0% to 100%. Results show that\ndiffusion-based models achieve competitive performance with as little as 5% of\nthe Dublin data and reach baseline-level performance around 20%, consistently\noutperforming models trained from scratch across metrics and visual\ninspections. Latent flow matching and latent diffusion models also benefit from\npretraining, though with more variable gains, while flow matching models show\nweaker generalization. Despite challenges in capturing rare trajectory\npatterns, these findings demonstrate the potential of transfer learning to\nsubstantially reduce data requirements for trajectory generation in ATM,\nenabling realistic synthetic data generation even in environments with limited\nhistorical records.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u98de\u673a\u8f68\u8ff9\u7684\u751f\u6210\u6a21\u578b\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u8fc1\u79fb\u5b66\u4e60\uff0c\u5c06\u6570\u636e\u4e30\u5bcc\u7684\u673a\u573a\u6570\u636e\u8fc1\u79fb\u5230\u6570\u636e\u7a00\u7f3a\u7684\u673a\u573a\uff0c\u8fd9\u4e0e\u5927\u6a21\u578b\u4e2d\u5229\u7528\u9884\u8bad\u7ec3\u7684\u601d\u60f3\u6709\u76f8\u4f3c\u4e4b\u5904\u3002\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86diffusion model\u548cflow matching\u7b49\u751f\u6210\u6a21\u578b\u3002", "keywords": ["trajectory prediction", "generative models", "transfer learning", "diffusion model", "flow matching", "aircraft trajectories"]}}
{"id": "2511.03773", "pdf": "https://arxiv.org/pdf/2511.03773", "abs": "https://arxiv.org/abs/2511.03773", "authors": ["Zhaorun Chen", "Zhuokai Zhao", "Kai Zhang", "Bo Liu", "Qi Qi", "Yifan Wu", "Tarun Kalluri", "Sara Cao", "Yuanhao Xiong", "Haibo Tong", "Huaxiu Yao", "Hengduo Li", "Jiacheng Zhu", "Xian Li", "Dawn Song", "Bo Li", "Jason Weston", "Dat Huynh"], "title": "Scaling Agent Learning via Experience Synthesis", "categories": ["cs.AI"], "comment": null, "summary": "While reinforcement learning (RL) can empower large language model (LLM)\nagents by enabling self-improvement through interaction, its practical adoption\nremains challenging due to costly rollouts, limited task diversity, unreliable\nreward signals, and infrastructure complexity, all of which obstruct the\ncollection of scalable experience data. To address these challenges, we\nintroduce DreamGym, the first unified framework designed to synthesize diverse\nexperiences with scalability in mind to enable effective online RL training for\nautonomous agents. Rather than relying on expensive real-environment rollouts,\nDreamGym distills environment dynamics into a reasoning-based experience model\nthat derives consistent state transitions and feedback signals through\nstep-by-step reasoning, enabling scalable agent rollout collection for RL. To\nimprove the stability and quality of transitions, DreamGym leverages an\nexperience replay buffer initialized with offline real-world data and\ncontinuously enriched with fresh interactions to actively support agent\ntraining. To improve knowledge acquisition, DreamGym adaptively generates new\ntasks that challenge the current agent policy, enabling more effective online\ncurriculum learning. Experiments across diverse environments and agent\nbackbones demonstrate that DreamGym substantially improves RL training, both in\nfully synthetic settings and in sim-to-real transfer scenarios. On non-RL-ready\ntasks like WebArena, DreamGym outperforms all baselines by over 30%. And in\nRL-ready but costly settings, it matches GRPO and PPO performance using only\nsynthetic interactions. When transferring a policy trained purely on synthetic\nexperiences to real-environment RL, DreamGym yields significant additional\nperformance gains while requiring far fewer real-world interactions, providing\na scalable warm-start strategy for general-purpose RL.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on reinforcement learning (RL) for large language model (LLM) agents, specifically addressing the challenges of scaling agent learning through experience synthesis. While it doesn't directly address trajectory prediction, the concepts of agent interaction with environments and improving policies could be relevant to trajectory prediction tasks in the context of autonomous agents (e.g., autonomous driving, robotics). The use of LLMs as agents and the scaling of their learning process are central to the paper, making it relevant to the large language model aspect. The connection to trajectory prediction is more indirect, through the general concept of agent behavior and planning.", "keywords": ["large language model", "LLM", "reinforcement learning", "RL", "autonomous agents", "experience synthesis", "agent learning"]}}
{"id": "2511.04042", "pdf": "https://arxiv.org/pdf/2511.04042", "abs": "https://arxiv.org/abs/2511.04042", "authors": ["Kailun Ji", "Xiaoyu Hu", "Xinyu Zhang", "Jun Chen"], "title": "An LLM-based Framework for Human-Swarm Teaming Cognition in Disaster Search and Rescue", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Large-scale disaster Search And Rescue (SAR) operations are persistently\nchallenged by complex terrain and disrupted communications. While Unmanned\nAerial Vehicle (UAV) swarms offer a promising solution for tasks like wide-area\nsearch and supply delivery, yet their effective coordination places a\nsignificant cognitive burden on human operators. The core human-machine\ncollaboration bottleneck lies in the ``intention-to-action gap'', which is an\nerror-prone process of translating a high-level rescue objective into a\nlow-level swarm command under high intensity and pressure. To bridge this gap,\nthis study proposes a novel LLM-CRF system that leverages Large Language Models\n(LLMs) to model and augment human-swarm teaming cognition. The proposed\nframework initially captures the operator's intention through natural and\nmulti-modal interactions with the device via voice or graphical annotations. It\nthen employs the LLM as a cognitive engine to perform intention comprehension,\nhierarchical task decomposition, and mission planning for the UAV swarm. This\nclosed-loop framework enables the swarm to act as a proactive partner,\nproviding active feedback in real-time while reducing the need for manual\nmonitoring and control, which considerably advances the efficacy of the SAR\ntask. We evaluate the proposed framework in a simulated SAR scenario.\nExperimental results demonstrate that, compared to traditional order and\ncommand-based interfaces, the proposed LLM-driven approach reduced task\ncompletion time by approximately $64.2\\%$ and improved task success rate by\n$7\\%$. It also leads to a considerable reduction in subjective cognitive\nworkload, with NASA-TLX scores dropping by $42.9\\%$. This work establishes the\npotential of LLMs to create more intuitive and effective human-swarm\ncollaborations in high-stakes scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u589e\u5f3a\u4eba\u4e0e\u65e0\u4eba\u673a\u96c6\u7fa4\u5728\u707e\u96be\u641c\u7d22\u548c\u6551\u63f4\u4e2d\u7684\u534f\u4f5c\uff0c\u7279\u522b\u662f\u901a\u8fc7LLM\u8fdb\u884c\u610f\u56fe\u7406\u89e3\u3001\u4efb\u52a1\u5206\u89e3\u548c\u4efb\u52a1\u89c4\u5212\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u65e0\u4eba\u673a\u96c6\u7fa4\u7684\u4efb\u52a1\u89c4\u5212\u548c\u63a7\u5236\u9690\u542b\u7740\u8f68\u8ff9\u751f\u6210\u548c\u9884\u6d4b\uff0c\u5e76\u4e14\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86LLM\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "UAV swarm", "mission planning"]}}
{"id": "2511.04357", "pdf": "https://arxiv.org/pdf/2511.04357", "abs": "https://arxiv.org/abs/2511.04357", "authors": ["Ma\u00eblic Neau", "Zoe Falomir", "Paulo E. Santos", "Anne-Gwenn Bosser", "C\u00e9dric Buche"], "title": "GraSP-VLA: Graph-based Symbolic Action Representation for Long-Horizon Planning with VLA Policies", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Deploying autonomous robots that can learn new skills from demonstrations is\nan important challenge of modern robotics. Existing solutions often apply\nend-to-end imitation learning with Vision-Language Action (VLA) models or\nsymbolic approaches with Action Model Learning (AML). On the one hand, current\nVLA models are limited by the lack of high-level symbolic planning, which\nhinders their abilities in long-horizon tasks. On the other hand, symbolic\napproaches in AML lack generalization and scalability perspectives. In this\npaper we present a new neuro-symbolic approach, GraSP-VLA, a framework that\nuses a Continuous Scene Graph representation to generate a symbolic\nrepresentation of human demonstrations. This representation is used to generate\nnew planning domains during inference and serves as an orchestrator for\nlow-level VLA policies, scaling up the number of actions that can be reproduced\nin a row. Our results show that GraSP-VLA is effective for modeling symbolic\nrepresentations on the task of automatic planning domain generation from\nobservations. In addition, results on real-world experiments show the potential\nof our Continuous Scene Graph representation to orchestrate low-level VLA\npolicies in long-horizon tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86Vision-Language Action (VLA) \u6a21\u578b\uff0c\u8fd9\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u5173\u7684\uff0c\u5e76\u4e14\u6d89\u53ca\u673a\u5668\u4eba\u957f\u65f6\u95f4\u89c4\u5212\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u89c4\u5212\u4efb\u52a1\u901a\u5e38\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u7684\u201clong-horizon tasks\u201d\u4e5f\u6697\u793a\u4e86\u5bf9\u672a\u6765\u72b6\u6001\u7684\u9884\u6d4b\u3002", "keywords": ["Vision-Language Action (VLA) models", "long-horizon planning", "Action Model Learning (AML)", "Continuous Scene Graph representation"]}}
{"id": "2511.04235", "pdf": "https://arxiv.org/pdf/2511.04235", "abs": "https://arxiv.org/abs/2511.04235", "authors": ["Zhengru Fang", "Yu Guo", "Jingjing Wang", "Yuang Zhang", "Haonan An", "Yinhai Wang", "Yuguang Fang"], "title": "Shared Spatial Memory Through Predictive Coding", "categories": ["cs.AI", "cs.CE"], "comment": "We have prepared the open-source code and video demonstration pages:\n  1. Code: github.com/fangzr/SSM-PC 2. Demo: fangzr.github.io/SSM-PC/index.html", "summary": "Sharing and reconstructing a consistent spatial memory is a critical\nchallenge in multi-agent systems, where partial observability and limited\nbandwidth often lead to catastrophic failures in coordination. We introduce a\nmulti-agent predictive coding framework that formulate coordination as the\nminimization of mutual uncertainty among agents. Instantiated as an information\nbottleneck objective, it prompts agents to learn not only who and what to\ncommunicate but also when. At the foundation of this framework lies a\ngrid-cell-like metric as internal spatial coding for self-localization,\nemerging spontaneously from self-supervised motion prediction. Building upon\nthis internal spatial code, agents gradually develop a bandwidth-efficient\ncommunication mechanism and specialized neural populations that encode\npartners' locations: an artificial analogue of hippocampal social place cells\n(SPCs). These social representations are further enacted by a hierarchical\nreinforcement learning policy that actively explores to reduce joint\nuncertainty. On the Memory-Maze benchmark, our approach shows exceptional\nresilience to bandwidth constraints: success degrades gracefully from 73.5% to\n64.4% as bandwidth shrinks from 128 to 4 bits/step, whereas a full-broadcast\nbaseline collapses from 67.6% to 28.6%. Our findings establish a theoretically\nprincipled and biologically plausible basis for how complex social\nrepresentations emerge from a unified predictive drive, leading to social\ncollective intelligence.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u7a7a\u95f4\u8bb0\u5fc6\u548c\u534f\u540c\uff0c\u4f7f\u7528\u4e86\u9884\u6d4b\u7f16\u7801\u6846\u67b6\u3002\u867d\u7136\u63d0\u5230\u4e86\u8fd0\u52a8\u9884\u6d4b\uff08motion prediction\uff09\uff0c\u4f46\u5176\u76ee\u7684\u662f\u4e3a\u4e86\u5b66\u4e60\u5185\u90e8\u7a7a\u95f4\u7f16\u7801\uff0c\u800c\u975e\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u8bba\u6587\u4e2d\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u6216\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u9884\u6d4b\u7f16\u7801\u7684\u601d\u60f3\u4e0e\u5927\u6a21\u578b\u4e2d\u7684\u4e00\u4e9b\u9884\u6d4b\u6027\u5b66\u4e60\u65b9\u6cd5\u6709\u4e00\u5b9a\u5173\u8054\u3002", "keywords": ["motion prediction", "predictive coding", "multi-agent systems", "self-localization"]}}
{"id": "2511.04307", "pdf": "https://arxiv.org/pdf/2511.04307", "abs": "https://arxiv.org/abs/2511.04307", "authors": ["Jian Mu", "Chaoyun Zhang", "Chiming Ni", "Lu Wang", "Bo Qiao", "Kartik Mathur", "Qianhui Wu", "Yuhang Xie", "Xiaojun Ma", "Mengyu Zhou", "Si Qin", "Liqun Li", "Yu Kang", "Minghua Ma", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents", "categories": ["cs.AI"], "comment": null, "summary": "We introduce GUI-360$^\\circ$, a large-scale, comprehensive dataset and\nbenchmark suite designed to advance computer-using agents (CUAs). CUAs present\nunique challenges and is constrained by three persistent gaps: a scarcity of\nreal-world CUA tasks, the lack of automated collection-and-annotation pipelines\nfor multi-modal trajectories, and the absence of a unified benchmark that\njointly evaluates GUI grounding, screen parsing, and action prediction.\n  GUI-360$^\\circ$ addresses these gaps with an LLM-augmented, largely automated\npipeline for query sourcing, environment-template construction, task\ninstantiation, batched execution, and LLM-driven quality filtering. The\nreleased corpus contains over 1.2M executed action steps across thousands of\ntrajectories in popular Windows office applications, and includes\nfull-resolution screenshots, accessibility metadata when available,\ninstantiated goals, intermediate reasoning traces, and both successful and\nfailed action trajectories. The dataset supports three canonical tasks, GUI\ngrounding, screen parsing, and action prediction, and a hybrid GUI+API action\nspace that reflects modern agent designs. Benchmarking state-of-the-art\nvision--language models on GUI-360$^\\circ$ reveals substantial out-of-the-box\nshortcomings in grounding and action prediction; supervised fine-tuning and\nreinforcement learning yield significant gains but do not close the gap to\nhuman-level reliability. We release GUI-360$^\\circ$ and accompanying code to\nfacilitate reproducible research and accelerate progress on robust desktop\nCUAs.\n  The full dataset has been made public on\nhttps://huggingface.co/datasets/vyokky/GUI-360.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper introduces a dataset for computer-using agents, focusing on GUI grounding, screen parsing, and action prediction. While it doesn't directly deal with trajectory prediction in the traditional sense (e.g., pedestrian or vehicle movement), it involves predicting action sequences, which can be considered a form of trajectory prediction in a broader context. The use of LLMs for data generation and filtering also contributes to the relevance. However, the primary focus is not on trajectory prediction or LLMs themselves, hence the moderate score.", "keywords": ["action prediction", "large language models", "LLM", "trajectories"]}}
{"id": "2511.04126", "pdf": "https://arxiv.org/pdf/2511.04126", "abs": "https://arxiv.org/abs/2511.04126", "authors": ["Venkata Manikanta Desu", "Syed Fawaz Ali"], "title": "Automated Tennis Player and Ball Tracking with Court Keypoints Detection (Hawk Eye System)", "categories": ["cs.CV", "cs.AI"], "comment": "14 pages, 11 figures, planning to submit for a coneference", "summary": "This study presents a complete pipeline for automated tennis match analysis.\nOur framework integrates multiple deep learning models to detect and track\nplayers and the tennis ball in real time, while also identifying court\nkeypoints for spatial reference. Using YOLOv8 for player detection, a\ncustom-trained YOLOv5 model for ball tracking, and a ResNet50-based\narchitecture for court keypoint detection, our system provides detailed\nanalytics including player movement patterns, ball speed, shot accuracy, and\nplayer reaction times. The experimental results demonstrate robust performance\nin varying court conditions and match scenarios. The model outputs an annotated\nvideo along with detailed performance metrics, enabling coaches, broadcasters,\nand players to gain actionable insights into the dynamics of the game.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on tracking tennis players and the ball, which falls under the domain of trajectory prediction (specifically, object tracking). While it uses deep learning models, it doesn't explicitly mention or utilize Large Language Models. Therefore, the relevance is moderate due to the trajectory prediction aspect.", "keywords": ["trajectory prediction", "object tracking", "player movement patterns", "ball tracking"]}}
{"id": "2511.04646", "pdf": "https://arxiv.org/pdf/2511.04646", "abs": "https://arxiv.org/abs/2511.04646", "authors": ["Narjes Nourzad", "Hanqing Yang", "Shiyu Chen", "Carlee Joe-Wong"], "title": "DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "Cooperative multi-agent planning requires agents to make joint decisions with\npartial information and limited communication. Coordination at the trajectory\nlevel often fails, as small deviations in timing or movement cascade into\nconflicts. Symbolic planning mitigates this challenge by raising the level of\nabstraction and providing a minimal vocabulary of actions that enable\nsynchronization and collective progress. We present DR. WELL, a decentralized\nneurosymbolic framework for cooperative multi-agent planning. Cooperation\nunfolds through a two-phase negotiation protocol: agents first propose\ncandidate roles with reasoning and then commit to a joint allocation under\nconsensus and environment constraints. After commitment, each agent\nindependently generates and executes a symbolic plan for its role without\nrevealing detailed trajectories. Plans are grounded in execution outcomes via a\nshared world model that encodes the current state and is updated as agents act.\nBy reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids\nbrittle step-level alignment and enables higher-level operations that are\nreusable, synchronizable, and interpretable. Experiments on cooperative\nblock-push tasks show that agents adapt across episodes, with the dynamic world\nmodel capturing reusable patterns and improving task completion rates and\nefficiency. Experiments on cooperative block-push tasks show that our dynamic\nworld model improves task completion and efficiency through negotiation and\nself-refinement, trading a time overhead for evolving, more efficient\ncollaboration strategies.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on multi-agent collaboration and planning using a symbolic world model and an embodied LLM. While it doesn't directly deal with trajectory prediction in the traditional sense (like predicting future positions), it does involve planning and executing actions in an environment, which can be seen as related to trajectory generation or high-level path planning. The use of LLM is also a key aspect. Therefore, there's some relevance, but it's not a direct application of trajectory prediction methods.", "keywords": ["Large Language Models", "LLM", "multi-agent planning", "symbolic planning", "world model"]}}
{"id": "2511.03728", "pdf": "https://arxiv.org/pdf/2511.03728", "abs": "https://arxiv.org/abs/2511.03728", "authors": ["Sanidhya Vijayvargiya", "Rahul Lokesh"], "title": "Efficient On-Device Agents via Adaptive Context Management", "categories": ["cs.HC", "cs.AI"], "comment": "27 pages, 5 figures", "summary": "On-device AI agents offer the potential for personalized, low-latency\nassistance, but their deployment is fundamentally constrained by limited memory\ncapacity, which restricts usable context. This reduced practical context window\ncreates a trade-off between supporting rich, stateful interactions with complex\ntool capabilities and maintaining on-device feasibility. We break this\ntrade-off with a framework for context-efficient on-device agents, driven by\nthree synergistic optimizations (1) a dynamic memory system using specialized\nLoRA adapters to distill conversational history into a compressed, and\nstructured Context State Object; (2) a minimalist serialization format for tool\nschemas to minimize token overhead per tool; and (3) a just-in-time\nschema-passing mechanism that loads full tool definitions only upon tool\nselection. We instantiate this framework by adapting a 3B parameter SLM to\ncontext-efficient trajectories and rigorously evaluate it against a\nconventional baseline on complex user tasks. Our agent matches, or exceeds, the\nperformance of a conventional baseline while dramatically compressing context,\nachieving more than a 6-fold reduction in initial system prompt context and a\n10- to 25-fold reduction in context growth rate based on the interaction\nverbosity, demonstrating that strategic context management is key to unlocking\ncapable and persistent on-device AI.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on efficient on-device agents using large language models (specifically, a 3B parameter SLM). While the abstract mentions \"context-efficient trajectories,\" it refers to the agent's ability to maintain context across interactions, not trajectory prediction in the traditional sense. Therefore, it has some relevance due to the use of LLMs and the mention of trajectories in the context of agent behavior, but it's not a direct contribution to trajectory prediction as a field.", "keywords": ["Large Language Models", "LLMs", "on-device agents", "context management", "trajectories"]}}
{"id": "2511.04332", "pdf": "https://arxiv.org/pdf/2511.04332", "abs": "https://arxiv.org/abs/2511.04332", "authors": ["Antti Koskela", "Tejas Kulkarni", "Laith Zumot"], "title": "Differentially Private In-Context Learning with Nearest Neighbor Search", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "NeurIPS Lock-LLM Workshop 2025", "summary": "Differentially private in-context learning (DP-ICL) has recently become an\nactive research topic due to the inherent privacy risks of in-context learning.\nHowever, existing approaches overlook a critical component of modern large\nlanguage model (LLM) pipelines: the similarity search used to retrieve relevant\ncontext data. In this work, we introduce a DP framework for in-context learning\nthat integrates nearest neighbor search of relevant examples in a privacy-aware\nmanner. Our method outperforms existing baselines by a substantial margin\nacross all evaluated benchmarks, achieving more favorable privacy-utility\ntrade-offs. To achieve this, we employ nearest neighbor retrieval from a\ndatabase of context data, combined with a privacy filter that tracks the\ncumulative privacy cost of selected samples to ensure adherence to a central\ndifferential privacy budget. Experimental results on text classification and\ndocument question answering show a clear advantage of the proposed method over\nexisting baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on differentially private in-context learning within the context of large language models (LLMs). While it doesn't directly address trajectory prediction, it's highly relevant to LLMs and their application, especially regarding privacy concerns and retrieval-based methods, which could potentially be adapted for trajectory prediction tasks. The connection to trajectory prediction is indirect but plausible.", "keywords": ["Large Language Models", "LLM", "In-Context Learning", "Nearest Neighbor Search"]}}
{"id": "2511.04595", "pdf": "https://arxiv.org/pdf/2511.04595", "abs": "https://arxiv.org/abs/2511.04595", "authors": ["Chen Shi", "Shaoshuai Shi", "Xiaoyang Lyu", "Chunyang Liu", "Kehua Sheng", "Bo Zhang", "Li Jiang"], "title": "UniSplat: Unified Spatio-Temporal Fusion via 3D Latent Scaffolds for Dynamic Driving Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Feed-forward 3D reconstruction for autonomous driving has advanced rapidly,\nyet existing methods struggle with the joint challenges of sparse,\nnon-overlapping camera views and complex scene dynamics. We present UniSplat, a\ngeneral feed-forward framework that learns robust dynamic scene reconstruction\nthrough unified latent spatio-temporal fusion. UniSplat constructs a 3D latent\nscaffold, a structured representation that captures geometric and semantic\nscene context by leveraging pretrained foundation models. To effectively\nintegrate information across spatial views and temporal frames, we introduce an\nefficient fusion mechanism that operates directly within the 3D scaffold,\nenabling consistent spatio-temporal alignment. To ensure complete and detailed\nreconstructions, we design a dual-branch decoder that generates dynamic-aware\nGaussians from the fused scaffold by combining point-anchored refinement with\nvoxel-based generation, and maintain a persistent memory of static Gaussians to\nenable streaming scene completion beyond current camera coverage. Extensive\nexperiments on real-world datasets demonstrate that UniSplat achieves\nstate-of-the-art performance in novel view synthesis, while providing robust\nand high-quality renderings even for viewpoints outside the original camera\ncoverage.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u52a8\u6001\u9a7e\u9a76\u573a\u666f\u76843D\u91cd\u5efa\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u5229\u7528\u9884\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff08foundation models\uff09\u8fdb\u884c\u573a\u666f\u7406\u89e3\uff0c\u5e76\u663e\u5f0f\u5efa\u6a21\u4e86\u65f6\u5e8f\u4fe1\u606f\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u573a\u666f\u7406\u89e3\u548c\u65f6\u5e8f\u5efa\u6a21\u6709\u4e00\u5b9a\u5173\u8054\u3002 \u53e6\u5916\uff0c\u91cd\u5efa\u7684\u52a8\u6001\u573a\u666f\u53ef\u4ee5\u7528\u4e8e\u540e\u7eed\u7684\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u3002 \u8bba\u6587\u63d0\u5230\u4e86\u57fa\u7840\u6a21\u578b\uff0c\u6240\u4ee5\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u4e9b\u76f8\u5173\u6027\u3002", "keywords": ["foundation models", "dynamic scene reconstruction", "spatio-temporal fusion", "autonomous driving"]}}
{"id": "2511.04668", "pdf": "https://arxiv.org/pdf/2511.04668", "abs": "https://arxiv.org/abs/2511.04668", "authors": ["Ellis Brown", "Arijit Ray", "Ranjay Krishna", "Ross Girshick", "Rob Fergus", "Saining Xie"], "title": "SIMS-V: Simulated Instruction-Tuning for Spatial Video Understanding", "categories": ["cs.CV"], "comment": "Project page: https://ellisbrown.github.io/sims-v", "summary": "Despite impressive high-level video comprehension, multimodal language models\nstruggle with spatial reasoning across time and space. While current spatial\ntraining approaches rely on real-world video data, obtaining diverse footage\nwith precise spatial annotations remains a bottleneck. To alleviate this\nbottleneck, we present SIMS-V -- a systematic data-generation framework that\nleverages the privileged information of 3D simulators to create spatially-rich\nvideo training data for multimodal language models. Using this framework, we\ninvestigate which properties of simulated data drive effective real-world\ntransfer through systematic ablations of question types, mixes, and scales. We\nidentify a minimal set of three question categories (metric measurement,\nperspective-dependent reasoning, and temporal tracking) that prove most\neffective for developing transferable spatial intelligence, outperforming\ncomprehensive coverage despite using fewer question types. These insights\nenable highly efficient training: our 7B-parameter video LLM fine-tuned on just\n25K simulated examples outperforms the larger 72B baseline and achieves\ncompetitive performance with proprietary models on rigorous real-world spatial\nreasoning benchmarks. Our approach demonstrates robust generalization,\nmaintaining performance on general video understanding while showing\nsubstantial improvements on embodied and real-world spatial tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on spatial video understanding using simulated data to train multimodal language models. While not directly addressing trajectory prediction, the 'temporal tracking' aspect and the use of 3D simulators for generating video data relate to the core concepts used in trajectory forecasting, especially in autonomous driving or robotics. The use of a large language model is also a key aspect.", "keywords": ["Large Language Models", "multimodal language models", "3D simulators", "temporal tracking", "spatial reasoning"]}}
