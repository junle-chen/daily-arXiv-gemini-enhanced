{"id": "2511.19830", "pdf": "https://arxiv.org/pdf/2511.19830", "abs": "https://arxiv.org/abs/2511.19830", "authors": ["Junhao Zhu", "Lu Chen", "Xiangyu Ke", "Ziquan Fang", "Tianyi Li", "Yunjun Gao", "Christian S. Jensen"], "title": "Beyond Relational: Semantic-Aware Multi-Modal Analytics with LLM-Native Query Optimization", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Multi-modal analytical processing has the potential to transform applications in e-commerce, healthcare, entertainment, and beyond. However, real-world adoption remains elusive due to the limited ability of traditional relational query operators to capture query semantics. The emergence of foundation models, particularly the large language models (LLMs), opens up new opportunities to develop flexible, semantic-aware data analytics systems that transcend the relational paradigm.\n  We present Nirvana, a multi-modal data analytics framework that incorporates programmable semantic operators while leveraging both logical and physical query optimization strategies, tailored for LLM-driven semantic query processing. Nirvana addresses two key challenges. First, it features an agentic logical optimizer that uses natural language-specified transformation rules and random-walk-based search to explore vast spaces of semantically equivalent query plans -- far beyond the capabilities of conventional optimizers. Second, it introduces a cost-aware physical optimizer that selects the most effective LLM backend for each operator using a novel improvement-score metric. To further enhance efficiency, Nirvana incorporates computation reuse and evaluation pushdown techniques guided by model capability hypotheses. Experimental evaluations on three real-world benchmarks demonstrate that Nirvana is able to reduce end-to-end runtime by 10%--85% and reduces system processing costs by 76% on average, outperforming state-of-the-art systems at both efficiency and scalability."}
{"id": "2511.20049", "pdf": "https://arxiv.org/pdf/2511.20049", "abs": "https://arxiv.org/abs/2511.20049", "authors": ["Yushuai Ji", "Sheng Wang", "Zhiyu Chen", "Yuan Sun", "Zhiyong Peng"], "title": "Updatable Balanced Index for Fast On-device Search with Auto-selection Model", "categories": ["cs.DB"], "comment": "Accepted for publication in the 42nd IEEE International Conference on Data Engineering (ICDE 2026). To appear", "summary": "Diverse types of edge data, such as 2D geo-locations and 3D point clouds, are collected by sensors like lidar and GPS receivers on edge devices. On-device searches, such as k-nearest neighbor (kNN) search and radius search, are commonly used to enable fast analytics and learning technologies, such as k-means dataset simplification using kNN. To maintain high search efficiency, a representative approach is to utilize a balanced multi-way KD-tree (BMKD-tree). However, the index has shown limited gains, mainly due to substantial construction overhead, inflexibility to real-time insertion, and inconsistent query performance. In this paper, we propose UnIS to address the above limitations. We first accelerate the construction process of the BMKD-tree by utilizing the dataset distribution to predict the splitting hyperplanes. To make the continuously generated data searchable, we propose a selective sub-tree rebuilding scheme to accelerate rebalancing during insertion by reducing the number of data points involved. We then propose an auto-selection model to improve query performance by automatically selecting the optimal search strategy among multiple strategies for an arbitrary query task. Experimental results show that UnIS achieves average speedups of 17.96x in index construction, 1.60x in insertion, 7.15x in kNN search, and 1.09x in radius search compared to the BMKD-tree. We further verify its effectiveness in accelerating dataset simplification on edge devices, achieving a speedup of 217x over Lloyd's algorithm."}
{"id": "2511.20084", "pdf": "https://arxiv.org/pdf/2511.20084", "abs": "https://arxiv.org/abs/2511.20084", "authors": ["Mariana M. Garcez Duarte", "Dwi P. A. Nugroho", "Georges Tod", "Evert Bevernage", "Pieter Moelans", "Emine Tas", "Esteban Zimanyi", "Mahmoud Sakr", "Steffen Zeuch", "Volker Markl"], "title": "Mobility Stream Processing on NebulaStream and MEOS", "categories": ["cs.DB"], "comment": null, "summary": "The increasing use of Internet-of-Things (IoT) sensors in moving objects has resulted in vast amounts of spatiotemporal streaming data. To analyze this data in situ, real-time spatiotemporal processing is needed. However, current stream processing systems designed for IoT environments often lack spatiotemporal processing capabilities, and existing spatiotemporal libraries primarily focus on analyzing historical data. This gap makes performing real-time spatiotemporal analytics challenging. In this demonstration, we present NebulaMEOS, which combines MEOS (Mobility Engine Open Source), a spatiotemporal processing library, with NebulaStream, a scalable data management system for IoT applications. By integrating MEOS into NebulaStream, NebulaMEOS utilizes spatiotemporal functionalities to process and analyze streaming data in real-time. We demonstrate NebulaMEOS by querying data streamed from edge devices on trains by the Soci\u00e9t\u00e9 Nationale des Chemins de fer Belges (SNCB). Visitors can experience demonstrations of geofencing and geospatial complex event processing, visualizing real-time train operations and environmental impacts."}
{"id": "2511.20125", "pdf": "https://arxiv.org/pdf/2511.20125", "abs": "https://arxiv.org/abs/2511.20125", "authors": ["Yihua Hu", "Hao Ding", "Wei Dong"], "title": "N2E: A General Framework to Reduce Node-Differential Privacy to Edge-Differential Privacy for Graph Analytics", "categories": ["cs.DB"], "comment": null, "summary": "Differential privacy (DP) has been widely adopted to protect sensitive information in graph analytics. While edge-DP, which protects privacy at the edge level, has been extensively studied, node-DP, offering stronger protection for entire nodes and their incident edges, remains largely underexplored due to its technical challenges. A natural way to bridge this gap is to develop a general framework for reducing node-DP graph analytical tasks to edge-DP ones, enabling the reuse of existing edge-DP mechanisms. A straightforward solution based on group privacy divides the privacy budget by a given degree upper bound, but this leads to poor utility when the bound is set conservatively large to accommodate worst-case inputs. To address this, we propose node-to-edge (N2E), a general framework that reduces any node-DP graph analytical task to an edge-DP one, with the error dependency on the graph's true maximum degree. N2E introduces two novel techniques: a distance-preserving clipping mechanism that bounds edge distance between neighboring graphs after clipping, and the first node-DP mechanism for maximum degree approximation, enabling tight, privacy-preserving clipping thresholds. By instantiating N2E with existing edge-DP mechanisms, we obtain the first node-DP solutions for tasks such as maximum degree estimation. For edge counting, our method theoretically matches the error of the state-of-the-art, which is provably optimal, and significantly outperforms existing approaches for degree distribution estimation. Experimental results demonstrate that our framework achieves up to a 2.5x reduction in error for edge counting and up to an 80x reduction for degree distribution estimation."}
{"id": "2511.20139", "pdf": "https://arxiv.org/pdf/2511.20139", "abs": "https://arxiv.org/abs/2511.20139", "authors": ["Mariana M Garcez Duarte", "Mahmoud Sakr"], "title": "An experimental study of existing tools for outlier detection and cleaning in trajectories", "categories": ["cs.DB"], "comment": null, "summary": "Outlier detection and cleaning are essential steps in data preprocessing to ensure the integrity and validity of data analyses. This paper focuses on outlier points within individual trajectories, i.e., points that deviate significantly inside a single trajectory. We experiment with ten open-source libraries to comprehensively evaluate available tools, comparing their efficiency and accuracy in identifying and cleaning outliers. This experiment considers the libraries as they are offered to end users, with real-world applicability. We compare existing outlier detection libraries, introduce a method for establishing ground-truth, and aim to guide users in choosing the most appropriate tool for their specific outlier detection needs. Furthermore, we survey the state-of-the-art algorithms for outlier detection and classify them into five types: Statistic-based methods, Sliding window algorithms, Clustering-based methods, Graph-based methods, and Heuristic-based methods. Our research provides insights into these libraries' performance and contributes to developing data preprocessing and outlier detection methodologies."}
{"id": "2511.20293", "pdf": "https://arxiv.org/pdf/2511.20293", "abs": "https://arxiv.org/abs/2511.20293", "authors": ["Chaowei He", "Yuanjun Liu", "Qingzhi Ma", "Shenyuan Ren", "Xizhao Luo", "Lei Zhao", "An Liu"], "title": "Forgetting by Pruning: Data Deletion in Join Cardinality Estimation", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": "AAAI26", "summary": "Machine unlearning in learned cardinality estimation (CE) systems presents unique challenges due to the complex distributional dependencies in multi-table relational data. Specifically, data deletion, a core component of machine unlearning, faces three critical challenges in learned CE models: attribute-level sensitivity, inter-table propagation and domain disappearance leading to severe overestimation in multi-way joins. We propose Cardinality Estimation Pruning (CEP), the first unlearning framework specifically designed for multi-table learned CE systems. CEP introduces Distribution Sensitivity Pruning, which constructs semi-join deletion results and computes sensitivity scores to guide parameter pruning, and Domain Pruning, which removes support for value domains entirely eliminated by deletion. We evaluate CEP on state-of-the-art architectures NeuroCard and FACE across IMDB and TPC-H datasets. Results demonstrate CEP consistently achieves the lowest Q-error in multi-table scenarios, particularly under high deletion ratios, often outperforming full retraining. Furthermore, CEP significantly reduces convergence iterations, incurring negligible computational overhead of 0.3%-2.5% of fine-tuning time."}
{"id": "2511.20419", "pdf": "https://arxiv.org/pdf/2511.20419", "abs": "https://arxiv.org/abs/2511.20419", "authors": ["Gianna Lisa Nicolai", "Patrick Hansert", "Sebastian Michel"], "title": "The Case for Intent-Based Query Rewriting", "categories": ["cs.DB"], "comment": "Published in the 2nd International Workshop on Data-driven AI (DATAI) 2025", "summary": "With this work, we describe the concept of intent-based query rewriting and present a first viable solution. The aim is to allow rewrites to alter the structure and syntactic outcome of an original query while keeping the obtainable insights intact. This drastically differs from traditional query rewriting, which typically aims to decrease query evaluation time by using strict equivalence rules and optimization heuristics on the query plan. Rewriting queries to queries that only provide a similar insight but otherwise can be entirely different can remedy inaccessible original data tables due to access control, privacy, or expensive data access regarding monetary cost or remote access. In this paper, we put forward INQURE, a system designed for INtent-based QUery REwriting. It uses access to a large language model (LLM) for the query understanding and human-like derivation of alternate queries. Around the LLM, INQURE employs upfront table filtering and subsequent candidate rewrite pruning and ranking. We report on the results of an evaluation using a benchmark set of over 900 database table schemas and discuss the pros and cons of alternate approaches regarding runtime and quality of the rewrites of a user study."}
{"id": "2511.20489", "pdf": "https://arxiv.org/pdf/2511.20489", "abs": "https://arxiv.org/abs/2511.20489", "authors": ["Kanchan Chowdhury", "Lixi Zhou", "Lulu Xie", "Xinwei Fu", "Jia Zou"], "title": "InferF: Declarative Factorization of AI/ML Inferences over Joins", "categories": ["cs.DB", "cs.LG"], "comment": "Accepted to SIGMOD 2026 as full research paper. This archived version has a full appendix", "summary": "Real-world AI/ML workflows often apply inference computations to feature vectors joined from multiple datasets. To avoid the redundant AI/ML computations caused by repeated data records in the join's output, factorized ML has been proposed to decompose ML computations into sub-computations to be executed on each normalized dataset. However, there is insufficient discussion on how factorized ML could impact AI/ML inference over multi-way joins. To address the limitations, we propose a novel declarative InferF system, focusing on the factorization of arbitrary inference workflows represented as analyzable expressions over the multi-way joins. We formalize our problem to flexibly push down partial factorized computations to qualified nodes in the join tree to minimize the overall inference computation and join costs and propose two algorithms to resolve the problem: (1) a greedy algorithm based on a per-node cost function that estimates the influence on overall latency if a subset of factorized computations is pushed to a node, and (2) a genetic algorithm for iteratively enumerating and evaluating promising factorization plans. We implement InferF on Velox, an open-sourced database engine from Meta, evaluate it on real-world datasets, observed up to 11.3x speedups, and systematically summarized the factors that determine when factorized ML can benefit AI/ML inference workflows."}
{"id": "2511.19453", "pdf": "https://arxiv.org/pdf/2511.19453", "abs": "https://arxiv.org/abs/2511.19453", "authors": ["Yuxin Wang", "Yuankai He", "Weisong Shi"], "title": "AVS: A Computational and Hierarchical Storage System for Autonomous Vehicles", "categories": ["cs.DC", "cs.DB", "cs.OS", "cs.RO"], "comment": null, "summary": "Autonomous vehicles (AVs) are evolving into mobile computing platforms, equipped with powerful processors and diverse sensors that generate massive heterogeneous data, for example 14 TB per day. Supporting emerging third-party applications calls for a general-purpose, queryable onboard storage system. Yet today's data loggers and storage stacks in vehicles fail to deliver efficient data storage and retrieval. This paper presents AVS, an Autonomous Vehicle Storage system that co-designs computation with a hierarchical layout: modality-aware reduction and compression, hot-cold tiering with daily archival, and a lightweight metadata layer for indexing. The design is grounded with system-level benchmarks on AV data that cover SSD and HDD filesystems and embedded indexing, and is validated on embedded hardware with real L4 autonomous driving traces. The prototype delivers predictable real-time ingest, fast selective retrieval, and substantial footprint reduction under modest resource budgets. The work also outlines observations and next steps toward more scalable and longer deployments to motivate storage as a first-class component in AV stacks."}
{"id": "2511.19837", "pdf": "https://arxiv.org/pdf/2511.19837", "abs": "https://arxiv.org/abs/2511.19837", "authors": ["Zhentao Zhan", "Xiaoliang Xu", "Jingjing Wang", "Junmei Wang"], "title": "GED-Consistent Disentanglement of Aligned and Unaligned Substructures for Graph Similarity Learning", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "Graph Similarity Computation (GSC) is a fundamental graph related task where Graph Edit Distance (GED) serves as a prevalent metric. GED is determined by an optimal alignment between a pair of graphs that partitions each into aligned (zero-cost) and unaligned (cost-incurring) substructures. Due to NP-hard nature of exact GED computation, GED approximations based on Graph Neural Network(GNN) have emerged. Existing GNN-based GED approaches typically learn node embeddings for each graph and then aggregate pairwise node similarities to estimate the final similarity. Despite their effectiveness, we identify a mismatch between this prevalent node-centric matching paradigm and the core principles of GED. This discrepancy leads to two critical limitations: (1) a failure to capture the global structural correspondence for optimal alignment, and (2) a misattribution of edit costs driven by spurious node level signals. To address these limitations, we propose GCGSim, a GED-consistent graph similarity learning framework centering on graph-level matching and substructure-level edit costs. Specifically, we make three core technical contributions. Extensive experiments on four benchmark datasets show that GCGSim achieves state-of-the-art performance. Our comprehensive analyses further validate that the framework effectively learns disentangled and semantically meaningful substructure representations."}
{"id": "2511.19949", "pdf": "https://arxiv.org/pdf/2511.19949", "abs": "https://arxiv.org/abs/2511.19949", "authors": ["Qingda Hu", "Xinjun Yang", "Feifei Li", "Junru Li", "Ya Lin", "Yuqi Zhou", "Yicong Zhu", "Junwei Zhang", "Rongbiao Xie", "Ling Zhou", "Bin Wu", "Wenchao Zhou"], "title": "PolarStore: High-Performance Data Compression for Large-Scale Cloud-Native Databases", "categories": ["cs.DC", "cs.DB"], "comment": "13 pages, accepted by FAST'26", "summary": "In recent years, resource elasticity and cost optimization have become essential for RDBMSs. While cloud-native RDBMSs provide elastic computing resources via disaggregated computing and storage, storage costs remain a critical user concern. Consequently, data compression emerges as an effective strategy to reduce storage costs. However, existing compression approaches in RDBMSs present a stark trade-off: software-based approaches incur significant performance overheads, while hardware-based alternatives lack the flexibility required for diverse database workloads. In this paper, we present PolarStore, a compressed shared storage system for cloud-native RDBMSs. PolarStore employs a dual-layer compression mechanism that combines in-storage compression in PolarCSD hardware with lightweight compression in software. This design leverages the strengths of both approaches. PolarStore also incorporates database-oriented optimizations to maintain high performance on critical I/O paths. Drawing from large-scale deployment experiences, we also introduce hardware improvements for PolarCSD to ensure host-level stability and propose a compression-aware scheduling scheme to improve cluster-level space efficiency. PolarStore is currently deployed on thousands of storage servers within PolarDB, managing over 100 PB of data. It achieves a compression ratio of 3.55 and reduces storage costs by approximately 60%. Remarkably, these savings are achieved while maintaining performance comparable to uncompressed clusters."}
{"id": "2511.19978", "pdf": "https://arxiv.org/pdf/2511.19978", "abs": "https://arxiv.org/abs/2511.19978", "authors": ["Junru Li", "Qing Wang", "Zhe Yang", "Shuo Liu", "Jiwu Shu", "Youyou Lu"], "title": "SwitchDelta: Asynchronous Metadata Updating for Distributed Storage with In-Network Data Visibility", "categories": ["cs.DC", "cs.DB"], "comment": "12 pages, accepted by ICDE'26", "summary": "Distributed storage systems typically maintain strong consistency between data nodes and metadata nodes by adopting ordered writes: 1) first installing data; 2) then updating metadata to make data visible.We propose SwitchDelta to accelerate ordered writes by moving metadata updates out of the critical path. It buffers in-flight metadata updates in programmable switches to enable data visibility in the network and retain strong consistency. SwitchDelta uses a best-effort data plane design to overcome the resource limitation of switches and designs a novel metadata update protocol to exploit the benefits of in-network data visibility. We evaluate SwitchDelta in three distributed in-memory storage systems: log-structured key-value stores, file systems, and secondary indexes. The evaluation shows that SwitchDelta reduces the latency of write operations by up to 52.4% and boosts the throughput by up to 126.9% under write-heavy workloads."}
