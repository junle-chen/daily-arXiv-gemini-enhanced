{"id": "2510.26023", "pdf": "https://arxiv.org/pdf/2510.26023", "abs": "https://arxiv.org/abs/2510.26023", "authors": ["Zhipeng Bao", "Qianwen Li"], "title": "Large Language Model-assisted Autonomous Vehicle Recovery from Immobilization", "categories": ["cs.AI", "cs.RO"], "comment": "8 pages", "summary": "Despite significant advancements in recent decades, autonomous vehicles (AVs)\ncontinue to face challenges in navigating certain traffic scenarios where human\ndrivers excel. In such situations, AVs often become immobilized, disrupting\noverall traffic flow. Current recovery solutions, such as remote intervention\n(which is costly and inefficient) and manual takeover (which excludes\nnon-drivers and limits AV accessibility), are inadequate. This paper introduces\nStuckSolver, a novel Large Language Model (LLM) driven recovery framework that\nenables AVs to resolve immobilization scenarios through self-reasoning and/or\npassenger-guided decision-making. StuckSolver is designed as a plug-in add-on\nmodule that operates on top of the AV's existing perception-planning-control\nstack, requiring no modification to its internal architecture. Instead, it\ninterfaces with standard sensor data streams to detect immobilization states,\ninterpret environmental context, and generate high-level recovery commands that\ncan be executed by the AV's native planner. We evaluate StuckSolver on the\nBench2Drive benchmark and in custom-designed uncertainty scenarios. Results\nshow that StuckSolver achieves near-state-of-the-art performance through\nautonomous self-reasoning alone and exhibits further improvements when\npassenger guidance is incorporated.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528Large Language Model (LLM) \u6765\u8f85\u52a9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4ece\u9759\u6b62\u72b6\u6001\u6062\u590d\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u6062\u590d\u5fc5\u7136\u6d89\u53ca\u5230\u8def\u5f84\u89c4\u5212\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u800c\u4e14\u4f7f\u7528\u4e86\u5927\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Model", "LLM", "autonomous vehicle", "recovery", "perception-planning-control", "path planning"]}}
{"id": "2510.26125", "pdf": "https://arxiv.org/pdf/2510.26125", "abs": "https://arxiv.org/abs/2510.26125", "authors": ["Runsheng Xu", "Hubert Lin", "Wonseok Jeon", "Hao Feng", "Yuliang Zou", "Liting Sun", "John Gorman", "Kate Tolstaya", "Sarah Tang", "Brandyn White", "Ben Sapp", "Mingxing Tan", "Jyh-Jing Hwang", "Drago Anguelov"], "title": "WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-based end-to-end (E2E) driving has garnered significant interest in\nthe research community due to its scalability and synergy with multimodal large\nlanguage models (MLLMs). However, current E2E driving benchmarks primarily\nfeature nominal scenarios, failing to adequately test the true potential of\nthese systems. Furthermore, existing open-loop evaluation metrics often fall\nshort in capturing the multi-modal nature of driving or effectively evaluating\nperformance in long-tail scenarios. To address these gaps, we introduce the\nWaymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021\ndriving segments (approximately 12 hours), specifically curated for challenging\nlong-tail scenarios that that are rare in daily life with an occurring\nfrequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the\nhigh-level routing information, ego states, and 360-degree camera views from 8\nsurrounding cameras. To evaluate the E2E driving performance on these long-tail\nsituations, we propose a novel open-loop evaluation metric: Rater Feedback\nScore (RFS). Unlike conventional metrics that measure the distance between\npredicted way points and the logs, RFS measures how closely the predicted\ntrajectory matches rater-annotated trajectory preference labels. We have\nreleased rater preference labels for all WOD-E2E validation set segments, while\nthe held out test set labels have been used for the 2025 WOD-E2E Challenge.\nThrough our work, we aim to foster state of the art research into\ngeneralizable, robust, and safe end-to-end autonomous driving agents capable of\nhandling complex real-world situations.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u7aef\u5230\u7aef\u9a7e\u9a76\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u53ca\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u8868\u660e\u5176\u4e0e\u5927\u6a21\u578b\u9886\u57df\u4e5f\u6709\u5173\u8054\u3002\u867d\u7136\u91cd\u70b9\u53ef\u80fd\u5728\u4e8e\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u4f46\u4e0e\u4e24\u4e2a\u4e3b\u9898\u90fd\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["end-to-end driving", "trajectory", "multimodal large language models", "E2E driving", "predicted trajectory"]}}
{"id": "2510.26242", "pdf": "https://arxiv.org/pdf/2510.26242", "abs": "https://arxiv.org/abs/2510.26242", "authors": ["Xinhang Li", "Qing Guo", "Junyu Chen", "Zheng Guo", "Shengzhe Xu", "Lei Li", "Lin Zhang"], "title": "Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles", "categories": ["cs.AI"], "comment": null, "summary": "With increasing urban traffic complexity, Traffic Signal Control (TSC) is\nessential for optimizing traffic flow and improving road safety. Large Language\nModels (LLMs) emerge as promising approaches for TSC. However, they are prone\nto hallucinations in emergencies, leading to unreliable decisions that may\ncause substantial delays for emergency vehicles. Moreover, diverse intersection\ntypes present substantial challenges for traffic state encoding and\ncross-intersection training, limiting generalization across heterogeneous\nintersections. Therefore, this paper proposes Retrieval Augmented Generation\n(RAG)-enhanced distributed LLM agents with Emergency response for Generalizable\nTSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning\nframework, which dynamically adjusts reasoning depth based on the emergency\nscenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to\ndistill specific knowledge and guidance from historical cases, enhancing the\nreliability and rationality of agents' emergency decisions. Secondly, this\npaper designs a type-agnostic traffic representation and proposes a\nReward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3\nadaptively samples training experience from diverse intersections with\nenvironment feedback-based priority and fine-tunes LLM agents with a designed\nreward-weighted likelihood loss, guiding REG-TSC toward high-reward policies\nacross heterogeneous intersections. On three real-world road networks with 17\nto 177 heterogeneous intersections, extensive experiments show that REG-TSC\nreduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle\nwaiting time by 83.16%, outperforming other state-of-the-art methods.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper combines Large Language Models (LLMs) with traffic signal control, which implicitly involves predicting the movement of vehicles (a form of trajectory prediction) to optimize traffic flow. While not directly focused on trajectory prediction algorithms, it uses LLMs to control systems that rely on understanding and predicting trajectories. The mention of emergency vehicles and their waiting time further reinforces the connection to trajectory-related decision-making.", "keywords": ["Large Language Models", "LLMs", "Traffic Signal Control", "emergency vehicles", "traffic flow", "trajectory prediction (implicit)"]}}
{"id": "2510.26173", "pdf": "https://arxiv.org/pdf/2510.26173", "abs": "https://arxiv.org/abs/2510.26173", "authors": ["Wontae Choi", "Jaelin Lee", "Hyung Sup Yun", "Byeungwoo Jeon", "Il Yong Chun"], "title": "MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models", "categories": ["cs.CV"], "comment": "10 pages, 6 figures", "summary": "Accurate estimation of motion information is crucial in diverse computational\nimaging and computer vision applications. Researchers have investigated various\nmethods to extract motion information from a single blurred image, including\nblur kernels and optical flow. However, existing motion representations are\noften of low quality, i.e., coarse-grained and inaccurate. In this paper, we\npropose the first high-resolution (HR) Motion Trajectory estimation framework\nusing Diffusion models (MoTDiff). Different from existing motion\nrepresentations, we aim to estimate an HR motion trajectory with high-quality\nfrom a single motion-blurred image. The proposed MoTDiff consists of two key\ncomponents: 1) a new conditional diffusion framework that uses multi-scale\nfeature maps extracted from a single blurred image as a condition, and 2) a new\ntraining method that can promote precise identification of a fine-grained\nmotion trajectory, consistent estimation of overall shape and position of a\nmotion path, and pixel connectivity along a motion trajectory. Our experiments\ndemonstrate that the proposed MoTDiff can outperform state-of-the-art methods\nin both blind image deblurring and coded exposure photography applications.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\u201cMotion Trajectory estimation\u201d\uff0c\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u6458\u8981\u4e2d\u4f7f\u7528\u4e86Diffusion Models\uff0c\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u4f7f\u7528Large Language Models\uff0c\u4f46\u6269\u6563\u6a21\u578b\u4e5f\u5c5e\u4e8e\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u4e0e\u5927\u89c4\u6a21\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "motion trajectory estimation", "diffusion models", "motion blur"]}}
{"id": "2510.26292", "pdf": "https://arxiv.org/pdf/2510.26292", "abs": "https://arxiv.org/abs/2510.26292", "authors": ["Lin Liu", "Guanyi Yu", "Ziying Song", "Junqiao Li", "Caiyan Jia", "Feiyang Jia", "Peiliang Wu", "Yandan Luo"], "title": "Beyond Imitation: Constraint-Aware Trajectory Generation with Flow Matching For End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Planning is a critical component of end-to-end autonomous driving. However,\nprevailing imitation learning methods often suffer from mode collapse, failing\nto produce diverse trajectory hypotheses. Meanwhile, existing generative\napproaches struggle to incorporate crucial safety and physical constraints\ndirectly into the generative process, necessitating an additional optimization\nstage to refine their outputs. To address these limitations, we propose CATG, a\nnovel planning framework that leverages Constrained Flow Matching. Concretely,\nCATG explicitly models the flow matching process, which inherently mitigates\nmode collapse and allows for flexible guidance from various conditioning\nsignals. Our primary contribution is the novel imposition of explicit\nconstraints directly within the flow matching process, ensuring that the\ngenerated trajectories adhere to vital safety and kinematic rules. Secondly,\nCATG parameterizes driving aggressiveness as a control signal during\ngeneration, enabling precise manipulation of trajectory style. Notably, on the\nNavSim v2 challenge, CATG achieved 2nd place with an EPDMS score of 51.31 and\nwas honored with the Innovation Award.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u8f68\u8ff9\u751f\u6210\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u89e3\u51b3\u7684\u95ee\u9898\u548c\u65b9\u6cd5\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5bc6\u5207\u76f8\u5173\uff0c\u4e14\u4f7f\u7528\u4e86Flow Matching\u8fd9\u79cd\u53ef\u4ee5\u501f\u9274\u5230\u5927\u6a21\u578b\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6280\u672f\u3002", "keywords": ["trajectory prediction", "autonomous driving", "trajectory generation", "flow matching", "planning"]}}
{"id": "2510.26004", "pdf": "https://arxiv.org/pdf/2510.26004", "abs": "https://arxiv.org/abs/2510.26004", "authors": ["Bai Li", "Achilleas Kourtellis", "Rong Cao", "Joseph Post", "Brian Porter", "Yu Zhang"], "title": "DARTS: A Drone-Based AI-Powered Real-Time Traffic Incident Detection System", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Preprint version. This manuscript is currently under review at\n  Transportation Research Part C: Emerging Technologies. The PDF corresponds to\n  the version submitted in June 2025. The main findings of this work were\n  recognized with the Best Intelligent Transportation Systems Paper Award at\n  the 2025 TRB Annual Meeting", "summary": "Rapid and reliable incident detection is critical for reducing crash-related\nfatalities, injuries, and congestion. However, conventional methods, such as\nclosed-circuit television, dashcam footage, and sensor-based detection,\nseparate detection from verification, suffer from limited flexibility, and\nrequire dense infrastructure or high penetration rates, restricting\nadaptability and scalability to shifting incident hotspots. To overcome these\nchallenges, we developed DARTS, a drone-based, AI-powered real-time traffic\nincident detection system. DARTS integrates drones' high mobility and aerial\nperspective for adaptive surveillance, thermal imaging for better\nlow-visibility performance and privacy protection, and a lightweight deep\nlearning framework for real-time vehicle trajectory extraction and incident\ndetection. The system achieved 99% detection accuracy on a self-collected\ndataset and supports simultaneous online visual verification, severity\nassessment, and incident-induced congestion propagation monitoring via a\nweb-based interface. In a field test on Interstate 75 in Florida, DARTS\ndetected and verified a rear-end collision 12 minutes earlier than the local\ntransportation management center and monitored incident-induced congestion\npropagation, suggesting potential to support faster emergency response and\nenable proactive traffic control to reduce congestion and secondary crash risk.\nCrucially, DARTS's flexible deployment architecture reduces dependence on\nfrequent physical patrols, indicating potential scalability and\ncost-effectiveness for use in remote areas and resource-constrained settings.\nThis study presents a promising step toward a more flexible and integrated\nreal-time traffic incident detection system, with significant implications for\nthe operational efficiency and responsiveness of modern transportation\nmanagement.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u3001\u4eba\u5de5\u667a\u80fd\u548c\u5b9e\u65f6\u4ea4\u901a\u4e8b\u4ef6\u68c0\u6d4b\u3002\u867d\u7136\u63d0\u5230\u4e86\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u8fdb\u884c\u8f66\u8f86\u8f68\u8ff9\u63d0\u53d6\uff0c\u4f46\u91cd\u70b9\u5728\u4e8e\u4e8b\u4ef6\u68c0\u6d4b\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u3002\u8bba\u6587\u4e2d\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["vehicle trajectory extraction"]}}
{"id": "2510.26040", "pdf": "https://arxiv.org/pdf/2510.26040", "abs": "https://arxiv.org/abs/2510.26040", "authors": ["Emily Steiner", "Daniel van der Spuy", "Futian Zhou", "Afereti Pama", "Minas Liarokapis", "Henry Williams"], "title": "Accelerating Real-World Overtaking in F1TENTH Racing Employing Reinforcement Learning Methods", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "While autonomous racing performance in Time-Trial scenarios has seen\nsignificant progress and development, autonomous wheel-to-wheel racing and\novertaking are still severely limited. These limitations are particularly\napparent in real-life driving scenarios where state-of-the-art algorithms\nstruggle to safely or reliably complete overtaking manoeuvres. This is\nimportant, as reliable navigation around other vehicles is vital for safe\nautonomous wheel-to-wheel racing. The F1Tenth Competition provides a useful\nopportunity for developing wheel-to-wheel racing algorithms on a standardised\nphysical platform. The competition format makes it possible to evaluate\novertaking and wheel-to-wheel racing algorithms against the state-of-the-art.\nThis research presents a novel racing and overtaking agent capable of learning\nto reliably navigate a track and overtake opponents in both simulation and\nreality. The agent was deployed on an F1Tenth vehicle and competed against\nopponents running varying competitive algorithms in the real world. The results\ndemonstrate that the agent's training against opponents enables deliberate\novertaking behaviours with an overtaking rate of 87% compared 56% for an agent\ntrained just to race.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on autonomous racing and overtaking using reinforcement learning. While it doesn't explicitly mention trajectory prediction or large language models, the task of overtaking inherently involves predicting the future trajectories of other vehicles. The use of reinforcement learning also suggests a degree of prediction involved in learning optimal actions. However, the lack of explicit mention of trajectory prediction techniques or the use of large language models lowers the relevance score.", "keywords": ["overtaking", "autonomous racing", "reinforcement learning", "trajectory prediction (implied)"]}}
{"id": "2510.25951", "pdf": "https://arxiv.org/pdf/2510.25951", "abs": "https://arxiv.org/abs/2510.25951", "authors": ["Sounak Banerjee", "Daphne Cornelisse", "Deepak Gopinath", "Emily Sumner", "Jonathan DeCastro", "Guy Rosman", "Eugene Vinitsky", "Mark K. Ho"], "title": "Estimating cognitive biases with attention-aware inverse planning", "categories": ["cs.AI"], "comment": null, "summary": "People's goal-directed behaviors are influenced by their cognitive biases,\nand autonomous systems that interact with people should be aware of this. For\nexample, people's attention to objects in their environment will be biased in a\nway that systematically affects how they perform everyday tasks such as driving\nto work. Here, building on recent work in computational cognitive science, we\nformally articulate the attention-aware inverse planning problem, in which the\ngoal is to estimate a person's attentional biases from their actions. We\ndemonstrate how attention-aware inverse planning systematically differs from\nstandard inverse reinforcement learning and how cognitive biases can be\ninferred from behavior. Finally, we present an approach to attention-aware\ninverse planning that combines deep reinforcement learning with computational\ncognitive modeling. We use this approach to infer the attentional strategies of\nRL agents in real-life driving scenarios selected from the Waymo Open Dataset,\ndemonstrating the scalability of estimating cognitive biases with\nattention-aware inverse planning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u901a\u8fc7\u89c2\u5bdf\u884c\u4e3a\u6765\u63a8\u65ad\u8ba4\u77e5\u504f\u5dee\uff0c\u5e76\u5e94\u7528\u4e8e\u9a7e\u9a76\u573a\u666f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u4e14\u5728Waymo Open Dataset\u4e2d\u7684\u9a7e\u9a76\u573a\u666f\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["inverse planning", "reinforcement learning", "driving scenarios", "Waymo Open Dataset"]}}
{"id": "2510.26027", "pdf": "https://arxiv.org/pdf/2510.26027", "abs": "https://arxiv.org/abs/2510.26027", "authors": ["Ali Rasekh", "Erfan Bagheri Soula", "Omid Daliran", "Simon Gottschalk", "Mohsen Fayyaz"], "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders", "categories": ["cs.CV"], "comment": "Accepted to NeurIPS 2025", "summary": "Despite significant advances in Multimodal Large Language Models (MLLMs),\nunderstanding complex temporal dynamics in videos remains a major challenge.\nOur experiments show that current Video Large Language Model (Video-LLM)\narchitectures have critical limitations in temporal understanding, struggling\nwith tasks that require detailed comprehension of action sequences and temporal\nprogression. In this work, we propose a Video-LLM architecture that introduces\nstacked temporal attention modules directly within the vision encoder. This\ndesign incorporates a temporal attention in vision encoder, enabling the model\nto better capture the progression of actions and the relationships between\nframes before passing visual tokens to the LLM. Our results show that this\napproach significantly improves temporal reasoning and outperforms existing\nmodels in video question answering tasks, specifically in action recognition.\nWe improve on benchmarks including VITATECS, MVBench, and Video-MME by up to\n+5.5%. By enhancing the vision encoder with temporal structure, we address a\ncritical gap in video understanding for Video-LLMs. Project page and code are\navailable at: https://alirasekh.github.io/STAVEQ2/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on improving temporal understanding in Video-LLMs, which is relevant to action recognition and video understanding. While not directly related to trajectory prediction, the enhanced temporal understanding could potentially be applied to trajectory prediction tasks in the future. The paper explicitly mentions Video Large Language Models (Video-LLM).", "keywords": ["Large Language Models", "Video-LLM", "temporal understanding", "action recognition", "vision encoder"]}}
{"id": "2510.26139", "pdf": "https://arxiv.org/pdf/2510.26139", "abs": "https://arxiv.org/abs/2510.26139", "authors": ["Minseo Kwon", "Young J. Kim"], "title": "Kinodynamic Task and Motion Planning using VLM-guided and Interleaved Sampling", "categories": ["cs.RO"], "comment": null, "summary": "Task and Motion Planning (TAMP) integrates high-level task planning with\nlow-level motion feasibility, but existing methods are costly in long-horizon\nproblems due to excessive motion sampling. While LLMs provide commonsense\npriors, they lack 3D spatial reasoning and cannot ensure geometric or dynamic\nfeasibility. We propose a kinodynamic TAMP framework based on a hybrid state\ntree that uniformly represents symbolic and numeric states during planning,\nenabling task and motion decisions to be jointly decided. Kinodynamic\nconstraints embedded in the TAMP problem are verified by an off-the-shelf\nmotion planner and physics simulator, and a VLM guides exploring a TAMP\nsolution and backtracks the search based on visual rendering of the states.\nExperiments on the simulated domains and in the real world show 32.14% -\n1166.67% increased average success rates compared to traditional and LLM-based\nTAMP planners and reduced planning time on complex problems, with ablations\nfurther highlighting the benefits of VLM guidance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6307\u5bfc\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\uff08TAMP\uff09\u3002\u867d\u7136TAMP\u53ef\u4ee5\u95f4\u63a5\u4e0e\u8fd0\u52a8\u8f68\u8ff9\u76f8\u5173\uff0c\u5e76\u4e14\u8bba\u6587\u4f7f\u7528\u4e86VLM\uff08\u4e00\u79cd\u5927\u6a21\u578b\uff09\uff0c\u4f46\u5176\u4e3b\u8981\u5173\u6ce8\u70b9\u5728\u4e8e\u4efb\u52a1\u89c4\u5212\u800c\u975e\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["VLM", "Large Language Models", "Task and Motion Planning", "Kinodynamic"]}}
{"id": "2510.26142", "pdf": "https://arxiv.org/pdf/2510.26142", "abs": "https://arxiv.org/abs/2510.26142", "authors": ["Hahjin Lee", "Young J. Kim"], "title": "Adaptive Trajectory Refinement for Optimization-based Local Planning in Narrow Passages", "categories": ["cs.RO"], "comment": null, "summary": "Trajectory planning for mobile robots in cluttered environments remains a\nmajor challenge due to narrow passages, where conventional methods often fail\nor generate suboptimal paths. To address this issue, we propose the adaptive\ntrajectory refinement algorithm, which consists of two main stages. First, to\nensure safety at the path-segment level, a segment-wise conservative collision\ntest is applied, where risk-prone trajectory path segments are recursively\nsubdivided until collision risks are eliminated. Second, to guarantee\npose-level safety, pose correction based on penetration direction and line\nsearch is applied, ensuring that each pose in the trajectory is collision-free\nand maximally clear from obstacles. Simulation results demonstrate that the\nproposed method achieves up to 1.69x higher success rates and up to 3.79x\nfaster planning times than state-of-the-art approaches. Furthermore, real-world\nexperiments confirm that the robot can safely pass through narrow passages\nwhile maintaining rapid planning performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory planning for mobile robots, which is related to trajectory prediction. While it doesn't explicitly mention or utilize large language models, the core topic of trajectory planning falls within the scope of trajectory prediction. The paper describes an optimization-based local planning approach for navigating narrow passages.", "keywords": ["trajectory planning", "mobile robots", "trajectory refinement", "optimization-based planning", "collision avoidance"]}}
{"id": "2510.26089", "pdf": "https://arxiv.org/pdf/2510.26089", "abs": "https://arxiv.org/abs/2510.26089", "authors": ["Fazel Arasteh", "Arian Haghparast", "Manos Papagelis"], "title": "Network-Constrained Policy Optimization for Adaptive Multi-agent Vehicle Routing", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "29 pages, 12 figures. Fazel Arasteh and Arian Haghparast contributed\n  equally to this research. Submitted to ACM Transactions on Spatial Algorithms\n  and Systems (TSAS). The code for this work is publicly available at\n  https://github.com/Arianhgh/HHAN", "summary": "Traffic congestion in urban road networks leads to longer trip times and\nhigher emissions, especially during peak periods. While the Shortest Path First\n(SPF) algorithm is optimal for a single vehicle in a static network, it\nperforms poorly in dynamic, multi-vehicle settings, often worsening congestion\nby routing all vehicles along identical paths. We address dynamic vehicle\nrouting through a multi-agent reinforcement learning (MARL) framework for\ncoordinated, network-aware fleet navigation. We first propose Adaptive\nNavigation (AN), a decentralized MARL model where each intersection agent\nprovides routing guidance based on (i) local traffic and (ii) neighborhood\nstate modeled using Graph Attention Networks (GAT). To improve scalability in\nlarge networks, we further propose Hierarchical Hub-based Adaptive Navigation\n(HHAN), an extension of AN that assigns agents only to key intersections\n(hubs). Vehicles are routed hub-to-hub under agent control, while SPF handles\nmicro-routing within each hub region. For hub coordination, HHAN adopts\ncentralized training with decentralized execution (CTDE) under the Attentive\nQ-Mixing (A-QMIX) framework, which aggregates asynchronous vehicle decisions\nvia attention. Hub agents use flow-aware state features that combine local\ncongestion and predictive dynamics for proactive routing. Experiments on\nsynthetic grids and real urban maps (Toronto, Manhattan) show that AN reduces\naverage travel time versus SPF and learning baselines, maintaining 100% routing\nsuccess. HHAN scales to networks with hundreds of intersections, achieving up\nto 15.9% improvement under heavy traffic. These findings highlight the\npotential of network-constrained MARL for scalable, coordinated, and\ncongestion-aware routing in intelligent transportation systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-agent vehicle routing using reinforcement learning, specifically addressing dynamic vehicle routing and coordinated fleet navigation. While it doesn't directly involve Large Language Models, the core problem of vehicle routing and the use of predictive dynamics for proactive routing are related to trajectory prediction. The use of Graph Attention Networks (GAT) for modeling neighborhood state is also relevant. Therefore, it has a moderate relevance to trajectory prediction but no relevance to Large Language Models.", "keywords": ["vehicle routing", "multi-agent reinforcement learning", "dynamic vehicle routing", "trajectory prediction", "Graph Attention Networks"]}}
{"id": "2510.26369", "pdf": "https://arxiv.org/pdf/2510.26369", "abs": "https://arxiv.org/abs/2510.26369", "authors": ["Kazuma Kano", "Yuki Mori", "Shin Katayama", "Kenta Urano", "Takuro Yonezawa", "Nobuo Kawaguchi"], "title": "CorVS: Person Identification via Video Trajectory-Sensor Correspondence in a Real-World Warehouse", "categories": ["cs.LG", "cs.CV", "cs.RO"], "comment": "7 pages, 3 figures, accepted to IPIN 2025", "summary": "Worker location data is key to higher productivity in industrial sites.\nCameras are a promising tool for localization in logistics warehouses since\nthey also offer valuable environmental contexts such as package status.\nHowever, identifying individuals with only visual data is often impractical.\nAccordingly, several prior studies identified people in videos by comparing\ntheir trajectories and wearable sensor measurements. While this approach has\nadvantages such as independence from appearance, the existing methods may break\ndown under real-world conditions. To overcome this challenge, we propose CorVS,\na novel data-driven person identification method based on correspondence\nbetween visual tracking trajectories and sensor measurements. Firstly, our deep\nlearning model predicts correspondence probabilities and reliabilities for\nevery pair of a trajectory and sensor measurements. Secondly, our algorithm\nmatches the trajectories and sensor measurements over time using the predicted\nprobabilities and reliabilities. We developed a dataset with actual warehouse\noperations and demonstrated the method's effectiveness for real-world\napplications.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on person identification using video trajectories and sensor data in a warehouse setting. While it involves trajectory analysis and prediction (implicitly through tracking), it doesn't directly relate to Large Language Models. The connection to trajectory prediction is present but not the primary focus, and there's no mention of LLMs.", "keywords": ["trajectory prediction", "visual tracking", "deep learning"]}}
{"id": "2510.26531", "pdf": "https://arxiv.org/pdf/2510.26531", "abs": "https://arxiv.org/abs/2510.26531", "authors": ["David Leprich", "Mario Rosenfelder", "Markus Herrmann-Wicklmayr", "Kathrin Fla\u00dfkamp", "Peter Eberhard", "Henrik Ebel"], "title": "Efficient Collision-Avoidance Constraints for Ellipsoidal Obstacles in Optimal Control: Application to Path-Following MPC and UAVs", "categories": ["eess.SY", "cs.RO", "cs.SY", "93-XX"], "comment": null, "summary": "This article proposes a modular optimal control framework for local\nthree-dimensional ellipsoidal obstacle avoidance, exemplarily applied to model\npredictive path-following control. Static as well as moving obstacles are\nconsidered. Central to the approach is a computationally efficient and\ncontinuously differentiable condition for detecting collisions with ellipsoidal\nobstacles. A novel two-stage optimization approach mitigates numerical issues\narising from the structure of the resulting optimal control problem. The\neffectiveness of the approach is demonstrated through simulations and\nreal-world experiments with the Crazyflie quadrotor. This represents the first\nhardware demonstration of an MPC controller of this kind for UAVs in a\nthree-dimensional task.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on collision avoidance and path-following control, which are related to trajectory prediction, especially in the context of UAVs. However, it doesn't involve large language models. The relevance stems from the trajectory planning and obstacle avoidance aspects, which are components of trajectory prediction.", "keywords": ["trajectory prediction", "path-following", "obstacle avoidance", "UAVs", "optimal control"]}}
{"id": "2510.26443", "pdf": "https://arxiv.org/pdf/2510.26443", "abs": "https://arxiv.org/abs/2510.26443", "authors": ["Rhodri Guerrier", "Adam W. Harley", "Dima Damen"], "title": "PointSt3R: Point Tracking through 3D Grounded Correspondence", "categories": ["cs.CV"], "comment": "http://rhodriguerrier.github.io/PointSt3R", "summary": "Recent advances in foundational 3D reconstruction models, such as DUSt3R and\nMASt3R, have shown great potential in 2D and 3D correspondence in static\nscenes. In this paper, we propose to adapt them for the task of point tracking\nthrough 3D grounded correspondence. We first demonstrate that these models are\ncompetitive point trackers when focusing on static points, present in current\npoint tracking benchmarks ($+33.5\\%$ on EgoPoints vs. CoTracker2). We propose\nto combine the reconstruction loss with training for dynamic correspondence\nalong with a visibility head, and fine-tuning MASt3R for point tracking using a\nrelatively small amount of synthetic data. Importantly, we only train and\nevaluate on pairs of frames where one contains the query point, effectively\nremoving any temporal context. Using a mix of dynamic and static point\ncorrespondences, we achieve competitive or superior point tracking results on\nfour datasets (e.g. competitive on TAP-Vid-DAVIS 73.8 $\\delta_{avg}$ / 85.8\\%\nocclusion acc. for PointSt3R compared to 75.7 / 88.3\\% for CoTracker2; and\nsignificantly outperform CoTracker3 on EgoPoints 61.3 vs 54.2 and RGB-S 87.0 vs\n82.8). We also present results on 3D point tracking along with several\nablations on training datasets and percentage of dynamic correspondences.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on point tracking, which is related to trajectory prediction. It leverages foundation models (DUSt3R and MASt3R) for 3D reconstruction and correspondence. While it doesn't directly address trajectory prediction as a primary goal, the underlying techniques are relevant. The mention of \"foundational 3D reconstruction models\" hints at the use of large models, but the core contribution is in point tracking, not the development or application of LLMs themselves.", "keywords": ["point tracking", "3D correspondence", "foundation models", "reconstruction"]}}
{"id": "2510.26580", "pdf": "https://arxiv.org/pdf/2510.26580", "abs": "https://arxiv.org/abs/2510.26580", "authors": ["Manjunath Prasad Holenarasipura Rajiv", "B. M. Vidyavathi"], "title": "Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios", "categories": ["cs.CV"], "comment": "Preprint under review at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), 2025", "summary": "In real-world environments, AI systems often face unfamiliar scenarios\nwithout labeled data, creating a major challenge for conventional scene\nunderstanding models. The inability to generalize across unseen contexts limits\nthe deployment of vision-based applications in dynamic, unstructured settings.\nThis work introduces a Dynamic Context-Aware Scene Reasoning framework that\nleverages Vision-Language Alignment to address zero-shot real-world scenarios.\nThe goal is to enable intelligent systems to infer and adapt to new\nenvironments without prior task-specific training. The proposed approach\nintegrates pre-trained vision transformers and large language models to align\nvisual semantics with natural language descriptions, enhancing contextual\ncomprehension. A dynamic reasoning module refines predictions by combining\nglobal scene cues and object-level interactions guided by linguistic priors.\nExtensive experiments on zero-shot benchmarks such as COCO, Visual Genome, and\nOpen Images demonstrate up to 18% improvement in scene understanding accuracy\nover baseline models in complex and unseen environments. Results also show\nrobust performance in ambiguous or cluttered scenes due to the synergistic\nfusion of vision and language. This framework offers a scalable and\ninterpretable approach for context-aware reasoning, advancing zero-shot\ngeneralization in dynamic real-world settings.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u8fdb\u884c\u573a\u666f\u7406\u89e3\uff0c\u4f7f\u7528\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9Transformer\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u5173\u6ce8\u70b9\u5728\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u5bf9\u573a\u666f\u7684\u7406\u89e3\u548c\u63a8\u7406\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e2d\u5bf9\u73af\u5883\u4e0a\u4e0b\u6587\u7684\u7406\u89e3\u6709\u4e00\u5b9a\u5173\u8054\u3002\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "Vision-Language Alignment", "Scene Reasoning", "Vision Transformers", "dynamic environments"]}}
{"id": "2510.26583", "pdf": "https://arxiv.org/pdf/2510.26583", "abs": "https://arxiv.org/abs/2510.26583", "authors": ["Yufeng Cui", "Honghao Chen", "Haoge Deng", "Xu Huang", "Xinghang Li", "Jirong Liu", "Yang Liu", "Zhuoyan Luo", "Jinsheng Wang", "Wenxuan Wang", "Yueze Wang", "Chengyuan Wang", "Fan Zhang", "Yingli Zhao", "Ting Pan", "Xianduo Li", "Zecheng Hao", "Wenxuan Ma", "Zhuo Chen", "Yulong Ao", "Tiejun Huang", "Zhongyuan Wang", "Xinlong Wang"], "title": "Emu3.5: Native Multimodal Models are World Learners", "categories": ["cs.CV"], "comment": "project page: https://emu.world", "summary": "We introduce Emu3.5, a large-scale multimodal world model that natively\npredicts the next state across vision and language. Emu3.5 is pre-trained\nend-to-end with a unified next-token prediction objective on a corpus of\nvision-language interleaved data containing over 10 trillion tokens, primarily\nderived from sequential frames and transcripts of internet videos. The model\nnaturally accepts interleaved vision-language inputs and generates interleaved\nvision-language outputs. Emu3.5 is further post-trained with large-scale\nreinforcement learning to enhance multimodal reasoning and generation. To\nimprove inference efficiency, we propose Discrete Diffusion Adaptation (DiDA),\nwhich converts token-by-token decoding into bidirectional parallel prediction,\naccelerating per-image inference by about 20x without sacrificing performance.\nEmu3.5 exhibits strong native multimodal capabilities, including long-horizon\nvision-language generation, any-to-image (X2I) generation, and complex\ntext-rich image generation. It also exhibits generalizable world-modeling\nabilities, enabling spatiotemporally consistent world exploration and\nopen-world embodied manipulation across diverse scenarios and tasks. For\ncomparison, Emu3.5 achieves performance comparable to Gemini 2.5 Flash Image\n(Nano Banana) on image generation and editing tasks and demonstrates superior\nresults on a suite of interleaved generation tasks. We open-source Emu3.5 at\nhttps://github.com/baaivision/Emu3.5 to support community research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper introduces Emu3.5, a large-scale multimodal world model that predicts the next state across vision and language. While it doesn't explicitly focus on trajectory prediction, its world-modeling abilities, spatiotemporally consistent world exploration, and open-world embodied manipulation suggest potential relevance to trajectory prediction, especially in the context of embodied agents or robots navigating complex environments. The mention of a large-scale model also connects it to the domain of large language models.", "keywords": ["large-scale multimodal world model", "vision-language", "next-token prediction", "world-modeling", "spatiotemporally consistent world exploration", "embodied manipulation"]}}
{"id": "2510.26641", "pdf": "https://arxiv.org/pdf/2510.26641", "abs": "https://arxiv.org/abs/2510.26641", "authors": ["Sayed Pedram Haeri Boroujeni", "Niloufar Mehrabi", "Hazim Alzorgan", "Ahmad Sarlak", "Mahlagha Fazeli", "Abolfazl Razi"], "title": "All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous Vehicles (AVs) are transforming the future of transportation\nthrough advances in intelligent perception, decision-making, and control\nsystems. However, their success is tied to one core capability, reliable object\ndetection in complex and multimodal environments. While recent breakthroughs in\nComputer Vision (CV) and Artificial Intelligence (AI) have driven remarkable\nprogress, the field still faces a critical challenge as knowledge remains\nfragmented across multimodal perception, contextual reasoning, and cooperative\nintelligence. This survey bridges that gap by delivering a forward-looking\nanalysis of object detection in AVs, emphasizing emerging paradigms such as\nVision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI\nrather than re-examining outdated techniques. We begin by systematically\nreviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR,\nand Radar) and their fusion strategies, highlighting not only their\ncapabilities and limitations in dynamic driving environments but also their\npotential to integrate with recent advances in LLM/VLM-driven perception\nframeworks. Next, we introduce a structured categorization of AV datasets that\nmoves beyond simple collections, positioning ego-vehicle, infrastructure-based,\nand cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a\ncross-analysis of data structures and characteristics. Ultimately, we analyze\ncutting-edge detection methodologies, ranging from 2D and 3D pipelines to\nhybrid sensor fusion, with particular attention to emerging transformer-driven\napproaches powered by Vision Transformers (ViTs), Large and Small Language\nModels (SLMs), and VLMs. By synthesizing these perspectives, our survey\ndelivers a clear roadmap of current capabilities, open challenges, and future\nopportunities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses object detection in autonomous vehicles and mentions the integration of Large Language Models (LLMs) and Vision-Language Models (VLMs) for perception. While it doesn't directly focus on trajectory prediction, object detection is a crucial component for trajectory prediction in autonomous driving scenarios. Therefore, there is some relevance, especially considering the discussion of LLMs/VLMs.", "keywords": ["Large Language Models", "LLMs", "Vision-Language Models", "VLMs", "Autonomous Vehicles", "object detection"]}}
