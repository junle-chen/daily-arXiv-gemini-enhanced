{"id": "2512.00105", "pdf": "https://arxiv.org/pdf/2512.00105", "abs": "https://arxiv.org/abs/2512.00105", "authors": ["Djawad Bekkoucha", "Lamine Diop", "Abdelkader Ouali", "Bruno Cr\u00e9milleux", "Patrice Boizumault"], "title": "Efficiently Sampling Interval Patterns from Numerical Databases", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Pattern sampling has emerged as a promising approach for information discovery in large databases, allowing analysts to focus on a manageable subset of patterns. In this approach, patterns are randomly drawn based on an interestingness measure, such as frequency or hyper-volume. This paper presents the first sampling approach designed to handle interval patterns in numerical databases. This approach, named Fips, samples interval patterns proportionally to their frequency. It uses a multi-step sampling procedure and addresses a key challenge in numerical data: accurately determining the number of interval patterns that cover each object. We extend this work with HFips, which samples interval patterns proportionally to both their frequency and hyper-volume. These methods efficiently tackle the well-known long-tail phenomenon in pattern sampling. We formally prove that Fips and HFips sample interval patterns in proportion to their frequency and the product of hyper-volume and frequency, respectively. Through experiments on several databases, we demonstrate the quality of the obtained patterns and their robustness against the long-tail phenomenon."}
{"id": "2512.00662", "pdf": "https://arxiv.org/pdf/2512.00662", "abs": "https://arxiv.org/abs/2512.00662", "authors": ["Christian Mancas", "Diana Christina Mancas"], "title": "MatBase algorithm for translating (E)MDM schemes into E-R data models", "categories": ["cs.DB"], "comment": "Submitted on 11/27/2025 to the Journal of Data Science and Intelligent Systems, BON VIEW PUB. PTE. LTD, Singapore", "summary": "This paper presents a pseudocode algorithm for translating (Elementary) Mathematical Data Model schemes into Entity-Relationship data models. We prove that this algorithm is linear, sound, complete, and semi-optimal. As an example, we apply this algorithm to an (Elementary) Mathematical Data Model scheme for a genealogical tree sub-universe. We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models."}
{"id": "2512.01092", "pdf": "https://arxiv.org/pdf/2512.01092", "abs": "https://arxiv.org/abs/2512.01092", "authors": ["Sofia Sideri", "Georgia Troullinou", "Elisjana Ymeralli", "Vasilis Efthymiou", "Dimitris Plexousakis", "Haridimos Kondylakis"], "title": "PG-HIVE: Hybrid Incremental Schema Discovery for Property Graphs", "categories": ["cs.DB"], "comment": null, "summary": "Property graphs have rapidly become the de facto standard for representing and managing complex, interconnected data, powering applications across domains from knowledge graphs to social networks. Despite the advantages, their schema-free nature poses major challenges for integration, exploration, visualization, and efficient querying. To bridge this gap, we present PG-HIVE, a novel framework for automatic schema discovery in property graphs. PG-HIVE goes beyond existing approaches by uncovering latent node and edge types, inferring property datatypes, constraints, and cardinalities, and doing so even in the absence of explicit labeling information. Leveraging a unique combination of Locality-Sensitive Hashing with property- and label-based clustering, PG-HIVE identifies structural similarities at scale. Moreover, it introduces incremental schema discovery, eliminating costly recomputation as new data arrives. Through extensive experimentation, we demonstrate that PG-HIVE consistently outperforms state-of-the-art solutions, in both accuracy (by up to 65% for nodes and 40% for edges), and efficiency (up to 1.95x faster execution), unlocking the full potential of schema-aware property graph management."}
{"id": "2512.01490", "pdf": "https://arxiv.org/pdf/2512.01490", "abs": "https://arxiv.org/abs/2512.01490", "authors": ["Marius Ottosen", "Magnus Keinicke Parlo", "Philippe Bonnet"], "title": "DuckDB on xNVMe", "categories": ["cs.DB"], "comment": null, "summary": "DuckDB is designed for portability. It is also designed to run anywhere, and possibly in contexts where it can be specialized for performance, e.g., as a cloud service or on a smart device. In this paper, we consider the way DuckDB interacts with local storage. Our long term research question is whether and how SSDs could be co-designed with DuckDB. As a first step towards vertical integration of DuckDB and programmable SSDs, we consider whether and how DuckDB can access NVMe SSDs directly. By default, DuckDB relies on the POSIX file interface. In contrast, we rely on the xNVMe library and explore how it can be leveraged in DuckDB. We leverage the block-based nature of the DuckDB buffer manager to bypass the synchronous POSIX I/O interface, the file system and the block manager. Instead, we directly issue asynchronous I/Os against the SSD logical block address space. Our preliminary experimental study compares different ways to manage asynchronous I/Os atop xNVMe. The speed-up we observe over the DuckDB baseline is significant, even for the simplest scan query over a TPC-H table. As expected, the speed-up increases with the scale factor, and the Linux NVMe passthru improves performance. Future work includes a more thorough experimental study, a flexible solution that combines raw NVMe access and legacy POSIX file interface as well the co-design of DuckDB and SSDs."}
{"id": "2512.01693", "pdf": "https://arxiv.org/pdf/2512.01693", "abs": "https://arxiv.org/abs/2512.01693", "authors": ["Honghui Kim", "Dohoon Kim", "Jihan Kim"], "title": "LLM-Driven Multi-Agent Curation and Expansion of Metal-Organic Frameworks Database", "categories": ["cs.DB", "cond-mat.mtrl-sci"], "comment": null, "summary": "Metal-organic framework (MOF) databases have grown rapidly through experimental deposition and large-scale literature extraction, but recent analyses show that nearly half of their entries contain substantial structural errors. These inaccuracies propagate through high-throughput screening and machine-learning workflows, limiting the reliability of data-driven MOF discovery. Correcting such errors is exceptionally difficult because true repairs require integrating crystallographic files, synthesis descriptions, and contextual evidence scattered across the literature. Here we introduce LitMOF, a large language model-driven multi-agent framework that validates crystallographic information directly from the original literature and cross-validates it with database entries to repair structural errors. Applying LitMOF to the experimental MOF database (the CSD MOF Subset), we constructed LitMOF-DB, a curated set 118,464 computation-ready structures, including corrections of 69% (6,161 MOFs) of the invalid MOFs in the latest CoRE MOF database. Additionally, the system uncovered 12,646 experimentally reported MOFs absent from existing resources, substantially expanding the known experimental design space. This work establishes a scalable pathway toward self-correcting scientific databases and a generalizable paradigm for LLM-driven curation in materials science."}
{"id": "2512.01733", "pdf": "https://arxiv.org/pdf/2512.01733", "abs": "https://arxiv.org/abs/2512.01733", "authors": ["Heyang Li", "Anthony Widjaja Lin", "Domagoj Vrgo\u010d"], "title": "Answering Constraint Path Queries over Graphs", "categories": ["cs.DB"], "comment": null, "summary": "Constraints are powerful declarative constructs that allow users to\n  conveniently restrict variable values that potentially range over an\n  infinite domain. In this paper, we propose a constraint path query language\n  over property graphs,\n  which extends Regular Path Queries (RPQs) with SMT constraints on data\n  attributes in the form of equality constraints and Linear\n  Real Arithmetic (LRA) constraints. We provide efficient algorithms\n  for evaluating such path queries over property graphs, which exploits\n  optimization of macro-states (among others, using theory-specific\n  techniques).\n  In particular, we demonstrate how such an algorithm may effectively utilize\n  highly optimized SMT solvers for resolving such constraints over paths.\n  We implement our algorithm in MillenniumDB, an open-source graph engine\n  supporting property graph queries and GQL. Our extensive empirical\n  evaluation in a real-world setting demonstrates the viability of our\n  approach."}
{"id": "2512.00645", "pdf": "https://arxiv.org/pdf/2512.00645", "abs": "https://arxiv.org/abs/2512.00645", "authors": ["Boyd Franken", "Hong-Hanh Nguyen-Le", "Nhien-An Le-Khac"], "title": "Blockchain-based vs. SQL Database Systems for Digital Twin Evidence Management: A Comparative Forensic Analysis", "categories": ["cs.CR", "cs.DB"], "comment": "Accepted at EAI International Conference on Digital Forensics & Cyber Crime 2025", "summary": "Digital forensics faces unprecedented challenges with the emergence of digital twins and metaverse technologies. This paper presents the first comparative analysis between blockchain-based and traditional database systems for managing digital twin evidence in forensic investigations. We conducted controlled experiments comparing the Ethereum blockchain with IPFS storage against traditional SQL databases for digital twin evidence management. Our findings reveal that while blockchain provides superior data integrity and immutability, crucial for forensic applications, traditional databases offer better performance consistency. The blockchain implementation showed faster average storage times but higher variability in retrieval operations. Both systems maintained forensic integrity through hash verification, though blockchain's immutable nature provides additional security guarantees essential for legal proceedings. This research contributes to the development of robust digital forensic methodologies for emerging technologies in the metaverse era."}
{"id": "2512.00804", "pdf": "https://arxiv.org/pdf/2512.00804", "abs": "https://arxiv.org/abs/2512.00804", "authors": ["Hao Wu", "Prateek Saxena"], "title": "Bias Injection Attacks on RAG Databases and Sanitization Defenses", "categories": ["cs.CR", "cs.AI", "cs.DB"], "comment": null, "summary": "This paper explores attacks and defenses on vector databases in retrieval-augmented generation (RAG) systems. Prior work on knowledge poisoning attacks primarily inject false or toxic content, which fact-checking or linguistic analysis easily detects. We reveal a new and subtle threat: bias injection attacks, which insert factually correct yet semantically biased passages into the knowledge base to covertly influence the ideological framing of answers generated by large language models (LLMs). We demonstrate that these adversarial passages, though linguistically coherent and truthful, can systematically crowd out opposing views from the retrieved context and steer LLM answers toward the attacker's intended perspective.\n  We precisely characterize this class of attacks and then develop a post-retrieval filtering defense, BiasDef. We construct a comprehensive benchmark based on public question answering datasets to evaluate them. Our results show that: (1) the proposed attack induces significant perspective shifts in LLM answers, effectively evading existing retrieval-based sanitization defenses; and (2) BiasDef outperforms existing methods by reducing adversarial passages retrieved by 15\\% which mitigates perspective shift by 6.2\\times in answers, while enabling the retrieval of 62\\% more benign passages."}
{"id": "2512.00870", "pdf": "https://arxiv.org/pdf/2512.00870", "abs": "https://arxiv.org/abs/2512.00870", "authors": ["Sven Groppe", "Valter Uotila", "Jinghua Groppe"], "title": "Opportunities and Challenges for Data Quality in the Era of Quantum Computing", "categories": ["quant-ph", "cs.DB"], "comment": "14 pages; 3 figures; 2 tables", "summary": "In an era where data underpins decision-making across science, politics, and economics, ensuring high data quality is of paramount importance. Conventional computing algorithms for enhancing data quality, including anomaly detection, demand substantial computational resources, lengthy processing times, and extensive training datasets. This work aims to explore the potential advantages of quantum computing for enhancing data quality, with a particular focus on detection. We begin by examining quantum techniques that could replace key subroutines in conventional anomaly detection frameworks to mitigate their computational intensity. We then provide practical demonstrations of quantum-based anomaly detection methods, highlighting their capabilities. We present a technical implementation for detecting volatility regime changes in stock market data using quantum reservoir computing, which is a special type of quantum machine learning model. The experimental results indicate that quantum-based embeddings are a competitive alternative to classical ones in this particular example. Finally, we identify unresolved challenges and limitations in applying quantum computing to data quality tasks. Our findings open up new avenues for innovative research and commercial applications that aim to advance data quality through quantum technologies."}
{"id": "2512.01769", "pdf": "https://arxiv.org/pdf/2512.01769", "abs": "https://arxiv.org/abs/2512.01769", "authors": ["Hafsa Billah"], "title": "VideoScoop: A Non-Traditional Domain-Independent Framework For Video Analysis", "categories": ["cs.CV", "cs.DB"], "comment": "This is a report submitted as part of PhD proposal defense of Hafsa Billah", "summary": "Automatically understanding video contents is important for several applications in Civic Monitoring (CM), general Surveillance (SL), Assisted Living (AL), etc. Decades of Image and Video Analysis (IVA) research have advanced tasks such as content extraction (e.g., object recognition and tracking). Identifying meaningful activities or situations (e.g., two objects coming closer) remains difficult and cannot be achieved by content extraction alone. Currently, Video Situation Analysis (VSA) is done manually with a human in the loop, which is error-prone and labor-intensive, or through custom algorithms designed for specific video types or situations. These algorithms are not general-purpose and require a new algorithm/software for each new situation or video from a new domain.\n  This report proposes a general-purpose VSA framework that overcomes the above limitations. Video contents are extracted once using state-of-the-art Video Content Extraction technologies. They are represented using two alternative models -- the extended relational model (R++) and graph models. When represented using R++, the extracted contents can be used as data streams, enabling Continuous Query Processing via the proposed Continuous Query Language for Video Analysis. The graph models complement this by enabling the detection of situations that are difficult or impossible to detect using the relational model alone. Existing graph algorithms and newly developed algorithms support a wide variety of situation detection. To support domain independence, primitive situation variants across domains are identified and expressed as parameterized templates. Extensive experiments were conducted across several interesting situations from three domains -- AL, CM, and SL-- to evaluate the accuracy, efficiency, and robustness of the proposed approach using a dataset of videos of varying lengths from these domains."}
