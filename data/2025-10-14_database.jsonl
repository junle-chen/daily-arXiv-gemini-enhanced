{"id": "2510.08863", "pdf": "https://arxiv.org/pdf/2510.08863", "abs": "https://arxiv.org/abs/2510.08863", "authors": ["Deep Bodra", "Sushil Khairnar"], "title": "Comparative Performance Analysis of Modern NoSQL Data Technologies: Redis, Aerospike, and Dragonfly", "categories": ["cs.DB", "cs.DC"], "comment": "NoSQL databases, performance benchmarking, cloud computing, Redis;\n  Aerospike, Dragonfly", "summary": "The rise of distributed applications and cloud computing has created a demand\nfor scalable, high-performance key-value storage systems. This paper presents a\nperformance evaluation of three prominent NoSQL key-value stores: Redis,\nAerospike, and Dragonfly, using the Yahoo! Cloud Serving Benchmark (YCSB)\nframework. We conducted extensive experiments across three distinct workload\npatterns (read-heavy, write-heavy), and balanced while systematically varying\nclient concurrency from 1 to 32 clients. Our evaluation methodology captures\nboth latency, throughput, and memory characteristics under realistic\noperational conditions, providing insights into the performance trade-offs and\nscalability behaviour of each system"}
{"id": "2510.08896", "pdf": "https://arxiv.org/pdf/2510.08896", "abs": "https://arxiv.org/abs/2510.08896", "authors": ["Suming Qiu", "Jing Li", "Zhicheng Zhou", "Junjie Huang", "Linyuan Qiu", "Zhijie Sun"], "title": "HES-SQL: Hybrid Reasoning for Efficient Text-to-SQL with Structural Skeleton Guidance", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "We present HES-SQL, a novel hybrid training framework that advances\nText-to-SQL generation through the integration of thinking-mode-fused\nsupervised fine-tuning (SFT) with Group Relative Policy Optimization (GRPO).\nOur approach introduces three key innovations: (1) a skeleton-completeness\nscoring mechanism that enhances preference alignment between generated queries\nand optimal SQL structures; (2) a query-latency-aware reward system that\nincentivizes the generation of computationally efficient SQL queries; (3) a\nself-distillation process for thinking-mode completion that prevents\ndegradation of the model's reasoning capabilities. This framework enables\nhybrid thinking models to switch between reasoning and non-reasoning modes\nwhile improving SQL query accuracy and execution efficiency.\n  Experimental evaluation, conducted on MySQL 8.0 and SQLite 3.42 under\ncontrolled single-user conditions, demonstrates that HES-SQL achieves\ncompetitive performance with execution accuracies of 79.14\\% and 54.9\\% on the\nBIRD and KaggleDBQA benchmarks, respectively. Query latency is measured as the\nend-to-end execution time of generated queries on the DBMS, averaged over\nmultiple runs to mitigate variance. Efficiency gains range from 11\\% to 20\\%\nrelative to supervised baselines. Our results establish a new paradigm for\nText-to-SQL systems that effectively balances semantic accuracy with\ncomputational efficiency through execution-informed reinforcement learning\n(RL). The proposed methodology has significant implications for developing\nrobust natural language interfaces to databases and can be extended to broader\nstructured generation tasks requiring both correctness and efficiency\noptimization."}
{"id": "2510.08747", "pdf": "https://arxiv.org/pdf/2510.08747", "abs": "https://arxiv.org/abs/2510.08747", "authors": ["Yihao Ang", "Peicheng Yao", "Yifan Bao", "Yushuo Feng", "Qiang Huang", "Anthony K. H. Tung", "Zhiyong Huang"], "title": "RFOD: Random Forest-based Outlier Detection for Tabular Data", "categories": ["cs.LG", "cs.DB"], "comment": "13 pages, 13 figures, and 4 tables", "summary": "Outlier detection in tabular data is crucial for safeguarding data integrity\nin high-stakes domains such as cybersecurity, financial fraud detection, and\nhealthcare, where anomalies can cause serious operational and economic impacts.\nDespite advances in both data mining and deep learning, many existing methods\nstruggle with mixed-type tabular data, often relying on encoding schemes that\nlose important semantic information. Moreover, they frequently lack\ninterpretability, offering little insight into which specific values cause\nanomalies. To overcome these challenges, we introduce \\textsf{\\textbf{RFOD}}, a\nnovel \\textsf{\\textbf{R}}andom \\textsf{\\textbf{F}}orest-based\n\\textsf{\\textbf{O}}utlier \\textsf{\\textbf{D}}etection framework tailored for\ntabular data. Rather than modeling a global joint distribution, \\textsf{RFOD}\nreframes anomaly detection as a feature-wise conditional reconstruction\nproblem, training dedicated random forests for each feature conditioned on the\nothers. This design robustly handles heterogeneous data types while preserving\nthe semantic integrity of categorical features. To further enable precise and\ninterpretable detection, \\textsf{RFOD} combines Adjusted Gower's Distance (AGD)\nfor cell-level scoring, which adapts to skewed numerical data and accounts for\ncategorical confidence, with Uncertainty-Weighted Averaging (UWA) to aggregate\ncell-level scores into robust row-level anomaly scores. Extensive experiments\non 15 real-world datasets demonstrate that \\textsf{RFOD} consistently\noutperforms state-of-the-art baselines in detection accuracy while offering\nsuperior robustness, scalability, and interpretability for mixed-type tabular\ndata."}
{"id": "2510.09050", "pdf": "https://arxiv.org/pdf/2510.09050", "abs": "https://arxiv.org/abs/2510.09050", "authors": ["Dildar Ali", "Rajibul Islam", "Suman Banerjee"], "title": "Multi-product Influence Maximization in Billboard Advertisement", "categories": ["cs.DS", "cs.DB"], "comment": "This paper has been accepted in ACM IKDD CODS-2025 conference", "summary": "Billboard Advertisement has emerged as an effective out-of-home advertisement\ntechnique where the goal is to select a limited number of slots and play\nadvertisement content over there with the hope that this will be observed by\nmany people, and effectively, a significant number of them will be influenced\ntowards the brand. Given a trajectory and a billboard database and a positive\ninteger $k$, how can we select $k$ highly influential slots to maximize\ninfluence? In this paper, we study a variant of this problem where a commercial\nhouse wants to make a promotion of multiple products, and there is an influence\ndemand for each product. We have studied two variants of the problem. In the\nfirst variant, our goal is to select $k$ slots such that the respective\ninfluence demand of each product is satisfied. In the other variant of the\nproblem, we are given with $\\ell$ integers $k_1,k_2, \\ldots, k_{\\ell}$, the\ngoal here is to search for $\\ell$ many set of slots $S_1, S_2, \\ldots,\nS_{\\ell}$ such that for all $i \\in [\\ell]$, $|S_{i}| \\leq k_i$ and for all $i\n\\neq j$, $S_i \\cap S_j=\\emptyset$ and the influence demand of each of the\nproducts gets satisfied. We model the first variant of the problem as a\nmulti-submodular cover problem and the second variant as its generalization.\nFor solving the first variant, we adopt the bi-criteria approximation\nalgorithm, and for the other variant, we propose a sampling-based approximation\nalgorithm. Extensive experiments with real-world trajectory and billboard\ndatasets highlight the effectiveness and efficiency of the proposed solution\napproach."}
{"id": "2510.09084", "pdf": "https://arxiv.org/pdf/2510.09084", "abs": "https://arxiv.org/abs/2510.09084", "authors": ["Dildar Ali", "Suman Benerjee", "Yamuna Prasad"], "title": "Approximately Bisubmodular Regret Minimization in Billboard and Social Media Advertising", "categories": ["cs.GT", "cs.DB", "cs.DS"], "comment": "12 Pages", "summary": "In a typical \\emph{billboard advertisement} technique, a number of digital\nbillboards are owned by an \\emph{influence provider}, and several commercial\nhouses approach the influence provider for a specific number of views of their\nadvertisement content on a payment basis. If the influence provider provides\nthe demanded or more influence, then he will receive the full payment else a\npartial payment. In the context of an influence provider, if he provides more\nor less than the advertisers demanded influence, it is a loss for him. This is\nformalized as 'Regret', and naturally, in the context of the influence\nprovider, the goal will be to allocate the billboard slots among the\nadvertisers such that the total regret is minimized. In this paper, we study\nthis problem as a discrete optimization problem and propose two solution\napproaches. The first one selects the billboard slots from the available ones\nin an incremental greedy manner, and we call this method the Budget Effective\nGreedy approach. In the second one, we introduce randomness in the first one,\nwhere we do it for a sample of slots instead of calculating the marginal gains\nof all the billboard slots. We analyze both algorithms to understand their time\nand space complexity. We implement them with real-life datasets and conduct a\nnumber of experiments. We observe that the randomized budget effective greedy\napproach takes reasonable computational time while minimizing the regret."}
{"id": "2510.09159", "pdf": "https://arxiv.org/pdf/2510.09159", "abs": "https://arxiv.org/abs/2510.09159", "authors": ["Tianyi Chen", "Mingcheng Zhu", "Zhiyao Luo", "Tingting Zhu"], "title": "Cross-Representation Benchmarking in Time-Series Electronic Health Records for Clinical Outcome Prediction", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "Electronic Health Records (EHRs) enable deep learning for clinical\npredictions, but the optimal method for representing patient data remains\nunclear due to inconsistent evaluation practices. We present the first\nsystematic benchmark to compare EHR representation methods, including\nmultivariate time-series, event streams, and textual event streams for LLMs.\nThis benchmark standardises data curation and evaluation across two distinct\nclinical settings: the MIMIC-IV dataset for ICU tasks (mortality, phenotyping)\nand the EHRSHOT dataset for longitudinal care (30-day readmission, 1-year\npancreatic cancer). For each paradigm, we evaluate appropriate modelling\nfamilies--including Transformers, MLP, LSTMs and Retain for time-series, CLMBR\nand count-based models for event streams, 8-20B LLMs for textual streams--and\nanalyse the impact of feature pruning based on data missingness. Our\nexperiments reveal that event stream models consistently deliver the strongest\nperformance. Pre-trained models like CLMBR are highly sample-efficient in\nfew-shot settings, though simpler count-based models can be competitive given\nsufficient data. Furthermore, we find that feature selection strategies must be\nadapted to the clinical setting: pruning sparse features improves ICU\npredictions, while retaining them is critical for longitudinal tasks. Our\nresults, enabled by a unified and reproducible pipeline, provide practical\nguidance for selecting EHR representations based on the clinical context and\ndata regime."}
{"id": "2510.09494", "pdf": "https://arxiv.org/pdf/2510.09494", "abs": "https://arxiv.org/abs/2510.09494", "authors": ["Nico Bistolfi", "Andreea Georgescu", "Dave Hodson"], "title": "The Data Enclave Advantage: A New Paradigm for Least-Privileged Data Access in a Zero-Trust World", "categories": ["cs.CR", "cs.DB", "cs.SE"], "comment": "11 pages, 2 figures, company whitepaper, no journal", "summary": "As cloud infrastructure evolves to support dynamic and distributed workflows,\naccelerated now by AI-driven processes, the outdated model of standing\npermissions has become a critical vulnerability. Based on the Cloud Security\nAlliance (CSA) Top Threats to Cloud Computing Deep Dive 2025 Report, our\nanalysis details how standing permissions cause catastrophic cloud breaches.\nWhile current security tools are addressing network and API security, the\nchallenge of securing granular data access remains. Removing standing\npermissions at the data level is as critical as it is at the network level,\nespecially for companies handling valuable data at scale.\n  In this white paper, we introduce an innovative architecture based on\non-demand data enclaves to address this gap directly. Our approach enables Zero\nStanding Privilege (ZSP) and Just-in-Time (JIT) principles at the data level.\nWe replace static permissions with temporary data contracts that enforce\nproactive protection. This means separation is built around the data requested\non-demand, providing precise access and real time monitoring for individual\nrecords instead of datasets. This solution drastically reduces the attack\nsurface, prevents privilege creep, and simplifies auditing, offering a vital\npath for enterprises to transition to a more secure and resilient data\nenvironment."}
{"id": "2510.09567", "pdf": "https://arxiv.org/pdf/2510.09567", "abs": "https://arxiv.org/abs/2510.09567", "authors": ["Jacopo Tagliabue", "Ciro Greco"], "title": "Safe, Untrusted, \"Proof-Carrying\" AI Agents: toward the agentic lakehouse", "categories": ["cs.AI", "cs.DB"], "comment": "IEEE Big Data, Workshop on Secure and Safe AI Agents for Big Data\n  Infrastructures", "summary": "Data lakehouses run sensitive workloads, where AI-driven automation raises\nconcerns about trust, correctness, and governance. We argue that API-first,\nprogrammable lakehouses provide the right abstractions for safe-by-design,\nagentic workflows. Using Bauplan as a case study, we show how data branching\nand declarative environments extend naturally to agents, enabling\nreproducibility and observability while reducing the attack surface. We present\na proof-of-concept in which agents repair data pipelines using correctness\nchecks inspired by proof-carrying code. Our prototype demonstrates that\nuntrusted AI agents can operate safely on production data and outlines a path\ntoward a fully agentic lakehouse."}
