{"id": "2510.05612", "pdf": "https://arxiv.org/pdf/2510.05612", "abs": "https://arxiv.org/abs/2510.05612", "authors": ["Utsav Pathak", "Amit Mankodi"], "title": "Redefining Cost Estimation in Database Systems: The Role of Execution Plan Features and Machine Learning", "categories": ["cs.DB"], "comment": "12 pages, 5 figures, conference", "summary": "Accurate query runtime prediction is a critical component of effective query\noptimization in modern database systems. Traditional cost models, such as those\nused in PostgreSQL, rely on static heuristics that often fail to reflect actual\nquery performance under complex and evolving workloads. This remains an active\narea of research, with recent work exploring machine learning techniques to\nreplace or augment traditional cost estimators. In this paper, we present a\nmachine learning-based framework for predicting SQL query runtimes using\nexecution plan features extracted from PostgreSQL. Our approach integrates\nscalar and structural features from execution plans and semantic\nrepresentations of SQL queries to train predictive models. We construct an\nautomated pipeline for data collection and feature extraction using\nparameterized TPC-H queries, enabling systematic evaluation of multiple\nmodeling techniques. Unlike prior efforts that focus either on cardinality\nestimation or on synthetic cost metrics, we model the actual runtimes using\nfine-grained plan statistics and query embeddings derived from execution\ntraces, to improve the model accuracy. We compare baseline regressors, a\nrefined XGBoost model, and a sequential LSTM-based model to assess their\neffectiveness in runtime prediction. Our dataset includes over 1000 queries\ngenerated from TPC-H query templates executed in PostgreSQL with EXPLAIN\nANALYZE. Experimental results show that the XGBoost model significantly\noutperforms others, achieving a mean squared error of 0.3002 and prediction\naccuracy within 10% of the true runtime in over 65% of cases. The findings\nhighlight the potential of tree-based learning combined with execution plan\nfeatures for improving cost estimation in query optimizers."}
{"id": "2510.05907", "pdf": "https://arxiv.org/pdf/2510.05907", "abs": "https://arxiv.org/abs/2510.05907", "authors": ["Dmitrii Radivonchik", "Yakov Kuzin", "Anton Chizhov", "Dmitriy Shcheka", "Mikhail Firsov", "Kirill Smirnov", "George Chernishev"], "title": "Speeding up SQL subqueries via decoupling of non-correlated predicate (extended version)", "categories": ["cs.DB", "cs.PF", "cs.SE", "H.2.4; E.1"], "comment": null, "summary": "In this paper, we discuss a novel technique for processing correlated\nsubqueries in SQL. The core idea is to isolate the non-correlated part of the\npredicate and use it to reduce the number of evaluations of the correlated\npart. We begin by providing an overview of several classes of queries that may\nbenefit from this technique. For each class, we propose a potential rewrite and\ndiscuss the conditions under which it is advantageous. Next, we address the\nevaluation aspects of the proposed rewrites: 1) we describe our approach to\nadapting the block-based Volcano query processing model, and 2) we discuss the\nbenefits of implementing that technique within a position-enabled column-store\nwith late materialization support. Finally, we present a simple cost model that\nallows estimation of the benefits of said rewrites.\n  Our evaluation has a quantitative part and a qualitative part. The former\nfocuses on studying the impact of non-correlated predicate selectivity on our\ntechnique. The latter identifies the limitations of our approach by comparing\nit with alternative approaches available in existing systems. Overall,\nexperiments conducted using PosDB (a position-enabled column-store) and\nPostgreSQL demonstrated that, under suitable conditions, our technique can\nachieve a 5x improvement."}
{"id": "2510.05805", "pdf": "https://arxiv.org/pdf/2510.05805", "abs": "https://arxiv.org/abs/2510.05805", "authors": ["Pafue Christy Nganjimi", "Andrew Soltan", "Danielle Belgrave", "Lei Clifton", "David A. Clifton", "Anshul Thakur"], "title": "Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates", "categories": ["cs.LG", "cs.CV", "cs.DB"], "comment": "20 pages, 4 figures, Submitted to AISTATS 2026", "summary": "Dataset condensation (DC) enables the creation of compact, privacy-preserving\nsynthetic datasets that can match the utility of real patient records,\nsupporting democratised access to highly regulated clinical data for developing\ndownstream clinical models. State-of-the-art DC methods supervise synthetic\ndata by aligning the training dynamics of models trained on real and those\ntrained on synthetic data, typically using full stochastic gradient descent\n(SGD) trajectories as alignment targets; however, these trajectories are often\nnoisy, high-curvature, and storage-intensive, leading to unstable gradients,\nslow convergence, and substantial memory overhead. We address these limitations\nby replacing full SGD trajectories with smooth, low-loss parametric surrogates,\nspecifically quadratic B\\'ezier curves that connect the initial and final model\nstates from real training trajectories. These mode-connected paths provide\nnoise-free, low-curvature supervision signals that stabilise gradients,\naccelerate convergence, and eliminate the need for dense trajectory storage. We\ntheoretically justify B\\'ezier-mode connections as effective surrogates for SGD\npaths and empirically show that the proposed method outperforms\nstate-of-the-art condensation approaches across five clinical datasets,\nyielding condensed datasets that enable clinically effective model development."}
