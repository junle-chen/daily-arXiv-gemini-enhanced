{"id": "2509.18181", "pdf": "https://arxiv.org/pdf/2509.18181", "abs": "https://arxiv.org/abs/2509.18181", "authors": ["Mustafa Sameen", "Xiaojian Zhang", "Xilei Zhao"], "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and\nimplementing effective traffic management policies for reducing congestion,\nimproving mobility, and allocating resources more efficiently. Existing models\nfor predicting ridesourcing mode choices often suffer from limited predictive\naccuracy due to their inability to capture key psychological factors, and are\nfurther challenged by severe class imbalance, as ridesourcing trips comprise\nonly a small fraction of individuals' daily travel. To address these\nlimitations, this paper introduces the Synthesizing Attitudes, Predicting\nActions (SAPA) framework, a hierarchical approach that uses Large Language\nModels (LLMs) to synthesize theory-grounded latent attitudes to predict\nridesourcing choices. SAPA first uses an LLM to generate qualitative traveler\npersonas from raw travel survey data and then trains a propensity-score model\non demographic and behavioral features, enriched by those personas, to produce\nan individual-level score. Next, the LLM assigns quantitative scores to\ntheory-driven latent variables (e.g., time and cost sensitivity), and a final\nclassifier integrates the propensity score, latent-variable scores (with their\ninteraction terms), and observable trip attributes to predict ridesourcing mode\nchoice. Experiments on a large-scale, multi-year travel survey show that SAPA\nsignificantly outperforms state-of-the-art baselines, improving ridesourcing\nchoice predictions by up to 75.9% in terms of PR-AUC on a held-out test set.\nThis study provides a powerful tool for accurately predicting ridesourcing mode\nchoices, and provides a methodology that is readily transferable to various\napplications.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant. It uses Large Language Models (LLMs) to predict ridesourcing mode choices, which can be considered a form of trajectory/behavior prediction. The paper explicitly mentions using LLMs to synthesize attitudes and predict actions, directly connecting large language models and a type of predictive task.", "keywords": ["trajectory prediction", "Large Language Models", "LLMs", "ridesourcing mode choices", "action prediction", "behavioral"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAPA\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5408\u6210\u6f5c\u5728\u6001\u5ea6\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\u3002", "motivation": "\u73b0\u6709\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\u6a21\u578b\u9884\u6d4b\u7cbe\u5ea6\u6709\u9650\uff0c\u65e0\u6cd5\u6355\u6349\u5173\u952e\u5fc3\u7406\u56e0\u7d20\uff0c\u4e14\u9762\u4e34\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faSAPA\u6846\u67b6\uff0c\u9996\u5148\u4f7f\u7528LLM\u4ece\u539f\u59cb\u51fa\u884c\u8c03\u67e5\u6570\u636e\u4e2d\u751f\u6210\u5b9a\u6027\u51fa\u884c\u8005\u89d2\u8272\uff0c\u7136\u540e\u8bad\u7ec3\u503e\u5411\u5f97\u5206\u6a21\u578b\uff0c\u5e76\u4f7f\u7528LLM\u4e3a\u7406\u8bba\u9a71\u52a8\u7684\u6f5c\u5728\u53d8\u91cf\u5206\u914d\u5b9a\u91cf\u5206\u6570\uff0c\u6700\u540e\u901a\u8fc7\u5206\u7c7b\u5668\u6574\u5408\u503e\u5411\u5f97\u5206\u3001\u6f5c\u5728\u53d8\u91cf\u5206\u6570\u548c\u53ef\u89c2\u5bdf\u7684\u51fa\u884c\u5c5e\u6027\u6765\u9884\u6d4b\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\u3002", "result": "\u5728\u5927\u578b\u591a\u5e74\u51fa\u884c\u8c03\u67e5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAPA\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u5728PR-AUC\u6307\u6807\u4e0a\uff0c\u7f51\u7ea6\u8f66\u9009\u62e9\u9884\u6d4b\u63d0\u9ad8\u4e86\u9ad8\u8fbe75.9%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u8f6c\u79fb\u5230\u5404\u79cd\u5e94\u7528\u7684\u65b9\u6cd5\u3002", "summary_zh": "\u4e3a\u4e86\u8bbe\u8ba1\u548c\u5b9e\u65bd\u6709\u6548\u7684\u4ea4\u901a\u7ba1\u7406\u653f\u7b56\uff0c\u51cf\u5c11\u62e5\u5835\uff0c\u6539\u5584\u4ea4\u901a\uff0c\u66f4\u6709\u6548\u5730\u5206\u914d\u8d44\u6e90\uff0c\u51c6\u786e\u5730\u6a21\u62df\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\u9884\u6d4b\u6a21\u578b\u901a\u5e38\u9884\u6d4b\u7cbe\u5ea6\u6709\u9650\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u6355\u6349\u5173\u952e\u7684\u5fc3\u7406\u56e0\u7d20\uff0c\u5e76\u4e14\u7531\u4e8e\u7f51\u7ea6\u8f66\u51fa\u884c\u4ec5\u5360\u4e2a\u4eba\u65e5\u5e38\u51fa\u884c\u7684\u5f88\u5c0f\u4e00\u90e8\u5206\uff0c\u56e0\u6b64\u9762\u4e34\u7740\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7efc\u5408\u6001\u5ea6\u3001\u9884\u6d4b\u884c\u4e3a\uff08SAPA\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u79cd\u5206\u5c42\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u7efc\u5408\u7406\u8bba\u9a71\u52a8\u7684\u6f5c\u5728\u6001\u5ea6\uff0c\u4ee5\u9884\u6d4b\u7f51\u7ea6\u8f66\u9009\u62e9\u3002SAPA\u9996\u5148\u4f7f\u7528LLM\u4ece\u539f\u59cb\u51fa\u884c\u8c03\u67e5\u6570\u636e\u4e2d\u751f\u6210\u5b9a\u6027\u51fa\u884c\u8005\u89d2\u8272\uff0c\u7136\u540e\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8e\u4eba\u53e3\u7edf\u8ba1\u548c\u884c\u4e3a\u7279\u5f81\uff08\u7531\u8fd9\u4e9b\u89d2\u8272\u4e30\u5bcc\uff09\u7684\u503e\u5411\u5f97\u5206\u6a21\u578b\uff0c\u4ee5\u751f\u6210\u4e2a\u4eba\u5c42\u9762\u7684\u5f97\u5206\u3002\u63a5\u4e0b\u6765\uff0cLLM\u4e3a\u7406\u8bba\u9a71\u52a8\u7684\u6f5c\u5728\u53d8\u91cf\uff08\u4f8b\u5982\uff0c\u65f6\u95f4\u548c\u6210\u672c\u654f\u611f\u6027\uff09\u5206\u914d\u5b9a\u91cf\u5206\u6570\uff0c\u6700\u7ec8\u7684\u5206\u7c7b\u5668\u96c6\u6210\u4e86\u503e\u5411\u5f97\u5206\u3001\u6f5c\u5728\u53d8\u91cf\u5f97\u5206\uff08\u53ca\u5176\u4ea4\u4e92\u9879\uff09\u548c\u53ef\u89c2\u5bdf\u7684\u51fa\u884c\u5c5e\u6027\uff0c\u4ee5\u9884\u6d4b\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\u3002\u5728\u5927\u578b\u591a\u5e74\u51fa\u884c\u8c03\u67e5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSAPA\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5728PR-AUC\u65b9\u9762\uff0c\u5728\u4fdd\u7559\u7684\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u7f51\u7ea6\u8f66\u9009\u62e9\u9884\u6d4b\u63d0\u9ad8\u4e86\u9ad8\u8fbe75.9%\u3002\u8fd9\u9879\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u7f51\u7ea6\u8f66\u6a21\u5f0f\u9009\u62e9\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6613\u4e8e\u8f6c\u79fb\u5230\u5404\u79cd\u5e94\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2509.18384", "pdf": "https://arxiv.org/pdf/2509.18384", "abs": "https://arxiv.org/abs/2509.18384", "authors": ["Yunhao Yang", "Junyuan Hong", "Gabriel Jacob Perin", "Zhiwen Fan", "Li Yin", "Zhangyang Wang", "Ufuk Topcu"], "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback", "categories": ["cs.RO", "cs.FL"], "comment": null, "summary": "Large language models (LLMs) can translate natural language instructions into\nexecutable action plans for robotics, autonomous driving, and other domains.\nYet, deploying LLM-driven planning in the physical world demands strict\nadherence to safety and regulatory constraints, which current models often\nviolate due to hallucination or weak alignment. Traditional data-driven\nalignment methods, such as Direct Preference Optimization (DPO), require costly\nhuman labeling, while recent formal-feedback approaches still depend on\nresource-intensive fine-tuning. In this paper, we propose LAD-VF, a\nfine-tuning-free framework that leverages formal verification feedback for\nautomated prompt engineering. By introducing a formal-verification-informed\ntext loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts\nrather than model parameters. This yields three key benefits: (i) scalable\nadaptation without fine-tuning; (ii) compatibility with modular LLM\narchitectures; and (iii) interpretable refinement via auditable prompts.\nExperiments in robot navigation and manipulation tasks demonstrate that LAD-VF\nsubstantially enhances specification compliance, improving success rates from\n60% to over 90%. Our method thus presents a scalable and interpretable pathway\ntoward trustworthy, formally-verified LLM-driven control systems.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u673a\u5668\u4eba\u89c4\u5212\uff0c\u5e76\u63d0\u5230\u4e86\u81ea\u4e3b\u9a7e\u9a76\u9886\u57df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u201c\u8f68\u8ff9\u9884\u6d4b\u201d\uff0c\u4f46\u673a\u5668\u4eba\u89c4\u5212\u548c\u81ea\u4e3b\u9a7e\u9a76\u901a\u5e38\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u91cd\u70b9\u5728\u4e8e\u5229\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u6765\u6539\u8fdbLLM\u7684prompt\uff0c\u4ee5\u63d0\u9ad8\u89c4\u5212\u7684\u5b89\u5168\u6027\uff0c\u4e5f\u6d89\u53ca\u4e86LLM\u7684\u5e94\u7528\u3002", "keywords": ["Large language models", "LLMs", "robot planning", "autonomous driving", "formal verification", "prompt engineering"]}, "AI": {"tldr": "LAD-VF \u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u6846\u67b6\uff0c\u5229\u7528\u5f62\u5f0f\u9a8c\u8bc1\u53cd\u9988\u8fdb\u884c\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8 LLM \u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u89c4\u8303\u4f9d\u4ece\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u4e16\u754c\u4e2d\u8fdb\u884c\u89c4\u5212\u65f6\uff0c\u7531\u4e8e\u5e7b\u89c9\u6216\u5bf9\u9f50\u4e0d\u8db3\uff0c\u5e38\u5e38\u8fdd\u53cd\u5b89\u5168\u548c\u6cd5\u89c4\u7ea6\u675f\u3002\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u6700\u8fd1\u7684\u5f62\u5f0f\u53cd\u9988\u65b9\u6cd5\u4ecd\u7136\u4f9d\u8d56\u4e8e\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u5fae\u8c03\u3002", "method": "LAD-VF \u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f62\u5f0f\u9a8c\u8bc1\u4fe1\u606f\u7684\u6587\u672c\u635f\u5931\uff0c\u5e76\u5c06\u5176\u4e0e LLM-AutoDiff \u96c6\u6210\uff0c\u4ece\u800c\u8fed\u4ee3\u5730\u4f18\u5316\u63d0\u793a\uff0c\u800c\u4e0d\u662f\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAD-VF \u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u8303\u4f9d\u4ece\u6027\uff0c\u5c06\u6210\u529f\u7387\u4ece 60% \u63d0\u9ad8\u5230 90% \u4ee5\u4e0a\u3002", "conclusion": "LAD-VF \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u9014\u5f84\uff0c\u53ef\u4ee5\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u3001\u7ecf\u8fc7\u5f62\u5f0f\u9a8c\u8bc1\u7684 LLM \u9a71\u52a8\u7684\u63a7\u5236\u7cfb\u7edf\u3002", "summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7ffb\u8bd1\u6210\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u5176\u4ed6\u9886\u57df\u7684\u53ef\u6267\u884c\u884c\u52a8\u8ba1\u5212\u3002\u7136\u800c\uff0c\u5728\u7269\u7406\u4e16\u754c\u4e2d\u90e8\u7f72 LLM \u9a71\u52a8\u7684\u89c4\u5212\u9700\u8981\u4e25\u683c\u9075\u5b88\u5b89\u5168\u548c\u6cd5\u89c4\u7ea6\u675f\uff0c\u4f46\u5f53\u524d\u7684\u6a21\u578b\u7531\u4e8e\u5e7b\u89c9\u6216\u5f31\u5bf9\u9f50\u800c\u7ecf\u5e38\u8fdd\u53cd\u8fd9\u4e9b\u7ea6\u675f\u3002\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982\u76f4\u63a5\u504f\u597d\u4f18\u5316 DPO\uff09\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u800c\u6700\u8fd1\u7684\u5f62\u5f0f\u53cd\u9988\u65b9\u6cd5\u4ecd\u7136\u4f9d\u8d56\u4e8e\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u5fae\u8c03\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u6846\u67b6 LAD-VF\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5f62\u5f0f\u9a8c\u8bc1\u53cd\u9988\u8fdb\u884c\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u3002\u901a\u8fc7\u5f15\u5165\u4e0e LLM-AutoDiff \u96c6\u6210\u7684\u5f62\u5f0f\u9a8c\u8bc1\u4fe1\u606f\u6587\u672c\u635f\u5931\uff0cLAD-VF \u8fed\u4ee3\u5730\u4f18\u5316\u63d0\u793a\uff0c\u800c\u4e0d\u662f\u6a21\u578b\u53c2\u6570\u3002\u8fd9\u4ea7\u751f\u4e86\u4e09\u4e2a\u5173\u952e\u597d\u5904\uff1a\uff08i\uff09\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u9002\u5e94\uff1b\uff08ii\uff09\u4e0e\u6a21\u5757\u5316 LLM \u67b6\u6784\u517c\u5bb9\uff1b\uff08iii\uff09\u901a\u8fc7\u53ef\u5ba1\u8ba1\u7684\u63d0\u793a\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u6539\u8fdb\u3002\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLAD-VF \u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u8303\u4f9d\u4ece\u6027\uff0c\u5c06\u6210\u529f\u7387\u4ece 60% \u63d0\u9ad8\u5230 90% \u4ee5\u4e0a\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u9014\u5f84\uff0c\u53ef\u4ee5\u6784\u5efa\u53ef\u4fe1\u8d56\u7684\u3001\u7ecf\u8fc7\u5f62\u5f0f\u9a8c\u8bc1\u7684 LLM \u9a71\u52a8\u7684\u63a7\u5236\u7cfb\u7edf\u3002"}}
{"id": "2509.18372", "pdf": "https://arxiv.org/pdf/2509.18372", "abs": "https://arxiv.org/abs/2509.18372", "authors": ["Reeshad Khan", "John Gauch"], "title": "TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning", "categories": ["cs.CV"], "comment": null, "summary": "We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework\nthat distills the full-stack capabilities of a large planning-oriented teacher\n(UniAD [19]) into a compact, real-time student model. Unlike prior efficient\ncamera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the\ncomplete autonomy stack 3D detection, HD-map segmentation, motion forecasting,\noccupancy prediction, and goal-directed planning within a streamlined\n28M-parameter backbone, achieving a 78% reduction in parameters over UniAD\n[19]. Our model-agnostic, multi-stage distillation strategy combines\nfeature-level, output-level, and adaptive region-aware supervision to\neffectively transfer high-capacity multi-modal knowledge to a lightweight BEV\nrepresentation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08\nminADE for motion forecasting, and a 0.32 collision rate, while running 5x\nfaster (11 FPS) and requiring only camera input. These results demonstrate that\nfull-stack driving intelligence can be retained in resource-constrained\nsettings, bridging the gap between large-scale, multi-modal perception-planning\nmodels and deployment-ready real-time autonomy.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u7684\u65b9\u5f0f\uff0c\u5c06\u4e00\u4e2a\u5927\u578b\u89c4\u5212\u6a21\u578b\uff08UniAD\uff09\u7684\u80fd\u529b\u8fc1\u79fb\u5230\u4e00\u4e2a\u66f4\u5c0f\u7684\u6a21\u578b\u4e2d\u3002UniAD\u672c\u8eab\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u57fa\u7840\u6a21\u578b\uff0c\u800c\u8be5\u8bba\u6587\u5173\u6ce8\u7684\u662f\u5982\u4f55\u9ad8\u6548\u5730\u5229\u7528\u548c\u538b\u7f29\u8fd9\u79cd\u6a21\u578b\u7684\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\"motion forecasting\"\uff08\u8fd0\u52a8\u9884\u6d4b\uff09\uff0c\u8fd9\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["motion forecasting", "knowledge distillation", "large planning-oriented teacher", "UniAD", "Bird's Eye View", "planning"]}, "AI": {"tldr": "TinyBEV\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u3001\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u7684BEV\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u5927\u578b\u89c4\u5212\u6a21\u578b\u7684\u80fd\u529b\u8f6c\u79fb\u5230\u5c0f\u578b\u5b9e\u65f6\u6a21\u578b\u4e2d\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u5168\u6808\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u591a\u9636\u6bb5\u84b8\u998f\u7b56\u7565\uff0c\u7ed3\u5408\u7279\u5f81\u7ea7\u3001\u8f93\u51fa\u7ea7\u548c\u81ea\u9002\u5e94\u533a\u57df\u611f\u77e5\u76d1\u7763\u3002", "result": "TinyBEV\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u68c0\u6d4b\u3001\u8fd0\u52a8\u9884\u6d4b\u548c\u78b0\u649e\u7387\u7ed3\u679c\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u63d0\u9ad8\u4e865\u500d\uff0811 FPS\uff09\uff0c\u5e76\u4e14\u4ec5\u9700\u8981\u6444\u50cf\u5934\u8f93\u5165\u3002", "conclusion": "TinyBEV\u8bc1\u660e\u4e86\u5168\u6808\u9a7e\u9a76\u667a\u80fd\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u4fdd\u7559\uff0c\u7f29\u5c0f\u4e86\u5927\u578b\u591a\u6a21\u6001\u611f\u77e5\u89c4\u5212\u6a21\u578b\u4e0e\u53ef\u90e8\u7f72\u7684\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "summary_zh": "\u6211\u4eec\u63d0\u51fa\u4e86TinyBEV\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u7684\u9e1f\u77b0\u56fe\uff08BEV\uff09\u6846\u67b6\uff0c\u5b83\u5c06\u5927\u578b\u9762\u5411\u89c4\u5212\u7684\u6559\u5e08\u6a21\u578b\uff08UniAD [19]\uff09\u7684\u5168\u6808\u80fd\u529b\u63d0\u70bc\u6210\u4e00\u4e2a\u7d27\u51d1\u7684\u3001\u5b9e\u65f6\u7684\u5b66\u751f\u6a21\u578b\u3002\u4e0e\u4e4b\u524d\u9ad8\u6548\u7684\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u7684\u57fa\u7ebf\uff08\u5982VAD[23]\u548cVADv2[7]\uff09\u4e0d\u540c\uff0cTinyBEV\u652f\u6301\u5b8c\u6574\u7684\u81ea\u4e3b\u5806\u6808\uff1a3D\u68c0\u6d4b\u3001HD\u5730\u56fe\u5206\u5272\u3001\u8fd0\u52a8\u9884\u6d4b\u3001 occupancy\u9884\u6d4b\u548c\u76ee\u6807\u5bfc\u5411\u7684\u89c4\u5212\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u5728\u4e00\u4e2a\u7b80\u5316\u768428M\u53c2\u6570\u9aa8\u5e72\u7f51\u7edc\u4e2d\u5b9e\u73b0\uff0c\u4e0eUniAD [19] \u76f8\u6bd4\uff0c\u53c2\u6570\u51cf\u5c11\u4e8678%\u3002\u6211\u4eec\u7684\u6a21\u578b\u65e0\u5173\u3001\u591a\u9636\u6bb5\u84b8\u998f\u7b56\u7565\u7ed3\u5408\u4e86\u7279\u5f81\u7ea7\u3001\u8f93\u51fa\u7ea7\u548c\u81ea\u9002\u5e94\u533a\u57df\u611f\u77e5\u76d1\u7763\uff0c\u4ee5\u6709\u6548\u5730\u5c06\u9ad8\u5bb9\u91cf\u591a\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7BEV\u8868\u793a\u4e2d\u3002\u5728nuScenes[4]\u4e0a\uff0cTiny-BEV\u5728\u68c0\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e8639.0 mAP\uff0c\u5728\u8fd0\u52a8\u9884\u6d4b\u65b9\u9762\u5b9e\u73b0\u4e861.08 minADE\uff0c\u78b0\u649e\u7387\u4e3a0.32\uff0c\u540c\u65f6\u8fd0\u884c\u901f\u5ea6\u63d0\u9ad8\u4e865\u500d\uff0811 FPS\uff09\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u6444\u50cf\u5934\u8f93\u5165\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5168\u6808\u9a7e\u9a76\u667a\u80fd\u53ef\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u4fdd\u7559\uff0c\u4ece\u800c\u5f25\u5408\u4e86\u5927\u89c4\u6a21\u591a\u6a21\u6001\u611f\u77e5\u89c4\u5212\u6a21\u578b\u4e0e\u53ef\u90e8\u7f72\u7684\u5b9e\u65f6\u81ea\u4e3b\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2509.18387", "pdf": "https://arxiv.org/pdf/2509.18387", "abs": "https://arxiv.org/abs/2509.18387", "authors": ["Thomas Gossard", "Filip Radovic", "Andreas Ziegler", "Andrea Zell"], "title": "BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Motion blur reduces the clarity of fast-moving objects, posing challenges for\ndetection systems, especially in racket sports, where balls often appear as\nstreaks rather than distinct points. Existing labeling conventions mark the\nball at the leading edge of the blur, introducing asymmetry and ignoring\nvaluable motion cues correlated with velocity. This paper introduces a new\nlabeling strategy that places the ball at the center of the blur streak and\nexplicitly annotates blur attributes. Using this convention, we release a new\ntable tennis ball detection dataset. We demonstrate that this labeling approach\nconsistently enhances detection performance across various models. Furthermore,\nwe introduce BlurBall, a model that jointly estimates ball position and motion\nblur attributes. By incorporating attention mechanisms such as\nSqueeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art\nresults in ball detection. Leveraging blur not only improves detection accuracy\nbut also enables more reliable trajectory prediction, benefiting real-time\nsports analytics.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8fd0\u52a8\u6a21\u7cca\u73af\u5883\u4e0b\u7684\u4e52\u4e53\u7403\u68c0\u6d4b\u548c\u8f68\u8ff9\u9884\u6d4b\u3002\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u8868\u660e\u5229\u7528\u8fd0\u52a8\u6a21\u7cca\u4fe1\u606f\u53ef\u4ee5\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u5728\u8fd0\u52a8\u76ee\u6807\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "motion blur", "ball tracking", "detection", "real-time sports analytics"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd0\u52a8\u6a21\u7cca\u6807\u6ce8\u7b56\u7565\uff0c\u5e76\u5c06\u6a21\u7cca\u5c5e\u6027\u52a0\u5165\u6807\u6ce8\u4fe1\u606f\u4e2d\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u7403\u7c7b\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u8f68\u8ff9\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6807\u6ce8\u65b9\u6cd5\u5c06\u8fd0\u52a8\u6a21\u7cca\u7403\u4f53\u6807\u6ce8\u5728\u6a21\u7cca\u8fb9\u7f18\uff0c\u5ffd\u7565\u4e86\u8fd0\u52a8\u7ebf\u7d22\uff0c\u5bfc\u81f4\u68c0\u6d4b\u7cfb\u7edf\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u7403\u62cd\u8fd0\u52a8\u4e2d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u5c06\u7403\u4f53\u6807\u6ce8\u5728\u6a21\u7cca\u6761\u7eb9\u7684\u4e2d\u5fc3\uff0c\u5e76\u660e\u786e\u6807\u6ce8\u6a21\u7cca\u5c5e\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86BlurBall\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u4ee5\u8054\u5408\u4f30\u8ba1\u7403\u7684\u4f4d\u7f6e\u548c\u8fd0\u52a8\u6a21\u7cca\u5c5e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6807\u6ce8\u65b9\u6cd5\u53ef\u4ee5\u63d0\u9ad8\u5404\u79cd\u6a21\u578b\u7684\u68c0\u6d4b\u6027\u80fd\u3002BlurBall\u6a21\u578b\u5728\u7403\u4f53\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "conclusion": "\u5229\u7528\u6a21\u7cca\u4fe1\u606f\u4e0d\u4ec5\u53ef\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u53ef\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4ece\u800c\u6709\u76ca\u4e8e\u5b9e\u65f6\u4f53\u80b2\u5206\u6790\u3002", "summary_zh": "\u8fd0\u52a8\u6a21\u7cca\u964d\u4f4e\u4e86\u5feb\u901f\u79fb\u52a8\u7269\u4f53\u7684\u6e05\u6670\u5ea6\uff0c\u7ed9\u68c0\u6d4b\u7cfb\u7edf\u5e26\u6765\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7403\u62cd\u8fd0\u52a8\u4e2d\uff0c\u7403\u901a\u5e38\u8868\u73b0\u4e3a\u6761\u7eb9\u800c\u4e0d\u662f\u6e05\u6670\u7684\u70b9\u3002\u73b0\u6709\u7684\u6807\u6ce8\u60ef\u4f8b\u5c06\u7403\u6807\u8bb0\u5728\u6a21\u7cca\u7684\u524d\u7f18\uff0c\u5f15\u5165\u4e86\u4e0d\u5bf9\u79f0\u6027\uff0c\u5ffd\u7565\u4e86\u4e0e\u901f\u5ea6\u76f8\u5173\u7684\u6709\u4ef7\u503c\u7684\u8fd0\u52a8\u7ebf\u7d22\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6807\u6ce8\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5c06\u7403\u653e\u7f6e\u5728\u6a21\u7cca\u6761\u7eb9\u7684\u4e2d\u5fc3\uff0c\u5e76\u660e\u786e\u5730\u6ce8\u91ca\u6a21\u7cca\u5c5e\u6027\u3002\u4f7f\u7528\u8fd9\u79cd\u60ef\u4f8b\uff0c\u6211\u4eec\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u4e52\u4e53\u7403\u68c0\u6d4b\u6570\u636e\u96c6\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u8fd9\u79cd\u6807\u6ce8\u65b9\u6cd5\u59cb\u7ec8\u5982\u4e00\u5730\u63d0\u9ad8\u4e86\u5404\u79cd\u6a21\u578b\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86BlurBall\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u8054\u5408\u4f30\u8ba1\u7403\u7684\u4f4d\u7f6e\u548c\u8fd0\u52a8\u6a21\u7cca\u5c5e\u6027\u3002\u901a\u8fc7\u5728\u591a\u5e27\u8f93\u5165\u4e0a\u7ed3\u5408\u8bf8\u5982Squeeze-and-Excitation\u4e4b\u7c7b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6211\u4eec\u5728\u7403\u68c0\u6d4b\u4e2d\u83b7\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u5229\u7528\u6a21\u7cca\u4e0d\u4ec5\u53ef\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u53ef\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4ece\u800c\u6709\u76ca\u4e8e\u5b9e\u65f6\u4f53\u80b2\u5206\u6790\u3002"}}
{"id": "2509.18136", "pdf": "https://arxiv.org/pdf/2509.18136", "abs": "https://arxiv.org/abs/2509.18136", "authors": ["Suqing Wang", "Zuchao Li", "Luohe Shi", "Bo Du", "Hai Zhao", "Yun Li", "Qianren Wang"], "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) have achieved remarkable success across various\ndomains, driving significant technological advancements and innovations.\nDespite the rapid growth in model scale and capability, systematic, data-driven\nresearch on how structural configurations affect performance remains scarce. To\naddress this gap, we present a large-scale dataset encompassing diverse\nopen-source LLM structures and their performance across multiple benchmarks.\nLeveraging this dataset, we conduct a systematic, data mining-driven analysis\nto validate and quantify the relationship between structural configurations and\nperformance. Our study begins with a review of the historical development of\nLLMs and an exploration of potential future trends. We then analyze how various\nstructural choices impact performance across benchmarks and further corroborate\nour findings using mechanistic interpretability techniques. By providing\ndata-driven insights into LLM optimization, our work aims to guide the targeted\ndevelopment and application of future models. We will release our dataset at\nhttps://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on the structure and performance of Large Language Models (LLMs). While it doesn't directly address trajectory prediction, its core subject is LLMs, which are a key component of the prompt. The paper analyzes the relationship between LLM structure and performance, providing insights that could potentially be relevant to LLMs used in trajectory prediction tasks.", "keywords": ["Large Language Models", "LLMs", "foundation models", "structure", "performance"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u914d\u7f6e\u4e0e\u5176\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u7ed3\u6784\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u3001\u6570\u636e\u9a71\u52a8\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5404\u79cd\u5f00\u6e90LLM\u7ed3\u6784\u53ca\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u7cfb\u7edf\u7684\u6570\u636e\u6316\u6398\u9a71\u52a8\u5206\u6790\u3002", "result": "\u7814\u7a76\u9a8c\u8bc1\u5e76\u91cf\u5316\u4e86\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u4f7f\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u7814\u7a76\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684LLM\u4f18\u5316\u89c1\u89e3\uff0c\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u6709\u9488\u5bf9\u6027\u5f00\u53d1\u548c\u5e94\u7528\u3002", "summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5404\u4e2a\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\uff0c\u63a8\u52a8\u4e86\u91cd\u5927\u7684\u6280\u672f\u8fdb\u6b65\u548c\u521b\u65b0\u3002\u5c3d\u7ba1\u6a21\u578b\u89c4\u6a21\u548c\u80fd\u529b\u8fc5\u901f\u589e\u957f\uff0c\u4f46\u5173\u4e8e\u7ed3\u6784\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u3001\u6570\u636e\u9a71\u52a8\u7684\u7814\u7a76\u4ecd\u7136\u7a00\u7f3a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u5404\u79cd\u5f00\u6e90LLM\u7ed3\u6784\u53ca\u5176\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u3002\u5229\u7528\u8be5\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u3001\u6570\u636e\u6316\u6398\u9a71\u52a8\u7684\u5206\u6790\uff0c\u4ee5\u9a8c\u8bc1\u548c\u91cf\u5316\u7ed3\u6784\u914d\u7f6e\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002\u6211\u4eec\u7684\u7814\u7a76\u9996\u5148\u56de\u987e\u4e86LLM\u7684\u5386\u53f2\u53d1\u5c55\uff0c\u5e76\u63a2\u8ba8\u4e86\u6f5c\u5728\u7684\u672a\u6765\u8d8b\u52bf\u3002\u7136\u540e\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5404\u79cd\u7ed3\u6784\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u8de8\u57fa\u51c6\u6d4b\u8bd5\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u7528\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6280\u672f\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6211\u4eec\u7684\u53d1\u73b0\u3002\u901a\u8fc7\u63d0\u4f9b\u6570\u636e\u9a71\u52a8\u7684LLM\u4f18\u5316\u89c1\u89e3\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u65e8\u5728\u6307\u5bfc\u672a\u6765\u6a21\u578b\u7684\u6709\u9488\u5bf9\u6027\u5f00\u53d1\u548c\u5e94\u7528\u3002\u6211\u4eec\u5c06\u5728https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset\u4e0a\u53d1\u5e03\u6211\u4eec\u7684\u6570\u636e\u96c6"}}
{"id": "2509.18282", "pdf": "https://arxiv.org/pdf/2509.18282", "abs": "https://arxiv.org/abs/2509.18282", "authors": ["Jesse Zhang", "Marius Memmel", "Kevin Kim", "Dieter Fox", "Jesse Thomason", "Fabio Ramos", "Erdem B\u0131y\u0131k", "Abhishek Gupta", "Anqi Li"], "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "11 pages", "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses Vision-Language Models (VLMs) to guide robot manipulation policies, specifically predicting end-effector paths, which can be considered a form of trajectory prediction. While the focus is on robot manipulation and not explicitly trajectory prediction, the use of VLMs to predict paths connects it to both themes.", "keywords": ["Vision-Language Models", "VLMs", "robot manipulation", "end-effector paths", "policy"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18198", "pdf": "https://arxiv.org/pdf/2509.18198", "abs": "https://arxiv.org/abs/2509.18198", "authors": ["Rui Liu", "Zikang Wang", "Peng Gao", "Yu Shen", "Pratap Tokekar", "Ming Lin"], "title": "MMCD: Multi-Modal Collaborative Decision-Making for Connected Autonomy with Knowledge Distillation", "categories": ["cs.AI", "cs.MA", "cs.RO"], "comment": null, "summary": "Autonomous systems have advanced significantly, but challenges persist in\naccident-prone environments where robust decision-making is crucial. A single\nvehicle's limited sensor range and obstructed views increase the likelihood of\naccidents. Multi-vehicle connected systems and multi-modal approaches,\nleveraging RGB images and LiDAR point clouds, have emerged as promising\nsolutions. However, existing methods often assume the availability of all data\nmodalities and connected vehicles during both training and testing, which is\nimpractical due to potential sensor failures or missing connected vehicles. To\naddress these challenges, we introduce a novel framework MMCD (Multi-Modal\nCollaborative Decision-making) for connected autonomy. Our framework fuses\nmulti-modal observations from ego and collaborative vehicles to enhance\ndecision-making under challenging conditions. To ensure robust performance when\ncertain data modalities are unavailable during testing, we propose an approach\nbased on cross-modal knowledge distillation with a teacher-student model\nstructure. The teacher model is trained with multiple data modalities, while\nthe student model is designed to operate effectively with reduced modalities.\nIn experiments on $\\textit{connected autonomous driving with ground vehicles}$\nand $\\textit{aerial-ground vehicles collaboration}$, our method improves\ndriving safety by up to ${\\it 20.7}\\%$, surpassing the best-existing baseline\nin detecting potential accidents and making safe driving decisions. More\ninformation can be found on our website https://ruiiu.github.io/mmcd.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-modal collaborative decision-making for connected autonomous driving, which is related to trajectory prediction and path planning for vehicles. Although it doesn't explicitly mention Large Language Models, the concept of knowledge distillation could potentially be relevant to how LLMs are used in autonomous driving scenarios. The core focus is on improving decision-making for autonomous systems, which indirectly relates to trajectory prediction.", "keywords": ["autonomous driving", "decision-making", "multi-modal", "knowledge distillation", "connected vehicles"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18506", "pdf": "https://arxiv.org/pdf/2509.18506", "abs": "https://arxiv.org/abs/2509.18506", "authors": ["Siyuan Yu", "Congkai Shen", "Yufei Xi", "James Dallas", "Michael Thompson", "John Subosits", "Hiroshi Yasuda", "Tulga Ersal"], "title": "Spatial Envelope MPC: High Performance Driving without a Reference", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a novel envelope based model predictive control (MPC)\nframework designed to enable autonomous vehicles to handle high performance\ndriving across a wide range of scenarios without a predefined reference. In\nhigh performance autonomous driving, safe operation at the vehicle's dynamic\nlimits requires a real time planning and control framework capable of\naccounting for key vehicle dynamics and environmental constraints when\nfollowing a predefined reference trajectory is suboptimal or even infeasible.\nState of the art planning and control frameworks, however, are predominantly\nreference based, which limits their performance in such situations. To address\nthis gap, this work first introduces a computationally efficient vehicle\ndynamics model tailored for optimization based control and a continuously\ndifferentiable mathematical formulation that accurately captures the entire\ndrivable envelope. This novel model and formulation allow for the direct\nintegration of dynamic feasibility and safety constraints into a unified\nplanning and control framework, thereby removing the necessity for predefined\nreferences. The challenge of envelope planning, which refers to maximally\napproximating the safe drivable area, is tackled by combining reinforcement\nlearning with optimization techniques. The framework is validated through both\nsimulations and real world experiments, demonstrating its high performance\nacross a variety of tasks, including racing, emergency collision avoidance and\noff road navigation. These results highlight the framework's scalability and\nbroad applicability across a diverse set of scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u548c\u63a7\u5236\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u5b9e\u73b0\u9ad8\u6027\u80fd\u81ea\u52a8\u9a7e\u9a76\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u3001\u8def\u5f84\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u8fd9\u4e9b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5bc6\u5207\u76f8\u5173\u3002\u5173\u952e\u8bcd\u5305\u62ec\u201cautonomous vehicles\u201d\u3001\u201cplanning and control\u201d\u3001\u201creinforcement learning\u201d\u7b49\uff0c\u8868\u660e\u5176\u5728\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u5185\u3002", "keywords": ["autonomous vehicles", "trajectory prediction", "planning and control", "reinforcement learning", "MPC", "model predictive control", "collision avoidance"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18592", "pdf": "https://arxiv.org/pdf/2509.18592", "abs": "https://arxiv.org/abs/2509.18592", "authors": ["Neel P. Bhatt", "Yunhao Yang", "Rohan Siva", "Pranay Samineni", "Daniel Milan", "Zhangyang Wang", "Ufuk Topcu"], "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/", "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on vision-language navigation (VLN) and uses vision-language models (VLMs) for robot navigation. While VLN involves planning and generating trajectories, the core contribution revolves around using VLMs to guide exploration and planning in unseen environments. The connection to trajectory prediction is indirect, but the navigation aspect makes it somewhat relevant. It also mentions vision-language models, which are related to large language models.", "keywords": ["vision-language navigation", "vision-language models", "robot navigation", "planning", "trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18119", "pdf": "https://arxiv.org/pdf/2509.18119", "abs": "https://arxiv.org/abs/2509.18119", "authors": ["Yifan Xu", "Xiao Liu", "Xinghan Liu", "Jiaqi Fu", "Hanchen Zhang", "Bohao Jing", "Shudan Zhang", "Yuting Wang", "Wenyi Zhao", "Yuxiao Dong"], "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Building general-purpose graphical user interface (GUI) agents has become\nincreasingly promising with the progress in vision language models. However,\ndeveloping effective mobile GUI agents with reinforcement learning (RL) remains\nchallenging due to the heavy-tailed distribution of task difficulty and the\ninefficiency of large-scale environment sampling. We present an online agentic\nreinforcement learning framework MOBILERL to enhance GUI agents in mobile\nenvironments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO)\nalgorithm. In ADAGRPO, we design difficulty-adaptive positive replay and\nfailure curriculum filtering to adapt the model to different task difficulties.\nWe introduce the shortest path reward adjustment strategy to reshape rewards\nconcerning the task length in multi-turn agentic tasks. Those strategies\njointly stabilize RL training, improve sample efficiency, and generate strong\nperformance across diverse mobile apps and tasks. We apply MOBILERL to two open\nmodels (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B\nmodel achieves state-of-the-art results in terms of success rates on both\nAndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted\nin the AutoGLM products, and also open-sourced at\nhttps://github.com/THUDM/MobileRL.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper explores reinforcement learning for mobile GUI agents, utilizing vision language models (VLMs) like Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base. While the primary focus is not trajectory prediction, the agent's interaction with the GUI can be viewed as a sequence of actions forming a 'trajectory' of interactions. The use of large language models increases its relevance.", "keywords": ["Large Language Models", "VLMs", "Qwen2.5-VL-7B-Instruct", "GLM-4.1V-9B-Base", "Reinforcement Learning", "agent"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18609", "pdf": "https://arxiv.org/pdf/2509.18609", "abs": "https://arxiv.org/abs/2509.18609", "authors": ["Chengran Yuan", "Zijian Lu", "Zhanqi Zhang", "Yimin Zhao", "Zefan Huang", "Shuo Sun", "Jiawei Sun", "Jiahui Li", "Christina Dao Wen Lee", "Dongen Li", "Marcelo H. Ang Jr"], "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving", "categories": ["cs.RO"], "comment": null, "summary": "End-to-end motion planning is promising for simplifying complex autonomous\ndriving pipelines. However, challenges such as scene understanding and\neffective prediction for decision-making continue to present substantial\nobstacles to its large-scale deployment. In this paper, we present PIE, a\npioneering framework that integrates advanced perception, reasoning, and\nintention modeling to dynamically capture interactions between the ego vehicle\nand surrounding agents. It incorporates a bidirectional Mamba fusion that\naddresses data compression losses in multimodal fusion of camera and LiDAR\ninputs, alongside a novel reasoning-enhanced decoder integrating Mamba and\nMixture-of-Experts to facilitate scene-compliant anchor selection and optimize\nadaptive trajectory inference. PIE adopts an action-motion interaction module\nto effectively utilize state predictions of surrounding agents to refine ego\nplanning. The proposed framework is thoroughly validated on the NAVSIM\nbenchmark. PIE, without using any ensemble and data augmentation techniques,\nachieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of\nprior state-of-the-art methods. Comprehensive quantitative and qualitative\nanalyses demonstrate that PIE is capable of reliably generating feasible and\nhigh-quality ego trajectories.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u7aef\u5230\u7aef\u7684\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\uff0c\u5176\u4e2d\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff08trajectory inference\uff09\u4ee5\u53ca\u5bf9\u5468\u56f4\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u9884\u6d4b\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u4f7f\u7528\u4e86Mamba\u67b6\u6784\uff0c\u53ef\u4ee5\u770b\u4f5ctransformer\u7684\u4e00\u79cd\u53d8\u4f53\uff0c\u4f46\u91cd\u70b9\u4e0d\u5728\u4e8e\u5927\u6a21\u578b\u672c\u8eab\u3002", "keywords": ["motion planning", "trajectory inference", "autonomous driving", "action prediction", "Mamba"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18610", "pdf": "https://arxiv.org/pdf/2509.18610", "abs": "https://arxiv.org/abs/2509.18610", "authors": ["Maximilian Adang", "JunEn Low", "Ola Shorinwa", "Mac Schwager"], "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones", "categories": ["cs.RO"], "comment": null, "summary": "Large vision-language models have driven remarkable progress in\nopen-vocabulary robot policies, e.g., generalist robot manipulation policies,\nthat enable robots to complete complex tasks specified in natural language.\nDespite these successes, open-vocabulary autonomous drone navigation remains an\nunsolved challenge due to the scarcity of large-scale demonstrations, real-time\ncontrol demands of drones for stabilization, and lack of reliable external pose\nestimation modules. In this work, we present SINGER for language-guided\nautonomous drone navigation in the open world using only onboard sensing and\ncompute. To train robust, open-vocabulary navigation policies, SINGER leverages\nthree central components: (i) a photorealistic language-embedded flight\nsimulator with minimal sim-to-real gap using Gaussian Splatting for efficient\ndata generation, (ii) an RRT-inspired multi-trajectory generation expert for\ncollision-free navigation demonstrations, and these are used to train (iii) a\nlightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior\nzero-shot sim-to-real transfer of our policy to unseen environments and unseen\nlanguage-conditioned goal objects. When trained on ~700k-1M observation action\npairs of language conditioned visuomotor data and deployed on hardware, SINGER\noutperforms a velocity-controlled semantic guidance baseline by reaching the\nquery 23.33% more on average, and maintains the query in the field of view\n16.67% more on average, with 10% fewer collisions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u5229\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6765\u5b8c\u6210\u81ea\u7136\u8bed\u8a00\u6307\u5b9a\u7684\u590d\u6742\u4efb\u52a1\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u65e0\u4eba\u673a\u5bfc\u822a\u672c\u8d28\u4e0a\u5305\u542b\u4e86\u8def\u5f84\u89c4\u5212\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large vision-language models", "navigation", "RRT", "trajectory generation", "autonomous drone navigation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18626", "pdf": "https://arxiv.org/pdf/2509.18626", "abs": "https://arxiv.org/abs/2509.18626", "authors": ["Jay Patrikar", "Apoorva Sharma", "Sushant Veer", "Boyi Li", "Sebastian Scherer", "Marco Pavone"], "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 5 figures", "summary": "Learning-based autonomous driving systems are trained mostly on incident-free\ndata, offering little guidance near safety-performance boundaries. Real crash\nreports contain precisely the contrastive evidence needed, but they are hard to\nuse: narratives are unstructured, third-person, and poorly grounded to sensor\nviews. We address these challenges by normalizing crash narratives to\nego-centric language and converting both logs and crashes into a unified\nscene-action representation suitable for retrieval. At decision time, our\nsystem adjudicates proposed actions by retrieving relevant precedents from this\nunified index; an agentic counterfactual extension proposes plausible\nalternatives, retrieves for each, and reasons across outcomes before deciding.\nOn a nuScenes benchmark, precedent retrieval substantially improves\ncalibration, with recall on contextually preferred actions rising from 24% to\n53%. The counterfactual variant preserves these gains while sharpening\ndecisions near risk.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses autonomous driving systems and uses crash reports to improve driving decisions. While it doesn't explicitly mention large language models, the use of narratives and reasoning about actions suggests potential applications of LLMs for processing and understanding the crash reports. It's related to trajectory prediction through the context of autonomous driving and decision-making.", "keywords": ["autonomous driving", "decision-making", "scene-action representation", "counterfactual reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18527", "pdf": "https://arxiv.org/pdf/2509.18527", "abs": "https://arxiv.org/abs/2509.18527", "authors": ["Ziwen Chen", "Zhong Wang"], "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "The sport of fencing, like many other sports, faces challenges in refereeing:\nsubjective calls, human errors, bias, and limited availability in practice\nenvironments. We present FERA (Fencing Referee Assistant), a prototype AI\nreferee for foil fencing which integrates pose-based multi-label action\nrecognition and rule-based reasoning. FERA extracts 2D joint positions from\nvideo, normalizes them, computes a 101-dimensional kinematic feature set, and\napplies a Transformer for multi-label move and blade classification. To\ndetermine priority and scoring, FERA applies a distilled language model with\nencoded right-of-way rules, producing both a decision and an explanation for\neach exchange. With limited hand-labeled data, a 5-fold cross-validation\nachieves an average macro-F1 score of 0.549, outperforming multiple baselines,\nincluding a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla\nTransformer. While not ready for deployment, these results demonstrate a\npromising path towards automated referee assistance in foil fencing and new\nopportunities for AI applications, such as coaching in the field of fencing.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses a Transformer model (related to large language models) and performs action recognition, which can be considered a form of trajectory prediction (predicting the future movement/action). The paper also mentions a distilled language model for rule reasoning, further increasing its relevance to large language models. While not directly focused on traditional trajectory prediction scenarios like pedestrian or vehicle movement, the underlying principles and the use of Transformer models connect it to the target themes.", "keywords": ["Transformer", "language model", "action recognition", "pose-based", "rule reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18666", "pdf": "https://arxiv.org/pdf/2509.18666", "abs": "https://arxiv.org/abs/2509.18666", "authors": ["Kaizer Rahaman", "Simran Kumari", "Ashish R. Hota"], "title": "Distributionally Robust Safe Motion Planning with Contextual Information", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We present a distributionally robust approach for collision avoidance by\nincorporating contextual information. Specifically, we embed the conditional\ndistribution of future trajectory of the obstacle conditioned on the motion of\nthe ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional\nkernel mean embedding operator. Then, we define an ambiguity set containing all\ndistributions whose embedding in the RKHS is within a certain distance from the\nempirical estimate of conditional mean embedding learnt from past data.\nConsequently, a distributionally robust collision avoidance constraint is\nformulated, and included in the receding horizon based motion planning\nformulation of the ego agent. Simulation results show that the proposed\napproach is more successful in avoiding collision compared to approaches that\ndo not include contextual information and/or distributional robustness in their\nformulation in several challenging scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u8fdb\u884c\u907f\u78b0\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u5230\u4e86\u8f68\u8ff9\u9884\u6d4b\u548c\u6982\u7387\u5206\u5e03\u5efa\u6a21\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8fd0\u52a8\u89c4\u5212", "\u78b0\u649e\u907f\u514d", "\u6761\u4ef6\u5206\u5e03", "\u4e0a\u4e0b\u6587\u4fe1\u606f"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18676", "pdf": "https://arxiv.org/pdf/2509.18676", "abs": "https://arxiv.org/abs/2509.18676", "authors": ["Sangjun Noh", "Dongwoo Nam", "Kangmin Kim", "Geonhyup Lee", "Yeonguk Yu", "Raeyoung Kang", "Kyoobin Lee"], "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "7 main scripts + 2 reference pages", "summary": "Learning robust visuomotor policies that generalize across diverse objects\nand interaction dynamics remains a central challenge in robotic manipulation.\nMost existing approaches rely on direct observation-to-action mappings or\ncompress perceptual inputs into global or object-centric features, which often\noverlook localized motion cues critical for precise and contact-rich\nmanipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework\nthat leverages scene-level 3D flow as a structured intermediate representation\nto capture fine-grained local motion cues. Our approach predicts the temporal\ntrajectories of sampled query points and conditions action generation on these\ninteraction-aware flows, implemented jointly within a unified diffusion\narchitecture. This design grounds manipulation in localized dynamics while\nenabling the policy to reason about broader scene-level consequences of\nactions. Extensive experiments on the MetaWorld benchmark show that 3D FDP\nachieves state-of-the-art performance across 50 tasks, particularly excelling\non medium and hard settings. Beyond simulation, we validate our method on eight\nreal-robot tasks, where it consistently outperforms prior baselines in\ncontact-rich and non-prehensile scenarios. These results highlight 3D flow as a\npowerful structural prior for learning generalizable visuomotor policies,\nsupporting the development of more robust and versatile robotic manipulation.\nRobot demonstrations, additional results, and code can be found at\nhttps://sites.google.com/view/3dfdp/home.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\uff0c\u901a\u8fc7\u9884\u6d4b3D\u7a7a\u95f4\u4e2d\u7684flow\u6765\u751f\u6210\u52a8\u4f5c\u3002\u867d\u7136\u6d89\u53ca\u5230\u8f68\u8ff9\uff08temporal trajectories of sampled query points\uff09\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u7684\u5b66\u4e60\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\uff08\u5982\u884c\u4eba\u6216\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff09\u3002\u4e0e\u5927\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\uff0c\u4f46diffusion model\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u53c2\u6570\u91cf\u8f83\u5927\u7684\u6a21\u578b\u3002", "keywords": ["trajectory", "diffusion model", "visuomotor policy learning", "robotic manipulation", "3D flow"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18937", "pdf": "https://arxiv.org/pdf/2509.18937", "abs": "https://arxiv.org/abs/2509.18937", "authors": ["Yanyuan Qiao", "Kieran Gilday", "Yutong Xie", "Josie Hughes"], "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands", "categories": ["cs.RO"], "comment": null, "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on robotic hand design using Large Language Models (LLMs). While it doesn't directly address trajectory prediction, it utilizes LLMs for task-specific design, which could potentially be relevant to future applications involving trajectory planning or manipulation tasks. The connection is indirect, but the use of LLMs in a robotics context warrants a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "robotic hand design"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18571", "pdf": "https://arxiv.org/pdf/2509.18571", "abs": "https://arxiv.org/abs/2509.18571", "authors": ["Yuhan Wang", "Cheng Liu", "Zihan Zhao", "Weichao Wu"], "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought", "categories": ["cs.CV"], "comment": null, "summary": "Real-time threat monitoring identifies threatening behaviors in video streams\nand provides reasoning and assessment of threat events through explanatory\ntext. However, prevailing methodologies, whether based on supervised learning\nor generative models, struggle to concurrently satisfy the demanding\nrequirements of real-time performance and decision explainability. To bridge\nthis gap, we introduce Live-E2T, a novel framework that unifies these two\nobjectives through three synergistic mechanisms. First, we deconstruct video\nframes into structured Human-Object-Interaction-Place semantic tuples. This\napproach creates a compact, semantically focused representation, circumventing\nthe information degradation common in conventional feature compression. Second,\nan efficient online event deduplication and updating mechanism is proposed to\nfilter spatio-temporal redundancies, ensuring the system's real time\nresponsiveness. Finally, we fine-tune a Large Language Model using a\nChain-of-Thought strategy, endow it with the capability for transparent and\nlogical reasoning over event sequences to produce coherent threat assessment\nreports. Extensive experiments on benchmark datasets, including XD-Violence and\nUCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art\nmethods in terms of threat detection accuracy, real-time efficiency, and the\ncrucial dimension of explainability.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a Large Language Model for threat assessment based on video streams, which involves reasoning about events that unfold over time. While not explicitly trajectory prediction, the reasoning process implicitly involves understanding the movement and interaction of objects. The use of LLMs is a key aspect of relevance.", "keywords": ["Large Language Model", "LLM", "video streams", "reasoning", "threat detection"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.18166", "pdf": "https://arxiv.org/pdf/2509.18166", "abs": "https://arxiv.org/abs/2509.18166", "authors": ["Xiaoqian Qi", "Haoye Chai", "Yong Li"], "title": "MobiGPT: A Foundation Model for Mobile Wireless Networks", "categories": ["cs.LG"], "comment": null, "summary": "With the rapid development of mobile communication technologies, future\nmobile networks will offer vast services and resources for commuting,\nproduction, daily life, and entertainment. Accurate and efficient forecasting\nof mobile data (e.g., cell traffic, user behavior, channel quality) helps\noperators monitor network state changes, orchestrate wireless resources, and\nschedule infrastructure and users, thereby improving supply efficiency and\nservice quality. However, current forecasting paradigms rely on customized\ndesigns with tailored models for exclusive data types. Such approaches increase\ncomplexity and deployment costs under large-scale, heterogeneous networks\ninvolving base stations, users, and channels. In this paper, we design a\nfoundation model for mobile data forecasting, MobiGPT, with a unified structure\ncapable of forecasting three data types: base station traffic, user app usage,\nand channel quality. We propose a soft-prompt learning method to help the model\nunderstand features of different data types, and introduce a temporal masking\nmechanism to guide the model through three forecasting tasks: short-term\nprediction, long-term prediction, and distribution generation, supporting\ndiverse optimization scenarios. Evaluations on real-world datasets with over\n100,000 samples show that MobiGPT achieves accurate multi-type forecasting.\nCompared to existing models, it improves forecasting accuracy by 27.37%,\n20.08%, and 7.27%, reflecting strong generalization. Moreover, MobiGPT exhibits\nsuperior zero/few-shot performance in unseen scenarios, with over 21.51%\nimprovement, validating its strong transferability as a foundation model.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86MobiGPT\uff0c\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u65e0\u7ebf\u7f51\u7edc\u7684\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u57fa\u7ad9\u6d41\u91cf\u3001\u7528\u6237\u5e94\u7528\u7a0b\u5e8f\u4f7f\u7528\u60c5\u51b5\u548c\u4fe1\u9053\u8d28\u91cf\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u5e7f\u4e49\u5f62\u5f0f\uff0c\u5e76\u4e14\u8bba\u6587\u660e\u786e\u63d0\u5230\u4e86\u201cfoundation model\u201d\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002\u76f8\u5173\u6027\u4e0d\u662f\u5f88\u9ad8\uff0c\u56e0\u4e3a\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u7f51\u7edc\u6570\u636e\u9884\u6d4b\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\u3002", "keywords": ["foundation model", "Large Language Models", "user behavior", "forecasting", "prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.19077", "pdf": "https://arxiv.org/pdf/2509.19077", "abs": "https://arxiv.org/abs/2509.19077", "authors": ["Zikang Tian", "Shaohui Peng", "Du Huang", "Jiaming Guo", "Ruizhi Chen", "Rui Zhang", "Xishan Zhang", "Yuxuan Guo", "Zidong Du", "Qi Guo", "Ling Li", "Yewen Pu", "Xing Hu", "Yunji Chen"], "title": "Code Driven Planning with Domain-Adaptive Critic", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have been widely adopted as task planners for AI\nagents in sequential decision-making problems, leveraging their extensive world\nknowledge. However, the gap between their general knowledge and\nenvironment-specific requirements often leads to inaccurate plans. To address\nthis, existing approaches rely on frequent LLM queries to iteratively refine\nplans based on immediate environmental feedback, which incurs substantial query\ncosts. However, this refinement is typically guided by short-term environmental\nfeedback, limiting LLMs from developing plans aligned with long-term rewards.\nWe propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of\nrelying on frequent queries, CoPiC employs LLMs to generate a diverse set of\nhigh-level planning programs, which iteratively produce and refine candidate\nplans. A trained domain-adaptive critic then evaluates these candidates and\nselects the one most aligned with long-term rewards for execution. Using\nhigh-level planning programs as planner and domain-adaptive critic as\nestimator, CoPiC improves planning while significantly reducing query costs.\nResults in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC\noutperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving\nan average (1) 23.33% improvement in success rate and (2) 91.27% reduction in\nquery costs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u9886\u57df\u81ea\u9002\u5e94\u8bc4\u8bba\u5bb6\u6765\u6539\u8fdbLLM\u89c4\u5212\u7684\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u5173\u6ce8\u7684planning\u95ee\u9898\u4e0e\u8def\u5f84\u89c4\u5212\u548c\u52a8\u4f5c\u9884\u6d4b\u7b49\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u9886\u57df\u5b58\u5728\u4e00\u5b9a\u8054\u7cfb\u3002\u6b64\u5916\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86Large Language Models (LLMs)\u4f5c\u4e3a\u6838\u5fc3\u6280\u672f\u3002", "keywords": ["Large Language Models", "LLMs", "planning", "sequential decision-making", "domain-adaptive critic"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
