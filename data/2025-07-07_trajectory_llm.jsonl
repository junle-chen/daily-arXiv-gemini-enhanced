{"id": "2507.02406", "pdf": "https://arxiv.org/pdf/2507.02406", "abs": "https://arxiv.org/abs/2507.02406", "authors": ["Caio Azevedo", "Lina Achaji", "Stefano Sabatini", "Nicola Poerio", "Grzegorz Bartyzel", "Sascha Hornauer", "Fabien Moutarde"], "title": "Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization", "categories": ["cs.LG"], "comment": "Accepted for publication at ITSC 2025", "summary": "Trajectory prediction is an essential step in the pipeline of an autonomous\nvehicle. Inaccurate or inconsistent predictions regarding the movement of\nagents in its surroundings lead to poorly planned maneuvers and potentially\ndangerous situations for the end-user. Current state-of-the-art\ndeep-learning-based trajectory prediction models can achieve excellent accuracy\non public datasets. However, when used in more complex, interactive scenarios,\nthey often fail to capture important interdependencies between agents, leading\nto inconsistent predictions among agents in the traffic scene. Inspired by the\nefficacy of incorporating human preference into large language models, this\nwork fine-tunes trajectory prediction models in multi-agent settings using\npreference optimization. By taking as input automatically calculated preference\nrankings among predicted futures in the fine-tuning process, our\nexperiments--using state-of-the-art models on three separate datasets--show\nthat we are able to significantly improve scene consistency while minimally\nsacrificing trajectory prediction accuracy and without adding any excess\ncomputational requirements at inference time.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u660e\u786e\u6d89\u53ca\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u6458\u8981\u4e2d\u4e5f\u63d0\u5230\u4e86\u8f68\u8ff9\u9884\u6d4b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u91cd\u8981\u6027\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8be5\u8bba\u6587\u53d7\u5230\u5c06\u4eba\u7c7b\u504f\u597d\u878d\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u542f\u53d1\uff0c\u5e76\u4f7f\u7528\u504f\u597d\u4f18\u5316\u6765\u5fae\u8c03\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u573a\u666f\u4e00\u81f4\u6027\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "vehicle trajectory prediction", "large language models", "preference optimization", "multi-agent settings", "scene consistency"]}}
{"id": "2507.02029", "pdf": "https://arxiv.org/pdf/2507.02029", "abs": "https://arxiv.org/abs/2507.02029", "authors": ["BAAI RoboBrain Team", "Mingyu Cao", "Huajie Tan", "Yuheng Ji", "Minglan Lin", "Zhiyu Li", "Zhou Cao", "Pengwei Wang", "Enshen Zhou", "Yi Han", "Yingbo Tang", "Xiangqi Xu", "Wei Guo", "Yaoxu Lyu", "Yijie Xu", "Jiayu Shi", "Cheng Chi", "Mengdi Zhao", "Xiaoshuai Hao", "Shanyu Rong", "Zhengliang Cai", "Bolun Zhang", "Shuyi Zhang", "Huaihai Lyu", "Mengfei Du", "Lingfeng Zhang", "Xi Feng", "Xiaodan Liu", "Yance Jiao", "Chenrui He", "Mengsi Lyu", "Zhuo Chen", "Yulong Ao", "Xue Sun", "Zheqi He", "Jingshu Zheng", "Xi Yang", "Donghai Shi", "Kunchang Xie", "Bochao Zhang", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "RoboBrain 2.0 Technical Report", "categories": ["cs.RO"], "comment": null, "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper introduces RoboBrain 2.0, an embodied vision-language foundation model. The abstract explicitly mentions 'trajectory forecasting' as one of its capabilities, and the model itself falls under the category of large models/foundation models. Therefore, it is highly relevant to both trajectory prediction and large language models.", "keywords": ["trajectory forecasting", "foundation models", "large language models", "embodied AI", "vision-language model"]}}
{"id": "2507.02190", "pdf": "https://arxiv.org/pdf/2507.02190", "abs": "https://arxiv.org/abs/2507.02190", "authors": ["Max Argus", "Jelena Bratulic", "Houman Masnavi", "Maxim Velikanov", "Nick Heppert", "Abhinav Valada", "Thomas Brox"], "title": "cVLA: Towards Efficient Camera-Space VLAs", "categories": ["cs.RO", "cs.LG"], "comment": "20 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models offer a compelling framework for tackling\ncomplex robotic manipulation tasks, but they are often expensive to train. In\nthis paper, we propose a novel VLA approach that leverages the competitive\nperformance of Vision Language Models (VLMs) on 2D images to directly infer\nrobot end-effector poses in image frame coordinates. Unlike prior VLA models\nthat output low-level controls, our model predicts trajectory waypoints, making\nit both more efficient to train and robot embodiment agnostic. Despite its\nlightweight design, our next-token prediction architecture effectively learns\nmeaningful and executable robot trajectories. We further explore the\nunderutilized potential of incorporating depth images, inference-time\ntechniques such as decoding strategies, and demonstration-conditioned action\ngeneration. Our model is trained on a simulated dataset and exhibits strong\nsim-to-real transfer capabilities. We evaluate our approach using a combination\nof simulated and real data, demonstrating its effectiveness on a real robotic\nsystem.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u867d\u7136\u4e3b\u8981\u5173\u6ce8Vision-Language-Action\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cpredicts trajectory waypoints\u201d\uff0c\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u540c\u65f6\uff0c\u4f7f\u7528\u4e86Vision Language Models (VLMs)\uff0c\u53ef\u4ee5\u8ba4\u4e3a\u662fLarge Language Models\u5728\u89c6\u89c9\u9886\u57df\u7684\u5e94\u7528\u3002", "keywords": ["trajectory waypoints", "Vision Language Models", "VLA", "action generation"]}}
{"id": "2507.02074", "pdf": "https://arxiv.org/pdf/2507.02074", "abs": "https://arxiv.org/abs/2507.02074", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u660e\u786e\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89c6\u9891\u4e2d\u7684\u78b0\u649e\u68c0\u6d4b\u5e94\u7528\u3002\u867d\u7136\u78b0\u649e\u68c0\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\uff08\u56e0\u4e3a\u53ef\u4ee5\u9884\u6d4b\u6f5c\u5728\u7684\u78b0\u649e\uff09\uff0c\u4f46\u8bba\u6587\u7684\u4e3b\u8981\u7126\u70b9\u662f\u5229\u7528LLM\u8fdb\u884c\u89c6\u9891\u7406\u89e3\u548c\u4e8b\u4ef6\u68c0\u6d4b\uff0c\u800c\u975e\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u5c5e\u4e8e\u4e2d\u7b49\u504f\u4e0a\u3002", "keywords": ["Large Language Models", "LLMs", "foundation models", "crash detection", "video understanding"]}}
{"id": "2507.01982", "pdf": "https://arxiv.org/pdf/2507.01982", "abs": "https://arxiv.org/abs/2507.01982", "authors": ["Siqing Long", "Xiangzhi Huang", "Jiemin Xie", "Ming Cai"], "title": "DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": "39 pages, 14 figures", "summary": "Accurate traffic demand forecasting enables transportation management\ndepartments to allocate resources more effectively, thereby improving their\nutilization efficiency. However, complex spatiotemporal relationships in\ntraffic systems continue to limit the performance of demand forecasting models.\nTo improve the accuracy of spatiotemporal traffic demand prediction, we propose\na new graph convolutional network structure called DKGCM. Specifically, we\nfirst consider the spatial flow distribution of different traffic nodes and\npropose a novel temporal similarity-based clustering graph convolution method,\nDK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering\nto group traffic nodes and more effectively capture spatial dependencies. On\nthe temporal scale, we integrate the Fast Fourier Transform (FFT) within the\nbidirectional Mamba deep learning framework to capture temporal dependencies in\ntraffic demand. To further optimize model training, we incorporate the GRPO\nreinforcement learning strategy to enhance the loss function feedback\nmechanism. Extensive experiments demonstrate that our model outperforms several\nadvanced methods and achieves strong results on three public datasets.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on traffic flow prediction, which falls under the umbrella of trajectory prediction. It utilizes a novel graph convolutional network and a bidirectional Mamba mechanism. While it doesn't directly involve Large Language Models, Mamba is a sequence modeling architecture that has been used in large language models, suggesting a potential connection. The paper's core focus is on spatiotemporal prediction, a key aspect of trajectory forecasting.", "keywords": ["trajectory prediction", "traffic flow prediction", "spatio-temporal prediction", "Mamba", "graph convolutional network"]}}
{"id": "2507.02252", "pdf": "https://arxiv.org/pdf/2507.02252", "abs": "https://arxiv.org/abs/2507.02252", "authors": ["Zeyu Lei", "Hongyuan Yu", "Jinlin Wu", "Zhen Chen"], "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Precise surgical interventions are vital to patient safety, and advanced\nenhancement algorithms have been developed to assist surgeons in\ndecision-making. Despite significant progress, these algorithms are typically\ndesigned for single tasks in specific scenarios, limiting their effectiveness\nin complex real-world situations. To address this limitation, we propose\nSurgVisAgent, an end-to-end intelligent surgical vision agent built on\nmultimodal large language models (MLLMs). SurgVisAgent dynamically identifies\ndistortion categories and severity levels in endoscopic images, enabling it to\nperform a variety of enhancement tasks such as low-light enhancement,\noverexposure correction, motion blur elimination, and smoke removal.\nSpecifically, to achieve superior surgical scenario understanding, we design a\nprior model that provides domain-specific knowledge. Additionally, through\nin-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent\ndelivers customized image enhancements tailored to a wide range of distortion\ntypes and severity levels, thereby addressing the diverse requirements of\nsurgeons. Furthermore, we construct a comprehensive benchmark simulating\nreal-world surgical distortions, on which extensive experiments demonstrate\nthat SurgVisAgent surpasses traditional single-task models, highlighting its\npotential as a unified solution for surgical assistance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on surgical visual enhancement using multimodal large language models (MLLMs). While it doesn't directly address trajectory prediction, the use of MLLMs and agentic models suggests a potential connection to future applications in areas like surgical robotics, where trajectory prediction might be relevant. The paper's focus on understanding and responding to dynamic visual information aligns with some aspects of trajectory prediction, albeit in a different domain.", "keywords": ["multimodal large language models (MLLMs)", "agentic model", "foundation models"]}}
{"id": "2507.02085", "pdf": "https://arxiv.org/pdf/2507.02085", "abs": "https://arxiv.org/abs/2507.02085", "authors": ["Wanjia Zhao", "Jiaqi Han", "Siyi Gu", "Mingjian Jiang", "James Zou", "Stefano Ermon"], "title": "GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Geometric diffusion models have shown remarkable success in molecular\ndynamics and structure generation. However, efficiently fine-tuning them for\ndownstream tasks with varying geometric controls remains underexplored. In this\nwork, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables\nflexible and parameter-efficient fine-tuning for controlled generative tasks\nwithout modifying the original model architecture. GeoAda introduces a\nstructured adapter design: control signals are first encoded through coupling\noperators, then processed by a trainable copy of selected pretrained model\nlayers, and finally projected back via decoupling operators followed by an\nequivariant zero-initialized convolution. By fine-tuning only these lightweight\nadapter modules, GeoAda preserves the model's geometric consistency while\nmitigating overfitting and catastrophic forgetting. We theoretically prove that\nthe proposed adapters maintain SE(3)-equivariance, ensuring that the geometric\ninductive biases of the pretrained diffusion model remain intact during\nadaptation. We demonstrate the wide applicability of GeoAda across diverse\ngeometric control types, including frame control, global control, subgraph\ncontrol, and a broad range of application domains such as particle dynamics,\nmolecular dynamics, human motion prediction, and molecule generation. Empirical\nresults show that GeoAda achieves state-of-the-art fine-tuning performance\nwhile preserving original task accuracy, whereas other baselines experience\nsignificant performance degradation due to overfitting and catastrophic\nforgetting.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on geometric diffusion models and efficient fine-tuning using equivariant adapters (GeoAda). While it doesn't directly involve large language models, it mentions \"human motion prediction\" as an application domain, which is related to trajectory prediction. The connection to trajectory prediction is indirect, and there is no mention of LLMs. Therefore, the relevance score is moderate.", "keywords": ["human motion prediction", "geometric diffusion models"]}}
{"id": "2507.02700", "pdf": "https://arxiv.org/pdf/2507.02700", "abs": "https://arxiv.org/abs/2507.02700", "authors": ["M\u00e1t\u00e9 B. Vizi", "D\u00e9nes T\u00e1k\u00e1cs", "G\u00e1bor St\u00e9p\u00e1n", "G\u00e1bor Orosz"], "title": "Integrating path-planning and control for robotic unicycles", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This article focuses on integrating path-planning and control with\nspecializing on the unique needs of robotic unicycles. A unicycle design is\npresented which is capable of accelerating/breaking and carrying out a variety\nof maneuvers. The proposed path-planning method segments the path into straight\nand curved path sections dedicated for accelerating/breaking and turning\nmaneuvers, respectively. The curvature profiles of the curved sections are\noptimized while considering the control performance and the slipping limits of\nthe wheel. The performance of the proposed integrated approach is demonstrated\nvia numerical simulations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on path planning for robotic unicycles, which falls under the broader category of trajectory prediction and path planning for mobile robots. While it doesn't explicitly mention large language models, the path planning aspect makes it somewhat relevant. It is related to trajectory prediction because path planning is a component of trajectory generation. The path planning method segments the path into straight and curved path sections, which is relevant to predicting how the unicycle will move.", "keywords": ["path-planning", "trajectory", "mobile robots"]}}
{"id": "2507.02761", "pdf": "https://arxiv.org/pdf/2507.02761", "abs": "https://arxiv.org/abs/2507.02761", "authors": ["Long Xu", "Choilam Wong", "Mengke Zhang", "Junxiao Lin", "Fei Gao"], "title": "Trajectory Optimization for Differential Drive Mobile Manipulators via Topological Paths Search and Arc Length-Yaw Parameterization", "categories": ["cs.RO"], "comment": "Technical Report", "summary": "We present an efficient hierarchical motion planning pipeline for\ndifferential drive mobile manipulators. Our approach first searches for\nmultiple collisionfree and topologically distinct paths for the mobile base to\nextract the space in which optimal solutions may exist. Further sampling and\noptimization are then conducted in parallel to explore feasible whole-body\ntrajectories. For trajectory optimization, we employ polynomial trajectories\nand arc length-yaw parameterization, enabling efficient handling of the\nnonholonomic dynamics while ensuring optimality.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5dee\u52a8\u9a71\u52a8\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u6d89\u53ca\u8f68\u8ff9\u89c4\u5212\u548c\u4f18\u5316\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u4f46\u662f\uff0c\u8be5\u8bba\u6587\u6ca1\u6709\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e0d\u9ad8\u3002", "keywords": ["trajectory optimization", "motion planning", "trajectory"]}}
{"id": "2507.02747", "pdf": "https://arxiv.org/pdf/2507.02747", "abs": "https://arxiv.org/abs/2507.02747", "authors": ["Jiawei He", "Danshi Li", "Xinqiang Yu", "Zekun Qi", "Wenyao Zhang", "Jiayi Chen", "Zhaoxiang Zhang", "Zhizheng Zhang", "Li Yi", "He Wang"], "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "As large models gain traction, vision-language-action (VLA) systems are\nenabling robots to tackle increasingly complex tasks. However, limited by the\ndifficulty of data collection, progress has mainly focused on controlling\nsimple gripper end-effectors. There is little research on functional grasping\nwith large models for human-like dexterous hands. In this paper, we introduce\nDexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction\naligned with language instructions using single-view RGBD input. To accomplish\nthis, we generate a dataset of 170 million dexterous grasp poses mapped to\nsemantic parts across 174,000 objects in simulation, paired with detailed\npart-level captions. This large-scale dataset, named DexGraspNet 3.0, is used\nto train a VLM and flow-matching-based pose head capable of producing\ninstruction-aligned grasp poses for tabletop objects. To assess DexVLG's\nperformance, we create benchmarks in physics-based simulations and conduct\nreal-world experiments. Extensive testing demonstrates DexVLG's strong\nzero-shot generalization capabilities-achieving over 76% zero-shot execution\nsuccess rate and state-of-the-art part-grasp accuracy in simulation-and\nsuccessful part-aligned grasps on physical objects in real-world scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using large vision-language models for dexterous grasp pose prediction. While it doesn't directly address trajectory prediction, the 'pose prediction' aspect shares some conceptual overlap. The core is using a large model (VLM) to predict the pose of a grasp, which can be seen as a form of action prediction, thus having some relevance. It also explicitly mentions \"large models\" which is relevant to the specified theme.", "keywords": ["large models", "vision-language model", "pose prediction"]}}
{"id": "2507.02771", "pdf": "https://arxiv.org/pdf/2507.02771", "abs": "https://arxiv.org/abs/2507.02771", "authors": ["Melanie Segado", "Felipe Parodi", "Jordan K. Matelsky", "Michael L. Platt", "Eva B. Dyer", "Konrad P. Kording"], "title": "Grounding Intelligence in Movement", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "9 pages, 2 figures", "summary": "Recent advances in machine learning have dramatically improved our ability to\nmodel language, vision, and other high-dimensional data, yet they continue to\nstruggle with one of the most fundamental aspects of biological systems:\nmovement. Across neuroscience, medicine, robotics, and ethology, movement is\nessential for interpreting behavior, predicting intent, and enabling\ninteraction. Despite its core significance in our intelligence, movement is\noften treated as an afterthought rather than as a rich and structured modality\nin its own right. This reflects a deeper fragmentation in how movement data is\ncollected and modeled, often constrained by task-specific goals and\ndomain-specific assumptions. But movement is not domain-bound. It reflects\nshared physical constraints, conserved morphological structures, and purposeful\ndynamics that cut across species and settings. We argue that movement should be\ntreated as a primary modeling target for AI. It is inherently structured and\ngrounded in embodiment and physics. This structure, often allowing for compact,\nlower-dimensional representations (e.g., pose), makes it more interpretable and\ncomputationally tractable to model than raw, high-dimensional sensory inputs.\nDeveloping models that can learn from and generalize across diverse movement\ndata will not only advance core capabilities in generative modeling and\ncontrol, but also create a shared foundation for understanding behavior across\nbiological and artificial systems. Movement is not just an outcome, it is a\nwindow into how intelligent systems engage with the world.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses the importance of movement as a primary modeling target for AI and mentions predicting intent. While it doesn't explicitly mention trajectory prediction or large language models, the general focus on movement and its role in interpreting behavior and predicting intent suggests some relevance to trajectory prediction. The absence of any mention of large language models lowers the score. The phrase \"generative modeling\" hints at a potential connection to the modeling aspects often involved in trajectory prediction.", "keywords": ["movement", "predicting intent", "generative modeling"]}}
{"id": "2507.02479", "pdf": "https://arxiv.org/pdf/2507.02479", "abs": "https://arxiv.org/abs/2507.02479", "authors": ["Teng Fu", "Yuwen Chen", "Zhuofan Chen", "Mengyang Zhao", "Bin Li", "Xiangyang Xue"], "title": "CrowdTrack: A Benchmark for Difficult Multiple Pedestrian Tracking in Real Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multi-object tracking is a classic field in computer vision. Among them,\npedestrian tracking has extremely high application value and has become the\nmost popular research category. Existing methods mainly use motion or\nappearance information for tracking, which is often difficult in complex\nscenarios. For the motion information, mutual occlusions between objects often\nprevent updating of the motion state; for the appearance information,\nnon-robust results are often obtained due to reasons such as only partial\nvisibility of the object or blurred images. Although learning how to perform\ntracking in these situations from the annotated data is the simplest solution,\nthe existing MOT dataset fails to satisfy this solution. Existing methods\nmainly have two drawbacks: relatively simple scene composition and\nnon-realistic scenarios. Although some of the video sequences in existing\ndataset do not have the above-mentioned drawbacks, the number is far from\nadequate for research purposes. To this end, we propose a difficult large-scale\ndataset for multi-pedestrian tracking, shot mainly from the first-person view\nand all from real-life complex scenarios. We name it ``CrowdTrack'' because\nthere are numerous objects in most of the sequences. Our dataset consists of 33\nvideos, containing a total of 5,185 trajectories. Each object is annotated with\na complete bounding box and a unique object ID. The dataset will provide a\nplatform to facilitate the development of algorithms that remain effective in\ncomplex situations. We analyzed the dataset comprehensively and tested multiple\nSOTA models on our dataset. Besides, we analyzed the performance of the\nfoundation models on our dataset. The dataset and project code is released at:\nhttps://github.com/loseevaya/CrowdTrack .", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper introduces a new dataset for multi-pedestrian tracking, which is related to trajectory prediction. While the paper doesn't directly use large language models, it mentions analyzing the performance of foundation models on the dataset, indicating a potential connection, albeit weak, to the realm of large models.", "keywords": ["pedestrian tracking", "multi-object tracking", "trajectories", "foundation models"]}}
