{"id": "2506.10092", "pdf": "https://arxiv.org/pdf/2506.10092", "abs": "https://arxiv.org/abs/2506.10092", "authors": ["Zezhou Huang", "Krystian Sakowski", "Hans Lehnert", "Wei Cui", "Carlo Curino", "Matteo Interlandi", "Marius Dumitru", "Rathijit Sen"], "title": "GPU Acceleration of SQL Analytics on Compressed Data", "categories": ["cs.DB"], "comment": null, "summary": "GPUs are uniquely suited to accelerate (SQL) analytics workloads thanks to\ntheir massive compute parallelism and High Bandwidth Memory (HBM) -- when\ndatasets fit in the GPU HBM, performance is unparalleled. Unfortunately, GPU\nHBMs remain typically small when compared with lower-bandwidth CPU main memory.\nBesides brute-force scaling across many GPUs, current solutions to accelerate\nqueries on large datasets include leveraging data partitioning and loading\nsmaller data batches in GPU HBM, and hybrid execution with a connected device\n(e.g., CPUs). Unfortunately, these approaches are exposed to the limitations of\nlower main memory and host-to-device interconnect bandwidths, introduce\nadditional I/O overheads, or incur higher costs. This is a substantial problem\nwhen trying to scale adoption of GPUs on larger datasets. Data compression can\nalleviate this bottleneck, but to avoid paying for costly\ndecompression/decoding, an ideal solution must include computation primitives\nto operate directly on data in compressed form.\n  This is the focus of our paper: a set of new methods for running queries\ndirectly on light-weight compressed data using schemes such as Run-Length\nEncoding (RLE), index encoding, bit-width reductions, and dictionary encoding.\nOur novelty includes operating on multiple RLE columns without decompression,\nhandling heterogeneous column encodings, and leveraging PyTorch tensor\noperations for portability across devices. Experimental evaluations show\nspeedups of an order of magnitude compared to state-of-the-art commercial\nCPU-only analytics systems, for real-world queries on a production dataset that\nwould not fit into GPU memory uncompressed. This work paves the road for GPU\nadoption in a much broader set of use cases, and it is complementary to most\nother scale-out or fallback mechanisms."}
{"id": "2506.10238", "pdf": "https://arxiv.org/pdf/2506.10238", "abs": "https://arxiv.org/abs/2506.10238", "authors": ["Mahmoud Abo Khamis", "Jesse Comer", "Phokion Kolaitis", "Sudeepa Roy", "Val Tannen"], "title": "A Unifying Algorithm for Hierarchical Queries", "categories": ["cs.DB"], "comment": null, "summary": "The class of hierarchical queries is known to define the boundary of the\ndichotomy between tractability and intractability for the following two\nextensively studied problems about self-join free Boolean conjunctive queries\n(SJF-BCQ): (i) evaluating a SJF-BCQ on a tuple-independent probabilistic\ndatabase; (ii) computing the Shapley value of a fact in a database on which a\nSJF-BCQ evaluates to true. Here, we establish that hierarchical queries define\nalso the boundary of the dichotomy between tractability and intractability for\na different natural algorithmic problem, which we call the \"bag-set\nmaximization\" problem. The bag-set maximization problem associated with a\nSJF-BCQ $Q$ asks: given a database $\\cal D$, find the biggest value that $Q$\ntakes under bag semantics on a database $\\cal D'$ obtained from $\\cal D$ by\nadding at most $\\theta$ facts from another given database $\\cal D^r$.\n  For non-hierarchical queries, we show that the bag-set maximization problem\nis an NP-complete optimization problem. More significantly, for hierarchical\nqueries, we show that all three aforementioned problems (probabilistic query\nevaluation, Shapley value computation, and bag-set maximization) admit a single\nunifying polynomial-time algorithm that operates on an abstract algebraic\nstructure, called a \"2-monoid\". Each of the three problems requires a different\ninstantiation of the 2-monoid tailored for the problem at hand."}
{"id": "2506.10422", "pdf": "https://arxiv.org/pdf/2506.10422", "abs": "https://arxiv.org/abs/2506.10422", "authors": ["Mayank Patel", "Minal Bhise"], "title": "A Hybrid Heuristic Framework for Resource-Efficient Querying of Scientific Experiments Data", "categories": ["cs.DB", "cs.DC", "cs.ET", "cs.PF"], "comment": null, "summary": "Scientific experiments and modern applications are generating large amounts\nof data every day. Most organizations utilize In-house servers or Cloud\nresources to manage application data and workload. The traditional database\nmanagement system (DBMS) and HTAP systems spend significant time & resources to\nload the entire dataset into DBMS before starting query execution. On the other\nhand, in-situ engines may reparse required data multiple times, increasing\nresource utilization and data processing costs. Additionally, over or\nunder-allocation of resources also increases application running costs. This\npaper proposes a lightweight Resource Availability &Workload aware Hybrid\nFramework (RAW-HF) to optimize querying raw data by utilizing existing finite\nresources efficiently. RAW-HF includes modules that help optimize the resources\nrequired to execute a given workload and maximize the utilization of existing\nresources. The impact of applying RAW-HF to real-world scientific dataset\nworkloads like Sloan Digital Sky Survey (SDSS) and Linked Observation Data\n(LOD) presented over 90% and 85% reduction in workload execution time (WET)\ncompared to widely used traditional DBMS PostgreSQL. The overall CPU, IO\nresource utilization, and WET have been reduced by 26%, 25%, and 26%,\nrespectively, while improving memory utilization by 33%, compared to the\nstate-of-the-art workload-aware partial loading technique (WA) proposed for\nhybrid systems. A comparison of MUAR technique used by RAW-HF with machine\nlearning based resource allocation techniques like PCC is also presented."}
{"id": "2506.10886", "pdf": "https://arxiv.org/pdf/2506.10886", "abs": "https://arxiv.org/abs/2506.10886", "authors": ["Steven Vasquez-Grinnell", "Alex Poliakov"], "title": "S3 Mirror: S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable with DBOS", "categories": ["cs.DB", "q-bio.GN"], "comment": null, "summary": "To meet the needs of a large pharmaceutical organization, we set out to\ncreate S3Mirror - an application for transferring large genomic sequencing\ndatasets between S3 buckets quickly, reliably, and observably. We used the\nDBOSTransact durable execution framework to achieve these goals and benchmarked\nthe performance and cost of the application. S3Mirror is an open source DBOS\nPython application that can run in a variety of environments, including DBOS\nCloud Pro where it runs as much as 40x faster than AWS DataSync at a fraction\nof the cost. Moreover, S3Mirror is resilient to failures and allows for\nreal-time filewise observability of ongoing and past transfers."}
