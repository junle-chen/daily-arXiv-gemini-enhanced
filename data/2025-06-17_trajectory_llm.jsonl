{"id": "2506.11234", "pdf": "https://arxiv.org/pdf/2506.11234", "abs": "https://arxiv.org/abs/2506.11234", "authors": ["Luke Rowe", "Rodrigue de Schaetzen", "Roger Girgis", "Christopher Pal", "Liam Paull"], "title": "Poutine: Vision-Language-Trajectory Pre-Training and Reinforcement Learning Post-Training Enable Robust End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "We present Poutine, a 3B-parameter vision-language model (VLM) tailored for\nend-to-end autonomous driving in long-tail driving scenarios. Poutine is\ntrained in two stages. To obtain strong base driving capabilities, we train\nPoutine-Base in a self-supervised vision-language-trajectory (VLT) next-token\nprediction fashion on 83 hours of CoVLA nominal driving and 11 hours of Waymo\nlong-tail driving. Accompanying language annotations are auto-generated with a\n72B-parameter VLM. Poutine is obtained by fine-tuning Poutine-Base with Group\nRelative Policy Optimization (GRPO) using less than 500 preference-labeled\nframes from the Waymo validation set. We show that both VLT pretraining and RL\nfine-tuning are critical to attain strong driving performance in the long-tail.\nPoutine-Base achieves a rater-feedback score (RFS) of 8.12 on the validation\nset, nearly matching Waymo's expert ground-truth RFS. The final Poutine model\nachieves an RFS of 7.99 on the official Waymo test set, placing 1st in the 2025\nWaymo Vision-Based End-to-End Driving Challenge by a significant margin. These\nresults highlight the promise of scalable VLT pre-training and lightweight RL\nfine-tuning to enable robust and generalizable autonomy.", "relevance_analysis": {"relevance_score": 1.0, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u6307\u51fa\u5176\u7814\u7a76\u5185\u5bb9\u6d89\u53ca\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u8f68\u8ff9\u9884\u6d4b\uff08trajectory\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff083B-parameter VLM\uff09\uff0c\u5e76\u4f7f\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002\u8bba\u6587\u76f4\u63a5\u5c06\u5927\u6a21\u578b\u548c\u8f68\u8ff9\u9884\u6d4b\u7ed3\u5408\uff0c\u56e0\u6b64\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "large language models", "vision-language model", "autonomous driving", "end-to-end driving", "reinforcement learning"]}}
{"id": "2506.11109", "pdf": "https://arxiv.org/pdf/2506.11109", "abs": "https://arxiv.org/abs/2506.11109", "authors": ["Yile Chen", "Yicheng Tao", "Yue Jiang", "Shuai Liu", "Han Yu", "Gao Cong"], "title": "Enhancing Large Language Models for Mobility Analytics with Semantic Location Tokenization", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KDD'25", "summary": "The widespread adoption of location-based services has led to the generation\nof vast amounts of mobility data, providing significant opportunities to model\nuser movement dynamics within urban environments. Recent advancements have\nfocused on adapting Large Language Models (LLMs) for mobility analytics.\nHowever, existing methods face two primary limitations: inadequate semantic\nrepresentation of locations (i.e., discrete IDs) and insufficient modeling of\nmobility signals within LLMs (i.e., single templated instruction fine-tuning).\nTo address these issues, we propose QT-Mob, a novel framework that\nsignificantly enhances LLMs for mobility analytics. QT-Mob introduces a\nlocation tokenization module that learns compact, semantically rich tokens to\nrepresent locations, preserving contextual information while ensuring\ncompatibility with LLMs. Furthermore, QT-Mob incorporates a series of\ncomplementary fine-tuning objectives that align the learned tokens with the\ninternal representations in LLMs, improving the model's comprehension of\nsequential movement patterns and location semantics. The proposed QT-Mob\nframework not only enhances LLMs' ability to interpret mobility data but also\nprovides a more generalizable approach for various mobility analytics tasks.\nExperiments on three real-world dataset demonstrate the superior performance in\nboth next-location prediction and mobility recovery tasks, outperforming\nexisting deep learning and LLM-based methods.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "This paper directly addresses the intersection of large language models and mobility analytics, specifically next-location prediction, which is a form of trajectory prediction. The abstract explicitly mentions adapting LLMs for mobility analytics and improving their ability to interpret mobility data.", "keywords": ["Large Language Models", "LLMs", "mobility analytics", "next-location prediction", "mobility data", "location tokenization", "sequential movement patterns"]}}
{"id": "2506.11124", "pdf": "https://arxiv.org/pdf/2506.11124", "abs": "https://arxiv.org/abs/2506.11124", "authors": ["Yifei Chen", "Ross Greer"], "title": "Technical Report for Argoverse2 Scenario Mining Challenges on Iterative Error Correction and Spatially-Aware Prompting", "categories": ["cs.CV", "cs.SE"], "comment": null, "summary": "Scenario mining from extensive autonomous driving datasets, such as Argoverse\n2, is crucial for the development and validation of self-driving systems. The\nRefAV framework represents a promising approach by employing Large Language\nModels (LLMs) to translate natural-language queries into executable code for\nidentifying relevant scenarios. However, this method faces challenges,\nincluding runtime errors stemming from LLM-generated code and inaccuracies in\ninterpreting parameters for functions that describe complex multi-object\nspatial relationships. This technical report introduces two key enhancements to\naddress these limitations: (1) a fault-tolerant iterative code-generation\nmechanism that refines code by re-prompting the LLM with error feedback, and\n(2) specialized prompt engineering that improves the LLM's comprehension and\ncorrect application of spatial-relationship functions. Experiments on the\nArgoverse 2 validation set with diverse LLMs-Qwen2.5-VL-7B, Gemini 2.5 Flash,\nand Gemini 2.5 Pro-show consistent gains across multiple metrics; most notably,\nthe proposed system achieves a HOTA-Temporal score of 52.37 on the official\ntest set using Gemini 2.5 Pro. These results underline the efficacy of the\nproposed techniques for reliable, high-precision scenario mining.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant as it explicitly combines Large Language Models (LLMs) with scenario mining in autonomous driving datasets, which inherently involves understanding and predicting trajectories of multiple agents. The use of Argoverse 2 dataset further strengthens the link to trajectory prediction, as it's a common benchmark for trajectory forecasting and motion planning. The focus on spatial relationships and temporal aspects also connects to the core aspects of trajectory prediction.", "keywords": ["Large Language Models", "LLMs", "Argoverse 2", "scenario mining", "autonomous driving", "spatial relationships", "temporal"]}}
{"id": "2506.11419", "pdf": "https://arxiv.org/pdf/2506.11419", "abs": "https://arxiv.org/abs/2506.11419", "authors": ["Bin Sun", "Boao Zhang", "Jiayi Lu", "Xinjie Feng", "Jiachen Shang", "Rui Cao", "Mengchao Zheng", "Chuanye Wang", "Shichun Yang", "Yaoguang Cao", "Ziying Song"], "title": "FocalAD: Local Motion Planning for End-to-End Autonomous Driving", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "In end-to-end autonomous driving,the motion prediction plays a pivotal role\nin ego-vehicle planning. However, existing methods often rely on globally\naggregated motion features, ignoring the fact that planning decisions are\nprimarily influenced by a small number of locally interacting agents. Failing\nto attend to these critical local interactions can obscure potential risks and\nundermine planning reliability. In this work, we propose FocalAD, a novel\nend-to-end autonomous driving framework that focuses on critical local\nneighbors and refines planning by enhancing local motion representations.\nSpecifically, FocalAD comprises two core modules: the Ego-Local-Agents\nInteractor (ELAI) and the Focal-Local-Agents Loss (FLA Loss). ELAI conducts a\ngraph-based ego-centric interaction representation that captures motion\ndynamics with local neighbors to enhance both ego planning and agent motion\nqueries. FLA Loss increases the weights of decision-critical neighboring\nagents, guiding the model to prioritize those more relevant to planning.\nExtensive experiments show that FocalAD outperforms existing state-of-the-art\nmethods on the open-loop nuScenes datasets and closed-loop Bench2Drive\nbenchmark. Notably, on the robustness-focused Adv-nuScenes dataset, FocalAD\nachieves even greater improvements, reducing the average colilision rate by\n41.9% compared to DiffusionDrive and by 15.6% compared to SparseDrive.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5c40\u90e8\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u6d89\u53ca\u8fd0\u52a8\u9884\u6d4b\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u4f46\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u6216\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u5e76\u975e\u5b8c\u5168\u9ad8\u3002\u5173\u952e\u8bcd\u5305\u62ec\u8fd0\u52a8\u9884\u6d4b\u548c\u5c40\u90e8\u8fd0\u52a8\u89c4\u5212\uff0c\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5bc6\u5207\u76f8\u5173\u3002", "keywords": ["motion prediction", "local motion planning", "autonomous driving", "trajectory prediction"]}}
{"id": "2506.11526", "pdf": "https://arxiv.org/pdf/2506.11526", "abs": "https://arxiv.org/abs/2506.11526", "authors": ["Yuan Gao", "Mattia Piccinini", "Yuchen Zhang", "Dingrui Wang", "Korbinian Moller", "Roberto Brusnicki", "Baha Zarrouki", "Alessio Gambi", "Jan Frederik Totz", "Kai Storms", "Steven Peters", "Andrea Stocco", "Bassam Alrifaee", "Marco Pavone", "Johannes Betz"], "title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "For autonomous vehicles, safe navigation in complex environments depends on\nhandling a broad range of diverse and rare driving scenarios. Simulation- and\nscenario-based testing have emerged as key approaches to development and\nvalidation of autonomous driving systems. Traditional scenario generation\nrelies on rule-based systems, knowledge-driven models, and data-driven\nsynthesis, often producing limited diversity and unrealistic safety-critical\ncases. With the emergence of foundation models, which represent a new\ngeneration of pre-trained, general-purpose AI models, developers can process\nheterogeneous inputs (e.g., natural language, sensor data, HD maps, and control\nactions), enabling the synthesis and interpretation of complex driving\nscenarios. In this paper, we conduct a survey about the application of\nfoundation models for scenario generation and scenario analysis in autonomous\ndriving (as of May 2025). Our survey presents a unified taxonomy that includes\nlarge language models, vision-language models, multimodal large language\nmodels, diffusion models, and world models for the generation and analysis of\nautonomous driving scenarios. In addition, we review the methodologies,\nopen-source datasets, simulation platforms, and benchmark challenges, and we\nexamine the evaluation metrics tailored explicitly to scenario generation and\nanalysis. Finally, the survey concludes by highlighting the open challenges and\nresearch questions, and outlining promising future research directions. All\nreviewed papers are listed in a continuously maintained repository, which\ncontains supplementary materials and is available at\nhttps://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper discusses the use of foundation models, including large language models, for scenario generation and analysis in autonomous driving. While it doesn't directly focus on trajectory prediction, scenario generation is a related field, as it involves creating realistic driving scenarios that would require trajectory prediction models to be evaluated. The paper also explicitly mentions large language models and other foundation models.", "keywords": ["foundation models", "large language models", "autonomous driving", "scenario generation", "scenario analysis"]}}
{"id": "2506.11842", "pdf": "https://arxiv.org/pdf/2506.11842", "abs": "https://arxiv.org/abs/2506.11842", "authors": ["Zhipeng Bao", "Qianwen Li"], "title": "Your Ride, Your Rules: Psychology and Cognition Enabled Automated Driving Systems", "categories": ["cs.RO"], "comment": "10 figures,29 pages, one colummn", "summary": "Despite rapid advances in autonomous driving, current autonomous vehicles\n(AVs) lack effective bidirectional communication with occupants, limiting\npersonalization and recovery from immobilization. This reduces comfort and\ntrust, potentially slowing broader AV adoption. We propose PACE-ADS (Psychology\nand Cognition Enabled Automated Driving Systems), a human-centered autonomy\nframework that enables AVs to sense, interpret, and respond to both external\ntraffic and internal occupant states. PACE-ADS comprises three foundation\nmodel-based agents: a Driver Agent that analyzes the driving context, a\nPsychologist Agent that interprets occupant psychological signals (e.g., EEG,\nheart rate, facial expressions) and cognitive commands (e.g., speech), and a\nCoordinator Agent that integrates these inputs to produce high-level behavior\ndecisions and operational parameters. Rather than replacing existing AV\nmodules, PACE-ADS complements them by operating at the behavioral level,\ndelegating low-level control to native AV systems. This separation enables\nclosed-loop adaptation and supports integration across diverse platforms. We\nevaluate PACE-ADS in simulation across varied scenarios involving traffic\nlights, pedestrians, work zones, and car following. Results show that PACE-ADS\nadapts driving styles to occupant states, improves ride comfort, and enables\nsafe recovery from immobilization via autonomous reasoning or human guidance.\nOur findings highlight the promise of LLM-based frameworks for bridging the gap\nbetween machine autonomy and human-centered driving.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5229\u7528LLM\u8fdb\u884c\u9a7e\u9a76\u884c\u4e3a\u51b3\u7b56\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u8bba\u6587\u4e2d\u63d0\u53ca\u4e86\u884c\u4eba\u3001\u8f66\u8f86\u7b49\u573a\u666f\uff0c\u4e5f\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u7814\u7a76\u5bf9\u8c61\u76f8\u5173\u3002", "keywords": ["Large Language Models", "LLMs", "automated driving systems", "autonomous vehicles", "foundation model", "driving context", "pedestrians", "car following"]}}
{"id": "2506.11613", "pdf": "https://arxiv.org/pdf/2506.11613", "abs": "https://arxiv.org/abs/2506.11613", "authors": ["Edward Turner", "Anna Soligo", "Mia Taylor", "Senthooran Rajamanoharan", "Neel Nanda"], "title": "Model Organisms for Emergent Misalignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language\nmodels on narrowly harmful datasets can lead them to become broadly misaligned.\nA survey of experts prior to publication revealed this was highly unexpected,\ndemonstrating critical gaps in our understanding of model alignment. In this\nwork, we both advance understanding and provide tools for future research.\nUsing new narrowly misaligned datasets, we create a set of improved model\norganisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B\nparameter models (vs. 32B), and that induce misalignment using a single rank-1\nLoRA adapter. We demonstrate that EM occurs robustly across diverse model\nsizes, three model families, and numerous training protocols including full\nsupervised fine-tuning. Leveraging these cleaner model organisms, we isolate a\nmechanistic phase transition and demonstrate that it corresponds to a robust\nbehavioural phase transition in all studied organisms. Aligning large language\nmodels is critical for frontier AI safety, yet EM exposes how far we are from\nachieving this robustly. By distilling clean model organisms that isolate a\nminimal alignment-compromising change, and where this is learnt, we establish a\nfoundation for future research into understanding and mitigating alignment\nrisks in LLMs.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on the alignment of large language models, which is a key aspect of working with LLMs. While it doesn't directly address trajectory prediction, the core topic of LLM alignment is highly relevant to the broader theme of large language models. The paper discusses fine-tuning, model safety, and mechanistic understanding of model behavior, all of which are crucial for responsible development and application of LLMs, potentially including their use in trajectory prediction tasks in the future.", "keywords": ["large language models", "LLMs", "model alignment", "fine-tuning", "foundation models"]}}
{"id": "2506.11261", "pdf": "https://arxiv.org/pdf/2506.11261", "abs": "https://arxiv.org/abs/2506.11261", "authors": ["Shizhe Chen", "Ricardo Garcia", "Paul Pacaud", "Cordelia Schmid"], "title": "Gondola: Grounded Vision Language Planning for Generalizable Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Robotic manipulation faces a significant challenge in generalizing across\nunseen objects, environments and tasks specified by diverse language\ninstructions. To improve generalization capabilities, recent research has\nincorporated large language models (LLMs) for planning and action execution.\nWhile promising, these methods often fall short in generating grounded plans in\nvisual environments. Although efforts have been made to perform visual\ninstructional tuning on LLMs for robotic manipulation, existing methods are\ntypically constrained by single-view image input and struggle with precise\nobject grounding. In this work, we introduce Gondola, a novel grounded\nvision-language planning model based on LLMs for generalizable robotic\nmanipulation. Gondola takes multi-view images and history plans to produce the\nnext action plan with interleaved texts and segmentation masks of target\nobjects and locations. To support the training of Gondola, we construct three\ntypes of datasets using the RLBench simulator, namely robot grounded planning,\nmulti-view referring expression and pseudo long-horizon task datasets. Gondola\noutperforms the state-of-the-art LLM-based method across all four\ngeneralization levels of the GemBench dataset, including novel placements,\nrigid objects, articulated objects and long-horizon tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper utilizes Large Language Models (LLMs) for robotic manipulation planning. While not directly focused on trajectory prediction, the planning aspect involves generating sequences of actions, which can be interpreted as a form of trajectory in the action space. The strong presence of LLMs makes it relevant.", "keywords": ["Large Language Models", "LLMs", "robotic manipulation", "planning"]}}
{"id": "2506.11040", "pdf": "https://arxiv.org/pdf/2506.11040", "abs": "https://arxiv.org/abs/2506.11040", "authors": ["Feifei Shi", "Xueyan Yin", "Kang Wang", "Wanyu Tu", "Qifu Sun", "Huansheng Ning"], "title": "Large Language models for Time Series Analysis: Techniques, Applications, and Challenges", "categories": ["cs.LG", "cs.CL", "cs.ET"], "comment": null, "summary": "Time series analysis is pivotal in domains like financial forecasting and\nbiomedical monitoring, yet traditional methods are constrained by limited\nnonlinear feature representation and long-term dependency capture. The\nemergence of Large Language Models (LLMs) offers transformative potential by\nleveraging their cross-modal knowledge integration and inherent attention\nmechanisms for time series analysis. However, the development of\ngeneral-purpose LLMs for time series from scratch is still hindered by data\ndiversity, annotation scarcity, and computational requirements. This paper\npresents a systematic review of pre-trained LLM-driven time series analysis,\nfocusing on enabling techniques, potential applications, and open challenges.\nFirst, it establishes an evolutionary roadmap of AI-driven time series\nanalysis, from the early machine learning era, through the emerging LLM-driven\nparadigm, to the development of native temporal foundation models. Second, it\norganizes and systematizes the technical landscape of LLM-driven time series\nanalysis from a workflow perspective, covering LLMs' input, optimization, and\nlightweight stages. Finally, it critically examines novel real-world\napplications and highlights key open challenges that can guide future research\nand innovation. The work not only provides valuable insights into current\nadvances but also outlines promising directions for future development. It\nserves as a foundational reference for both academic and industrial\nresearchers, paving the way for the development of more efficient,\ngeneralizable, and interpretable systems of LLM-driven time series analysis.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u5e8f\u5217\u5206\u6790\uff0c\u5e76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\u7684\u5e94\u7528\u3002\u867d\u7136\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u76f8\u5173\uff08\u8f68\u8ff9\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff09\uff0c\u4f46\u8bba\u6587\u672c\u8eab\u5e76\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u6216\u4fa7\u91cd\u4e8e\u8f68\u8ff9\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u8bc4\u5206\u4e3a0.6\u3002\u8bba\u6587\u660e\u786e\u6d89\u53ca\u5927\u6a21\u578b\u3002", "keywords": ["Large Language Models", "LLMs", "time series analysis", "foundation models"]}}
{"id": "2506.11302", "pdf": "https://arxiv.org/pdf/2506.11302", "abs": "https://arxiv.org/abs/2506.11302", "authors": ["H\u00e9ctor Carri\u00f3n", "Yutong Bai", "V\u00edctor A. Hern\u00e1ndez Castro", "Kishan Panaganti", "Ayush Zenith", "Matthew Trang", "Tony Zhang", "Pietro Perona", "Jitendra Malik"], "title": "TARDIS STRIDE: A Spatio-Temporal Road Image Dataset for Exploration and Autonomy", "categories": ["cs.CV", "cs.AI"], "comment": "Computer Vision, Pattern Recognition, LLMs, Dataset, Data\n  Augmentation", "summary": "World models aim to simulate environments and enable effective agent\nbehavior. However, modeling real-world environments presents unique challenges\nas they dynamically change across both space and, crucially, time. To capture\nthese composed dynamics, we introduce a Spatio-Temporal Road Image Dataset for\nExploration (STRIDE) permuting 360-degree panoramic imagery into rich\ninterconnected observation, state and action nodes. Leveraging this structure,\nwe can simultaneously model the relationship between egocentric views,\npositional coordinates, and movement commands across both space and time. We\nbenchmark this dataset via TARDIS, a transformer-based generative world model\nthat integrates spatial and temporal dynamics through a unified autoregressive\nframework trained on STRIDE. We demonstrate robust performance across a range\nof agentic tasks such as controllable photorealistic image synthesis,\ninstruction following, autonomous self-control, and state-of-the-art\ngeoreferencing. These results suggest a promising direction towards\nsophisticated generalist agents--capable of understanding and manipulating the\nspatial and temporal aspects of their material environments--with enhanced\nembodied reasoning capabilities. Training code, datasets, and model checkpoints\nare made available at https://huggingface.co/datasets/Tera-AI/STRIDE.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86STRIDE\u6570\u636e\u96c6\u548cTARDIS\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u73af\u5883\u5e76\u5b9e\u73b0\u6709\u6548\u7684agent\u884c\u4e3a\u3002\u867d\u7136\u8bba\u6587\u4fa7\u91cd\u4e8e\u4e16\u754c\u6a21\u578b\u548cagent\u63a7\u5236\uff0c\u4f46\u5b83\u6d89\u53ca\u5230\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u52a8\u6001\u7684\u5efa\u6a21\uff0c\u4ee5\u53ca\u57fa\u4e8etransformer\u7684\u751f\u6210\u6a21\u578b\u3002\u8003\u8651\u5230transformer\u67b6\u6784\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5173\u8054\uff0c\u4ee5\u53ca\u5176\u5bf9agent\u8fd0\u52a8\u8f68\u8ff9\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u4f46\u8bba\u6587\u66f4\u504f\u5411\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u73af\u5883\u5efa\u6a21\uff0c\u800c\u975e\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u6a21\u578b\u7814\u7a76\u3002", "keywords": ["world models", "transformer", "agent", "autonomous self-control", "embodied reasoning", "Spatio-Temporal"]}}
{"id": "2506.11558", "pdf": "https://arxiv.org/pdf/2506.11558", "abs": "https://arxiv.org/abs/2506.11558", "authors": ["Bo-Cheng Chiu", "Jen-Jee Chen", "Yu-Chee Tseng", "Feng-Chi Chen"], "title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been extended to the video domain,\nenabling sophisticated video-language understanding. However, existing Video\nLLMs often exhibit limitations in fine-grained temporal reasoning, restricting\ntheir ability to precisely attribute responses to specific video moments,\nespecially under constrained supervision. We introduce DaMO, a data-efficient\nVideo LLM explicitly designed for accurate temporal reasoning and multimodal\nunderstanding. At its core, the proposed Temporal-aware Fuseformer employs a\nhierarchical dual-stream architecture that progressively captures temporal\ndynamics within each modality and effectively fuses complementary visual and\naudio information. To further enhance computational efficiency, DaMO integrates\na global residual that reduces spatial redundancy while preserving essential\nsemantic details. We train DaMO via a structured four-stage progressive\ntraining paradigm, incrementally equipping the model with multimodal alignment,\nsemantic grounding, and temporal reasoning capabilities. This work also\ncontributes multiple datasets augmented from existing ones with GPT-generated\ntemporally grounded QA pairs for tasks requiring temporal supervision.\nComprehensive experiments on temporal grounding and video QA benchmarks\ndemonstrate that DaMO consistently surpasses prior methods, particularly in\ntasks demanding precise temporal alignment and reasoning. Our work establishes\na promising direction for data-efficient video-language modeling.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on video understanding using Large Language Models (LLMs) and temporal reasoning. While it doesn't directly address trajectory prediction, the temporal reasoning aspect and the use of video data are relevant. It uses LLMs for video-language tasks.", "keywords": ["Large Language Models", "LLMs", "Video LLMs", "temporal reasoning", "video-language understanding"]}}
{"id": "2506.11512", "pdf": "https://arxiv.org/pdf/2506.11512", "abs": "https://arxiv.org/abs/2506.11512", "authors": ["Wei Li", "Yunyao Cheng", "Xinli Hao", "Chaohong Ma", "Yuxuan Liang", "Bin Yang", "Christian S. Jensen", "Xiaofeng Meng"], "title": "Prioritizing Alignment Paradigms over Task-Specific Model Customization in Time-Series LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have enabled unprecedented\ncapabilities for time-series reasoning in diverse real-world applications,\nincluding medical, financial, and spatio-temporal domains. However, existing\napproaches typically focus on task-specific model customization, such as\nforecasting and anomaly detection, while overlooking the data itself, referred\nto as time-series primitives, which are essential for in-depth reasoning. This\nposition paper advocates a fundamental shift in approaching time-series\nreasoning with LLMs: prioritizing alignment paradigms grounded in the intrinsic\nprimitives of time series data over task-specific model customization. This\nrealignment addresses the core limitations of current time-series reasoning\napproaches, which are often costly, inflexible, and inefficient, by\nsystematically accounting for intrinsic structure of data before task\nengineering. To this end, we propose three alignment paradigms: Injective\nAlignment, Bridging Alignment, and Internal Alignment, which are emphasized by\nprioritizing different aspects of time-series primitives: domain,\ncharacteristic, and representation, respectively, to activate time-series\nreasoning capabilities of LLMs to enable economical, flexible, and efficient\nreasoning. We further recommend that practitioners adopt an alignment-oriented\nmethod to avail this instruction to select an appropriate alignment paradigm.\nAdditionally, we categorize relevant literature into these alignment paradigms\nand outline promising research directions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5206\u6790\u5728\u8f68\u8ff9\u9884\u6d4b\u4e2d\u975e\u5e38\u91cd\u8981\u3002\u8bba\u6587\u63d0\u5230\u4e86\u65f6\u7a7a\u57df\uff0c\u6697\u793a\u4e86\u53ef\u80fd\u7684\u5e94\u7528\u573a\u666f\u5305\u62ec\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u91cd\u70b9\u5728\u4e8e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5904\u7406\u548cLLM\u7684\u5bf9\u9f50\u3002", "keywords": ["Large Language Models", "LLMs", "time-series reasoning", "time-series data", "spatio-temporal domains", "foundation models"]}}
{"id": "2506.11528", "pdf": "https://arxiv.org/pdf/2506.11528", "abs": "https://arxiv.org/abs/2506.11528", "authors": ["Zijian Wang", "Peng Tao", "Luonan Chen"], "title": "Delayformer: spatiotemporal transformation for predicting high-dimensional dynamics", "categories": ["cs.LG"], "comment": "This paper is currently under review", "summary": "Predicting time-series is of great importance in various scientific and\nengineering fields. However, in the context of limited and noisy data,\naccurately predicting dynamics of all variables in a high-dimensional system is\na challenging task due to their nonlinearity and also complex interactions.\nCurrent methods including deep learning approaches often perform poorly for\nreal-world systems under such circumstances. This study introduces the\nDelayformer framework for simultaneously predicting dynamics of all variables,\nby developing a novel multivariate spatiotemporal information (mvSTI)\ntransformation that makes each observed variable into a delay-embedded state\n(vector) and further cross-learns those states from different variables. From\ndynamical systems viewpoint, Delayformer predicts system states rather than\nindividual variables, thus theoretically and computationally overcoming such\nnonlinearity and cross-interaction problems. Specifically, it first utilizes a\nsingle shared Visual Transformer (ViT) encoder to cross-represent dynamical\nstates from observed variables in a delay embedded form and then employs\ndistinct linear decoders for predicting next states, i.e. equivalently\npredicting all original variables parallelly. By leveraging the theoretical\nfoundations of delay embedding theory and the representational capabilities of\nTransformers, Delayformer outperforms current state-of-the-art methods in\nforecasting tasks on both synthetic and real-world datasets. Furthermore, the\npotential of Delayformer as a foundational time-series model is demonstrated\nthrough cross-domain forecasting tasks, highlighting its broad applicability\nacross various scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u4e86Transformer\u67b6\u6784\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u65b9\u6cd5\u5177\u6709\u5e94\u7528\u4e8e\u9ad8\u7ef4\u52a8\u6001\u7cfb\u7edf\uff08\u5982\u8f68\u8ff9\u9884\u6d4b\uff09\u7684\u6f5c\u529b\u3002\u4f7f\u7528\u4e86Visual Transformer (ViT)\uff0c\u53ef\u4ee5\u89c6\u4f5c\u4e0eTransformer\u76f8\u5173\uff0c\u4f46\u4e0d\u662f\u901a\u5e38\u610f\u4e49\u4e0a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["time-series prediction", "Transformer", "Visual Transformer", "dynamical systems"]}}
{"id": "2506.11773", "pdf": "https://arxiv.org/pdf/2506.11773", "abs": "https://arxiv.org/abs/2506.11773", "authors": ["Zikang Leng", "Megha Thukral", "Yaqi Liu", "Hrudhai Rajasekhar", "Shruthi K. Hiremath", "Thomas Pl\u00f6tz"], "title": "AgentSense: Virtual Sensor Data Generation Using LLM Agent in Simulated Home Environments", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "A major obstacle in developing robust and generalizable smart home-based\nHuman Activity Recognition (HAR) systems is the lack of large-scale, diverse\nlabeled datasets. Variability in home layouts, sensor configurations, and user\nbehavior adds further complexity, as individuals follow varied routines and\nperform activities in distinct ways. Building HAR systems that generalize well\nrequires training data that captures the diversity across users and\nenvironments. To address these challenges, we introduce AgentSense, a virtual\ndata generation pipeline where diverse personas are generated by leveraging\nLarge Language Models. These personas are used to create daily routines, which\nare then decomposed into low-level action sequences. Subsequently, the actions\nare executed in a simulated home environment called VirtualHome that we\nextended with virtual ambient sensors capable of recording the agents\nactivities as they unfold. Overall, AgentSense enables the generation of rich,\nvirtual sensor datasets that represent a wide range of users and home settings.\nAcross five benchmark HAR datasets, we show that leveraging our virtual sensor\ndata substantially improves performance, particularly when real data are\nlimited. Notably, models trained on a combination of virtual data and just a\nfew days of real data achieve performance comparable to those trained on the\nentire real datasets. These results demonstrate and prove the potential of\nvirtual data to address one of the most pressing challenges in ambient sensing,\nwhich is the distinct lack of large-scale, annotated datasets without requiring\nany manual data collection efforts.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528LLM\u751f\u6210\u865a\u62df\u4f20\u611f\u5668\u6570\u636e\uff0c\u7528\u4e8e\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u4f7f\u7528\u4e86LLM\uff08Large Language Models\uff09\u751f\u6210\u884c\u4e3a\u5e8f\u5217\uff0c\u8fd9\u4e9b\u884c\u4e3a\u5e8f\u5217\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5e7f\u4e49\u7684\u8f68\u8ff9\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLM", "VirtualHome", "action sequences"]}}
{"id": "2506.11973", "pdf": "https://arxiv.org/pdf/2506.11973", "abs": "https://arxiv.org/abs/2506.11973", "authors": ["Ankit Bhardwaj", "Rohail Asim", "Sachin Chauhan", "Yasir Zaki", "Lakshminarayanan Subramanian"], "title": "Self-Regulating Cars: Automating Traffic Control in Free Flow Road Networks", "categories": ["cs.LG"], "comment": null, "summary": "Free-flow road networks, such as suburban highways, are increasingly\nexperiencing traffic congestion due to growing commuter inflow and limited\ninfrastructure. Traditional control mechanisms, such as traffic signals or\nlocal heuristics, are ineffective or infeasible in these high-speed,\nsignal-free environments. We introduce self-regulating cars, a reinforcement\nlearning-based traffic control protocol that dynamically modulates vehicle\nspeeds to optimize throughput and prevent congestion, without requiring new\nphysical infrastructure. Our approach integrates classical traffic flow theory,\ngap acceptance models, and microscopic simulation into a physics-informed RL\nframework. By abstracting roads into super-segments, the agent captures\nemergent flow dynamics and learns robust speed modulation policies from\ninstantaneous traffic observations. Evaluated in the high-fidelity PTV Vissim\nsimulator on a real-world highway network, our method improves total throughput\nby 5%, reduces average delay by 13%, and decreases total stops by 3% compared\nto the no-control setting. It also achieves smoother, congestion-resistant flow\nwhile generalizing across varied traffic patterns, demonstrating its potential\nfor scalable, ML-driven traffic management.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses traffic control using reinforcement learning to modulate vehicle speeds, which is related to trajectory prediction and path planning. While it doesn't directly involve large language models, the use of RL for traffic management and speed modulation policies connects it to the broader field of intelligent transportation systems, which can benefit from trajectory prediction techniques. The connection to large language models is weak but potentially exists if LLMs are used for higher-level traffic management strategies that could influence the RL agent.", "keywords": ["traffic control", "reinforcement learning", "vehicle speeds", "traffic flow", "speed modulation", "path planning"]}}
{"id": "2506.11475", "pdf": "https://arxiv.org/pdf/2506.11475", "abs": "https://arxiv.org/abs/2506.11475", "authors": ["Syeda Kisaa Fatima", "Tehreem Zubair", "Noman Ahmed", "Asifullah Khan"], "title": "AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction", "categories": ["cs.MA", "cs.CL", "cs.CV"], "comment": null, "summary": "This paper introduces LUCID-MA (Learning and Understanding Crime through\nDialogue of Multiple Agents), an innovative AI powered framework where multiple\nAI agents collaboratively analyze and understand crime data. Our system that\nconsists of three core components: an analysis assistant that highlights\nspatiotemporal crime patterns, a feedback component that reviews and refines\nanalytical results and a prediction component that forecasts future crime\ntrends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it\nruns completely offline and allows the agents undergo self-improvement through\n100 rounds of communication with less human interaction. A scoring function is\nincorporated to evaluate agent's performance, providing visual plots to track\nlearning progress. This work demonstrates the potential of AutoGen-style agents\nfor autonomous, scalable, and iterative analysis in social science domains\nmaintaining data privacy through offline execution.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a multi-agent framework driven by AutoGen and LLMs (specifically LLaMA-2) for crime data analysis and prediction. While the core task is crime prediction, the \"spatiotemporal crime patterns\" and \"forecasts future crime trends\" aspects touch upon the idea of predicting events in space and time, which is related to trajectory prediction. The use of LLaMA-2 and AutoGen clearly links it to large language models. However, it's not directly trajectory prediction, but rather a prediction task leveraging LLMs.", "keywords": ["Large Language Models", "LLMs", "AutoGen", "Prediction", "LLaMA-2"]}}
{"id": "2506.11925", "pdf": "https://arxiv.org/pdf/2506.11925", "abs": "https://arxiv.org/abs/2506.11925", "authors": ["M. Manzour", "Catherine M. Elias", "Omar M. Shehata", "R. Izquierdo", "M. A. Sotelo"], "title": "Real-World Deployment of a Lane Change Prediction Architecture Based on Knowledge Graph Embeddings and Bayesian Inference", "categories": ["cs.AR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Research on lane change prediction has gained a lot of momentum in the last\ncouple of years. However, most research is confined to simulation or results\nobtained from datasets, leaving a gap between algorithmic advances and on-road\ndeployment. This work closes that gap by demonstrating, on real hardware, a\nlane-change prediction system based on Knowledge Graph Embeddings (KGEs) and\nBayesian inference. Moreover, the ego-vehicle employs a longitudinal braking\naction to ensure the safety of both itself and the surrounding vehicles. Our\narchitecture consists of two modules: (i) a perception module that senses the\nenvironment, derives input numerical features, and converts them into\nlinguistic categories; and communicates them to the prediction module; (ii) a\npretrained prediction module that executes a KGE and Bayesian inference model\nto anticipate the target vehicle's maneuver and transforms the prediction into\nlongitudinal braking action. Real-world hardware experimental validation\ndemonstrates that our prediction system anticipates the target vehicle's lane\nchange three to four seconds in advance, providing the ego vehicle sufficient\ntime to react and allowing the target vehicle to make the lane change safely.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on lane change prediction using knowledge graph embeddings and Bayesian inference. While it doesn't directly use large language models, it utilizes knowledge graphs, which are sometimes used in conjunction with LLMs. The core focus is trajectory prediction (lane change), making it moderately relevant.", "keywords": ["lane change prediction", "trajectory prediction", "knowledge graph embeddings", "Bayesian inference"]}}
{"id": "2506.11305", "pdf": "https://arxiv.org/pdf/2506.11305", "abs": "https://arxiv.org/abs/2506.11305", "authors": ["Mohammad Hammoud", "Devang Acharya"], "title": "Don't Pay Attention", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence length and improved handling of\nlong-range dependencies, albeit with limited parallelism due to their\ninherently recurrent nature. In this paper, we propose Avey, a new neural\nfoundational architecture that breaks away from both attention and recurrence.\nAvey comprises a ranker and an autoregressive neural processor, which\ncollaboratively identify and contextualize only the most relevant tokens for\nany given token, regardless of their positions in the sequence. Specifically,\nAvey decouples sequence length from context width, thus enabling effective\nprocessing of arbitrarily long sequences. Experimental results show that Avey\ncompares favorably to the Transformer across a variety of standard short-range\nNLP benchmarks, while notably excelling at capturing long-range dependencies.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving the efficiency of Transformer architectures for large language models. While it doesn't directly address trajectory prediction, it is highly relevant to the underlying technology (LLMs) that could potentially be used in trajectory prediction tasks. The paper explores alternatives to the attention mechanism, which is a core component of many LLMs.", "keywords": ["Large Language Models", "Transformer", "Foundation Models", "Neural Architecture", "Autoregressive"]}}
