# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-11-30

## 目录

- [计算语言学 (Computation and Language) (2)](#cs-cl)
- [cs.CR (1)](#cs-cr)
- [cs.DB (3)](#cs-db)
- [cs.DC (2)](#cs-dc)

## 计算语言学 (Computation and Language) [cs.CL]
### [1] [Prompt Engineering Techniques for Context-dependent Text-to-SQL in Arabic](https://arxiv.org/abs/2511.20677)
*Saleh Almohaimeed, May Alsofyani, Saad Almohaimeed, Mansour Al Ghanim, Liqiang Wang*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: In recent years, the task of cross-domain, context-dependent text-to-SQL has received significant attention. Enables users with no prior knowledge of SQL to have a conversation with databases using natural language. However, most of the available datasets and research have been conducted in English, along with some work in Chinese. To this date, no effort has been made to address this task in the Arabic language. In this paper, we introduce Ar-SParC, the first Arabic cross-domain, context-dependent text-to-SQL dataset. The dataset consists of 3,450 sequences of interrelated questions, each sequence containing an average of approximately three questions, which results in a total of 10225 questions along with their corresponding SQL queries. We conducted 40 experiments on the Ar-SParC dataset using two large language models, GPT-3.5-turbo and GPT-4.5-turbo, applying 10 different prompt engineering techniques, including four question representation methods and six in-context learning techniques. Furthermore, we developed a novel approach named GAT corrector, which enhanced the performance across all 40 experiments, yielding an average improvement of 1.9% in execution accuracy (EX) and 1.9% in interaction accuracy (IX) under zero-shot settings, and an average increase of 1.72% EX and 0.92% IX under in-context learning settings. Finally, we conducted an ablation study with two more experiments to explain why the GAT corrector outperformed the previous GAT verifier technique, particularly for the Arabic language.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.20677) | **Categories:** cs.CL, cs.DB

---

### [2] [LLMs-Powered Accurate Extraction, Querying and Intelligent Management of Literature derived 2D Materials Data](https://arxiv.org/abs/2511.20691)
*Lijun Shang, Yadong Yu, Wenqiang Kang, Jian Zhou, Dongyue Gao, Pan Xiang, Zhe Liu, Mengyan Dai, Zhonglu Guo, Zhimei Sun*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Two-dimensional (2D) materials have showed widespread applications in energy storage and conversion owning to their unique physicochemical, and electronic properties. Most of the valuable information for the materials, such as their properties and preparation methods, is included in the published research papers. However, due to the dispersion of synthe

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.20691) | **Categories:** cs.CL, cond-mat.mtrl-sci, cs.DB

---


## cs.CR [cs.CR]
### [1] [Constructing and Benchmarking: a Labeled Email Dataset for Text-Based Phishing and Spam Detection Framework](https://arxiv.org/abs/2511.21448)
*Rebeka Toth, Tamas Bisztray, Richard Dubniczky*

Main category: cs.CR

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Phishing and spam emails remain a major cybersecurity threat, with attackers increasingly leveraging Large Language Models (LLMs) to craft highly deceptive content. This study presents a comprehensive email dataset containing phishing, spam, and legitimate messages, explicitly distinguishing between human- and LLM-generated content. Each email is annotated with its category, emotional appeal (e.g., urgency, fear, authority), and underlying motivation (e.g., link-following, credential theft, financial fraud). We benchmark multiple LLMs on their ability to identify these emotional and motivational cues and select the most reliable model to annotate the full dataset. To evaluate classification robustness, emails were also rephrased using several LLMs while preserving meaning and intent. A state-of-the-art LLM was then assessed on its performance across both original and rephrased emails using expert-labeled ground truth. The results highlight strong phishing detection capabilities but reveal persistent challenges in distinguishing spam from legitimate emails. Our dataset and evaluation framework contribute to improving AI-assisted email security systems. To support open science, all code, templates, and resources are available on our project site.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.21448) | **Categories:** cs.CR, cs.AI, cs.DB

---


## cs.DB [cs.DB]
### [1] [MorphingDB: A Task-Centric AI-Native DBMS for Model Management and Inference](https://arxiv.org/abs/2511.21160)
*Wu Sai, Xia Ruichen, Yang Dingyu, Wang Rui, Lai Huihang, Guan Jiarui, Bai Jiameng, Zhang Dongxiang, Tang Xiu, Xie Zhongle, Lu Peng, Chen Gang*

Main category: cs.DB

TL;DR: MorphingDB是一个AI原生数据库管理系统，它自动化了PostgreSQL中深度学习模型的存储、选择和推理。


<details>
  <summary>Details</summary>
Motivation: 现有AI原生数据库管理系统存在开发开销高或计算成本高、DBMS集成差的问题。

Method: MorphingDB设计了一个迁移学习框架用于模型选择，并提出了带有向量共享的预嵌入和基于DAG的批处理流水线来优化推理吞吐量。

Result: MorphingDB在九个公共数据集上优于其他AI原生数据库管理系统和AutoML平台，并在模型选择的准确性、资源消耗和时间成本之间实现了稳健的平衡，并在吞吐量和资源效率方面获得了显著的提升。

Conclusion: MorphingDB在数据库环境中实现了高效且自动化的深度学习模型管理和推理。

Abstract: 对数据库环境内深度神经网络推理的需求不断增长，推动了AI原生数据库管理系统的出现。然而，现有的解决方案要么依赖于以模型为中心的设计，需要开发人员手动选择、配置和维护模型，导致开发开销高，要么采用以任务为中心的AutoML方法，但计算成本高且与DBMS集成不良。我们提出了MorphingDB，这是一个以任务为中心的AI原生数据库管理系统，它可以自动化PostgreSQL中模型的存储、选择和推理。为了实现灵活、I/O高效的深度学习模型存储，我们首先引入了专门的模式和多维张量数据类型，以支持基于BLOB的一体化和解耦模型存储。然后，我们设计了一个用于模型选择的迁移学习框架，该框架通过离线嵌入历史任务来构建可迁移性子空间，并通过特征感知映射进行在线投影以用于实时任务。为了进一步优化推理吞吐量，我们提出了带有向量共享的预嵌入来消除冗余计算，并提出了具有成本感知调度的基于DAG的批处理流水线来最大限度地减少推理时间。MorphingDB作为一个带有LibTorch的PostgreSQL扩展实现，在包含序列、NLP和图像任务的九个公共数据集上优于AI原生数据库管理系统（EvaDB、Madlib、GaussML）和AutoML平台（AutoGluon、AutoKeras、AutoSklearn）。我们的评估表明，在模型选择的准确性、资源消耗和时间成本之间实现了稳健的平衡，并在吞吐量和资源效率方面获得了显著的提升。

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.21160) | **Categories:** cs.DB

---

### [2] [HIRE: A Hybrid Learned Index for Robust and Efficient Performance under Mixed Workloads](https://arxiv.org/abs/2511.21307)
*Xinyi Zhang, Liang Liang, Anastasia Ailamaki, Jianliang Xu*

Main category: cs.DB

TL;DR: HIRE提出了一种混合内存索引结构，结合传统索引的鲁棒性和模型预测能力，以实现高效稳定的性能。


<details>
  <summary>Details</summary>
Motivation: 现有学习索引在尾部延迟、范围查询性能和跨工作负载的一致性方面存在不足。

Method: HIRE采用混合叶节点、模型加速的内部节点、非阻塞重校准机制和跨层优化的批量加载算法。

Result: 在多个真实数据集上的实验结果表明，HIRE在范围查询吞吐量、尾部延迟和整体稳定性方面优于最先进的学习索引和传统结构。

Conclusion: HIRE通过结合传统索引和学习索引的优点，显著提高了索引性能和稳定性。

Abstract: 索引对于现代数据库中高效的数据检索和更新至关重要。机器学习的最新进展催生了学习索引的开发，它通过对数据的累积分布函数进行建模来预测搜索位置并加速查询处理。虽然学习索引在点查询方面大大优于传统结构，但它们通常存在尾部延迟高、范围查询性能欠佳以及在不同工作负载中效果不一致的问题。为了应对这些挑战，本文提出了一种混合内存索引结构HIRE，旨在提供一致的高效性能。 HIRE结合了传统索引的结构和性能鲁棒性以及基于模型的预测的预测能力，以减少搜索开销，同时保持最坏情况下的稳定性。具体来说，它采用（1）适应不同数据分布和工作负载的混合叶节点，（2）通过基于日志的更新来增强的模型加速内部节点，以实现高效更新，（3）用于动态数据的非阻塞、成本驱动的重新校准机制，以及（4）考虑叶和内部节点错误的层间优化批量加载算法。在多个真实世界数据集上的实验结果表明，HIRE在范围查询吞吐量、尾部延迟和整体稳定性方面优于最先进的学习索引和传统结构。与最先进的学习索引和传统索引相比，HIRE在混合工作负载下实现了高达41.7倍的吞吐量，并在不同场景下将尾部延迟降低了高达98%。

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.21307) | **Categories:** cs.DB

---

### [3] [Beyond Accuracy: An Empirical Study of Uncertainty Estimation in Imputation](https://arxiv.org/abs/2511.21607)
*Zarin Tahia Hossain, Mostafa Milani*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Handling missing data is a central challenge in data-driven analysis. Modern imputation methods not only aim for accurate reconstruction but also differ in how they represent and quantify uncertainty. Yet, the reliability and calibration of these uncertainty estimates remain poorly understood. This paper presents a systematic empirical study of uncertainty in imputation, comparing representative methods from three major families: statistical (MICE, SoftImpute), distribution alignment (OT-Impute), and deep generative (GAIN, MIWAE, TabCSDI). Experiments span multiple datasets, missingness mechanisms (MCAR, MAR, MNAR), and missingness rates. Uncertainty is estimated through three complementary routes: multi-run variability, conditional sampling, and predictive-distribution modeling, and evaluated using calibration curves and the Expected Calibration Error (ECE). Results show that accuracy and calibration are often misaligned: models with high reconstruction accuracy do not necessarily yield reliable uncertainty. We analyze method-specific trade-offs among accuracy, calibration, and runtime, identify stable configurations, and offer guidelines for selecting uncertainty-aware imputers in data cleaning and downstream machine learning pipelines.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.21607) | **Categories:** cs.DB, cs.LG

---


## cs.DC [cs.DC]
### [1] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen, Robert Keßler, Roland Pabel, Viktor Achter, Stefan Wesner*

Main category: cs.DC

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.21413) | **Categories:** cs.DC, cs.AI, cs.DB, cs.PF

---

### [2] [AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI](https://arxiv.org/abs/2511.21661)
*Beth Plale, Neelesh Karthikeyan, Isuru Gamage, Joe Stubbs, Sachith Withana*

Main category: cs.DC

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.21661) | **Categories:** cs.DC, cs.DB

---
