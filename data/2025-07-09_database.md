# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-07-09

## 目录

- [计算语言学 (Computation and Language) (1)](#cs-cl)
- [cs.DB (6)](#cs-db)

## 计算语言学 (Computation and Language) [cs.CL]
### [1] [Graph Repairs with Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.03410)
*Hrishikesh Terdalkar, Angela Bonifati, Andrea Mauri*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Property graphs are widely used in domains such as healthcare, finance, and social networks, but they often contain errors due to inconsistencies, missing data, or schema violations. Traditional rule-based and heuristic-driven graph repair methods are limited in their adaptability as they need to be tailored for each dataset. On the other hand, interactive human-in-the-loop approaches may become infeasible when dealing with large graphs, as the cost--both in terms of time and effort--of involving users becomes too high. Recent advancements in Large Language Models (LLMs) present new opportunities for automated graph repair by leveraging contextual reasoning and their access to real-world knowledge. We evaluate the effectiveness of six open-source LLMs in repairing property graphs. We assess repair quality, computational cost, and model-specific performance. Our experiments show that LLMs have the potential to detect and correct errors, with varying degrees of accuracy and efficiency. We discuss the strengths, limitations, and challenges of LLM-driven graph repair and outline future research directions for improving scalability and interpretability.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.03410) | **Categories:** cs.CL, cs.DB, cs.ET

---


## cs.DB [cs.DB]
### [1] [LLM4Hint: Leveraging Large Language Models for Hint Recommendation in Offline Query Optimization](https://arxiv.org/abs/2507.03384)
*Suchen Liu, Jun Gao, Yinjun Han, Yang Lin*

Main category: cs.DB

TL;DR: LLM4Hint 利用大型语言模型 (LLM) 来提升学习优化器的泛化能力，从而更有效地推荐查询优化提示。


<details>
  <summary>Details</summary>
Motivation: 现有的传统优化器在复杂工作负载所需的手动调整方面存在困难，并且基于学习的方法在确保泛化方面面临限制。大型语言模型 (LLM) 在各种下游任务中取得了巨大的成功，本文探讨了如何整合 LLM 以增强学习优化器的泛化能力。

Method: 该论文提出 LLM4Hint，它利用中等规模的骨干 LLM 来推荐查询优化提示。LLM4Hint 通过以下方式实现目标：(i) 集成一个轻量级模型来生成软提示，捕获 DBMS 中的数据分布和 SQL 谓词，以提供足够的优化特征，同时减少输入到 LLM 的上下文长度，(ii) 设计使用更大的商业 LLM 的查询重写策略，从而简化骨干 LLM 的 SQL 语义并降低微调成本，以及 (iii) 引入显式匹配提示以促进 LLM 和轻量级模型之间的对齐，从而加速组合模型的收敛。

Result: LLM4Hint 在有效性和泛化性方面优于最先进的学习优化器。

Conclusion: 实验表明，LLM4Hint 通过利用 LLM 更强的理解查询语句的能力，在有效性和泛化性方面优于最先进的学习优化器。

Abstract: 查询优化对于 DBMS 中高效的 SQL 查询执行至关重要，并且由于数据量的增长和硬件的进步，随着时间的推移仍然具有吸引力。现有的传统优化器在复杂工作负载所需的手动调整方面存在困难，并且基于学习的方法在确保泛化方面面临限制。 随着大型语言模型 (LLM) 在各种下游任务中取得了巨大的成功，本文探讨了如何整合 LLM 以增强学习优化器的泛化能力。 尽管前景广阔，但这种整合仍然存在挑战，主要包括高模型推理延迟，以及由于 LLM 中令牌序列与具有丰富数值特征的结构化 SQL 执行计划之间的固有差异而导致的大量微调成本和次优性能。在本文中，我们专注于离线优化中的重复查询，以缓解高推理延迟的问题，并提出 LLM4Hint，它利用中等规模的骨干 LLM 来推荐查询优化提示。 LLM4Hint 通过以下方式实现目标：(i) 集成一个轻量级模型来生成软提示，捕获 DBMS 中的数据分布和 SQL 谓词，以提供足够的优化特征，同时减少输入到 LLM 的上下文长度，(ii) 设计使用更大的商业 LLM 的查询重写策略，从而简化骨干 LLM 的 SQL 语义并降低微调成本，以及 (iii) 引入显式匹配提示以促进 LLM 和轻量级模型之间的对齐，从而加速组合模型的收敛。 实验表明，LLM4Hint 通过利用 LLM 更强的理解查询语句的能力，在有效性和泛化性方面优于最先进的学习优化器。

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.03384) | **Categories:** cs.DB, cs.AI

---

### [2] [PFCS: Prime Factorization Cache System for Deterministic Data Relationship Discovery](https://arxiv.org/abs/2507.03919)
*Duy Le*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Cache systems fundamentally limit modern computing performance due to their inability to precisely capture data relationships. While achieving 85-92% hit rates, traditional systems rely on statistical heuristics that cannot guarantee relationship discovery, leading to suboptimal prefetching and resource waste. We present PFCS (Prime Factorization Cache System), which leverages the mathematical uniqueness of prime factorization to achieve deterministic relationship discovery with zero false positives. PFCS assigns unique primes to data elements and represents relationships as composite numbers, enabling the recovery of perfect relationships through factorization. A comprehensive evaluation across database, ML, and HPC workloads demonstrates an average performance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction compared to state-of-the-art systems. The mathematical foundation provides formal guarantees impossible with approximation-based approaches, establishing a new paradigm for cache system design

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.03919) | **Categories:** cs.DB, cs.CC

---

### [3] [OneDB: A Distributed Multi-Metric Data Similarity Search System](https://arxiv.org/abs/2507.04256)
*Tang Qian, Yifan Zhu, Lu Chen, Xiangyu Ke, Jingwen Zhao, Tianyi Li, Yunjun Gao, Christian S. Jensen*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Increasingly massive volumes of multi-modal data are being accumulated in many {real world} settings, including in health care and e-commerce. This development calls for effective general-purpose data management solutions for multi-modal data. Such a solution must facilitate user-friendly and accurate retrieval of any multi-modal data according to diverse application requirements. Further, such a solution must be capable of efficient and scalable retrieval.   To address this need, we present OneDB, a distributed multi-metric data similarity retrieval system. This system exploits the fact that data of diverse modalities, such as text, images, and video, can be represented as metric data. The system thus affords each data modality its own metric space with its own distance function and then uses a multi-metric model to unify multi-modal data. The system features several innovations: (i) an extended Spart SQL query interface; (ii) lightweight means of learning appropriate weights of different modalities when retrieving multi-modal data to enable accurate retrieval; (iii) smart search-space pruning strategies that improve efficiency; (iv) two-layered indexing of data to ensure load-balancing during distributed processing; and (v) end-to-end system parameter autotuning.   Experiments on three real-life datasets and two synthetic datasets offer evidence that the system is capable of state-of-the-art performance: (i) efficient and effective weight learning; (ii) retrieval accuracy improvements of 12.63\%--30.75\% over the state-of-the-art vector similarity search system at comparable efficiency; (iii) accelerated search by 2.5--5.75x over state-of-the-art single- or multi-metric solutions; (iv) demonstrated high scalability; and (v) parameter tuning that enables performance improvements of 15+%.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.04256) | **Categories:** cs.DB

---

### [4] [AKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset Discovery in Data Lakes](https://arxiv.org/abs/2507.04687)
*Zhenwei Dai, Chuan Lei, Asterios Katsifodimos, Xiao Qin, Christos Faloutsos, Huzefa Rangwala*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: How to generate a large, realistic set of tables along with joinability relationships, to stress-test dataset discovery methods? Dataset discovery methods aim to automatically identify related data assets in a data lake. The development and evaluation of such solutions for customers from a wide range of business domains, relies on diverse, high quality and domain-specific tabular benchmarks. Large language models (LLMs) are trained on a wide variety of text data, which can provide a strong foundation of general and domain-specific knowledge. In this paper, we ask the question -- \textit{can we leverage LLMs to generate a tabular benchmark adequate for evaluating the dataset discovery solutions?} In particular, we focus on the task of finding joinable tables which is the cornerstone of virtually every dataset discovery method. Current corpora for evaluating dataset discovery methods are mainly based on subsets of open data, and they suffer from three important issues: $i)$ they focus on very common and generic data types (e.g., address, id, name, etc.); $ii)$ they do not contain human-annotated column pairs; instead, practitioners synthesize ground truth using table splits (e.g., horizontal for table union search and vertical ones for joinability) and $iii)$ they do not focus on semantic column relationships.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.04687) | **Categories:** cs.DB

---

### [5] [SHARP: Shared State Reduction for Efficient Matching of Sequential Patterns](https://arxiv.org/abs/2507.04872)
*Cong Yu, Tuo Shi, Matthias Weidlich, Bo Zhao*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: The detection of sequential patterns in data is a basic functionality of modern data processing systems for complex event processing (CEP), OLAP, and retrieval-augmented generation (RAG). In practice, pattern matching is challenging, since common applications rely on a large set of patterns that shall be evaluated with tight latency bounds. At the same time, matching needs to maintain state, i.e., intermediate results, that grows exponentially in the input size. Hence, systems turn to best-effort processing, striving for maximal recall under a latency bound. Existing techniques, however, consider each pattern in isolation, neglecting the optimization potential induced by state sharing in pattern matching.   In this paper, we present SHARP, a library that employs state reduction to achieve efficient best-effort pattern matching. To this end, SHARP incorporates state sharing between patterns through a new abstraction, coined pattern-sharing degree (PSD). At runtime, this abstraction facilitates the categorization and indexing of partial pattern matches. Based thereon, once a latency bound is exceeded, SHARP realizes best-effort processing by selecting a subset of partial matches for further processing in constant time. In experiments with real-world data, SHARP achieves a recall of 97%, 96% and 73% for pattern matching in CEP, OLAP, and RAG applications, under a bound of 50% of the average processing latency.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.04872) | **Categories:** cs.DB

---

### [6] [The Case for Instance-Optimized LLMs in OLAP Databases](https://arxiv.org/abs/2507.04967)
*Bardia Mohammadi, Laurent Bindschaedler*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Large Language Models (LLMs) can enhance analytics systems with powerful data summarization, cleaning, and semantic transformation capabilities. However, deploying LLMs at scale -- processing millions to billions of rows -- remains prohibitively expensive in computation and memory. We present IOLM-DB, a novel system that makes LLM-enhanced database queries practical through query-specific model optimization. Instead of using general-purpose LLMs, IOLM-DB generates lightweight, specialized models tailored to each query's specific needs using representative data samples. IOLM-DB reduces model footprints by up to 76% and increases throughput by up to 3.31$\times$ while maintaining accuracy through aggressive compression techniques, including quantization, sparsification, and structural pruning. We further show how our approach enables higher parallelism on existing hardware and seamlessly supports caching and batching strategies to reduce overheads. Our prototype demonstrates that leveraging LLM queries inside analytics systems is feasible at scale, opening new possibilities for future OLAP applications.

</details>

[**[PDF]**](https://arxiv.org/pdf/2507.04967) | **Categories:** cs.DB, cs.LG

---
