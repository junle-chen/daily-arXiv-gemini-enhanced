{"id": "2510.03314", "pdf": "https://arxiv.org/pdf/2510.03314", "abs": "https://arxiv.org/abs/2510.03314", "authors": ["Shucheng Zhang", "Yan Shi", "Bingzhang Wang", "Yuang Zhang", "Muhammad Monjurul Karim", "Kehua Chen", "Chenxi Liu", "Mehrdad Nasri", "Yinhai Wang"], "title": "A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages, 4 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, remains a critical global challenge, as conventional\ninfrastructure-based measures often prove inadequate in dynamic urban\nenvironments. Recent advances in artificial intelligence (AI), particularly in\nvisual perception and reasoning, open new opportunities for proactive and\ncontext-aware VRU protection. However, existing surveys on AI applications for\nVRUs predominantly focus on detection, offering limited coverage of other\nvision-based tasks that are essential for comprehensive VRU understanding and\nprotection. This paper presents a state-of-the-art review of recent progress in\ncamera-based AI sensing systems for VRU safety, with an emphasis on\ndevelopments from the past five years and emerging research trends. We\nsystematically examine four core tasks, namely detection and classification,\ntracking and reidentification, trajectory prediction, and intent recognition\nand prediction, which together form the backbone of AI-empowered proactive\nsolutions for VRU protection in intelligent transportation systems. To guide\nfuture research, we highlight four major open challenges from the perspectives\nof data, model, and deployment. By linking advances in visual AI with practical\nconsiderations for real-world implementation, this survey aims to provide a\nfoundational reference for the development of next-generation sensing systems\nto enhance VRU safety.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u884c\u4eba/\u9a91\u884c\u8005\u5b89\u5168\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5176\u4e2d\u660e\u786e\u63d0\u5230\u4e86\u8f68\u8ff9\u9884\u6d4b\uff08trajectory prediction\uff09\u662f\u6838\u5fc3\u4efb\u52a1\u4e4b\u4e00\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u8ba8\u8bba\u4e86AI\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u8868\u660e\u5b58\u5728\u6f5c\u5728\u7684\u7ed3\u5408\u70b9\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e2d\u7b49\u504f\u4e0a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "AI", "intelligent transportation systems", "intent recognition and prediction", "vulnerable road users"]}, "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u89c6\u89c9\u4eba\u5de5\u667a\u80fd\u7684\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u5b89\u5168\u611f\u77e5\u7cfb\u7edf\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u68c0\u6d4b\u3001\u8ddf\u8e2a\u3001\u8f68\u8ff9\u9884\u6d4b\u548c\u610f\u56fe\u8bc6\u522b\u7b49\u6838\u5fc3\u4efb\u52a1\u3002", "motivation": "\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u63aa\u65bd\u4e0d\u8db3\u4ee5\u786e\u4fdd\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u7684\u5b89\u5168\u3002\u4eba\u5de5\u667a\u80fd\u5728\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u65b9\u9762\u7684\u8fdb\u6b65\u4e3a\u4e3b\u52a8\u548c\u60c5\u5883\u611f\u77e5\u7684VRU\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u56de\u987e\u4e86\u8fd1\u4e94\u5e74\u57fa\u4e8e\u89c6\u89c9\u4eba\u5de5\u667a\u80fd\u7684VRU\u5b89\u5168\u611f\u77e5\u7cfb\u7edf\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u68c0\u6d4b\u4e0e\u5206\u7c7b\u3001\u8ddf\u8e2a\u4e0e\u91cd\u8bc6\u522b\u3001\u8f68\u8ff9\u9884\u6d4b\u4ee5\u53ca\u610f\u56fe\u8bc6\u522b\u4e0e\u9884\u6d4b\u8fd9\u56db\u4e2a\u6838\u5fc3\u4efb\u52a1\u3002", "result": "\u672c\u6587\u603b\u7ed3\u4e86\u6570\u636e\u3001\u6a21\u578b\u548c\u90e8\u7f72\u7b49\u89d2\u5ea6\u7684\u56db\u4e2a\u4e3b\u8981\u5f00\u653e\u6311\u6218\uff0c\u65e8\u5728\u4e3a\u4e0b\u4e00\u4ee3VRU\u5b89\u5168\u611f\u77e5\u7cfb\u7edf\u7684\u5f00\u53d1\u63d0\u4f9b\u53c2\u8003\u3002", "conclusion": "\u901a\u8fc7\u5c06\u89c6\u89c9\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u4e0e\u5b9e\u9645\u5e94\u7528\u76f8\u7ed3\u5408\uff0c\u672c\u6587\u65e8\u5728\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u589e\u5f3aVRU\u5b89\u5168\u6027\u7684\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u53c2\u8003\u3002", "summary_zh": "\u786e\u4fdd\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u5982\u884c\u4eba\u53ca\u9a91\u81ea\u884c\u8f66\u8005\u7684\u5b89\u5168\uff0c\u4ecd\u7136\u662f\u4e00\u9879\u5173\u952e\u7684\u5168\u7403\u6027\u6311\u6218\uff0c\u56e0\u4e3a\u4f20\u7edf\u7684\u57fa\u4e8e\u57fa\u7840\u8bbe\u65bd\u7684\u63aa\u65bd\u5728\u52a8\u6001\u57ce\u5e02\u73af\u5883\u4e2d\u5e38\u5e38\u663e\u5f97\u4e0d\u8db3\u3002\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u611f\u77e5\u548c\u63a8\u7406\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4e3a\u4e3b\u52a8\u548c\u60c5\u5883\u611f\u77e5\u7684VRU\u4fdd\u62a4\u5f00\u542f\u4e86\u65b0\u7684\u673a\u9047\u3002\u7136\u800c\uff0c\u73b0\u6709\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u5728VRU\u5e94\u7528\u65b9\u9762\u7684\u8c03\u67e5\u4e3b\u8981\u96c6\u4e2d\u5728\u68c0\u6d4b\u4e0a\uff0c\u5bf9\u5176\u4ed6\u57fa\u4e8e\u89c6\u89c9\u7684\u4efb\u52a1\u8986\u76d6\u6709\u9650\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u5bf9\u4e8e\u5168\u9762\u7406\u89e3\u548c\u4fdd\u62a4VRU\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u5bf9\u8fd1\u4e94\u5e74\u57fa\u4e8e\u6444\u50cf\u5934\u7684VRU\u5b89\u5168\u4eba\u5de5\u667a\u80fd\u611f\u77e5\u7cfb\u7edf\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u6700\u5148\u8fdb\u7684\u7efc\u8ff0\uff0c\u5e76\u7740\u91cd\u5173\u6ce8\u4e86\u65b0\u5174\u7684\u7814\u7a76\u8d8b\u52bf\u3002\u6211\u4eec\u7cfb\u7edf\u5730\u8003\u5bdf\u4e86\u56db\u4e2a\u6838\u5fc3\u4efb\u52a1\uff0c\u5373\u68c0\u6d4b\u4e0e\u5206\u7c7b\u3001\u8ddf\u8e2a\u4e0e\u91cd\u8bc6\u522b\u3001\u8f68\u8ff9\u9884\u6d4b\u4ee5\u53ca\u610f\u56fe\u8bc6\u522b\u4e0e\u9884\u6d4b\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5171\u540c\u6784\u6210\u4e86\u4eba\u5de5\u667a\u80fd\u8d4b\u80fd\u7684\u4e3b\u52a8\u89e3\u51b3\u65b9\u6848\u7684\u652f\u67f1\uff0c\u4ee5\u5728\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u4fdd\u62a4VRU\u3002\u4e3a\u4e86\u6307\u5bfc\u672a\u6765\u7684\u7814\u7a76\uff0c\u6211\u4eec\u4ece\u6570\u636e\u3001\u6a21\u578b\u548c\u90e8\u7f72\u7684\u89d2\u5ea6\u5f3a\u8c03\u4e86\u56db\u4e2a\u4e3b\u8981\u7684\u5f00\u653e\u6311\u6218\u3002\u901a\u8fc7\u5c06\u89c6\u89c9\u4eba\u5de5\u667a\u80fd\u7684\u8fdb\u6b65\u4e0e\u5b9e\u9645\u5e94\u7528\u76f8\u7ed3\u5408\uff0c\u672c\u7efc\u8ff0\u65e8\u5728\u4e3a\u5f00\u53d1\u4e0b\u4e00\u4ee3\u589e\u5f3aVRU\u5b89\u5168\u6027\u7684\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u57fa\u7840\u53c2\u8003\u3002"}}
{"id": "2510.03496", "pdf": "https://arxiv.org/pdf/2510.03496", "abs": "https://arxiv.org/abs/2510.03496", "authors": ["Vadivelan Murugesan", "Rajasundaram Mathiazhagan", "Sanjana Joshi", "Aliasghar Arab"], "title": "Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*", "categories": ["cs.RO"], "comment": null, "summary": "Human-robot collaboration requires precise prediction of human motion over\nextended horizons to enable proactive collision avoidance. Unlike existing\nplanners that rely solely on kinodynamic models, we present a prediction-driven\nsafe planning framework that leverages granular, joint-by-joint human motion\nforecasting validated in a physics-based digital twin. A capsule-based\nartificial potential field (APF) converts these granular predictions into\ncollision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when\nthresholds are exceeded. The depth camera is used to extract 3D skeletal poses\nand a convolutional neural network-bidirectional long short-term memory\n(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A\ndigital twin model integrates real-time human posture prediction placed in\nfront of a simulated robot to evaluate motions and physical contacts. The\nproposed method enables validation of planned trajectories ahead of time and\nbridging potential latency gaps in updating planned trajectories in real-time.\nIn 50 trials, our method achieved 100% proactive avoidance with > 250 mm\nclearance and sub-2 s replanning, demonstrating superior precision and\nreliability compared to existing kinematic-only planners through the\nintegration of predictive human modeling with digital twin validation.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u9884\u6d4b\u7684\u4eba\u673a\u534f\u4f5c\u78b0\u649e\u907f\u514d\uff0c\u5176\u4e2d\u4f7f\u7528\u4e86CNN-BiLSTM\u6a21\u578b\u8fdb\u884c\u4eba\u4f53\u8fd0\u52a8\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8fd0\u52a8\u9884\u6d4b", "CNN-BiLSTM", "\u4eba\u673a\u534f\u4f5c", "\u78b0\u649e\u907f\u514d"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03776", "pdf": "https://arxiv.org/pdf/2510.03776", "abs": "https://arxiv.org/abs/2510.03776", "authors": ["Tiago Rodrigues de Almeida", "Yufei Zhu", "Andrey Rudenko", "Tomasz P. Kucner", "Johannes A. Stork", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets", "categories": ["cs.RO", "cs.LG"], "comment": "This paper has been accepted to the IEEE Robotics and Automation\n  Letters journal and presented at the 40th Anniversary of the IEEE\n  International Conference on Robotics and Automation, which was held in\n  Rotterdam, Netherlands on 23-26 September, 2024", "summary": "Robots and other intelligent systems navigating in complex dynamic\nenvironments should predict future actions and intentions of surrounding agents\nto reach their goals efficiently and avoid collisions. The dynamics of those\nagents strongly depends on their tasks, roles, or observable labels.\nClass-conditioned motion prediction is thus an appealing way to reduce forecast\nuncertainty and get more accurate predictions for heterogeneous agents.\nHowever, this is hardly explored in the prior art, especially for mobile robots\nand in limited data applications. In this paper, we analyse different\nclass-conditioned trajectory prediction methods on two datasets. We propose a\nset of conditional pattern-based and efficient deep learning-based baselines,\nand evaluate their performance on robotics and outdoors datasets (TH\\\"OR-MAGNI\nand Stanford Drone Dataset). Our experiments show that all methods improve\naccuracy in most of the settings when considering class labels. More\nimportantly, we observe that there are significant differences when learning\nfrom imbalanced datasets, or in new environments where sufficient data is not\navailable. In particular, we find that deep learning methods perform better on\nbalanced datasets, but in applications with limited data, e.g., cold start of a\nrobot in a new environment, or imbalanced classes, pattern-based methods may be\npreferable.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5f02\u6784agent\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u5206\u6790\u4e86\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8f68\u8ff9\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u8fd9\u4e0e\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u9047\u5230\u7684\u95ee\u9898\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "heterogeneous agents", "motion prediction", "deep learning", "robotics"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03895", "pdf": "https://arxiv.org/pdf/2510.03895", "abs": "https://arxiv.org/abs/2510.03895", "authors": ["Zheng Huang", "Mingyu Liu", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Xiaoman Li", "Yiduo Jia", "Hao Zhong", "Hao Chen", "Chunhua Shen"], "title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models represent a pivotal advance in embodied\nintelligence, yet they confront critical barriers to real-world deployment,\nmost notably catastrophic forgetting. This issue stems from their overreliance\non continuous action sequences or action chunks, which inadvertently create\nisolated data silos that disrupt knowledge retention across tasks. To tackle\nthese challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)\nframework: a novel approach that narrows its focus to sparse trajectories,\nthereby avoiding the catastrophic forgetting associated with dense trajectory\nfine-tuning. A key innovation of NoTVLA lies in its trajectory planning\nstrategy: instead of centering on the target object's trajectory, it leverages\ntemporal compression and spatial reasoning pruning specifically for the robot\nend effector's trajectory. Furthermore, training is conducted using these\nsparse trajectories rather than dense action trajectories, an optimization that\ndelivers remarkable practical advantages with better performance in zero-shot.\nIn multi-task evaluation scenarios, NoTVLA achieves superior performance and\ngeneralization compared to pi0 while operating under two critical constraints:\nit uses over an order of magnitude less computing power than pi0 and requires\nno wrist-mounted camera. This design ensures that NoTVLA's operational accuracy\nclosely approximates that of single-task expert models. Crucially, it also\npreserves the model's inherent language capabilities, enabling zero-shot\ngeneralization in specific scenarios, supporting unified model deployment\nacross multiple robot platforms, and fostering a degree of generalization even\nwhen perceiving tasks from novel perspectives.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper discusses Vision-Language-Action (VLA) models, which are related to Large Language Models as they often leverage language understanding. It also focuses on action trajectories for robot manipulation, which falls under the umbrella of trajectory prediction. The paper proposes a method to narrow action trajectories, indicating a focus on optimizing the trajectory planning process. While not directly trajectory prediction in the sense of predicting the future path of an agent, it is related to planning and executing trajectories.", "keywords": ["Vision-Language-Action models", "VLA", "action trajectories", "trajectory planning", "robot manipulation", "Large Language Models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03545", "pdf": "https://arxiv.org/pdf/2510.03545", "abs": "https://arxiv.org/abs/2510.03545", "authors": ["Sixten Norelius", "Aaron O. Feldman", "Mac Schwager"], "title": "SketchPlan: Diffusion Based Drone Planning From Human Sketches", "categories": ["cs.CV", "cs.RO"], "comment": "Code available at https://github.com/sixnor/SketchPlan", "summary": "We propose SketchPlan, a diffusion-based planner that interprets 2D\nhand-drawn sketches over depth images to generate 3D flight paths for drone\nnavigation. SketchPlan comprises two components: a SketchAdapter that learns to\nmap the human sketches to projected 2D paths, and DiffPath, a diffusion model\nthat infers 3D trajectories from 2D projections and a first person view depth\nimage. Our model achieves zero-shot sim-to-real transfer, generating accurate\nand safe flight paths in previously unseen real-world environments. To train\nthe model, we build a synthetic dataset of 32k flight paths using a diverse set\nof photorealistic 3D Gaussian Splatting scenes. We automatically label the data\nby computing 2D projections of the 3D flight paths onto the camera plane, and\nuse this to train the DiffPath diffusion model. However, since real human 2D\nsketches differ significantly from ideal 2D projections, we additionally label\n872 of the 3D flight paths with real human sketches and use this to train the\nSketchAdapter to infer the 2D projection from the human sketch. We demonstrate\nSketchPlan's effectiveness in both simulated and real-world experiments, and\nshow through ablations that training on a mix of human labeled and auto-labeled\ndata together with a modular design significantly boosts its capabilities to\ncorrectly interpret human intent and infer 3D paths. In real-world drone tests,\nSketchPlan achieved 100\\% success in low/medium clutter and 40\\% in unseen\nhigh-clutter environments, outperforming key ablations by 20-60\\% in task\ncompletion.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\uff0c\u6d89\u53ca\u5230\u8f68\u8ff9\u9884\u6d4b\uff08\u98de\u884c\u8def\u5f84\u751f\u6210\uff09\u548c\u6269\u6563\u6a21\u578b\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5927\u89c4\u6a21AI\u6a21\u578b\u7684\u4e00\u79cd\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "diffusion model", "drone planning", "flight path"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.04076", "pdf": "https://arxiv.org/pdf/2510.04076", "abs": "https://arxiv.org/abs/2510.04076", "authors": ["Amin Vahidi-Moghaddam", "Sayed Pedram Haeri Boroujeni", "Iman Jebellat", "Ehsan Jebellat", "Niloufar Mehrabi", "Zhaojian Li"], "title": "From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "One of the main challenges in modern control applications, particularly in\nrobot and vehicle motion control, is achieving accurate, fast, and safe\nmovement. To address this, optimal control policies have been developed to\nenforce safety while ensuring high performance. Since basic first-principles\nmodels of real systems are often available, model-based controllers are widely\nused. Model predictive control (MPC) is a leading approach that optimizes\nperformance while explicitly handling safety constraints. However, obtaining\naccurate models for complex systems is difficult, which motivates data-driven\nalternatives. ML-based MPC leverages learned models to reduce reliance on\nhand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal\npolicies directly from interaction data. Data-enabled predictive control\n(DeePC) goes further by bypassing modeling altogether, directly learning safe\npolicies from raw input-output data. Recently, large language model (LLM)\nagents have also emerged, translating natural language instructions into\nstructured formulations of optimal control problems. Despite these advances,\ndata-driven policies face significant limitations. They often suffer from slow\nresponse times, high computational demands, and large memory needs, making them\nless practical for real-world systems with fast dynamics, limited onboard\ncomputing, or strict memory constraints. To address this, various technique,\nsuch as reduced-order modeling, function-approximated policy learning, and\nconvex relaxations, have been proposed to reduce computational complexity. In\nthis paper, we present eight such approaches and demonstrate their\neffectiveness across real-world applications, including robotic arms, soft\nrobots, and vehicle motion control.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper discusses the use of Large Language Models (LLMs) in the context of control policies for motion control, including vehicle motion control. While it doesn't explicitly focus on trajectory prediction as its primary objective, the connection to motion control and the inclusion of LLMs make it relevant. The paper also mentions Model Predictive Control (MPC), which is often used in trajectory prediction.", "keywords": ["LLM", "Large Language Model", "MPC", "Model Predictive Control", "vehicle motion control", "motion control", "optimal control"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.04436", "pdf": "https://arxiv.org/pdf/2510.04436", "abs": "https://arxiv.org/abs/2510.04436", "authors": ["Jushan Chen", "Santiago Paternain"], "title": "PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Recently, diffusion models have gained popularity and attention in trajectory\noptimization due to their capability of modeling multi-modal probability\ndistributions. However, addressing nonlinear equality constraints, i.e, dynamic\nfeasi- bility, remains a great challenge in diffusion-based trajectory\noptimization. Recent diffusion-based trajectory optimization frameworks rely on\na single-shooting style approach where the denoised control sequence is applied\nto forward propagate the dynamical system, which cannot explicitly enforce\nconstraints on the states and frequently leads to sub-optimal solutions. In\nthis work, we propose a novel direct trajectory optimization approach via\nmodel-based diffusion, which directly generates a sequence of states. To ensure\ndynamic feasibility, we propose a gradient-free projection mechanism that is\nincorporated into the reverse diffusion process. Our results show that,\ncompared to a recent state-of-the-art baseline, our approach leads to zero\ndynamic feasibility error and approximately 4x higher success rate in a\nquadrotor waypoint navigation scenario involving dense static obstacles.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u201ctrajectory optimization\u201d\u548c\u201cdiffusion models\u201d\uff0c\u800c\u6269\u6563\u6a21\u578b\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u88ab\u89c6\u4e3a\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u201cLarge Language Models\u201d\uff0c\u4f46\u5176\u4f7f\u7528\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u4e0e\u5927\u6a21\u578b\u9886\u57df\u7684\u6280\u672f\u6709\u76f8\u4f3c\u4e4b\u5904\u3002\u8bba\u6587\u5173\u6ce8\u8f68\u8ff9\u4f18\u5316\u548c\u52a8\u6001\u53ef\u884c\u6027\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u76f8\u5173\u9886\u57df\u3002", "keywords": ["trajectory optimization", "diffusion models", "trajectory", "dynamic feasibility"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03342", "pdf": "https://arxiv.org/pdf/2510.03342", "abs": "https://arxiv.org/abs/2510.03342", "authors": ["Abbas Abdolmaleki", "Saminda Abeyruwan", "Joshua Ainslie", "Jean-Baptiste Alayrac", "Montserrat Gonzalez Arenas", "Ashwin Balakrishna", "Nathan Batchelor", "Alex Bewley", "Jeff Bingham", "Michael Bloesch", "Konstantinos Bousmalis", "Philemon Brakel", "Anthony Brohan", "Thomas Buschmann", "Arunkumar Byravan", "Serkan Cabi", "Ken Caluwaerts", "Federico Casarini", "Christine Chan", "Oscar Chang", "London Chappellet-Volpini", "Jose Enrique Chen", "Xi Chen", "Hao-Tien Lewis Chiang", "Krzysztof Choromanski", "Adrian Collister", "David B. D'Ambrosio", "Sudeep Dasari", "Todor Davchev", "Meet Kirankumar Dave", "Coline Devin", "Norman Di Palo", "Tianli Ding", "Carl Doersch", "Adil Dostmohamed", "Yilun Du", "Debidatta Dwibedi", "Sathish Thoppay Egambaram", "Michael Elabd", "Tom Erez", "Xiaolin Fang", "Claudio Fantacci", "Cody Fong", "Erik Frey", "Chuyuan Fu", "Ruiqi Gao", "Marissa Giustina", "Keerthana Gopalakrishnan", "Laura Graesser", "Oliver Groth", "Agrim Gupta", "Roland Hafner", "Steven Hansen", "Leonard Hasenclever", "Sam Haves", "Nicolas Heess", "Brandon Hernaez", "Alex Hofer", "Jasmine Hsu", "Lu Huang", "Sandy H. Huang", "Atil Iscen", "Mithun George Jacob", "Deepali Jain", "Sally Jesmonth", "Abhishek Jindal", "Ryan Julian", "Dmitry Kalashnikov", "M. Emre Karagozler", "Stefani Karp", "Matija Kecman", "J. Chase Kew", "Donnie Kim", "Frank Kim", "Junkyung Kim", "Thomas Kipf", "Sean Kirmani", "Ksenia Konyushkova", "Li Yang Ku", "Yuheng Kuang", "Thomas Lampe", "Antoine Laurens", "Tuan Anh Le", "Isabel Leal", "Alex X. Lee", "Tsang-Wei Edward Lee", "Guy Lever", "Jacky Liang", "Li-Heng Lin", "Fangchen Liu", "Shangbang Long", "Caden Lu", "Sharath Maddineni", "Anirudha Majumdar", "Kevis-Kokitsi Maninis", "Andrew Marmon", "Sergio Martinez", "Assaf Hurwitz Michaely", "Niko Milonopoulos", "Joss Moore", "Robert Moreno", "Michael Neunert", "Francesco Nori", "Joy Ortiz", "Kenneth Oslund", "Carolina Parada", "Emilio Parisotto", "Amaris Paryag", "Acorn Pooley", "Thomas Power", "Alessio Quaglino", "Haroon Qureshi", "Rajkumar Vasudeva Raju", "Helen Ran", "Dushyant Rao", "Kanishka Rao", "Isaac Reid", "David Rendleman", "Krista Reymann", "Miguel Rivas", "Francesco Romano", "Yulia Rubanova", "Peter Pastor Sampedro", "Pannag R Sanketi", "Dhruv Shah", "Mohit Sharma", "Kathryn Shea", "Mohit Shridhar", "Charles Shu", "Vikas Sindhwani", "Sumeet Singh", "Radu Soricut", "Rachel Sterneck", "Ian Storz", "Razvan Surdulescu", "Jie Tan", "Jonathan Tompson", "Saran Tunyasuvunakool", "Jake Varley", "Grace Vesom", "Giulia Vezzani", "Maria Bauza Villalonga", "Oriol Vinyals", "Ren\u00e9 Wagner", "Ayzaan Wahid", "Stefan Welker", "Paul Wohlhart", "Chengda Wu", "Markus Wulfmeier", "Fei Xia", "Ted Xiao", "Annie Xie", "Jinyu Xie", "Peng Xu", "Sichun Xu", "Ying Xu", "Zhuo Xu", "Jimmy Yan", "Sherry Yang", "Skye Yang", "Yuxiang Yang", "Hiu Hong Yu", "Wenhao Yu", "Wentao Yuan", "Yuan Yuan", "Jingwei Zhang", "Tingnan Zhang", "Zhiyuan Zhang", "Allan Zhou", "Guangyao Zhou", "Yuxiang Zhou"], "title": "Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer", "categories": ["cs.RO"], "comment": null, "summary": "General-purpose robots need a deep understanding of the physical world,\nadvanced reasoning, and general and dexterous control. This report introduces\nthe latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,\na multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER\n1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together\nthree major innovations. First, Gemini Robotics 1.5 features a novel\narchitecture and a Motion Transfer (MT) mechanism, which enables it to learn\nfrom heterogeneous, multi-embodiment robot data and makes the VLA more general.\nSecond, Gemini Robotics 1.5 interleaves actions with a multi-level internal\nreasoning process in natural language. This enables the robot to \"think before\nacting\" and notably improves its ability to decompose and execute complex,\nmulti-step tasks, and also makes the robot's behavior more interpretable to the\nuser. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for\nembodied reasoning, i.e., for reasoning capabilities that are critical for\nrobots, such as visual and spatial understanding, task planning, and progress\nestimation. Together, this family of models takes us a step towards an era of\nphysical agents-enabling robots to perceive, think and then act so they can\nsolve complex multi-step tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u63cf\u8ff0\u4e86Gemini Robotics 1.5\uff0c\u4e00\u4e2a\u591a\u6a21\u6001 Vision-Language-Action (VLA) \u6a21\u578b\uff0c\u4ee5\u53caGemini Robotics-ER 1.5\uff0c\u4e00\u4e2a\u5177\u8eab\u63a8\u7406 (ER) \u6a21\u578b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u5173\u6ce8\u7684\u91cd\u70b9\u662f\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\u3001\u4efb\u52a1\u89c4\u5212\u548c\u6267\u884c\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5b58\u5728\u4e00\u5b9a\u5173\u8054\u3002 \u540c\u65f6\uff0c\u8be5\u8bba\u6587\u660e\u786e\u63d0\u53ca\u4e86\u5927\u578b\u6a21\u578b\uff08Gemini Robotics 1.5\uff09\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u76f8\u5173\u3002\u603b\u4f53\u6765\u8bf4\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u6a21\u578b\u90fd\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5e76\u975e\u6838\u5fc3\u4e3b\u9898\u3002", "keywords": ["Large Language Models", "VLA", "embodied reasoning", "motion transfer", "task planning", "robot"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03244", "pdf": "https://arxiv.org/pdf/2510.03244", "abs": "https://arxiv.org/abs/2510.03244", "authors": ["Yanlong Wang", "Hang Yu", "Jian Xu", "Fei Ma", "Hongkang Zhang", "Tongtong Feng", "Zijian Zhang", "Shao-Lun Huang", "Danny Dongning Sun", "Xiao-Ping Zhang"], "title": "VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Large time series foundation models often adopt channel-independent\narchitectures to handle varying data dimensions, but this design ignores\ncrucial cross-channel dependencies. Concurrently, existing multimodal\napproaches have not fully exploited the power of large vision models (LVMs) to\ninterpret spatiotemporal data. Additionally, there remains significant\nunexplored potential in leveraging the advantages of information extraction\nfrom different modalities to enhance time series forecasting performance. To\naddress these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO\nuniquely renders multivariate time series into image, enabling pre-trained LVM\nto extract complex cross-channel patterns that are invisible to\nchannel-independent models. These visual features are then aligned and fused\nwith representations from the time series modality. By freezing the LVM and\ntraining only 7.45% of its parameters, VIFO achieves competitive performance on\nmultiple benchmarks, offering an efficient and effective solution for capturing\ncross-variable relationships in", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses multivariate time series forecasting and leverages large vision models (LVMs) for feature extraction. While it doesn't directly address trajectory prediction, the use of LVMs aligns with the 'Large Models' theme. The connection to trajectory prediction is weak but plausible, as time series forecasting can be applied in trajectory prediction contexts. It also mentions cross-modal fusion which could be used in trajectory prediction.", "keywords": ["Large vision models", "multivariate time series forecasting", "cross-modal fusion", "foundation models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03246", "pdf": "https://arxiv.org/pdf/2510.03246", "abs": "https://arxiv.org/abs/2510.03246", "authors": ["Xinyuan Song", "Guangji Bai", "Liang Zhao"], "title": "StructPrune: Structured Global Pruning asymptotics with $\\mathcal{O}(\\sqrt{N})$ GPU Memory", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Pruning is critical for scaling large language models (LLMs). Global pruning\nachieves strong performance but requires $\\mathcal{O}(N)$ memory, which is\ninfeasible for billion-parameter models. Local pruning reduces GPU memory usage\nto that of a single layer by pruning layers independently, but it neglects\ninter-layer dependencies and often leads to suboptimal performance in\nhigh-sparsity regimes. Unlike unstructured pruning, structured pruning produces\nregular sparsity patterns that align well with GPU kernels and library\noptimizations, making it more hardware-efficient. However, structured pruning\ntypically relies on global pruning, since structured patterns are more prone to\nsevere performance degradation under local optimization. To jointly achieve\nstructured pruning and the memory efficiency of local pruning, we propose a\ndivide-and-conquer strategy that decomposes the global pruning problem into\ncoordinated subproblems across different modules, each of which fits within\nlimited GPU memory. Building on this idea, we design \\textbf{STRUPRUNE}, an\nADMM-based framework that integrates structured sparsity into the pruning\nprocess, combining the memory efficiency of local pruning with the hardware\ncompatibility of structured methods. We derive a closed-form analytical\nsolution for structured pruning masks that provides an explicit rule for\nlayer-wise sparsity allocation, and further develop an energy-based asymptotic\nframework yielding a softmax-form allocation scheme that simplifies\noptimization while adapting to heterogeneous layer importance. Experiments\ndemonstrate that STRUPRUNE matches the perplexity of global structured pruning\nwhile reducing memory cost from $\\mathcal{O}(N)$ to $\\mathcal{O}(\\sqrt{N})$,\nenabling practical deployment at the billion-parameter scale.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u526a\u679d\u4f18\u5316\uff0c\u7279\u522b\u662f\u7ed3\u6784\u5316\u526a\u679d\uff0c\u4ee5\u964d\u4f4eGPU\u5185\u5b58\u9700\u6c42\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u5bc6\u5207\u76f8\u5173\uff0c\u800c\u5927\u6a21\u578b\u6709\u65f6\u4f1a\u88ab\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["large language models", "LLMs", "pruning", "structured pruning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03485", "pdf": "https://arxiv.org/pdf/2510.03485", "abs": "https://arxiv.org/abs/2510.03485", "authors": ["Xiaofei Wen", "Wenjie Jacky Mo", "Yanan Xie", "Peng Qi", "Muhao Chen"], "title": "Towards Policy-Compliant Agents: Learning Efficient Guardrails For Policy Violation Detection", "categories": ["cs.AI", "I.2.7"], "comment": "16 pages, 5 figures", "summary": "Autonomous web agents need to operate under externally imposed or\nhuman-specified policies while generating long-horizon trajectories. However,\nlittle work has examined whether these trajectories comply with such policies,\nor whether policy violations persist across different contexts such as domains\n(e.g., shopping or coding websites) and subdomains (e.g., product search and\norder management in shopping). To address this gap, we introduce\nPolicyGuardBench, a benchmark of about 60k examples for detecting policy\nviolations in agent trajectories. From diverse agent runs, we generate a broad\nset of policies and create both within subdomain and cross subdomain pairings\nwith violation labels. In addition to full-trajectory evaluation,\nPolicyGuardBench also includes a prefix-based violation detection task where\nmodels must anticipate policy violations from truncated trajectory prefixes\nrather than complete sequences. Using this dataset, we train PolicyGuard-4B, a\nlightweight guardrail model that delivers strong detection accuracy across all\ntasks while keeping inference efficient. Notably, PolicyGuard-4B generalizes\nacross domains and preserves high accuracy on unseen settings. Together,\nPolicyGuardBench and PolicyGuard-4B provide the first comprehensive framework\nfor studying policy compliance in web agent trajectories, and show that\naccurate and generalizable guardrails are feasible at small scales.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on policy compliance of web agent trajectories, which can be considered a form of trajectory prediction with constraints. It uses a 4B parameter model (PolicyGuard-4B), indicating the involvement of a relatively large model, though not explicitly a Large Language Model. The connection to trajectory prediction is through the analysis of agent trajectories, and the connection to large models is through the mention of a 4B parameter model. However, the core focus is on policy compliance rather than trajectory prediction or LLMs themselves.", "keywords": ["agent trajectories", "policy compliance", "4B parameter model"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03253", "pdf": "https://arxiv.org/pdf/2510.03253", "abs": "https://arxiv.org/abs/2510.03253", "authors": ["Heyang Gao", "Zexu Sun", "Erxue Min", "Hengyi Cai", "Shuaiqiang Wang", "Dawei Yin", "Xu Chen"], "title": "Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents", "categories": ["cs.LG", "cs.AI", "I.2.7"], "comment": "Preprint", "summary": "Large Language Models (LLMs) as autonomous agents are increasingly tasked\nwith solving complex, long-horizon problems. Aligning these agents via\npreference-based offline methods like Direct Preference Optimization (DPO) is a\npromising direction, yet it faces a critical granularity mismatch.\nTrajectory-level DPO provides a signal that is too coarse for precise credit\nassignment, while step-level DPO is often too myopic to capture the value of\nmulti-step behaviors. To resolve this challenge, we introduce Hierarchical\nPreference Learning (HPL), a hierarchical framework that optimizes LLM agents\nby leveraging preference signals at multiple, synergistic granularities. While\nHPL incorporates trajectory- and step-level DPO for global and local policy\nstability, its core innovation lies in group-level preference optimization\nguided by a dual-layer curriculum. Our approach first decomposes expert\ntrajectories into semantically coherent action groups and then generates\ncontrasting suboptimal groups to enable preference learning at a fine-grained,\nsub-task level. Then, instead of treating all preference pairs equally, HPL\nintroduces a curriculum scheduler that organizes the learning process from\nsimple to complex. This curriculum is structured along two axes: the group\nlength, representing sub-task complexity, and the sample difficulty, defined by\nthe reward gap between preferred and dispreferred action groups. Experiments on\nthree challenging agent benchmarks show that HPL outperforms existing\nstate-of-the-art methods. Our analyses demonstrate that the hierarchical DPO\nloss effectively integrates preference signals across multiple granularities,\nwhile the dual-layer curriculum is crucial for enabling the agent to solve a\nwide range of tasks, from simple behaviors to complex multi-step sequences.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving LLM agents for long-horizon tasks by using hierarchical preference learning. While it doesn't directly deal with trajectory prediction in the traditional sense (e.g., predicting the future path of an object), it does involve learning optimal action sequences (trajectories) for LLM agents in complex environments. The core idea of optimizing agent behavior over 'trajectories' of actions connects to the broader concept of trajectory optimization, even if not explicitly in the context of physical object movement. The strong presence of LLMs justifies the relevance score.", "keywords": ["Large Language Models", "LLMs", "autonomous agents", "long-horizon problems", "trajectory", "Direct Preference Optimization", "preference learning", "action sequences"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03255", "pdf": "https://arxiv.org/pdf/2510.03255", "abs": "https://arxiv.org/abs/2510.03255", "authors": ["Wen Wu", "Ziyang Zhang", "Liwei Liu", "Xuenan Xu", "Junlin Liu", "Ke Fan", "Qitan Lv", "Jimin Zhuang", "Chen Zhang", "Zheqi Yuan", "Siyuan Hou", "Tianyi Lin", "Kai Chen", "Bowen Zhou", "Chao Zhang"], "title": "SciTS: Scientific Time Series Understanding and Generation with LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The scientific reasoning ability of large language models (LLMs) has recently\nattracted significant attention. Time series, as a fundamental modality in\nscientific data, presents unique challenges that are often overlooked in\ncurrent multimodal LLMs, which either encode numerical sequences as text or\nconvert them into images. Such approaches may be insufficient for comprehensive\nscientific time series understanding and generation. Existing unified time\nseries models typically specialise in either forecasting or analysis, and their\neffectiveness on non-periodic, heterogeneous scientific signals remains\nunclear. To address these gaps, we introduce SciTS, a benchmark spanning 12\nscientific domains and 43 tasks, with over 50k+ instances, both univariate and\nmultivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz\nin frequency. We benchmark 17 models, including text-only LLMs, multimodal\nLLMs, and unified time series models, and find that general-purpose LLMs\nexhibit stronger generalisability than specialised time series models, while\nrepresenting time series as text or images limits their performance due to\nexcessively long sequences and loss of numerical precision, respectively. We\nthen introduce TimeOmni, a framework that equips LLMs with the ability to\nunderstand and generate time series while remaining compatible with\ngeneral-purpose LLM training. This work fills a gap in both dedicated\nbenchmarks and modelling frameworks for scientific time series, paving the way\nfor LLMs to understand and generate complex temporal scientific data.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper explores the use of LLMs for understanding and generating scientific time series data. While it doesn't directly focus on trajectory prediction, the methodology and findings could potentially be relevant to trajectory prediction tasks, as trajectory data is a type of time series data. The paper explicitly discusses LLMs and their application to time series, making it moderately relevant.", "keywords": ["LLMs", "Large Language Models", "Time series", "Scientific Time Series"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03640", "pdf": "https://arxiv.org/pdf/2510.03640", "abs": "https://arxiv.org/abs/2510.03640", "authors": ["Mostafa Emam", "Matthias Gerdts"], "title": "Safety-Oriented Dynamic Path Planning for Automated Vehicles", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Published in 2025 IEEE 101st Vehicular Technology Conference\n  (VTC2025-Spring), Oslo, Norway, June 17-20, 2025. Received Best Conference\n  Paper Award", "summary": "Ensuring safety in autonomous vehicles necessitates advanced path planning\nand obstacle avoidance capabilities, particularly in dynamic environments. This\npaper introduces a bi-level control framework that efficiently augments road\nboundaries by incorporating time-dependent grid projections of obstacle\nmovements, thus enabling precise and adaptive path planning. The main control\nloop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path\noptimization, wherein homotopy-based constraint relaxation is employed to\nimprove the solvability of the optimal control problem (OCP). Furthermore, an\nindependent backup loop runs concurrently to provide safe fallback trajectories\nwhen an optimal trajectory cannot be computed by the main loop within a\ncritical time frame, thus enhancing safety and real-time performance. Our\nevaluation showcases the benefits of the proposed methods in various driving\nscenarios, highlighting the real-time applicability and robustness of our\napproach. Overall, the framework represents a significant step towards safer\nand more reliable autonomous driving in complex and dynamic environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on dynamic path planning for autonomous vehicles, which is related to trajectory prediction. It does not mention or utilize large language models. However, the core problem of path planning inherently involves predicting future states, hence the moderate relevance.", "keywords": ["path planning", "obstacle avoidance", "trajectory optimization", "autonomous vehicles"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03727", "pdf": "https://arxiv.org/pdf/2510.03727", "abs": "https://arxiv.org/abs/2510.03727", "authors": ["Xuehai He"], "title": "Bridging the Gap Between Multimodal Foundation Models and World Models", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "PhD thesis", "summary": "Humans understand the world through the integration of multiple sensory\nmodalities, enabling them to perceive, reason about, and imagine dynamic\nphysical processes. Inspired by this capability, multimodal foundation models\n(MFMs) have emerged as powerful tools for multimodal understanding and\ngeneration. However, today's MFMs fall short of serving as effective world\nmodels. They lack the essential ability such as perform counterfactual\nreasoning, simulate dynamics, understand the spatiotemporal information,\ncontrol generated visual outcomes, and perform multifaceted reasoning. We\ninvestigates what it takes to bridge the gap between multimodal foundation\nmodels and world models. We begin by improving the reasoning capabilities of\nMFMs through discriminative tasks and equipping MFMs with structured reasoning\nskills, such as causal inference, counterfactual thinking, and spatiotemporal\nreasoning, enabling them to go beyond surface correlations and understand\ndeeper relationships within visual and textual data. Next, we explore\ngenerative capabilities of multimodal foundation models across both image and\nvideo modalities, introducing new frameworks for structured and controllable\ngeneration. Our approaches incorporate scene graphs, multimodal conditioning,\nand multimodal alignment strategies to guide the generation process, ensuring\nconsistency with high-level semantics and fine-grained user intent. We further\nextend these techniques to controllable 4D generation, enabling interactive,\neditable, and morphable object synthesis over time and space.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on bridging the gap between multimodal foundation models and world models, which touches upon spatiotemporal reasoning and simulating dynamics. While it doesn't explicitly mention trajectory prediction, the ability to simulate dynamics and understand spatiotemporal information is crucial for trajectory prediction. The paper heavily involves Large Models (Multimodal Foundation Models). Therefore, there is a moderate relevance.", "keywords": ["multimodal foundation models", "world models", "spatiotemporal reasoning", "dynamics", "large models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.03267", "pdf": "https://arxiv.org/pdf/2510.03267", "abs": "https://arxiv.org/abs/2510.03267", "authors": ["Xianglong Yan", "Chengzhu Bao", "Zhiteng Li", "Tianao Zhang", "Kaicheng Yang", "Haotong Qin", "Ruobing Xie", "Xingwu Sun", "Yulun Zhang"], "title": "PT$^2$-LLM: Post-Training Ternarization for Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown impressive capabilities across\ndiverse tasks, but their large memory and compute demands hinder deployment.\nTernarization has gained attention as a promising compression technique,\ndelivering substantial size reduction and high computational efficiency.\nHowever, its potential in the post-training quantization (PTQ) setting remains\nunderexplored, due to the challenge of training-free parameter optimization and\nthe quantization difficulty posed by outliers and dispersed weights. To address\nthese issues, we propose PT$^2$-LLM, a post-training ternarization framework\ntailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with\na two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which\nalternates between optimal ternary grid construction and flexible rounding to\nminimize quantization error, and (2) Activation-aware Grid Alignment (AGA),\nwhich further refines the ternary grid to better match full-precision outputs.\nIn addition, we propose a plug-and-play Structural Similarity-based Reordering\n(SSR) strategy that leverages inter-column structural similarity to ease\nquantization and mitigate outlier effects, further enhancing overall\nperformance. Extensive experiments demonstrate that PT$^2$-LLM delivers\ncompetitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with\nlower memory cost, while also accelerating both prefill and decoding to achieve\nend-to-end speedup. The code and models will be available at\nhttps://github.com/XIANGLONGYAN/PT2-LLM.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4e09\u5143\u5316\u538b\u7f29\uff0c\u4ee5\u964d\u4f4e\u5185\u5b58\u548c\u8ba1\u7b97\u9700\u6c42\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u7814\u7a76\u5bf9\u8c61\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "post-training quantization", "PTQ", "ternarization", "compression"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.04041", "pdf": "https://arxiv.org/pdf/2510.04041", "abs": "https://arxiv.org/abs/2510.04041", "authors": ["Ayudh Saxena", "Harsh Shah", "Sandeep Routray", "Rishi Rajesh Shah", "Esha Pahwa"], "title": "SITCOM: Scaling Inference-Time COMpute for VLAs", "categories": ["cs.RO"], "comment": "Accepted at the NeurIPS 2025 Workshop on Space in Vision, Language,\n  and Embodied AI (SpaVLE). *Equal contribution", "summary": "Learning robust robotic control policies remains a major challenge due to the\nhigh cost of collecting labeled data, limited generalization to unseen\nenvironments, and difficulties in planning over long horizons. While\nVision-Language-Action (VLA) models offer a promising solution by grounding\nnatural language instructions into single-step control commands, they often\nlack mechanisms for lookahead and struggle with compounding errors in dynamic\ntasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs\n(SITCOM), a framework that augments any pretrained VLA with model-based\nrollouts and reward-based trajectory selection, inspired by Model Predictive\nControl algorithm. SITCOM leverages a learned dynamics model to simulate\nmulti-step action rollouts to select the best candidate plan for real-world\nexecution, transforming one-shot VLAs into robust long-horizon planners. We\ndevelop an efficient transformer-based dynamics model trained on large-scale\nBridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim\ngap, and score candidate rollouts using rewards from simulator. Through\ncomprehensive evaluation across multiple tasks and settings in the SIMPLER\nenvironment, we demonstrate that SITCOM when combined with a good reward\nfunction can significantly improve task completion rate from 48% to 72% using\ntrained dynamics model.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8 Vision-Language-Action (VLA) \u6a21\u578b\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236 (MPC) \u7684\u6846\u67b6 SITCOM \u6765\u63d0\u5347 VLA \u6a21\u578b\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b\u3002\u867d\u7136\u8bba\u6587\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u52a8\u4f5c\u63a8\u6f14\uff0c\u5e76\u9009\u62e9\u6700\u4f73\u8f68\u8ff9\u8fdb\u884c\u6267\u884c\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u5bc6\u5207\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4f7f\u7528\u4e86Transformer\u6a21\u578b\uff0c\u5e76\u4e14VLA\u6a21\u578b\u672c\u8eab\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\uff0c\u6240\u4ee5\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language-Action (VLA)", "Model Predictive Control (MPC)", "dynamics model", "transformer", "long-horizon planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.04017", "pdf": "https://arxiv.org/pdf/2510.04017", "abs": "https://arxiv.org/abs/2510.04017", "authors": ["Sumanth Varambally", "Marshall Fisher", "Jas Thakker", "Yiwei Chen", "Zhirui Xia", "Yasaman Jafari", "Ruijia Niu", "Manas Jain", "Veeramakali Vignesh Manivannan", "Zachary Novack", "Luyu Han", "Srikar Eranky", "Salva R\u00fchling Cachay", "Taylor Berg-Kirkpatrick", "Duncan Watson-Parris", "Yi-An Ma", "Rose Yu"], "title": "Zephyrus: An Agentic Framework for Weather Science", "categories": ["cs.AI", "cs.LG", "physics.ao-ph"], "comment": null, "summary": "Foundation models for weather science are pre-trained on vast amounts of\nstructured numerical data and outperform traditional weather forecasting\nsystems. However, these models lack language-based reasoning capabilities,\nlimiting their utility in interactive scientific workflows. Large language\nmodels (LLMs) excel at understanding and generating text but cannot reason\nabout high-dimensional meteorological datasets. We bridge this gap by building\na novel agentic framework for weather science. Our framework includes a Python\ncode-based environment for agents (ZephyrusWorld) to interact with weather\ndata, featuring tools like an interface to WeatherBench 2 dataset, geoquerying\nfor geographical masks from natural language, weather forecasting, and climate\nsimulation capabilities. We design Zephyrus, a multi-turn LLM-based weather\nagent that iteratively analyzes weather datasets, observes results, and refines\nits approach through conversational feedback loops. We accompany the agent with\na new benchmark, ZephyrusBench, with a scalable data generation pipeline that\nconstructs diverse question-answer pairs across weather-related tasks, from\nbasic lookups to advanced forecasting, extreme event detection, and\ncounterfactual reasoning. Experiments on this benchmark demonstrate the strong\nperformance of Zephyrus agents over text-only baselines, outperforming them by\nup to 35 percentage points in correctness. However, on harder tasks, Zephyrus\nperforms similarly to text-only baselines, highlighting the challenging nature\nof our benchmark and suggesting promising directions for future work.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Large Language Models (LLMs) in weather science, specifically for tasks involving weather data analysis, forecasting, and reasoning. While it doesn't directly address trajectory prediction, the concept of forecasting is related, and the use of LLMs is a core element. The 'climate simulation capabilities' could potentially involve predicting future states, which has some overlap with trajectory prediction, although it's not the main focus. Therefore, the relevance score is moderate.", "keywords": ["Large language models (LLMs)", "foundation models", "weather forecasting", "climate simulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.04190", "pdf": "https://arxiv.org/pdf/2510.04190", "abs": "https://arxiv.org/abs/2510.04190", "authors": ["Jian-jie Zheng", "Chih-kai Yang", "Po-han Chen", "Lyn Chao-ling Chen"], "title": "Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification", "categories": ["cs.RO"], "comment": null, "summary": "In the study, the social robot act as a patrol to recognize and notify\nillegal parking in real-time. Dual-model pipeline method and large multimodal\nmodel were compared, and the GPT-4o multimodal model was adopted in license\nplate recognition without preprocessing. For moving smoothly on a flat ground,\nthe robot navigated in a simulated parking lot in the experiments. The robot\nchanges angle view of the camera automatically to capture the images around\nwith the format of license plate number. From the captured images of the robot,\nthe numbers on the plate are recognized through the GPT-4o model, and\nidentifies legality of the numbers. When an illegal parking is detected, the\nrobot sends Line messages to the system manager immediately. The contribution\nof the work is that a novel multimodal deep learning method has validated with\nhigh accuracy in license plate recognition, and a social assistive robot is\nalso provided for solving problems in a real scenario, and can be applied in an\nindoor parking lot.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a large multimodal model (GPT-4o) for license plate recognition, which is part of a robot patrol system. While the core task is not trajectory prediction, the robot navigates and moves, suggesting a potential, albeit weak, connection. The use of a large multimodal model makes it relevant to the large language model domain.", "keywords": ["large multimodal model", "GPT-4o", "robot navigation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.04246", "pdf": "https://arxiv.org/pdf/2510.04246", "abs": "https://arxiv.org/abs/2510.04246", "authors": ["Huiwon Jang", "Sihyun Yu", "Heeseung Kwon", "Hojin Jeon", "Younggyo Seo", "Jinwoo Shin"], "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context", "categories": ["cs.RO", "cs.AI"], "comment": "Project page: https://huiwon-jang.github.io/contextvla", "summary": "Leveraging temporal context is crucial for success in partially observable\nrobotic tasks. However, prior work in behavior cloning has demonstrated\ninconsistent performance gains when using multi-frame observations. In this\npaper, we introduce ContextVLA, a policy model that robustly improves robotic\ntask performance by effectively leveraging multi-frame observations. Our\napproach is motivated by the key observation that Vision-Language-Action models\n(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more\neffectively utilize multi-frame observations for action generation. This\nsuggests that VLMs' inherent temporal understanding capability enables them to\nextract more meaningful context from multi-frame observations. However, the\nhigh dimensionality of video inputs introduces significant computational\noverhead, making VLA training and inference inefficient. To address this,\nContextVLA compresses past observations into a single context token, allowing\nthe policy to efficiently leverage temporal context for action generation. Our\nexperiments show that ContextVLA consistently improves over single-frame VLAs\nand achieves the benefits of full multi-frame training but with reduced\ntraining and inference times.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on a Vision-Language-Action model (VLA) for robotic tasks, leveraging multi-frame observations. While it doesn't explicitly mention trajectory prediction, the action generation aspect and use of temporal context (multi-frame observations) are weakly related to trajectory prediction. The paper also explicitly mentions Vision-Language Models (VLMs), which are related to Large Language Models. Thus, there is some relevance to both trajectory prediction and large language models, but not a strong direct connection.", "keywords": ["Vision-Language Model", "VLA", "temporal context", "action generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.04195", "pdf": "https://arxiv.org/pdf/2510.04195", "abs": "https://arxiv.org/abs/2510.04195", "authors": ["Puzhen Zhang", "Xuyang Chen", "Yu Feng", "Yuhan Jiang", "Liqiu Meng"], "title": "Constructing coherent spatial memory in LLM agents through graph rectification", "categories": ["cs.AI"], "comment": null, "summary": "Given a map description through global traversal navigation instructions\n(e.g., visiting each room sequentially with action signals such as north, west,\netc.), an LLM can often infer the implicit spatial layout of the environment\nand answer user queries by providing a shortest path from a start to a\ndestination (for instance, navigating from the lobby to a meeting room via the\nhall and elevator). However, such context-dependent querying becomes incapable\nas the environment grows much longer, motivating the need for incremental map\nconstruction that builds a complete topological graph from stepwise\nobservations. We propose a framework for LLM-driven construction and map\nrepair, designed to detect, localize, and correct structural inconsistencies in\nincrementally constructed navigation graphs. Central to our method is the\nVersion Control, which records the full history of graph edits and their source\nobservations, enabling fine-grained rollback, conflict tracing, and repair\nevaluation. We further introduce an Edge Impact Score to prioritize\nminimal-cost repairs based on structural reachability, path usage, and conflict\npropagation. To properly evaluate our approach, we create a refined version of\nthe MANGO benchmark dataset by systematically removing non-topological actions\nand inherent structural conflicts, providing a cleaner testbed for LLM-driven\nconstruction and map repair. Our approach significantly improves map\ncorrectness and robustness, especially in scenarios with entangled or chained\ninconsistencies. Our results highlight the importance of introspective,\nhistory-aware repair mechanisms for maintaining coherent spatial memory in LLM\nagents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using LLMs to construct and repair spatial maps for navigation. While not directly addressing trajectory prediction, it involves the creation of spatial representations and path planning, which are related. The core focus is on LLMs and their ability to build and maintain spatial memory, making it relevant to the 'Large Language Models' aspect. The connection to trajectory prediction is more indirect, through the navigation aspect.", "keywords": ["LLM", "Large Language Models", "navigation", "path planning", "spatial memory", "graph"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.04206", "pdf": "https://arxiv.org/pdf/2510.04206", "abs": "https://arxiv.org/abs/2510.04206", "authors": ["Hanchen Zhang", "Xiao Liu", "Bowen Lv", "Xueqiao Sun", "Bohao Jing", "Iat Long Iong", "Zhenyu Hou", "Zehan Qi", "Hanyu Lai", "Yifan Xu", "Rui Lu", "Hongning Wang", "Jie Tang", "Yuxiao Dong"], "title": "AgentRL: Scaling Agentic Reinforcement Learning with a Multi-Turn, Multi-Task Framework", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin building generalist agents that can learn through online interactions.\nHowever, applying reinforcement learning (RL) to train LLM agents in\nmulti-turn, multi-task settings remains challenging due to lack of scalable\ninfrastructure and stable training algorithms. In this work, we present the\nAgentRL framework for scalable multi-turn, multi-task agentic RL training. On\nthe infrastructure side, AgentRL features a fully-asynchronous\ngeneration-training pipeline for efficient multi-turn RL. To support\nheterogeneous environment development in multi-task RL, we design a unified\nfunction-call based API interface, containerized environment development, and a\ncentralized controller. On the algorithm side, we propose cross-policy sampling\nto encourage model exploration in multi-turn settings and task advantage\nnormalization to stabilize multi-task training. Experiments show that AgentRL,\ntrained on open LLMs across five agentic tasks, significantly outperforms\nGPT-5, Clause-Sonnet-4, DeepSeek-R1, and other open-source LLM agents.\nMulti-task training with AgentRL matches the best results among all\ntask-specific models. AgentRL is open-sourced at\nhttps://github.com/THUDM/AgentRL. The algorithm and framework are adopted in\nbuilding \\textsc{\\href{https://autoglm.zhipuai.cn}{AutoGLM}}.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Reinforcement Learning to train Large Language Models for multi-turn, multi-task agentic tasks. While not directly about trajectory prediction, it's highly relevant because it explores the application of LLMs in an agentic setting, which could be potentially applied to trajectory prediction tasks in the future. The paper discusses scaling and training LLM agents which are core concepts in the field of LLMs.", "keywords": ["Large Language Models", "LLMs", "Reinforcement Learning", "agentic tasks", "multi-turn", "multi-task"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
