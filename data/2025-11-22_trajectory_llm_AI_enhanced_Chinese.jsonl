{"id": "2511.15992", "pdf": "https://arxiv.org/pdf/2511.15992", "abs": "https://arxiv.org/abs/2511.15992", "authors": ["Shahin Zanbaghi", "Ryan Rostampour", "Farhan Abid", "Salim Al Jarmakani"], "title": "Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis", "categories": ["cs.AI"], "comment": "7 pages, 3 figures, 1 table", "summary": "Large Language Models (LLMs) can be backdoored to exhibit malicious behavior under specific deployment conditions while appearing safe during training a phenomenon known as \"sleeper agents.\" Recent work by Hubinger et al. demonstrated that these backdoors persist through safety training, yet no practical detection methods exist. We present a novel dual-method detection system combining semantic drift analysis with canary baseline comparison to identify backdoored LLMs in real-time. Our approach uses Sentence-BERT embeddings to measure semantic deviation from safe baselines, complemented by injected canary questions that monitor response consistency. Evaluated on the official Cadenza-Labs dolphin-llama3-8B sleeper agent model, our system achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall. The combined detection method operates in real-time (<1s per query), requires no model modification, and provides the first practical solution to LLM backdoor detection. Our work addresses a critical security gap in AI deployment and demonstrates that embedding-based detection can effectively identify deceptive model behavior without sacrificing deployment efficiency.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on detecting backdoors in Large Language Models (LLMs). While it doesn't directly deal with trajectory prediction, it is highly relevant to the area of Large Language Models, specifically concerning their safety and security.", "keywords": ["Large Language Models", "LLMs", "backdoor detection", "semantic drift analysis", "embedding-based detection"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16048", "pdf": "https://arxiv.org/pdf/2511.16048", "abs": "https://arxiv.org/abs/2511.16048", "authors": ["Qing Zhang", "Jing Huang", "Mingyang Xu", "Jun Rekimoto"], "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems", "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper describes a robotic art installation that uses a Multimodal Large Language Model for navigation, which is related to trajectory prediction. The abstract mentions autonomous navigation and planning, although it emphasizes a 'lo-fi' approach and qualitative understanding rather than precise metric prediction. The use of LLMs for navigation contributes to the relevance.", "keywords": ["Large Language Model", "autonomous navigation", "Multimodal Large Language Model", "autonomous pipeline", "plan to execution"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16518", "pdf": "https://arxiv.org/pdf/2511.16518", "abs": "https://arxiv.org/abs/2511.16518", "authors": ["Xiaoshuai Hao", "Lei Zhou", "Zhijian Huang", "Zhiwen Hou", "Yingbo Tang", "Lingfeng Zhang", "Guang Li", "Zheng Lu", "Shuhuai Ren", "Xianhui Meng", "Yuchen Zhang", "Jing Wu", "Jinghui Lu", "Chenxu Dang", "Jiayi Guan", "Jianhua Wu", "Zhiyi Hou", "Hanbing Li", "Shumeng Xia", "Mingliang Zhou", "Yinan Zheng", "Zihao Yue", "Shuhao Gu", "Hao Tian", "Yuannan Shen", "Jianwei Cui", "Wen Zhang", "Shaoqing Xu", "Bing Wang", "Haiyang Sun", "Zeyu Zhu", "Yuncheng Jiang", "Zibin Guo", "Chuhong Gong", "Chaofan Zhang", "Wenbo Ding", "Kun Ma", "Guang Chen", "Rui Cai", "Diyun Xiang", "Heng Qu", "Fuli Luo", "Hangjun Ye", "Long Chen"], "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "Code: https://github.com/XiaomiMiMo/MiMo-Embodied Model: https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B", "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86MiMo-Embodied\uff0c\u4e00\u4e2a\u8de8\u5177\u8eab\u57fa\u7840\u6a21\u578b\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eabAI\u4e2d\u90fd\u53d6\u5f97\u4e86state-of-the-art\u7684\u6027\u80fd\u3002\u867d\u7136\u8bba\u6587\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u6db5\u76d6\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u73af\u5883\u611f\u77e5\u3001\u72b6\u6001\u9884\u6d4b\u548c\u9a7e\u9a76\u89c4\u5212\uff0c\u8fd9\u4e9b\u90fd\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u6b64\u5916\uff0c\u5b83\u4e5f\u5c5e\u4e8e\u5927\u6a21\u578b\uff08\u57fa\u7840\u6a21\u578b\uff09\u7684\u8303\u7574\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["foundation model", "autonomous driving", "embodied AI", "driving planning", "status prediction"]}, "AI": {"tldr": "MiMo-Embodied \u662f\u9996\u4e2a\u8de8\u5177\u8eab\u57fa\u7840\u6a21\u578b\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eab\u667a\u80fd\u9886\u57df\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eab\u667a\u80fd\u9886\u57df\u7f3a\u4e4f\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u591a\u9636\u6bb5\u5b66\u4e60\u3001\u7cbe\u9009\u6570\u636e\u6784\u5efa\u548c CoT/RL \u5fae\u8c03\uff0c\u6574\u5408\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eab\u667a\u80fd\u3002", "result": "MiMo-Embodied \u5728 17 \u4e2a\u5177\u8eab AI \u57fa\u51c6\u6d4b\u8bd5\u548c 12 \u4e2a\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eab\u667a\u80fd\u8fd9\u4e24\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6b63\u5411\u8fc1\u79fb\u548c\u76f8\u4e92\u4fc3\u8fdb\u4f5c\u7528\u3002", "summary_zh": "\u6211\u4eec\u5f00\u6e90\u4e86 MiMo-Embodied\uff0c\u8fd9\u662f\u9996\u4e2a\u8de8\u5177\u8eab\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u6210\u529f\u5730\u6574\u5408\u4e86\u81ea\u52a8\u9a7e\u9a76\u548c\u5177\u8eab\u667a\u80fd\uff0c\u5e76\u5728\u4e24\u4e2a\u9886\u57df\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002MiMo-Embodied \u5728\u4efb\u52a1\u89c4\u5212\u3001\u53ef\u4f9b\u6027\u9884\u6d4b\u548c\u7a7a\u95f4\u7406\u89e3\u7b49 17 \u4e2a\u5177\u8eab AI \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee5\u53ca\u5728\u73af\u5883\u611f\u77e5\u3001\u72b6\u6001\u9884\u6d4b\u548c\u9a7e\u9a76\u89c4\u5212\u7b49 12 \u4e2a\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u90fd\u521b\u9020\u4e86\u65b0\u7684\u8bb0\u5f55\u3002\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e2d\uff0cMiMo-Embodied \u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u3001\u95ed\u6e90\u548c\u4e13\u7528\u57fa\u7ebf\u6a21\u578b\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u5b66\u4e60\u3001\u7cbe\u9009\u6570\u636e\u6784\u5efa\u548c CoT/RL \u5fae\u8c03\uff0c\u8fd9\u4e24\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6b63\u5411\u8fc1\u79fb\u548c\u76f8\u4e92\u4fc3\u8fdb\u4f5c\u7528\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u6a21\u578b\u8bbe\u8ba1\u548c\u8bad\u7ec3\u65b9\u6cd5\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002\u4ee3\u7801\u548c\u6a21\u578b\u53ef\u5728 https://github.com/XiaomiMiMo/MiMo-Embodied \u83b7\u53d6\u3002"}}
{"id": "2511.16105", "pdf": "https://arxiv.org/pdf/2511.16105", "abs": "https://arxiv.org/abs/2511.16105", "authors": ["Yuanbo Tang", "Yan Tang", "Zixuan Zhang", "Zihui Zhao", "Yang Li"], "title": "Pathlet Variational Auto-Encoder for Robust Trajectory Generation", "categories": ["cs.LG"], "comment": null, "summary": "Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints.\n  Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\\%$ and $26.3\\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\\%$ of the time and $56.5\\%$ of GPU memory compared to previous approaches.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on trajectory generation using a deep generative model (VAE) but doesn't explicitly mention or utilize Large Language Models. While it touches on trajectory prediction as a downstream task and uses deep learning, the core focus is on robust trajectory generation and the use of a Variational Autoencoder (VAE) for this purpose. The connection to LLMs is weak, but the trajectory generation aspect and its link to trajectory prediction warrant a moderate relevance score.", "keywords": ["trajectory generation", "trajectory prediction", "Variational Autoencoder", "deep generative model", "mobility patterns"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.15914", "pdf": "https://arxiv.org/pdf/2511.15914", "abs": "https://arxiv.org/abs/2511.15914", "authors": ["Debasmita Ghose", "Oz Gitelson", "Ryan Jin", "Grace Abawe", "Marynel Vazquez", "Brian Scassellati"], "title": "I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration", "categories": ["cs.RO"], "comment": "Accepted to RA-L", "summary": "For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4eba\u673a\u534f\u4f5c\u4e2d\u673a\u5668\u4eba\u5982\u4f55\u9002\u5e94\u4eba\u7c7b\u76ee\u6807\u7684\u53d8\u5316\uff0c\u6d89\u53ca\u52a8\u4f5c\u9884\u6d4b\u548c\u89c4\u5212\uff0c\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u4e2a\u5e94\u7528\u573a\u666f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u4e2d\u6d89\u53ca\u7684\u7b56\u7565\u9009\u62e9\u548c\u76ee\u6807\u8bc6\u522b\u7b49\u4efb\u52a1\uff0c\u672a\u6765\u53ef\u4ee5\u8003\u8651\u4f7f\u7528\u5927\u6a21\u578b\u6765\u589e\u5f3a\u5176\u6027\u80fd\u3002", "keywords": ["\u52a8\u4f5c\u9884\u6d4b", "\u4eba\u673a\u534f\u4f5c", "\u76ee\u6807\u9884\u6d4b", "Receding Horizon Planning", "\u7b56\u7565\u9009\u62e9"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.15995", "pdf": "https://arxiv.org/pdf/2511.15995", "abs": "https://arxiv.org/abs/2511.15995", "authors": ["Zili Tang", "Ying Zhang", "Meng Guo"], "title": "PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization", "categories": ["cs.RO"], "comment": "20 pages, 24 figures. Accepted to IEEE Transactions on Robotics (T-RO), 2025", "summary": "Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u534f\u540c\u63a8\u7269\uff0c\u6d89\u53ca\u8def\u5f84\u89c4\u5212\u3001\u52a8\u4f5c\u6267\u884c\u548c\u73af\u5883\u4ea4\u4e92\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u63a7\u5236\u673a\u5668\u4eba\u5c06\u7269\u4f53\u63a8\u5230\u7279\u5b9a\u76ee\u6807\u4f4d\u7f6e\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u79cd\u8f68\u8ff9\u751f\u6210\u548c\u63a7\u5236\u95ee\u9898\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\"diffusion-based accelerator\"\uff0c\u8fd9\u53ef\u80fd\u4e0e\u751f\u6210\u6a21\u578b\u6709\u5173\uff0c\u4f46\u5e76\u975e\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u5c5e\u4e8e\u4e2d\u7b49\u3002", "keywords": ["pushing", "multi-robot systems", "path planning", "hybrid control", "diffusion-based accelerator"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16200", "pdf": "https://arxiv.org/pdf/2511.16200", "abs": "https://arxiv.org/abs/2511.16200", "authors": ["Kewei Chen", "Yayu Long", "Mingsheng Shang"], "title": "PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks", "categories": ["cs.RO"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Multi-robot systems in complex physical collaborations face a \"shared brain dilemma\": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace \"raw data communication\" with \"semantic communication\" by performing \"semantic distillation\" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the \"shared brain dilemma\" in resource-constrained multi-robot systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on physical interaction prediction in multi-robot systems, which can be considered related to trajectory prediction. The abstract mentions using a Physical Interaction Prediction Network (PIPN) derived from large model knowledge distillation, indicating a connection to large language models, although the connection isn't direct usage of LLMs for trajectory prediction. The primary focus is on efficient communication and control in multi-robot systems, with prediction as a component.", "keywords": ["physical interaction prediction", "multi-robot systems", "knowledge distillation", "large model"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16223", "pdf": "https://arxiv.org/pdf/2511.16223", "abs": "https://arxiv.org/abs/2511.16223", "authors": ["Vincenzo Pomponi", "Paolo Franceschi", "Stefano Baraldo", "Loris Roveda", "Oliver Avram", "Luca Maria Gambardella", "Anna Valente"], "title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on generating data for robot learning of dynamic tasks, using Dynamic Movement Primitives (DMPs) to create trajectories. While it involves trajectory generation and adaptation, it does not directly use or focus on Large Language Models. The connection to trajectory prediction is present but not the central theme, and the link to large language models is absent. The DMPs can be seen as a form of trajectory prediction in a dynamic environment, justifying a moderate relevance score.", "keywords": ["trajectory", "Dynamic Movement Primitives", "dynamic tasks", "robot learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16233", "pdf": "https://arxiv.org/pdf/2511.16233", "abs": "https://arxiv.org/abs/2511.16233", "authors": ["Kewei Chen", "Yayu Long", "Shuai Li", "Mingsheng Shang"], "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models", "categories": ["cs.RO"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on improving the efficiency of Vision-Language-Action (VLA) models through data distillation. While it doesn't explicitly mention trajectory prediction, the 'Action' component of VLA models can relate to predicting future actions in a scene, which is related to trajectory prediction. The paper also deals with a large model (VLA model), making it relevant to the large model aspect. However, the connection to trajectory prediction is somewhat indirect.", "keywords": ["Vision-Language-Action models", "VLA models", "data distillation", "large models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16372", "pdf": "https://arxiv.org/pdf/2511.16372", "abs": "https://arxiv.org/abs/2511.16372", "authors": ["Bowen Xu", "Zexuan Yan", "Minghao Lu", "Xiyu Fan", "Yi Luo", "Youshen Lin", "Zhiqiang Chen", "Yeke Chen", "Qiyuan Qiao", "Peng Lu"], "title": "Flow-Aided Flight Through Dynamic Clutters From Point To Motion", "categories": ["cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L), November, 2025", "summary": "Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u65e0\u4eba\u673a\u7684\u52a8\u6001\u73af\u5883\u4e0b\u7684\u98de\u884c\uff0c\u91cd\u70b9\u5728\u4e8e\u901a\u8fc7\u70b9\u4e91\u6570\u636e\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u907f\u969c\u548c\u8def\u5f84\u89c4\u5212\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u662f\u5176\u76ee\u6807\u4e0e\u8f68\u8ff9\u9884\u6d4b\uff08\u7279\u522b\u662f\u79fb\u52a8\u7269\u4f53\u7684\u8f68\u8ff9\uff09\u76f8\u5173\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7b56\u7565\u4f18\u5316\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u5e94\u7528\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u5f3a\u5316\u5b66\u4e60", "\u8def\u5f84\u89c4\u5212", "\u52a8\u6001\u73af\u5883", "\u907f\u969c"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16407", "pdf": "https://arxiv.org/pdf/2511.16407", "abs": "https://arxiv.org/abs/2511.16407", "authors": ["Xizhou Bu", "Jiexi Lyu", "Fulei Sun", "Ruichen Yang", "Zhiqiang Ma", "Wei Li"], "title": "LAOF: Robust Latent Action Learning with Optical Flow Constraints", "categories": ["cs.RO"], "comment": "Code can be found at https://github.com/XizoB/LAOF", "summary": "Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u5b66\u4e60latent actions\uff0c\u5e76\u5229\u7528optical flow\u8fdb\u884c\u7ea6\u675f\uff0c\u8fd9\u4e0e\u52a8\u4f5c\u9884\u6d4b\u548c\u79fb\u52a8\u7269\u4f53\u7684\u884c\u4e3a\u5b66\u4e60\u76f8\u5173\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u4e2a\u76f8\u5173\u9886\u57df\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cembodied foundation models\u201d\uff0c\u8868\u660e\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002\u4f46\u662f\uff0c\u8bba\u6587\u5e76\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u800c\u662f\u4fa7\u91cd\u4e8e\u5b66\u4e60\u66f4\u597d\u7684\u52a8\u4f5c\u8868\u5f81\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u8bc4\u5206\u4e3a0.6\u3002", "keywords": ["latent action learning", "optical flow", "foundation models", "action prediction", "imitation learning", "reinforcement learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16049", "pdf": "https://arxiv.org/pdf/2511.16049", "abs": "https://arxiv.org/abs/2511.16049", "authors": ["Pei Liu", "Songtao Wang", "Lang Zhang", "Xingyue Peng", "Yuandong Lyu", "Jiaxin Deng", "Songxin Lu", "Weiliang Ma", "Xueyang Zhang", "Yifei Zhan", "XianPeng Lang", "Jun Ma"], "title": "LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u751f\u6210\u6a21\u578b\u5408\u62104D LiDAR\u6570\u636e\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u4eff\u771f\u73af\u5883\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u5230\u4e86\u751f\u6210\u6a21\u578b\u548c\u9884\u6d4b\uff08prediction\uff09\uff0c\u5e76\u4e14\u4e0e\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u5176\u6838\u5fc3\u5728\u4e8e\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u9884\u6d4b\u548c\u751f\u6210\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002", "keywords": ["autonomous driving", "prediction", "generative model", "4D LiDAR", "world model"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16651", "pdf": "https://arxiv.org/pdf/2511.16651", "abs": "https://arxiv.org/abs/2511.16651", "authors": ["Yang Tian", "Yuyin Yang", "Yiman Xie", "Zetao Cai", "Xu Shi", "Ning Gao", "Hangxu Liu", "Xuekun Jiang", "Zherui Qiu", "Feng Yuan", "Yaping Li", "Ping Wang", "Junhao Cai", "Jia Zeng", "Hao Dong", "Jiangmiao Pang"], "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy", "categories": ["cs.RO"], "comment": null, "summary": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $\u03c0$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $\u03c0_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $\u03c0_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating synthetic data for pre-training Vision-Language-Action (VLA) models. While it doesn't explicitly mention trajectory prediction, the use of trajectories in the synthetic dataset (630k trajectories) and its application to robotic manipulation tasks suggests a connection to trajectory generation and prediction. The reference to large-scale pre-training and VLA models connects it to the realm of large models, although not directly large language models. The embodiment AI aspect also has some relevance to trajectory prediction in physical systems.", "keywords": ["trajectories", "VLA models", "pre-training", "embodied AI", "robotic manipulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16043", "pdf": "https://arxiv.org/pdf/2511.16043", "abs": "https://arxiv.org/abs/2511.16043", "authors": ["Peng Xia", "Kaide Zeng", "Jiaqi Liu", "Can Qin", "Fang Wu", "Yiyang Zhou", "Caiming Xiong", "Huaxiu Yao"], "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on training LLM agents through self-evolution and tool integration. While it doesn't directly address trajectory prediction, it explores methods to improve LLM reasoning and problem-solving, which could potentially be applied to trajectory prediction tasks in the future. The core focus is on LLMs and agent development, hence the moderate relevance score.", "keywords": ["Large Language Model", "LLM Agents", "reasoning", "self-evolution", "tool integration"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16183", "pdf": "https://arxiv.org/pdf/2511.16183", "abs": "https://arxiv.org/abs/2511.16183", "authors": ["Jeremie Ochin", "Raphael Chekroun", "Bogdan Stanciulescu", "Sotiris Manitsaris"], "title": "FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on soccer video understanding and play-by-play action spotting, which involves multi-agent tracking and tactical modeling. The abstract mentions trajectory forecasting as a related area. While it doesn't directly use or discuss large language models, the mention of tactical modeling and long time horizons suggests potential for future integration with LLMs for strategic analysis. Therefore, there is some relevance to trajectory prediction, but no direct relevance to large language models.", "keywords": ["trajectory forecasting", "multi-agent tracking", "tactical modeling", "action spotting"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16150", "pdf": "https://arxiv.org/pdf/2511.16150", "abs": "https://arxiv.org/abs/2511.16150", "authors": ["Chunxu Liu", "Jiyuan Yang", "Ruopeng Gao", "Yuhan Zhu", "Feng Zhu", "Rui Zhao", "Limin Wang"], "title": "Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u68c0\u7d22\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u63a8\u7406\u80fd\u529b\u6765\u6539\u8fdb\u5d4c\u5165\u8868\u793a\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e14\u5d4c\u5165\u8868\u793a\u7684\u6982\u5ff5\u53ef\u4ee5\u5e94\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Multimodal Large Language Models", "MLLMs", "embedding", "reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16160", "pdf": "https://arxiv.org/pdf/2511.16160", "abs": "https://arxiv.org/abs/2511.16160", "authors": ["Yibin Huang", "Wang Xu", "Wanyue Zhang", "Helu Zhi", "Jingjing Huang", "Yangbin Xu", "Yangang Sun", "Conghui Zhu", "Tiejun Zhao"], "title": "Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM) \u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u89c6\u9891\u91cd\u5efa\u5ea6\u91cf\u7a7a\u95f4\u5e03\u5c40\u7684\u6846\u67b6\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u6784\u5efa\u4e86\u7528\u4e8e\u7a7a\u95f4\u7406\u89e3\u7684\u8ba4\u77e5\u5730\u56fe\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u73af\u5883\u7406\u89e3\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8bba\u6587\u660e\u786e\u63d0\u5230\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLM)\u3002", "keywords": ["Multimodal Large Language Models", "MLLMs", "spatial reasoning", "cognitive map", "foundation models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16166", "pdf": "https://arxiv.org/pdf/2511.16166", "abs": "https://arxiv.org/abs/2511.16166", "authors": ["Zeting Liu", "Zida Yang", "Zeyu Zhang", "Hao Tang"], "title": "EvoVLA: Self-Evolving Vision-Language-Action Model", "categories": ["cs.CV"], "comment": null, "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses a Vision-Language-Action (VLA) model for robotic manipulation. While it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting pedestrian or vehicle trajectories), it does involve predicting and controlling robot actions over a long horizon, which can be seen as a form of trajectory planning/prediction in the action space. It utilizes a large model (Gemini) for generating hard negatives, indicating a connection to large language models. Therefore, there is some relevance to both trajectory prediction and large models, although not a direct and strong connection.", "keywords": ["Vision-Language-Action Model", "VLA", "robotic manipulation", "long-horizon", "Gemini", "action prediction", "trajectory planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16175", "pdf": "https://arxiv.org/pdf/2511.16175", "abs": "https://arxiv.org/abs/2511.16175", "authors": ["Yi Yang", "Xueqi Li", "Yiyang Chen", "Jin Song", "Yihan Wang", "Zipeng Xiao", "Jiadi Su", "You Qiaoben", "Pengfei Liu", "Zhijie Deng"], "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $\u03c0_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8Vision-Language-Action\u6a21\u578b\uff0c\u7279\u522b\u662f\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u9884\u6d4b\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002\u867d\u7136\u6d89\u53ca\u89c6\u89c9\u72b6\u6001\u7684\u9884\u6d4b\uff0c\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5e7f\u4e49\u4e0a\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7684\u7ed3\u5408\uff0c\u4ee5\u53ca\u4f7f\u7528Diffusion Transformer (DiT)\u3002\u4e0e\u7eaf\u7cb9\u7684\u8f68\u8ff9\u9884\u6d4b\u76f8\u6bd4\uff0c\u66f4\u4fa7\u91cd\u4e8e\u52a8\u4f5c\u9884\u6d4b\u548c\u89c6\u89c9\u7406\u89e3\u3002\u540c\u65f6\uff0c\u63d0\u5230\u4e86\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u8bc4\u5206\u4e3a0.6\u3002", "keywords": ["Vision-Language-Action model", "visual foresight", "diffusion Transformer", "action prediction", "large-scale pretraining"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16258", "pdf": "https://arxiv.org/pdf/2511.16258", "abs": "https://arxiv.org/abs/2511.16258", "authors": ["Yang Xu", "Zuliang Yang", "Kai Ming Ting"], "title": "GeoPTH: A Lightweight Approach to Category-Based Trajectory Retrieval via Geometric Prototype Trajectory Hashing", "categories": ["cs.LG"], "comment": null, "summary": "Trajectory similarity retrieval is an important part of spatiotemporal data mining, however, existing methods have the following limitations: traditional metrics are computationally expensive, while learning-based methods suffer from substantial training costs and potential instability. This paper addresses these problems by proposing \\textbf{Geo}metric \\textbf{P}rototype \\textbf{T}rajectory \\textbf{H}ashing (GeoPTH), a novel, lightweight, and non-learning framework for efficient category-based trajectory retrieval. GeoPTH constructs data-dependent hash functions by using representative trajectory prototypes, i.e., small point sets preserving geometric characteristics, as anchors. The hashing process is efficient, which involves mapping a new trajectory to its closest prototype via a robust, \\textit{Hausdorff} metric. Extensive experiments show that GeoPTH's retrieval accuracy is highly competitive with both traditional metrics and state-of-the-art learning methods, and it significantly outperforms binary codes generated through simple binarization of the learned embeddings. Critically, GeoPTH consistently outperforms all competitors in terms of efficiency. Our work demonstrates that a lightweight, prototype-centric approach offers a practical and powerful alternative, achieving an exceptional retrieval performance and computational efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory retrieval using geometric hashing. While it deals with trajectories, it doesn't involve trajectory prediction or large language models directly. However, trajectory retrieval is a related area, hence the moderate relevance score.", "keywords": ["trajectory retrieval", "trajectory hashing", "geometric prototype trajectory", "spatiotemporal data mining"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16227", "pdf": "https://arxiv.org/pdf/2511.16227", "abs": "https://arxiv.org/abs/2511.16227", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "SwiTrack: Tri-State Switch for Cross-Modal Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\\% and 4.3\\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on cross-modal object tracking, which involves predicting the target's movement (trajectory prediction) when the input modality changes. While it doesn't directly involve large language models, the 'consistency trajectory prediction module' indicates a connection to trajectory prediction. The primary focus is on RGB-NIR tracking and modality switching, making the connection to trajectory prediction somewhat indirect. It does not mention anything related to large language models.", "keywords": ["trajectory prediction", "object tracking", "cross-modal object tracking", "consistency trajectory prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16264", "pdf": "https://arxiv.org/pdf/2511.16264", "abs": "https://arxiv.org/abs/2511.16264", "authors": ["Sinan Mutlu", "Georgios F. Angelis", "Savas Ozkan", "Paul Wisbey", "Anastasios Drosou", "Mete Ozay"], "title": "Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs", "categories": ["cs.CV"], "comment": null, "summary": "Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce83D\u4eba\u4f53\u8fd0\u52a8\u751f\u6210\uff0c\u8fd9\u662f\u4e00\u4e2a\u52a8\u4f5c\u9884\u6d4b/\u8f68\u8ff9\u9884\u6d4b\u7684\u53d8\u4f53\u3002\u867d\u7136\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86\u795e\u7ecf\u7f51\u7edc\uff08MLP\uff09\uff0c\u4f46\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u57fa\u7840\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["3D Human Motion Generation", "Motion Prediction", "Neural Network", "MLP"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16333", "pdf": "https://arxiv.org/pdf/2511.16333", "abs": "https://arxiv.org/abs/2511.16333", "authors": ["Mohammad Areeb Qazi", "Maryam Nadeem", "Mohammad Yaqub"], "title": "Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning", "categories": ["cs.LG"], "comment": "2 Figures, 1 Table", "summary": "Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86World Models\u5728\u533b\u7597\u5065\u5eb7\u9886\u57df\u7684\u5e94\u7528\uff0c\u5305\u62ec\u9884\u6d4b\u3001\u53cd\u4e8b\u5b9e\u5206\u6790\u548c\u89c4\u5212\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u4f20\u7edf\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46World Models\u7684\u5b66\u4e60\u9884\u6d4b\u52a8\u6001\uff0c\u8fdb\u884c\u591a\u6b65\u5c55\u5f00\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u6982\u5ff5\u6709\u76f8\u4f3c\u4e4b\u5904\u3002\u8bba\u6587\u63d0\u5230\u4e86transformers\u7b49\u751f\u6210\u6a21\u578b\uff0c\u4e0e\u5927\u6a21\u578b\u6709\u5173\u3002\u603b\u4f53\u6765\u8bf4\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5e76\u975e\u76f4\u63a5\u805a\u7126\u4e8e\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u6a21\u578b\u3002", "keywords": ["world models", "prediction", "planning", "transformers", "generative models", "temporal reasoning", "action-conditioned"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16426", "pdf": "https://arxiv.org/pdf/2511.16426", "abs": "https://arxiv.org/abs/2511.16426", "authors": ["Seyed Mohamad Moghadas", "Bruno Cornelis", "Adrian Munteanu"], "title": "FreqFlow: Long-term forecasting using lightweight flow matching", "categories": ["cs.LG"], "comment": "Accepted at EurIPS, 2025", "summary": "Multivariate time-series (MTS) forecasting is fundamental to applications ranging from urban mobility and resource management to climate modeling. While recent generative models based on denoising diffusion have advanced state-of-the-art performance in capturing complex data distributions, they suffer from significant computational overhead due to iterative stochastic sampling procedures that limit real-time deployment. Moreover, these models can be brittle when handling high-dimensional, non-stationary, and multi-scale periodic patterns characteristic of real-world sensor networks. We introduce FreqFlow, a novel framework that leverages conditional flow matching in the frequency domain for deterministic MTS forecasting. Unlike conventional approaches that operate in the time domain, FreqFlow transforms the forecasting problem into the spectral domain, where it learns to model amplitude and phase shifts through a single complex-valued linear layer. This frequency-domain formulation enables the model to efficiently capture temporal dynamics via complex multiplication, corresponding to scaling and temporal translations. The resulting architecture is exceptionally lightweight with only 89k parameters - an order of magnitude smaller than competing diffusion-based models-while enabling single-pass deterministic sampling through ordinary differential equation (ODE) integration. Our approach decomposes MTS signals into trend, seasonal, and residual components, with the flow matching mechanism specifically designed for residual learning to enhance long-term forecasting accuracy. Extensive experiments on real-world traffic speed, volume, and flow datasets demonstrate that FreqFlow achieves state-of-the-art forecasting performance, on average 7\\% RMSE improvements, while being significantly faster and more parameter-efficient than existing methods", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7279\u522b\u662f\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86flow matching\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4e14flow matching\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u751f\u6210\u6a21\u578b\u3002", "keywords": ["time-series forecasting", "traffic speed", "traffic volume", "flow", "flow matching"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
