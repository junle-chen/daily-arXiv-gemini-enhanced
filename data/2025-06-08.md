# 每日 ArXiv 摘要速递: 2025-06-08

### [Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training](https://arxiv.org/abs/2506.04263)

**一句话总结:** 提出了一种动态Epsilon调度（DES）框架，用于自适应调整对抗扰动预算，以提高模型的鲁棒性和泛化能力。

**Authors:** Alan Mitkiy, James Smith, Hana Satou, Hiroshi Tanaka, Emily Johnson, F Monkey
**Categories:** `cs.CV`, `cs.LG`

[**[PDF]**](https://arxiv.org/pdf/2506.04263)

#### 中文摘要 (Abstract in Chinese)

> 对抗训练是防御深度神经网络免受对抗样本攻击的最有效策略之一。 现有对抗训练方法的一个主要限制在于它们依赖于固定的扰动预算，这无法解释特定于实例的鲁棒性特征。 虽然诸如IAAT和MMA之类的先前工作引入了实例级别的自适应，但它们通常依赖于数据鲁棒性的启发式或静态近似。 在本文中，我们提出了一种动态Epsilon调度（DES）框架，该框架可以自适应地调整每个实例和每个训练迭代的对抗扰动预算。 DES集成了三个关键因素：（1）通过基于梯度的代理来近似到决策边界的距离，（2）从softmax熵得出的预测置信度，以及（3）通过蒙特卡洛dropout估计的模型不确定性。 通过将这些线索整合到统一的调度策略中，DES可以动态地调整扰动预算，以指导更有效的对抗学习。 在CIFAR-10和CIFAR-100上的实验结果表明，与固定epsilon基线和先前的自适应方法相比，我们的方法持续提高了对抗鲁棒性和标准准确性。 此外，我们还提供了有关调度策略的稳定性和收敛性的理论见解。 这项工作为实例感知，数据驱动的对抗训练方法开辟了一条新途径。

---

### [RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought](https://arxiv.org/abs/2506.04277)

**一句话总结:** RSVP 框架通过结合多模态推理和视觉分割，提升了多模态大语言模型在视觉理解任务中的性能。

**Authors:** Yi Lu, Jiawang Cao, Yongliang Wu, Bozheng Li, Licheng Tang, Yangguang Ji, Chong Wu, Jay Wu, Wenbo Zhu
**Categories:** `cs.CV`, `cs.AI`

[**[PDF]**](https://arxiv.org/pdf/2506.04277)

#### 中文摘要 (Abstract in Chinese)

> 多模态大型语言模型（MLLM）在展示出卓越的推理能力的同时，缺乏显式的视觉 grounding 和分割机制，从而在认知推理和视觉感知之间造成了差距。为了弥合这一差距，我们提出了一种通过视觉提示进行推理分割的新框架（RSVP），该框架将多步骤多模态推理与有依据的视觉理解相结合。RSVP 是一个两阶段结构化框架，它集成了推理驱动的定位和分割细化。在推理阶段，RSVP 采用多模态链式思考视觉提示来帮助 MLLM 理解查询并推断目标，从而生成可解释的区域提议，从而增强视觉 grounding。在分割阶段，RSVP 使用视觉-语言分割模块（VLSM）细化这些提议，无缝集成文本和视觉线索以生成精确的分割掩码。通过显式建模多模态推理和分割之间的交互，RSVP 为可解释的推理分割引入了一种新范例。它利用 MLLM 固有的定位能力，使模型不仅能够推理对象，还能够生成结构化的视觉表示。我们广泛的实验表明，RSVP 实现了最先进的性能，在 ReasonSeg 上超越了最先进的方法高达 +6.5 gIoU 和 +9.2 cIoU，并在零样本设置下在 SegInW 上实现了 49.7 mAP。这些结果验证了 RSVP 作为一个有效的和可扩展的框架，用于集成认知推理与结构化的视觉理解。

---

### [Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark](https://arxiv.org/abs/2506.04280)

**一句话总结:** MMRB基准测试揭示了开源多模态大语言模型在多图推理能力上与商业模型的差距，并指出现有奖励模型在多图奖励排序方面的不足。

**Authors:** Ziming Cheng, Binrui Xu, Lisheng Gong, Zuhe Song, Tianshuo Zhou, Shiqi Zhong, Siyu Ren, Mingxiang Chen, Xiangchao Meng, Yuxin Zhang, Yanlin Li, Lei Ren, Wei Chen, Zhiyuan Huang, Mingjie Zhan, Xiaojie Wang, Fangxiang Feng
**Categories:** `cs.CV`, `cs.AI`, `68T50`, `I.2.7`

[**[PDF]**](https://arxiv.org/pdf/2506.04280)

#### 中文摘要 (Abstract in Chinese)

> 随着多模态大语言模型（MLLM）能力的增强和应用的普及，越来越需要它们能够同时处理和推理多个图像。然而，现有的MLLM基准测试主要集中在单图视觉推理或多图理解任务的最终答案评估上，对多图输入的多模态大语言模型的推理能力探索不足。为了解决这个问题，我们推出了多模态多图推理基准（MMRB），这是第一个旨在评估跨多个图像的结构化视觉推理的基准。MMRB包含92个子任务，覆盖空间、时间、语义推理，并使用GPT-4o生成多解、CoT风格的注释，并通过人工专家改进。一个衍生的子集被设计用于评估多图场景中的多模态奖励模型。为了支持快速和可扩展的评估，我们提出了一个句子级匹配框架，使用开源LLM。在40个多模态大语言模型（包括9个推理专用模型和8个奖励模型）上进行了广泛的基线实验，结果表明开源多模态大语言模型在多图推理任务中仍然显著落后于商业模型。此外，当前的多模态奖励模型几乎无法处理多图奖励排序任务。

---

### [HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting](https://arxiv.org/abs/2506.04351)

**一句话总结:** 该论文提出了一种弱监督流程，通过图像扩散模型、特征映射和点云扩散模型，实现了快速、高质量和可控的3D人体生成。

**Authors:** Maksym Ivashechkin, Oscar Mendez, Richard Bowden
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04351)

#### 中文摘要 (Abstract in Chinese)

> 3D人体生成是计算机视觉和图形学中一个重要的问题，有着广泛的应用。尽管生成式人工智能（如扩散模型）或神经辐射场或高斯溅射等渲染方法最近取得了进展，但从文本提示控制精确的3D人体的生成仍然是一个开放的挑战。目前的方法在细节、手和面部的准确渲染、人体真实感和外观可控性方面存在困难。缺乏多样性、真实感和注释的人体图像数据仍然是一个挑战，阻碍了基础3D人体模型的发展。我们提出了一种弱监督流程，试图解决这些挑战。第一步，我们使用最先进的图像扩散模型生成一个具有可控属性（如外观、种族、性别等）的照片级真实的人体图像数据集。接下来，我们提出了一种有效的从图像特征到3D点云的映射方法，使用基于Transformer的架构。最后，我们通过训练一个以用于生成原始样本的相同文本提示为条件的点云扩散模型来闭环。我们证明，与最先进的方法相比，3D人体生成的速度提高了几个数量级，文本提示对齐、真实感和渲染质量也得到了显著提高。我们将提供代码和数据集。

---

### [ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding](https://arxiv.org/abs/2506.04353)

**一句话总结:** ReXVQA是一个新的胸部X光VQA基准，AI性能首次超越了人类专家。

**Authors:** Ankit Pal, Jung-Oh Lee, Xiaoman Zhang, Malaikannan Sankarasubbu, Seunghyeon Roh, Won Jung Kim, Meesun Lee, Pranav Rajpurkar
**Categories:** `cs.CV`, `cs.AI`, `cs.CE`, `cs.CL`, `cs.LG`

[**[PDF]**](https://arxiv.org/pdf/2506.04353)

#### 中文摘要 (Abstract in Chinese)

> 本文介绍了ReXVQA，这是一个最大、最全面的胸部放射学视觉问答（VQA）基准，包含约696,000个问题，并配有来自训练、验证和测试集的160,000个胸部X光研究。与之前严重依赖于基于模板的查询的工作不同，ReXVQA引入了一个多样化且临床真实的测试套件，反映了五个核心放射学推理技能：存在评估、位置分析、否定检测、鉴别诊断和几何推理。我们评估了八个最先进的多模态大型语言模型，包括MedGemma-4B-it、Qwen2.5-VL、Janus-Pro-7B和Eagle2-9B。性能最佳的模型（MedGemma）实现了83.24%的总体准确率。为了弥合AI性能和临床专业知识之间的差距，我们对200个随机抽样的病例进行了全面的人类读者研究，涉及3名放射科住院医师。我们的评估表明，与人类读者（最佳放射科住院医师：77.27%）相比，MedGemma取得了优异的性能（83.84%的准确率），这代表着一个重要的里程碑，即AI性能超过了胸部X光片解读方面的专家人类评估。读者研究揭示了AI模型和人类专家之间不同的性能模式，放射科医生之间具有很强的一致性，而人类读者和AI模型之间的协议模式则变化更大。ReXVQA为评估通用放射学AI系统建立了一个新标准，提供公共排行榜、细粒度评估拆分、结构化解释和类别级别细分。该基准为下一代AI系统奠定了基础，这些系统能够模仿专家级别的临床推理，超越狭窄的病理分类。我们的数据集将在https://huggingface.co/datasets/rajpurkarlab/ReXVQA上开源。

---

### [WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning](https://arxiv.org/abs/2506.04363)

**一句话总结:** WorldPrediction是一个新的视频基准，用于评估AI模型的世界建模和程序规划能力，结果表明当前模型与人类水平差距很大。

**Authors:** Delong Chen, Willy Chung, Yejin Bang, Ziwei Ji, Pascale Fung
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04363)

#### 中文摘要 (Abstract in Chinese)

> 人类拥有一个内在的“世界模型”，使我们能够根据世界状态进行行动规划。人工智能代理也需要拥有这样的世界模型来进行行动规划。目前尚不清楚当前的人工智能模型，特别是生成模型，是如何学习世界模型并在不同的环境中进行程序规划的。我们引入了WorldPrediction，这是一个基于视频的基准，用于评估不同AI模型的世界建模和程序规划能力。与之前主要关注低级世界建模和机器人运动规划的基准相比，WorldPrediction是第一个强调具有时间和语义抽象的动作的基准。给定初始和最终世界状态，任务是从一组反事实干扰因素中区分正确的动作 (WorldPrediction-WM) 或正确排序的动作序列 (WorldPrediction-PP)。这种判别性任务设置使我们能够评估不同类型的世界模型和规划器，并实现对不同假设的全面比较。该基准使用视觉观察来表示状态和动作。为了防止模型利用背景场景中的低级连续性线索，我们提供了“动作等价物”——在不同上下文中观察到的相同动作——作为选择的候选对象。该基准基于部分可观察半MDP的形式框架，确保了评估的更好可靠性和鲁棒性。我们对我们的基准进行了广泛的人工过滤和验证，并表明当前的前沿模型在WorldPrediction-WM上的准确率仅为57%，在WorldPrediction-PP上的准确率仅为38%，而人类可以完美解决这两个任务。

---

### [Ice Hockey Puck Localization Using Contextual Cues](https://arxiv.org/abs/2506.04365)

**一句话总结:** 提出了一种利用球员上下文线索进行冰球检测的新方法PLUCC，并在冰球检测任务上取得了state-of-the-art的结果。

**Authors:** Liam Salass, Jerrin Bright, Amir Nazemi, Yuhao Chen, John Zelek, David Clausi
**Categories:** `cs.CV`, `cs.AI`

[**[PDF]**](https://arxiv.org/pdf/2506.04365)

#### 中文摘要 (Abstract in Chinese)

> 冰球广播视频中的冰球检测由于冰球尺寸小、频繁遮挡、运动模糊、广播伪影以及因相机变焦和广播相机视点变化而导致的不一致性而面临重大挑战。以往的研究主要集中在基于外观或基于运动的冰球线索，而没有明确地对来自球员行为的线索进行建模。球员们总是会转动身体，并将目光投向冰球。受到这种强烈的上下文线索的启发，我们提出了一种名为PLUCC的新方法，用于尺度感知和上下文驱动的单帧冰球检测。PLUCC包括三个组成部分：（a）上下文编码器，它利用球员的朝向和位置作为有用的先验；（b）特征金字塔编码器，它从双编码器中提取多尺度特征；（c）门控解码器，它将潜在特征与通道门控机制相结合。为了进行评估，除了标准的平均精度外，我们还提出了Rink Space Localization Error (RSLE)，这是一种基于尺度不变单应性的度量，用于消除冰场空间评估中的透视偏差。PLUCC在PuckDataset数据集上的实验结果表明，其检测性能达到了最先进的水平，超过了之前的基线方法，平均精度提高了12.2%，RSLE平均精度提高了25%。我们的研究表明，上下文理解在提高冰球检测性能方面起着关键作用，对自动化体育分析具有广泛的意义。

---

### [Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A Comparative Analysis for Classification Tasks](https://arxiv.org/abs/2506.04367)

**一句话总结:** 本研究通过微调视频Transformer模型，显著提高了孟加拉手语识别的准确率，为听障人士的交流提供了更有效的工具。

**Authors:** Jubayer Ahmed Bhuiyan Shawon, Hasan Mahmud, Kamrul Hasan
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04367)

#### 中文摘要 (Abstract in Chinese)

> 该研究旨在使用视频Transformer架构改进孟加拉手语识别。研究人员在两个孟加拉手语数据集（BdSLW60和BdSLW401）上微调了VideoMAE、ViViT和TimeSformer模型，并采用了数据增强和交叉验证等技术。实验结果表明，视频Transformer模型优于传统方法，其中VideoMAE模型表现最佳。

---

### [Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization](https://arxiv.org/abs/2506.04379)

**一句话总结:** 激活最大化可以成功应用于基于DNN的编码模型，以表征和调节人类视觉系统中的反应。

**Authors:** Matthew W. Shinkle, Mark D. Lescroart
**Categories:** `cs.CV`, `cs.AI`, `q-bio.NC`

[**[PDF]**](https://arxiv.org/pdf/2506.04379)

#### 中文摘要 (Abstract in Chinese)

> 在视觉任务上训练的深度神经网络（DNN）会发展出类似于人类视觉系统的特征表示。虽然基于DNN的编码模型可以准确地预测大脑对视觉刺激的反应，但它们对驱动这些反应的特定特征的洞察力有限。在这里，我们证明了激活最大化——一种旨在解释视觉DNN的技术——可以应用于基于DNN的人脑编码模型。我们提取并自适应地对预训练的Inception V3网络的多个层的激活进行下采样，然后使用线性回归来预测fMRI反应。这产生了一个完整的大脑反应图像可计算模型。接下来，我们应用激活最大化来生成针对单个皮质体素中预测反应进行优化的图像。我们发现这些图像包含与已知选择性定性对应的视觉特征，并能够探索整个视觉皮层的选择性。我们将该方法进一步扩展到整个大脑感兴趣区域（ROI），并通过在fMRI研究中向人类参与者展示这些图像来验证其有效性。我们发现生成的图像可靠地驱动了低级和高级视觉区域以及受试者中目标区域的活动。这些结果表明，激活最大化可以成功应用于基于DNN的编码模型。通过解决需要原生生成模型的替代方法的关键局限性，我们的方法能够灵活地表征和调节整个人类视觉系统中的反应。

---

### [Is Perturbation-Based Image Protection Disruptive to Image Editing?](https://arxiv.org/abs/2506.04394)

**一句话总结:** 基于扰动的图像保护方法可能无法有效阻止基于扩散模型的图像编辑，甚至可能适得其反。

**Authors:** Qiuyu Tang, Bonor Ayambem, Mooi Choo Chuah, Aparna Bharati
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04394)

#### 中文摘要 (Abstract in Chinese)

> 目前最先进的扩散模型（如Stable Diffusion）具有卓越的图像生成能力，但也可能被滥用于传播虚假信息和剽窃受版权保护的材料。为了减轻图像编辑带来的潜在风险，当前的图像保护方法依赖于向图像添加难以察觉的扰动，以阻止基于扩散的编辑。对图像的完全成功保护意味着编辑尝试的输出将是不良的、嘈杂的图像，与参考图像完全无关。在我们在多个领域（自然场景图像和艺术品）和编辑任务（图像到图像生成和风格编辑）中使用各种基于扰动的图像保护方法进行的实验中，我们发现这种保护并没有完全实现这一目标。在大多数情况下，对受保护图像的基于扩散的编辑会生成理想的输出图像，该图像精确地符合引导提示。我们的研究结果表明，在生成过程中，向图像添加噪声可能会适得其反地增加它们与给定文本提示的关联，从而导致意想不到的后果，例如更好的编辑结果。因此，我们认为基于扰动的方法可能无法为针对基于扩散的编辑提供强大的图像保护。

---

### [Normalize Filters! Classical Wisdom for Deep Vision](https://arxiv.org/abs/2506.04401)

**一句话总结:** 该论文提出了一种滤波器归一化方法，通过将经典滤波原则集成到深度学习中，显著提高了模型在图像强度变化下的鲁棒性和泛化能力。

**Authors:** Gustavo Perez, Stella X. Yu
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04401)

#### 中文摘要 (Abstract in Chinese)

> 经典图像滤波器，例如用于平均或差分的滤波器，经过仔细归一化以确保一致性、可解释性，并避免诸如强度偏移、光晕或振铃之类的伪影。相比之下，在深度网络中端到端学习的卷积滤波器缺乏这种约束。虽然它们可能类似于小波和斑点/边缘检测器，但它们没有以相同或任何方式进行归一化。因此，当图像经历大气传递时，它们的响应会失真，导致不正确的结果。我们通过提出滤波器归一化，然后进行可学习的缩放和移位（类似于批量归一化）来解决这一限制。这种简单而有效的修改确保了滤波器是大气等变的，从而实现共域对称。通过将经典滤波原则集成到深度学习中（适用于卷积神经网络和卷积相关的视觉转换器），我们的方法在人工和自然强度变化基准上取得了显着改进。我们的 ResNet34 甚至可以大幅优于 CLIP。我们的分析表明，未归一化的滤波器会降低性能，而滤波器归一化可以规范学习，促进多样性，并提高鲁棒性和泛化能力。

---

### [HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation](https://arxiv.org/abs/2506.04421)

**一句话总结:** HMAR通过分层掩码自回归建模，实现了高质量、快速采样的图像生成，并在速度、内存占用和灵活性方面优于现有方法。

**Authors:** Hermann Kumbong, Xian Liu, Tsung-Yi Lin, Ming-Yu Liu, Xihui Liu, Ziwei Liu, Daniel Y. Fu, Christopher Ré, David W. Romero
**Categories:** `cs.CV`, `cs.AI`, `cs.LG`

[**[PDF]**](https://arxiv.org/pdf/2506.04421)

#### 中文摘要 (Abstract in Chinese)

> 视觉自回归建模 (VAR) 在弥合自回归图像模型和扩散模型之间的速度和质量差距方面显示出希望。VAR 通过将图像分解为连续的分辨率尺度来重新制定自回归建模。在推理过程中，通过预测下一个（更高分辨率）尺度中的所有tokens来生成图像，该预测以所有先前（较低分辨率）尺度中的所有tokens为条件。然而，由于并行生成分辨率尺度中的所有tokens，这种公式的图像质量降低；序列长度以超线性方式缩放图像分辨率；并且需要重新训练才能更改采样计划。 我们引入了分层掩码自回归建模 (HMAR)，这是一种新的图像生成算法，它使用下一尺度预测和掩码预测来缓解这些问题，从而生成具有快速采样的高质量图像。HMAR 将下一尺度预测重新表述为马尔可夫过程，其中每个分辨率尺度的预测仅以其直接前置分辨率中的tokens为条件，而不是以所有前置分辨率中的tokens为条件。在预测分辨率尺度时，HMAR 使用可控的多步掩码生成程序来生成每个步骤中tokens的子集。在 ImageNet 256x256 和 512x512 基准测试中，HMAR 模型与参数匹配的 VAR、扩散和自回归基线模型相匹配或超过了它们的性能。我们开发了高效的 IO 感知块稀疏注意力内核，使 HMAR 能够实现比 VAR 更快的训练和推理时间，分别超过 2.5 倍和 1.75 倍，并且推理内存占用降低超过 3 倍。最后，HMAR 比 VAR 具有更大的灵活性；它的采样计划可以在没有进一步训练的情况下进行更改，并且可以以零样本方式应用于图像编辑任务。

---

### [Photoreal Scene Reconstruction from an Egocentric Device](https://arxiv.org/abs/2506.04444)

**一句话总结:** 该论文提出了一种结合视觉惯性束调整（VIBA）和基于物理的图像形成模型的高动态范围场景光真实感重建方法，有效提升了重建质量。

**Authors:** Zhaoyang Lv, Maurizio Monge, Ka Chen, Yufeng Zhu, Michael Goesele, Jakob Engel, Zhao Dong, Richard Newcombe
**Categories:** `cs.CV`, `cs.AI`, `cs.GR`, `cs.HC`, `cs.MM`

[**[PDF]**](https://arxiv.org/pdf/2506.04444)

#### 中文摘要 (Abstract in Chinese)

> 本文研究了使用第一人称视角设备在高动态范围下进行照片级真实感场景重建所面临的挑战。现有的方法通常假设使用从设备的视觉惯性里程计系统估计的帧率 6DoF 姿势，这可能会忽略像素精确重建所需的关键细节。本研究提出了两个重要的发现。首先，与将 RGB 相机视为全局快门帧率相机的主流工作不同，我们强调了采用视觉惯性束调整（VIBA）来校准滚动快门 RGB 传感相机在高频轨迹格式下的精确时间戳和运动的重要性，这确保了滚动快门相机的物理特性的精确校准。其次，我们将基于物理的图像形成模型整合到高斯溅射中，有效地解决了传感器特性，包括 RGB 相机的滚动快门效应和传感器测量的动态范围。我们提出的公式适用于广泛使用的高斯溅射表示变体。我们使用开源 Project Aria 设备在各种室内和室外照明条件下对我们的流程进行了全面的评估，并在 Meta Quest3 设备上进一步验证了它。在所有实验中，我们观察到通过结合 VIBA 可以持续视觉增强 +1 dB 的 PSNR，通过我们提出的图像形成模型可以额外实现 +1 dB。我们的完整实现、评估数据集和记录配置文件可在 http://www.projectaria.com/photoreal-reconstruction/ 获取。

---

### [Towards Large-Scale Pose-Invariant Face Recognition Using Face Defrontalization](https://arxiv.org/abs/2506.04496)

**一句话总结:** 提出面部去正面化方法以增强面部识别模型的训练数据，并在大型数据集上取得了比现有面部正面化方法更好的效果。

**Authors:** Patrik Mesec, Alan Jović
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04496)

#### 中文摘要 (Abstract in Chinese)

> 在极端头部姿势下的面部识别是一项具有挑战性的任务。理想情况下，面部识别系统应在不同的头部姿势下表现良好，这被称为姿势不变面部识别。为了实现姿势不变性，当前的方法依赖于复杂的方法，例如面部正面化和各种面部特征提取模型架构。然而，这些方法在实际环境中有些不切实际，并且通常在小型科学数据集（如Multi-PIE）上进行评估。在这项工作中，我们提出了面部正面化的逆方法，称为面部去正面化，以扩充面部特征提取模型的训练数据集。该方法在推理步骤中不会引入任何时间开销。该方法包括：1) 在正面-侧面人脸对数据集上训练一个改进的面部去正面化FFWM模型，该数据集已使用我们提出的面部对齐方法进行预处理；2) 在原始和随机去正面化的大规模数据集上，基于ArcFace损失训练ResNet-50面部特征提取模型，其中去正面化是使用我们之前训练的面部去正面化模型执行的。我们的方法在四个开放数据集（LFW、AgeDB、CFP和Multi-PIE）上与现有方法进行了比较。与没有去正面化的模型相比，去正面化显示出改进的结果，而所提出的调整在三个较大的开放数据集上明显优于最先进的面部正面化FFWM方法，但在用于极端姿势（75度和90度）的小型Multi-PIE数据集上则不然。结果表明，至少某些当前方法可能过度拟合小型数据集。

---

### [FALO: Fast and Accurate LiDAR 3D Object Detection on Resource-Constrained Devices](https://arxiv.org/abs/2506.04499)

**一句话总结:** FALO是一种硬件友好的LiDAR 3D检测方法，它在保持竞争力的性能的同时，显著提高了在资源受限设备上的推理速度。

**Authors:** Shizhong Han, Hsin-Pai Cheng, Hong Cai, Jihad Masri, Soyeb Nagori, Fatih Porikli
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04499)

#### 中文摘要 (Abstract in Chinese)

> 现有的LiDAR 3D物体检测方法主要依赖于稀疏卷积和/或Transformer，由于不规则的内存访问模式和高计算成本，这可能难以在资源受限的边缘设备上运行。在本文中，我们提出了一种硬件友好的LiDAR 3D检测方法FALO，它既能提供最先进的（SOTA）检测精度，又能提供快速的推理速度。更具体地说，给定3D点云并在体素化之后，FALO首先根据3D体素的坐标和邻近度将稀疏3D体素排列成1D序列。然后，该序列由我们提出的ConvDotMix块处理，该块由大核卷积、Hadamard积和线性层组成。ConvDotMix在空间和嵌入维度上提供了足够的混合能力，并在空间特征之间引入了更高阶的非线性交互。此外，在通过ConvDotMix层时，我们引入了隐式分组，这平衡了张量维度，以实现更高效的推理，并考虑了不断增长的感受野。所有这些操作都易于在资源受限的平台上运行，并且提出的FALO可以很容易地部署在紧凑的嵌入式设备上。我们在nuScenes和Waymo等LiDAR 3D检测基准上的广泛评估表明，FALO取得了有竞争力的性能。同时，在移动图形处理单元（GPU）和移动神经处理单元（NPU）上，FALO比最新的SOTA快1.6~9.8倍。

---

### [AuthGuard: Generalizable Deepfake Detection via Language Guidance](https://arxiv.org/abs/2506.04501)

**一句话总结:** AuthGuard：一种利用语言引导和视觉编码器相结合的深度伪造检测框架，显著提升了检测的准确性和泛化能力。

**Authors:** Guangyu Shen, Zhihua Li, Xiang Xu, Tianchen Zhao, Zheng Zhang, Dongsheng An, Zhuowen Tu, Yifan Xing, Qin Zhang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04501)

#### 中文摘要 (Abstract in Chinese)

> 现有的深度伪造检测技术难以跟上不断发展的新型、未见过的伪造方法。这种局限性源于它们依赖于训练过程中学习的统计伪影，这些伪影通常与特定的生成过程相关联，而这些过程可能不代表测试时遇到的新的、未见过的深度伪造生成方法的样本。我们提出，结合语言指导可以通过整合类似人类的常识推理（例如识别逻辑不一致和感知异常）以及统计线索来提高深度伪造检测的泛化能力。为了实现这一目标，我们通过将判别分类与图像-文本对比学习相结合来训练专家深度伪造视觉编码器，其中文本由通用MLLM使用少量样本提示生成。这使得编码器能够从像素级分布中提取语言可描述的、常识性的深度伪造伪影和统计伪造伪影。为了进一步提高鲁棒性，我们将数据不确定性学习集成到视觉-语言对比学习中，从而减轻图像-文本监督中的噪声。我们的专家视觉编码器与LLM无缝连接，进一步实现了更通用和可解释的深度伪造检测，同时也提高了准确性。由此产生的框架AuthGuard在同分布和异分布设置下均实现了state-of-the-art的深度伪造检测准确率，在DFDC数据集上实现了6.15%的AUC增益，在DF40数据集上实现了16.68%的AUC增益。此外，AuthGuard显著提高了深度伪造推理能力，在DDVQA数据集上提高了24.69%的性能。

---

### [Pruning Everything, Everywhere, All at Once](https://arxiv.org/abs/2506.04513)

**一句话总结:** 该论文提出了一种新的剪枝方法，可以同时剪枝模型中的不同结构，并在减少计算量的同时保持或提高模型精度和鲁棒性，并减少碳排放。

**Authors:** Gustavo Henrique do Nascimento, Ian Pons, Anna Helena Reali Costa, Artur Jordao
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04513)

#### 中文摘要 (Abstract in Chinese)

> 深度学习是解决认知任务的现代范例。然而，随着问题复杂性的增加，模型变得越来越深，计算成本也越来越高，这阻碍了在现实世界和资源受限的应用中的发展。大量的研究表明，剪枝模型中的结构可以有效地降低模型复杂性并提高计算效率。这一领域成功的策略包括移除神经元（即滤波器、注意力头）或层，但不能同时移除两者。因此，同时剪枝不同的结构仍然是一个开放的问题。为了填补这一空白，并利用同时消除神经元和层的好处，我们提出了一种新的方法，该方法能够剪枝模型中的不同结构，具体如下：给定两个候选子网络（剪枝后的模型），一个来自层剪枝，另一个来自神经元剪枝，我们的方法通过使用 Centered Kernel Alignment 指标选择与父网络（生成子网络的网络）具有最高表示相似度的子网络来决定选择哪一个。迭代地重复这个过程可以提供高度稀疏的模型，同时保留原始的预测能力。通过在标准架构和基准测试中进行的大量实验，我们证实了我们方法的有效性，并表明它优于最先进的层和滤波器剪枝技术。在高水平的浮点运算减少的情况下，大多数最先进的方法会降低准确性，而我们的方法要么提高准确性，要么只有最小的下降。值得注意的是，在流行的 ResNet56 和 ResNet110 上，我们实现了 86.37% 和 95.82% 的 FLOPs 减少的里程碑。此外，我们剪枝后的模型获得了对抗性和分布外样本的鲁棒性，并朝着 GreenAI 迈出了重要一步，减少了高达 83.31% 的碳排放。总的来说，我们相信我们的工作开启了剪枝的新篇章。

---

### [EECD-Net: Energy-Efficient Crack Detection with Spiking Neural Networks and Gated Attention](https://arxiv.org/abs/2506.04526)

**一句话总结:** EECD-Net通过结合SRCNN、SCU和GAT模块，实现了高精度和低功耗的道路裂缝检测。

**Authors:** Shuo Zhang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04526)

#### 中文摘要 (Abstract in Chinese)

> 道路表面的裂缝检测是仪器仪表领域的一项关键测量技术，对于确保基础设施安全和运输可靠性至关重要。然而，由于智能终端设备能量有限且图像分辨率较低，难以维持实时监测性能。为了克服这些挑战，本文提出了一种用于道路裂缝检测的多阶段检测方法EECD-Net，以提高仪器检测的准确性和能效。具体来说，采用精密的超分辨率卷积神经网络（SRCNN）来解决低质量图像的固有挑战，有效提高图像分辨率，同时保留关键的结构细节。同时，提出了一种具有连续积分-激发（CIF）神经元的脉冲卷积单元（SCU），将这些图像转换为稀疏脉冲序列，从而显著降低功耗。此外，设计了一个门控注意力Transformer（GAT）模块，通过自适应注意力机制策略性地融合多尺度特征表示，有效地捕获远程依赖关系和复杂的局部裂缝模式，并显著提高各种裂缝形态的检测鲁棒性。在CrackVision12K基准测试中进行的实验表明，EECD-Net实现了98.6%的显著检测精度，超过了最先进的Hybrid-Segmentor 1.5%。值得注意的是，EECD-Net保持了卓越的能源效率，仅消耗5.6 mJ，与基线实现相比大幅降低了33%。这项工作在基于仪器的裂缝检测中开创了一种变革性的方法，为资源受限环境中的实时、大规模基础设施监测提供了一种可扩展的低功耗解决方案。

---

### [Enhancing Frequency for Single Image Super-Resolution with Learnable Separable Kernels](https://arxiv.org/abs/2506.04555)

**一句话总结:** 本文提出了一种可学习分离内核（LSKs）的即插即用模块，用于直接增强单图像超分辨率任务中的图像频率分量，从而减少参数和计算量并提高性能。

**Authors:** Heng Tian
**Categories:** `cs.CV`, `cs.MM`

[**[PDF]**](https://arxiv.org/pdf/2506.04555)

#### 中文摘要 (Abstract in Chinese)

> 现有的方法通常通过引入辅助结构，例如专门设计的损失函数，来间接提升低分辨率图像的质量，以此增强单图像超分辨率（SISR）方法的性能。本文提出了一种即插即用的模块，名为可学习分离内核（LSKs），它在形式上是秩一矩阵，设计用于直接增强图像的频率分量。首先，我们从频率的角度解释了为什么LSKs特别适合SISR任务。结合LSKs的基线方法在参数数量和计算需求方面都显著减少了60%以上。这种减少是通过将LSKs分解为正交且可合并的一维内核来实现的。此外，我们对LSKs生成的特征图进行了可解释性分析。可视化结果表明LSKs能够有效地增强图像的频率分量。大量的实验表明，引入LSKs不仅减少了参数数量和计算负载，还提高了整体模型性能。此外，这些实验表明，使用LSKs的模型表现出更优越的性能，尤其是在放大倍数增加时。

---

### [Perceptual Decoupling for Scalable Multi-modal Reasoning via Reward-Optimized Captioning](https://arxiv.org/abs/2506.04559)

**一句话总结:** RACRO通过推理引导的强化学习优化视觉提取器的标题生成，以提升多模态LLM的推理能力，避免了昂贵的重对齐。

**Authors:** Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Xin Jin, Zhenguo Li, James T. Kwok, Yu Zhang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04559)

#### 中文摘要 (Abstract in Chinese)

> 最近在慢思考语言模型（例如，OpenAI-o1和DeepSeek-R1）方面的进展已经证明了通过模仿类人反思认知在复杂推理任务中表现出的卓越能力。然而，由于升级底层推理器LLM时重新训练视觉-语言对齐的成本很高，因此将这种能力扩展到多模态大型语言模型（MLLM）仍然具有挑战性。一个直接的解决方案是将感知与推理分离，即将视觉输入转换为语言表示（例如，标题），然后将其传递给强大的纯文本推理器。然而，这种解耦引入了一个关键挑战：视觉提取器必须生成既忠实于图像又具有足够信息量的描述，以支持准确的下游推理。为了解决这个问题，我们提出了一种通过标题奖励优化进行推理对齐感知解耦（RACRO）——一种推理引导的强化学习策略，该策略使提取器的标题行为与推理目标对齐。通过基于奖励的优化来闭合感知-推理循环，RACRO显著增强了视觉基础并提取了推理优化的表示。在多模态数学和科学基准测试上的实验表明，所提出的RACRO方法实现了最先进的平均性能，同时实现了卓越的可扩展性和即插即用适应性，而无需昂贵的多模态重新对齐。

---

### [LGM-Pose: A Lightweight Global Modeling Network for Real-time Human Pose Estimation](https://arxiv.org/abs/2506.04561)

**一句话总结:** 提出了一种轻量级的单分支全局建模网络LGM-Pose，用于多人姿态估计，该网络在性能和速度上都优于现有方法。

**Authors:** Biao Guo, Fangmin Guo, Guibo Luo, Xiaonan Luo, Feng Zhang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04561)

#### 中文摘要 (Abstract in Chinese)

> 目前大多数自顶向下的多人姿态估计轻量级方法都基于多分支并行纯CNN网络架构，这种架构通常难以捕捉检测语义复杂关键点所需的全局上下文，并且由于其复杂和冗余的结构而导致高延迟。为了解决这些挑战，本文提出了一种近似单分支轻量级全局建模网络（LGM-Pose）。在该网络中，设计了一个轻量级MobileViM块，其中包含一个轻量级注意力表示模块（LARM），该模块使用非参数变换操作（NPT-Op）整合patch内部和之间的信息，以提取全局信息。此外，还引入了一种新颖的Shuffle集成融合模块（SFusion），以有效整合多尺度信息，减轻通常在单分支结构中观察到的性能下降。在COCO和MPII数据集上的实验评估表明，我们的方法不仅减少了与现有主流轻量级方法相比的参数数量，而且实现了卓越的性能和更快的处理速度。

---

### [Follow-Your-Creation: Empowering 4D Creation through Video Inpainting](https://arxiv.org/abs/2506.04590)

**一句话总结:** 该论文提出了一种名为Follow-Your-Creation的4D视频创作框架，它通过视频修复和自迭代调整策略，实现了从单目视频生成和编辑高质量4D内容。

**Authors:** Yue Ma, Kunyu Feng, Xinhua Zhang, Hongyu Liu, David Junhao Zhang, Jinbo Xing, Yinhan Zhang, Ayden Yang, Zeyu Wang, Qifeng Chen
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04590)

#### 中文摘要 (Abstract in Chinese)

> 该论文介绍了一种名为Follow-Your-Creation的全新4D视频创作框架，该框架能够从单个单目视频输入生成和编辑4D内容。通过利用强大的视频修复基础模型作为生成先验，我们将4D视频创建重新定义为视频修复任务，从而使模型能够填充由相机轨迹变化或用户编辑引起的缺失内容。为了实现这一点，我们生成了复合掩码修复视频数据，以有效地微调模型以进行4D视频生成。给定输入视频及其相关的相机轨迹，我们首先执行基于深度的点云渲染，以获得指示应完成区域的不可见性掩码。同时，引入编辑掩码以指定用户定义的修改，并将这些掩码与不可见性掩码组合以创建复合掩码数据集。在训练过程中，我们随机采样不同类型的掩码以构建多样且具有挑战性的修复场景，从而增强模型在各种4D编辑和生成任务中的泛化性和鲁棒性。为了处理大型相机运动下的时间一致性，我们设计了一种自迭代调整策略，该策略在训练期间逐渐增加视角，其中该模型用于在每次微调迭代后生成下一阶段的训练数据。此外，我们在推理过程中引入了一个时间打包模块，以提高生成质量。我们的方法有效地利用了基础模型的先验知识，而不会降低其原始性能，从而能够生成具有一致多视角连贯性的4D视频。此外，我们的方法支持基于提示的内容编辑，展示了强大的灵活性，并且在质量和多功能性方面显著优于现有技术。

---

### [Hierarchical-Task-Aware Multi-modal Mixture of Incremental LoRA Experts for Embodied Continual Learning](https://arxiv.org/abs/2506.04595)

**一句话总结:** 提出了一种分层具身持续学习框架和任务感知的LoRA专家混合模型，以解决具身智能中持续学习高级规划和多级知识的问题，并有效减少灾难性遗忘。

**Authors:** Ziqi Jia, Anmin Wang, Xiaoyang Qu, Xiaowen Yang, Jianzong Wang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04595)

#### 中文摘要 (Abstract in Chinese)

> 以往的具身智能持续学习设置侧重于执行基于人类指令的低级动作，忽略了学习高级规划和多级知识的能力。为了解决这些问题，我们提出了分层具身持续学习设置（HEC），将智能体的持续学习过程分为两个层次：高级指令和低级动作，并定义了五个具身持续学习子设置。在此基础上，我们引入了任务感知的增量LoRA专家混合模型（Task-aware MoILE）方法。该方法通过聚类视觉-文本嵌入实现任务识别，并使用任务级和令牌级路由器选择合适的LoRA专家。为了有效解决灾难性遗忘问题，对先前任务获得的LoRA参数应用奇异值分解（SVD），保留关键组件，同时正交训练剩余部分。实验结果表明，与其他方法相比，该方法在减少旧任务遗忘方面表现出色，有效支持智能体在持续学习新任务的同时保留先前的知识。

---

### [SmartAvatar: Text- and Image-Guided Human Avatar Generation with VLM AI Agents](https://arxiv.org/abs/2506.04606)

**一句话总结:** SmartAvatar 是一个通过视觉-语言-代理驱动的框架，从单张照片或文本提示生成可用于动画的 3D 人体头像。

**Authors:** Alexander Huang-Menders, Xinhang Liu, Andy Xu, Yuyao Zhang, Chi-Keung Tang, Yu-Wing Tai
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04606)

#### 中文摘要 (Abstract in Chinese)

> SmartAvatar 是一个由视觉-语言-代理驱动的框架，用于从单张照片或文本提示生成完全绑定、可用于动画的 3D 人体头像。虽然基于扩散的方法在通用 3D 对象生成方面取得了进展，但它们仍然难以精确控制人体身份、体型和动画就绪度。相比之下，SmartAvatar 利用大型视觉-语言模型 (VLM) 的常识推理能力，结合现成的参数化人体生成器来提供高质量、可定制的头像。一个关键的创新是自主验证循环，其中代理渲染草图头像，评估面部相似性、解剖学合理性和提示对齐，并迭代调整生成参数以实现收敛。这种交互式的、AI 引导的细化过程促进了对面部和身体特征的精细控制，使用户能够通过自然语言对话迭代地细化他们的头像。与依赖静态预训练数据集并提供有限灵活性的扩散模型不同，SmartAvatar 将用户带入建模循环，并通过 LLM 驱动的程序生成和验证系统确保持续改进。生成的头像已完全绑定并支持姿势操作，具有一致的身份和外观，使其适用于下游动画和交互式应用程序。定量基准测试和用户研究表明，SmartAvatar 在重建网格质量、身份保真度、属性准确性和动画就绪度方面优于最近的文本和图像驱动的头像生成系统，使其成为在消费级硬件上创建逼真、可定制头像的多功能工具。

---

### [Perfecting Depth: Uncertainty-Aware Enhancement of Metric Depth](https://arxiv.org/abs/2506.04612)

**一句话总结:** 提出了一种新的两阶段框架Perfecting Depth，用于传感器深度增强，该框架结合了随机不确定性建模和确定性细化，提高了深度图的可靠性和准确性。

**Authors:** Jinyoung Jun, Lei Chu, Jiahao Li, Yan Lu, Chang-Su Kim
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04612)

#### 中文摘要 (Abstract in Chinese)

> 我们提出了一种新颖的两阶段传感器深度增强框架，称为Perfecting Depth。该框架利用扩散模型的随机性来自动检测不可靠的深度区域，同时保留几何线索。在第一阶段（随机估计），该方法通过利用训练-推理域的差距来识别不可靠的测量并推断几何结构。在第二阶段（确定性细化），它使用从第一阶段导出的不确定性图来强制结构一致性和像素级精度。通过结合随机不确定性建模和确定性细化，我们的方法生成具有改进可靠性的密集、无伪影的深度图。实验结果表明了其在各种真实场景中的有效性。此外，理论分析、各种实验和定性可视化验证了其鲁棒性和可扩展性。我们的框架为传感器深度增强设定了新的基线，在自动驾驶、机器人和沉浸式技术领域具有潜在的应用。

---

### [Deep Learning Reforms Image Matching: A Survey and Outlook](https://arxiv.org/abs/2506.04619)

**一句话总结:** 本综述全面概述了深度学习如何变革传统图像匹配流程，并突出了未来创新的关键方向。

**Authors:** Shihua Zhang, Zizhuo Li, Kaining Zhang, Yifan Lu, Yuxin Deng, Linfeng Tang, Xingyu Jiang, Jiayi Ma
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04619)

#### 中文摘要 (Abstract in Chinese)

> 图像匹配是计算机视觉的基石，它在两幅图像之间建立对应关系，以恢复 3D 结构和相机几何。它支撑着广泛的应用，包括视觉定位、3D 重建和同步定位与地图构建 (SLAM)。由“检测器-描述符、特征匹配器、异常值滤波器和几何估计器”组成的传统流程在具有挑战性的场景中会失效。最近深度学习的进展显着提高了鲁棒性和准确性。本综述采用独特的视角，全面回顾了深度学习如何逐步改变传统的图像匹配流程。我们的分类在两个关键方面与传统流程高度一致：i) 用可学习的替代方案替换传统流程中的各个步骤，包括可学习的检测器-描述符、异常值滤波器和几何估计器；ii) 将多个步骤合并到端到端的可学习模块中，包括中间端稀疏匹配器、端到端半密集/密集匹配器和姿势回归器。我们首先检查这两个方面的设计原则、优点和局限性，然后在相对姿势恢复、单应性估计和视觉定位任务上对代表性方法进行基准测试。最后，我们讨论了开放的挑战，并概述了未来研究的有希望的方向。通过系统地分类和评估深度学习驱动的策略，本综述清晰地概述了不断发展的图像匹配领域，并强调了进一步创新的关键途径。

---

### [Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations](https://arxiv.org/abs/2506.04633)

**一句话总结:** STARE 基准测试揭示了多模态大型语言模型在复杂空间推理任务中利用视觉模拟的不足。

**Authors:** Linjie Li, Mahtab Bigverdi, Jiawei Gu, Zixian Ma, Yinuo Yang, Ziang Li, Yejin Choi, Ranjay Krishna
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04633)

#### 中文摘要 (Abstract in Chinese)

> 空间认知对于人类智能至关重要，它可以通过视觉模拟进行问题解决，而不仅仅依赖于语言推理。然而，现有的 AI 基准主要评估语言推理，忽略了非语言、多步骤视觉模拟的复杂性。我们引入了 STARE(Spatial Transformations and Reasoning Evaluation) 基准，旨在严格评估多模态大型语言模型在通过多步骤视觉模拟更好地解决的任务上的能力。STARE 包含 4K 个任务，涵盖基础几何变换（2D 和 3D）、综合空间推理（立方体网折叠和七巧板拼图）和真实世界空间推理（透视和时间推理），反映了实际的认知挑战，如对象组装、机械图解释和日常空间导航。我们的评估表明，模型在简单的 2D 变换推理方面表现出色，但在需要多步骤视觉模拟的复杂任务（如 3D 立方体网折叠和七巧板拼图）上的表现接近随机水平。人类在复杂任务上实现了接近完美的准确率，但需要相当长的时间（最多 28.9 秒），并且通过中间视觉模拟可以显着加快速度（平均减少 7.5 秒）。相比之下，模型从视觉模拟中获得的好处并不一致，在大多数任务上都有所提高，但在特定情况下（如七巧板拼图 (GPT-4o, o1) 和立方体网折叠 (Claude-3.5, Gemini-2.0 Flash)）有所下降，这表明模型可能不知道如何有效地利用中间视觉信息。

---

### [Text-Aware Real-World Image Super-Resolution via Diffusion Model with Joint Segmentation Decoders](https://arxiv.org/abs/2506.04641)

**一句话总结:** 提出了一种名为TADiSR的基于扩散的超分辨率框架，该框架集成了文本感知注意力机制和联合分割解码器，以提高文本区域的结构保真度。

**Authors:** Qiming Hu, Linlong Fan, Yiyan Luo, Yuhang Yu, Xiaojie Guo, Qingnan Fan
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04641)

#### 中文摘要 (Abstract in Chinese)

> 生成模型在处理真实世界退化图像的超分辨率（SR）方面取得了显著进展。然而，它们经常会产生与保真度相关的问题，尤其是在扭曲文本结构方面。在本文中，我们介绍了一种新颖的基于扩散的超分辨率框架，即TADiSR，它集成了文本感知注意力机制和联合分割解码器，不仅可以恢复自然细节，还可以恢复退化的真实世界图像中文本区域的结构保真度。此外，我们提出了一个完整的pipeline，用于合成具有精细的全图像文本mask的高质量图像，将逼真的前景文本区域与详细的背景内容相结合。大量实验表明，我们的方法在超分辨率图像中显著提高了文本的可读性，并在多个评估指标上取得了最先进的性能，同时对真实场景表现出强大的泛化能力。我们的代码可在\href{https://github.com/mingcv/TADiSR}{here}获取。

---

### [FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion](https://arxiv.org/abs/2506.04648)

**一句话总结:** FPSAttention通过训练感知的FP8量化和稀疏化协同设计，显著加速了视频生成，同时保持了生成质量。

**Authors:** Akide Liu, Zeyu Zhang, Zhexin Li, Xuehai Bai, Yizeng Han, Jiasheng Tang, Yuanjie Xing, Jichao Wu, Mingyang Yang, Weihua Chen, Jiahao He, Yuanyu He, Fan Wang, Gholamreza Haffari, Bohan Zhuang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04648)

#### 中文摘要 (Abstract in Chinese)

> 扩散生成模型已成为生成高质量、连贯视频内容的标准，但其推理速度慢和计算需求高阻碍了实际部署。虽然量化和稀疏化可以独立加速推理并保持生成质量，但由于缺乏联合优化，现有方法将这些技术结合会导致显著的性能下降。我们引入了FPSAttention，这是一种新颖的训练感知FP8量化和稀疏化协同设计方法，专注于3D双向注意力机制。我们的方法包含三个关键创新：1）统一的3D tile-wise粒度，同时支持量化和稀疏化；2）去噪步骤感知策略，适应噪声计划，解决量化/稀疏化误差与去噪步骤之间的强相关性；3）原生、硬件友好的内核，利用FlashAttention并采用优化的Hopper架构特性实现高效执行。在Wan2.1的1.3B和14B模型上进行训练，并在VBench基准上进行评估，FPSAttention在720p分辨率下实现了7.09倍的内核加速和4.96倍的端到端视频生成加速，且不牺牲生成质量。

---

### [Feature-Based Lie Group Transformer for Real-World Applications](https://arxiv.org/abs/2506.04668)

**一句话总结:** 该论文提出了一种结合特征提取和对象分割的群分解方法，以实现更现实场景下的表征学习。

**Authors:** Takayuki Komatsu, Yoshiyuki Ohmura, Kayato Nishitsunoi, Yasuo Kuniyoshi
**Categories:** `cs.CV`, `cs.AI`

[**[PDF]**](https://arxiv.org/pdf/2506.04668)

#### 中文摘要 (Abstract in Chinese)

> 表征学习的主要目标是从现实世界的感官输入中获得有意义的表征，而无需监督。表征学习解释了人类发展的某些方面。已经提出了各种神经网络（NN）模型，这些模型获得了经验上良好的表征。但是，尚未建立良好表征的公式。我们最近提出了一种对一对感官输入之间的变化进行分类的方法。该方法的独特之处在于，学习两个感官输入之间的转换以满足代数结构约束。传统的表征学习通常假设解缠结的独立特征轴是一个好的表征。但是，我们发现这种表征无法解释条件独立性。为了克服这个问题，我们提出了一种在伽罗瓦代数理论中使用群分解的新方法。虽然该方法有希望定义一种更通用的表征，但它假设没有特征提取的像素到像素的转换，并且只能处理没有背景的低分辨率图像，这阻碍了实际应用。在这项研究中，我们提供了一种简单的方法，通过结合特征提取和对象分割，将我们的群分解理论应用于更现实的场景。我们用特征转换代替像素转换，并将对象分割公式化为同一转换下的分组特征。我们在包含真实世界对象和背景的实际数据集上验证了所提出的方法。我们相信我们的模型将使人们更好地理解现实世界中物体识别的人类发展。

---

### [Interpretable Few-Shot Image Classification via Prototypical Concept-Guided Mixture of LoRA Experts](https://arxiv.org/abs/2506.04673)

**一句话总结:** 该论文提出了一个少样本原型概念分类框架，通过参数高效适配、跨模块概念引导和多层次特征保留等策略，提高了自解释模型在数据稀缺环境下的准确性和可解释性。

**Authors:** Zhong Ji, Rongshuai Wei, Jingren Liu, Yanwei Pang, Jungong Han
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04673)

#### 中文摘要 (Abstract in Chinese)

> 自解释模型（SEM）依赖于原型概念学习（PCL）来实现更具可解释性的视觉识别过程，但它们在数据稀缺的环境中表现不佳，因为训练样本不足会导致性能下降。为了解决这个问题，我们提出了一个少样本原型概念分类（FSPCC）框架，该框架系统地缓解了低数据状态下的两个关键挑战：参数不平衡和表示不对齐。具体来说，我们的方法利用混合LoRA专家（MoLE）进行参数高效的适配，确保主干网络和PCL模块之间可训练参数的平衡分配。同时，跨模块概念引导加强了主干网络的特征表示和原型概念激活模式之间的紧密对齐。此外，我们还结合了多层次特征保留策略，融合了跨越各个层的空间和语义线索，从而丰富了学习到的表示，并减轻了有限数据可用性带来的挑战。最后，为了提高可解释性并最小化概念重叠，我们引入了几何感知概念判别损失，该损失强制概念之间的正交性，从而鼓励更解耦和透明的决策边界。在六个流行的基准测试（CUB-200-2011、mini-ImageNet、CIFAR-FS、Stanford Cars、FGVC-Aircraft和DTD）上的实验结果表明，我们的方法始终优于现有的SEM，在5-way 5-shot分类中相对提高了4.2%-8.7%。这些发现强调了将概念学习与少样本自适应相结合以实现更高准确性和更清晰模型可解释性的有效性，为更透明的视觉识别系统铺平了道路。

---

### [Gen-n-Val: Agentic Image Data Generation and Validation](https://arxiv.org/abs/2506.04676)

**一句话总结:** Gen-n-Val 提出了一种新的 agentic 数据生成框架，通过结合 LD、LLM 和 VLLM 来生成高质量合成数据，从而提高目标检测和实例分割的性能。

**Authors:** Jing-En Huang, I-Sheng Fang, Tzuhsuan Huang, Chih-Yu Wang, Jun-Cheng Chen
**Categories:** `cs.CV`, `cs.AI`, `cs.LG`, `cs.MA`

[**[PDF]**](https://arxiv.org/pdf/2506.04676)

#### 中文摘要 (Abstract in Chinese)

> 最近，大型语言模型 (LLM) 和视觉大型语言模型 (VLLM) 在各种任务中作为 Agent 表现出了令人印象深刻的性能，但数据稀缺和标签噪声仍然是计算机视觉任务中的重大挑战，例如目标检测和实例分割。 解决这些问题的一个常见方案是生成合成数据。 然而，当前的合成数据生成方法存在诸如每个掩码多个对象、不准确的分割和不正确的类别标签等问题，限制了它们的有效性。 为了解决这些问题，我们介绍 Gen-n-Val，这是一种新型的 Agentic 数据生成框架，它利用 Layer Diffusion (LD)、LLM 和 VLLM 来生成高质量的单物体掩码和多样化的背景。 Gen-n-Val 由两个 Agent 组成：(1) LD 提示 Agent，一个 LLM，优化 LD 的提示，以生成高质量的前景实例图像和分割掩码。 这些优化的提示确保生成具有精确实例掩码和干净背景的单对象合成数据。 (2) 数据验证 Agent，一个 VLLM，它过滤掉低质量的合成实例图像。 两个 Agent 的系统提示都通过 TextGrad 进行优化。 此外，我们使用图像协调来组合场景中的多个实例。 与最先进的合成数据方法（如 MosaicFusion）相比，我们的方法将无效合成数据从 50% 减少到 7%，并在 COCO 实例分割中使用 YOLOv9c 和 YOLO11m 将稀有类别的性能提高了 1% mAP。 此外，在开放词汇目标检测基准测试中，Gen-n-Val 相比 YOLO-Worldv2-M 取得了显著的改进（7. 1% mAP，使用 YOLO11m）。 此外，Gen-n-Val 提高了 YOLOv9 和 YOLO11 系列在实例分割和目标检测方面的性能。

---

### [MARS: Radio Map Super-resolution and Reconstruction Method under Sparse Channel Measurements](https://arxiv.org/abs/2506.04682)

**一句话总结:** MARS：一种多尺度感知无线电地图超分辨率方法，结合CNN和Transformer以提高重建精度。

**Authors:** Chuyun Deng, Na Liu, Wei Xie, Lianming Xu, Li Wang
**Categories:** `cs.CV`, `eess.SP`

[**[PDF]**](https://arxiv.org/pdf/2506.04682)

#### 中文摘要 (Abstract in Chinese)

> 无线电地图反映了信号强度的空间分布，对于智慧城市、物联网和无线网络规划等应用至关重要。然而，从稀疏测量中重建精确的无线电地图仍然具有挑战性。传统的插值和修复方法缺乏环境感知，而许多深度学习方法依赖于详细的场景数据，限制了泛化能力。为了解决这个问题，我们提出了一种多尺度感知无线电地图超分辨率方法MARS，该方法结合了CNN和Transformer，具有多尺度特征融合和残差连接。MARS侧重于全局和局部特征提取，增强了不同感受野的特征表示，提高了重建精度。在不同场景和天线位置的实验表明，MARS在MSE和SSIM方面均优于基线模型，同时保持较低的计算成本，展示了强大的实际应用潜力。

---

### [HoliSafe: Holistic Safety Benchmarking and Modeling with Safety Meta Token for Vision-Language Model](https://arxiv.org/abs/2506.04704)

**一句话总结:** 本文提出了HoliSafe数据集和SafeLLaVA模型，以提高视觉-语言模型(VLM)的安全性，并在多个基准测试中取得了SOTA结果。

**Authors:** Youngwan Lee, Kangsan Kim, Kwanyong Park, Ilcahe Jung, Soojin Jang, Seanie Lee, Yong-Ju Lee, Sung Ju Hwang
**Categories:** `cs.CV`, `cs.AI`

[**[PDF]**](https://arxiv.org/pdf/2506.04704)

#### 中文摘要 (Abstract in Chinese)

> 为了提高视觉-语言模型(VLM)的安全性，目前的方法存在两个主要缺点。1)现有的安全调整数据集和基准测试仅部分考虑了图像-文本交互如何产生有害内容，通常忽略了看似良性的配对在上下文中的不安全结果。这种狭窄的覆盖范围使VLM容易受到未见过的配置中的越狱攻击。2)以往的方法主要依赖于以数据为中心的调整，而对内在加强安全性的架构创新有限。为了解决这些差距，我们引入了一个全面的安全数据集和基准测试HoliSafe，它涵盖了所有五种安全/不安全的图像-文本组合，为训练和评估提供了更强大的基础。我们进一步提出了SafeLLaVA，一种新型VLM，它通过可学习的安全元令牌和专用安全头进行增强。元令牌在训练期间编码有害的视觉线索，从本质上引导语言模型产生更安全的回应，而安全头提供可解释的有害性分类，与拒绝理由相一致。实验表明，在HoliSafe上训练的SafeLLaVA在多个VLM基准测试中实现了最先进的安全性能。此外，HoliSafe基准测试本身揭示了现有模型的关键漏洞。我们希望HoliSafe和SafeLLaVA将刺激对鲁棒和可解释的VLM安全性的进一步研究，从而扩展多模态对齐的未来途径。

---

### [Line of Sight: On Linear Representations in VLLMs](https://arxiv.org/abs/2506.04706)

**一句话总结:** 该论文研究了 LLaVA-Next 模型中图像概念的表示，发现图像特征可以通过线性解码并在深层中与其他模态共享。

**Authors:** Achyuta Rajaram, Sarah Schwettmann, Jacob Andreas, Arthur Conmy
**Categories:** `cs.CV`, `cs.AI`

[**[PDF]**](https://arxiv.org/pdf/2506.04706)

#### 中文摘要 (Abstract in Chinese)

> 通过对视觉输入的嵌入进行微调，可以使语言模型具备多模态能力。但是，这种多模态模型如何在隐藏激活中表示图像呢？我们探索了 LLaVA-Next（一种流行的开源 VLLM）中图像概念的表示。我们发现残差流中存在一组通过线性可解码特征表示的各种 ImageNet 类。我们通过对模型输出执行有针对性的编辑，证明了这些特征是因果关系。为了增加所研究的线性特征的多样性，我们训练了多模态稀疏自编码器 (SAE)，创建了一个高度可解释的文本和图像特征字典。我们发现，尽管跨模态的模型表示非常不相交，但它们在更深的层中变得越来越共享。

---

### [Robust Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2506.04713)

**一句话总结:** 本文提出了一种名为 SRAPF 的方法，通过分阶段的检索增强和对抗性部分微调，提升了视觉语言模型在 ImageNet OOD 基准测试上的同分布和分布外泛化性能。

**Authors:** Hanxin Wang, Tian Liu, Shu Kong
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04713)

#### 中文摘要 (Abstract in Chinese)

> 当预训练的 VLM 仅用少量标记的例子进行调整时，就能在下游任务上实现强大的性能。由于调整后的模型不可避免地会遇到与同分布 (ID) 特定任务训练数据不同的分布外 (OOD) 测试数据，因此增强少样本调整中的 OOD 泛化能力至关重要。本文研究了鲁棒的少样本 VLM 调整，旨在提高 ID 和 OOD 准确率。通过比较不同的调整方法（例如，提示调整、线性探测、对比微调和完全微调），我们发现了三个关键发现：(1) 使用适当的超参数进行微调明显优于流行的 VLM 调整方法，即提示调整和线性探测；(2) 仅微调视觉编码器比对比微调视觉和文本编码器更有效、更准确；(3) 微调视觉编码器的顶层可以在 ID 和 OOD 准确率之间实现最佳平衡。在这些发现的基础上，我们提出了一种基于两种简单增强技术的部分视觉编码器微调方法：(1) 检索增强，从 VLM 的预训练数据集中检索任务相关数据以增强调整，以及 (2) 对抗扰动，在微调期间提高鲁棒性。结果表明，前者/后者提高了 OOD/ID 准确率，同时略微牺牲了 ID/OOD 准确率。然而，也许可以理解的是，天真地将两者结合起来并不能保持它们最佳的 OOD/ID 准确率。我们通过开发的 SRAPF（基于阶段式检索增强的对抗性部分微调）解决了这一难题。SRAPF 包括两个阶段：(1) 使用 ID 和检索数据部分微调视觉编码器，以及 (2) 使用少量 ID 数据进行对抗性部分微调。大量实验表明，SRAPF 在 ImageNet OOD 基准测试上实现了最先进的 ID 和 OOD 准确率。

---

### [Towards Holistic Visual Quality Assessment of AI-Generated Videos: A LLM-Based Multi-Dimensional Evaluation Model](https://arxiv.org/abs/2506.04715)

**一句话总结:** 该论文提出了一种基于多维度分解和大型语言模型（LLM）的AI生成视频质量评估方法，并在NTIRE 2025挑战赛中取得了优异成绩。

**Authors:** Zelu Qi, Ping Shi, Chaoyang Zhang, Shuqi Wang, Fei Zhao, Da Pan, Zefeng Ying
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04715)

#### 中文摘要 (Abstract in Chinese)

> 近年来，AI生成视频（AIGV）技术发展显著，极大地改变了视频内容生产的模式。然而，AIGV仍然存在明显的视觉质量缺陷，例如噪声、模糊、帧抖动和动态度低，严重影响用户观看体验。因此，有效的自动视觉质量评估对于AIGV内容监管和生成模型改进至关重要。在这项工作中，我们将AIGV的视觉质量分解为三个维度：技术质量、运动质量和视频语义。对于每个维度，我们设计了相应的编码器以实现有效的特征表示。此外，考虑到大型语言模型（LLM）在各种视觉和语言任务中的出色表现，我们引入LLM作为质量回归模块。为了更好地使LLM能够建立多维特征与视觉质量之间的推理关联，我们提出了专门设计的多模态提示工程框架。此外，我们在训练阶段采用了LoRA微调技术，使LLM能够更好地适应特定任务。我们提出的方法在NTIRE 2025 AI生成内容质量评估挑战赛：AI生成视频赛道2中获得第二名，证明了其有效性。代码可在https://github.com/QiZelu/AIGVEval获取。

---

### [Learning dissection trajectories from expert surgical videos via imitation learning with equivariant diffusion](https://arxiv.org/abs/2506.04716)

**一句话总结:** 提出了一种基于隐式扩散策略和等变表示的模仿学习方法 (iDPOE)，用于预测内镜黏膜下剥离术 (ESD) 中的解剖轨迹。

**Authors:** Hongyu Wang, Yonghao Long, Yueyao Chen, Hon-Chi Yip, Markus Scheppach, Philip Wai-Yan Chiu, Yeung Yam, Helen Mei-Ling Meng, Qi Dou
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04716)

#### 中文摘要 (Abstract in Chinese)

> 内镜黏膜下剥离术 (ESD) 是一种成熟的去除上皮病变的技术。预测 ESD 视频中的解剖轨迹对于加强手术技能培训和简化学习过程具有巨大的潜力，但该领域仍有待探索。虽然模仿学习在从专家演示中获取技能方面显示出希望，但在处理不确定的未来运动、学习几何对称性和推广到不同的手术场景方面仍然存在挑战。为了解决这些问题，我们引入了一种新方法：具有等变表示的隐式扩散策略，用于模仿学习 (iDPOE)。我们的方法通过联合状态动作分布对专家行为进行建模，捕获解剖轨迹的随机性质，并实现跨各种内镜视图的鲁棒视觉表示学习。通过将扩散模型整合到策略学习中，iDPOE 确保了高效的训练和采样，从而实现更准确的预测和更好的泛化。此外，我们通过将等变性嵌入到学习过程中，增强了模型泛化到几何对称性的能力。为了解决状态不匹配问题，我们开发了一种前向过程引导的动作推理策略，用于条件采样。使用包含近 2000 个剪辑的 ESD 视频数据集，实验结果表明，我们的方法在轨迹预测方面超越了最先进的方法，包括显式和隐式方法。据我们所知，这是模仿学习在解剖轨迹预测的手术技能开发中的首次应用。

---

### [Using In-Context Learning for Automatic Defect Labelling of Display Manufacturing Data](https://arxiv.org/abs/2506.04717)

**一句话总结:** 本文提出了一种AI辅助自动标注系统，用于显示面板缺陷检测，通过增强SegGPT架构和引入涂鸦注释机制，实现了与人工标注数据相匹配的性能，从而减少了人工标注工作。

**Authors:** Babar Hussain, Qiang Liu, Gang Chen, Bihai She, Dahai Yu
**Categories:** `cs.CV`, `cs.AI`, `cs.LG`

[**[PDF]**](https://arxiv.org/pdf/2506.04717)

#### 中文摘要 (Abstract in Chinese)

> 本文介绍了一种用于显示面板缺陷检测的AI辅助自动标注系统，该系统利用了上下文学习能力。我们采用并通过几种特定领域的训练技术增强了SegGPT架构，并引入了一种基于涂鸦的注释机制来简化标注过程。我们的两阶段训练方法在工业显示面板数据集上得到了验证，与基线模型相比，在多个产品类型上实现了平均IoU增加0.22和召回率提高14％的显着改进，同时保持了大约60％的自动标注覆盖率。实验结果表明，在我们的自动标注数据上训练的模型与在人工标注数据上训练的模型性能相匹配，为减少工业检测系统中的人工标注工作提供了一个实用的解决方案。

---

### [Bridging Annotation Gaps: Transferring Labels to Align Object Detection Datasets](https://arxiv.org/abs/2506.04737)

**一句话总结:** LAT 提出了一种标签对齐转移框架，通过伪标签和特征融合，实现了跨异构数据集的标签空间对齐，从而提升了目标域检测性能。

**Authors:** Mikhail Kennerley, Angelica Alives-Reviro, Carola-Bibiane Schönlieb, Robby T. Tan
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04737)

#### 中文摘要 (Abstract in Chinese)

> 组合多个目标检测数据集可以提高泛化能力，但受到类语义和边界框注释不一致的阻碍。一些解决此问题的方法假设共享标签分类法并仅解决空间不一致问题；其他方法需要手动重新标记，或生成统一的标签空间，当需要固定的目标标签空间时，这可能不合适。我们提出了标签对齐转移 (LAT)，这是一个标签转移框架，可系统地将来自不同源数据集的注释投影到目标数据集的标签空间中。LAT 首先训练特定于数据集的检测器以生成伪标签，然后通过特权提案生成器 (PPG) 将其与地面实况注释相结合，该生成器取代了两阶段检测器中的区域提案网络。为了进一步细化区域特征，语义特征融合 (SFF) 模块使用置信度加权注意力机制从重叠提案中注入类感知上下文和特征。该管道保留了特定于数据集的注释粒度，同时实现了跨异构数据集的多对一标签空间转移，从而产生了适用于训练下游检测器的语义和空间对齐表示。因此，LAT 共同解决了类级别的不对齐和边界框不一致问题，而无需依赖共享标签空间或手动注释。在多个基准测试中，LAT 证明了目标域检测性能的持续改进，与半监督基线相比，增益高达 +4.8AP。

---

### [SRD: Reinforcement-Learned Semantic Perturbation for Backdoor Defense in VLMs](https://arxiv.org/abs/2506.04743)

**一句话总结:** 提出了一种针对视觉-语言模型后门攻击的语义奖励防御框架，可在降低攻击成功率的同时保持模型性能。

**Authors:** Shuhan Xu, Siyuan Liang, Hongling Zheng, Yong Luo, Aishan Liu, Dacheng Tao
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04743)

#### 中文摘要 (Abstract in Chinese)

> 视觉-语言模型（VLM）在图像描述任务中取得了显著的性能，但最近的研究表明它们容易受到后门攻击。攻击者可以将不易察觉的扰动（例如局部像素触发器或全局语义短语）注入到训练数据中，导致模型为特定输入生成恶意、受攻击者控制的描述。由于这些攻击的隐蔽性和跨模态性质，因此难以检测和防御。通过分析攻击样本，我们发现了两个关键漏洞：（1）对特定图像区域的异常注意力集中，以及（2）生成的描述中的语义漂移和不连贯。为了应对这种情况，我们提出了一种语义奖励防御（SRD）方法，这是一个强化学习框架，可以在没有触发器先验知识的情况下减轻后门行为。SRD使用深度Q网络来学习策略，以对敏感图像区域应用离散扰动（例如，遮挡、颜色掩蔽），旨在破坏恶意路径的激活。我们设计了一个语义保真度评分作为奖励信号，该信号共同评估输出的语义一致性和语言流畅性，从而引导agent生成稳健而忠实的描述。在主流VLM和数据集上进行的实验表明，SRD将攻击成功率降低至5.6％，同时在干净输入上保持了描述质量，性能下降不到10％。SRD提供了一种触发器无关、可解释的防御范例，可以防御多模态生成模型中隐蔽的后门威胁。

---

### [Physics Informed Capsule Enhanced Variational AutoEncoder for Underwater Image Enhancement](https://arxiv.org/abs/2506.04753)

**一句话总结:** 该论文提出了一种双流架构，通过结合物理模型和胶囊聚类，实现了最先进的水下图像增强。

**Authors:** Niki Martinel, Rita Pucci
**Categories:** `cs.CV`, `cs.AI`, `eess.IV`

[**[PDF]**](https://arxiv.org/pdf/2506.04753)

#### 中文摘要 (Abstract in Chinese)

> 本文提出了一种新颖的双流架构，通过将Jaffe-McGlamery物理模型与基于胶囊聚类的特征表示学习显式结合，实现了最先进的水下图像增强。我们的方法通过专用的物理估计器同时估计透射图和空间变化背景光，同时通过并行流中的胶囊聚类提取实体级特征。这种物理引导的方法能够实现无需参数的增强，既尊重水下形成约束，又保留语义结构和精细细节。我们的方法还具有一种新颖的优化目标，可确保跨多个空间频率的物理依从性和感知质量。为了验证我们的方法，我们进行了跨六个具有挑战性的基准的广泛实验。结果表明，与现有最佳方法相比，PSNR持续提高+0.5dB，而计算复杂度（FLOP）仅为其三分之一；或者，与具有相似计算预算的方法相比，PSNR提高超过+1dB。

---

### [Truth in the Few: High-Value Data Selection for Efficient Multi-Modal Reasoning](https://arxiv.org/abs/2506.04755)

**一句话总结:** RAP方法通过选择认知样本，仅用少量数据就实现了优越的多模态推理性能，并显著降低了计算成本。

**Authors:** Shenshen Li, Kaiyuan Deng, Lei Wang, Hao Yang, Chong Peng, Peng Yan, Fumin Shen, Heng Tao Shen, Xing Xu
**Categories:** `cs.CV`, `cs.AI`, `cs.MM`

[**[PDF]**](https://arxiv.org/pdf/2506.04755)

#### 中文摘要 (Abstract in Chinese)

> 虽然多模态大型语言模型（MLLM）通过强化学习在复杂推理任务中取得了显著进展，但人们普遍认为，大量的训练数据对于提高多模态推理能力是必要的，这不可避免地导致数据冗余和巨大的计算成本。然而，较小的高价值数据集能否在 MLLM 的多模态推理中与完整语料库相媲美或超越？在这项工作中，我们通过一个关键观察挑战了这一假设：有意义的多模态推理仅由训练样本的一个稀疏子集（称为认知样本）触发，而大多数样本的贡献微乎其微。基于这一洞察，我们提出了一个名为推理激活潜力（RAP）的新型数据选择范例，通过两个互补的估计器识别认知样本：1）基于潜在结果模型原理的因果差异估计器（CDE），通过比较多模态和仅文本输入之间的输出来消除过度依赖语言先验的样本；2）注意力置信度估计器（ACE），它利用令牌级自注意力来丢弃被中间推理阶段不相关但过度强调的令牌支配的样本。此外，我们还引入了一个难度感知替换模块（DRM），用认知上具有挑战性的实例替换微不足道的实例，从而确保了强大的多模态推理的复杂性。在六个数据集上的实验表明，我们的 RAP 方法仅使用 9.3% 的训练数据，始终取得卓越的性能，同时降低了超过 43% 的计算成本。我们的代码可在 https://github.com/Leo-ssl/RAP 获取。

---

### [Toward Better SSIM Loss for Unsupervised Monocular Depth Estimation](https://arxiv.org/abs/2506.04758)

**一句话总结:** 提出了一种新的SSIM损失函数，通过加法组合亮度、对比度和结构相似性分量，提高了无监督单目深度估计的性能。

**Authors:** Yijun Cao, Fuya Luo, Yongjie Li
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04758)

#### 中文摘要 (Abstract in Chinese)

> 无监督单目深度学习通常依赖于时间上相邻图像之间的光度关系。先前的大多数工作使用平均绝对误差（MAE）和具有传统形式的结构相似性指数度量（SSIM）作为训练损失。然而，它们忽略了SSIM函数中不同组成部分以及相应的超参数对训练的影响。为了解决这些问题，这项工作提出了一种新的SSIM形式。与原始SSIM函数相比，提出的新形式使用加法而不是乘法来组合SSIM中与亮度、对比度和结构相似性相关的分量。用这种方案构建的损失函数有助于产生更平滑的梯度，并在无监督深度估计方面获得更高的性能。我们进行了广泛的实验，以确定我们新的SSIM的相对最佳参数组合。基于流行的MonoDepth方法，优化的SSIM损失函数可以显着优于KITTI-2015户外数据集上的基线。

---

### [HypeVPR: Exploring Hyperbolic Space for Perspective to Equirectangular Visual Place Recognition](https://arxiv.org/abs/2506.04764)

**一句话总结:** HypeVPR是一种新的双曲空间分层嵌入框架，通过分层特征聚合和由粗到精的搜索策略，提高了透视到等矩形视觉地点识别的速度和精度。

**Authors:** Suhan Woo, Seongwon Lee, Jinwoo Jang, Euntai Kim
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04764)

#### 中文摘要 (Abstract in Chinese)

> 本文提出了一种名为HypeVPR的在双曲空间中的分层嵌入框架，旨在解决透视到等矩形（P2E）视觉地点识别（VPR）的挑战。该方法利用全景视图捕获的视觉环境所呈现的内在分层结构，采用双曲空间表示分层特征关系并保持特征空间内的距离属性。HypeVPR采用分层特征聚合机制组织局部到全局的特征表示，并采用高效的由粗到精的搜索策略，从而在保证鲁棒匹配的同时，优化速度和精度。实验结果表明，HypeVPR在各种基准数据集上优于现有技术方法，并且显著减少了检索时间，实现了高达5倍的检索速度提升。代码和模型将在https://github.com/suhan-woo/HypeVPR.git 上发布。

---

### [Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations](https://arxiv.org/abs/2506.04789)

**一句话总结:** Object-X是一种通用的多模态对象表示框架，能够编码丰富的对象嵌入并解码回详细的几何和视觉重建。

**Authors:** Gaia Di Lorenzo, Federico Tombari, Marc Pollefeys, Daniel Barath
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04789)

#### 中文摘要 (Abstract in Chinese)

> 学习有效的对象多模态3D表示对于增强现实和机器人等众多应用至关重要。现有的方法通常依赖于特定任务的嵌入，这些嵌入是为语义理解或几何重建量身定制的。因此，这些嵌入通常无法解码为显式几何体重建，也无法在任务中重复使用。在本文中，我们提出了一种通用的多模态对象表示框架Object-X，它能够编码丰富的对象嵌入（例如，图像、点云、文本），并将它们解码回详细的几何和视觉重建。Object-X通过在3D体素网格中几何对齐捕获的模态，并学习融合体素和对象属性信息的非结构化嵌入来实现。学习到的嵌入支持基于3D高斯溅射的对象重建，同时也支持一系列下游任务，包括场景对齐、单图像3D对象重建和定位。在两个具有挑战性的真实世界数据集上的评估表明，Object-X产生了与标准3D高斯溅射相当的高保真新视角合成，同时显著提高了几何精度。此外，Object-X在场景对齐和定位方面达到了与专用方法相媲美的性能。重要的是，与传统的基于图像或点云的方法相比，我们的对象中心描述符所需的存储空间减少了3-4个数量级，这使得Object-X成为一种可扩展且高度实用的多模态3D场景表示解决方案。

---

### [LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table](https://arxiv.org/abs/2506.04790)

**一句话总结:** LotusFilter 是一个快速的后处理模块，用于多样化近似最近邻搜索（ANNS）结果。

**Authors:** Yusuke Matsui
**Categories:** `cs.CV`, `cs.IR`, `cs.LG`

[**[PDF]**](https://arxiv.org/pdf/2506.04790)

#### 中文摘要 (Abstract in Chinese)

> 近似最近邻搜索（ANNS）是 RAG 等应用必不可少的组成部分，但有时会产生过于相似的结果。在某些情况下，搜索结果应该与查询相似但又具有多样性。我们提出了 LotusFilter，这是一个用于多样化 ANNS 结果的后处理模块。我们预先计算一个 cutoff 表，用于记录彼此接近的向量。在过滤过程中，LotusFilter 贪婪地查找该表，以删除候选结果中的冗余向量。我们证明了 LotusFilter 在类似于真实 RAG 应用的设置中运行速度很快（0.02 毫秒/查询），并利用了 OpenAI embeddings 等特征。我们的代码已在 https://github.com/matsui528/lotf 公开。

---

### [SupeRANSAC: One RANSAC to Rule Them All](https://arxiv.org/abs/2506.04803)

**一句话总结:** SupeRANSAC是一个统一的RANSAC流程，它在各种视觉任务中实现了持续的高精度，并在多个数据集上优于现有技术。

**Authors:** Daniel Barath
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04803)

#### 中文摘要 (Abstract in Chinese)

> 稳健估计是计算机视觉的基石，尤其是在运动结构恢复和同步定位与地图构建等任务中。RANSAC及其变体是从受异常值污染的数据中估计几何模型（例如，单应性、相对/绝对姿势）的黄金标准。尽管RANSAC表面上很简单，但在不同问题上实现始终如一的高性能具有挑战性。虽然最近的研究通常侧重于改进特定的RANSAC组件（例如，采样、评分），但总体性能通常更多地受到给定库中“花里胡哨”（即，实现细节和特定于问题的优化）的影响。像OpenCV和PoseLib这样的流行框架表现出不同的性能，在某些任务中表现出色，但在其他任务中则落后。我们介绍了SupeRANSAC，一种新的统一RANSAC流程，并详细分析了使RANSAC有效用于特定视觉任务的技术，包括单应性、基础/本质矩阵和绝对/刚性姿势估计。SupeRANSAC旨在在这些任务中实现一致的准确性，例如，在基本矩阵估计上的平均AUC提高了6个点，从而改进了现有最佳方法。我们证明了在多个问题和数据集上相对于最新技术的显着性能改进。

---

### [MegaHan97K: A Large-Scale Dataset for Mega-Category Chinese Character Recognition with over 97K Categories](https://arxiv.org/abs/2506.04807)

**一句话总结:** MegaHan97K是一个新的超大类别汉字数据集，包含97,455个类别，旨在推进文化遗产保护和数字应用中的汉字识别技术。

**Authors:** Yuyi Zhang, Yongxin Shi, Peirong Zhang, Yixin Zhao, Zhenhua Yang, Lianwen Jin
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04807)

#### 中文摘要 (Abstract in Chinese)

> 汉字作为中国语言和文化的基础，包含极其广泛且不断扩展的类别，最新的中国GB18030-2022标准包含87,887个类别。准确识别如此庞大数量的字符，被称为超大类别识别，对于文化遗产保护和数字应用来说，是一个艰巨而至关重要的挑战。尽管光学字符识别（OCR）取得了显著进展，但由于缺乏全面的数据集，超大类别识别仍未被探索，现有最大的数据集仅包含16,151个类别。为了弥合这一关键差距，我们推出了MegaHan97K，这是一个超大类别、大规模数据集，涵盖了前所未有的97,455个汉字类别。我们的工作提供了三个主要贡献：（1）MegaHan97K是第一个完全支持最新GB18030-2022标准的数据集，提供的类别是现有数据集的至少六倍；（2）它通过其三个不同的子集：手写子集、历史子集和合成子集，有效地解决了长尾分布问题，从而在所有类别中提供平衡的样本；（3）全面的基准实验揭示了超大类别场景中的新挑战，包括增加的存储需求、形态相似的字符识别和零样本学习困难，同时也为未来的研究释放了巨大的机会。据我们所知，MetaHan97K可能是类别最多的数据集，不仅在OCR领域，而且可能在更广泛的模式识别领域。该数据集可在https://github.com/SCUT-DLVCLab/MegaHan97K获取。

---

### [Spike-TBR: a Noise Resilient Neuromorphic Event Representation](https://arxiv.org/abs/2506.04817)

**一句话总结:** Spike-TBR是一种新颖的事件编码策略，它结合了时间二进制表示和脉冲神经元，以实现更强大的噪声过滤和事件流表示。

**Authors:** Gabriele Magrini. Federico Becattini, Luca Cultrera, Lorenzo Berlincioni, Pietro Pala, Alberto Del Bimbo
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04817)

#### 中文摘要 (Abstract in Chinese)

> 事件相机相比传统相机，在时间分辨率、延迟和动态范围上更有优势。但是，如何将事件流高效转换为标准计算机视觉流程可以兼容的格式仍然是个难题，尤其是在有噪声的情况下。本文提出Spike-TBR，一种基于时间二进制表示（TBR）的事件编码策略，通过集成脉冲神经元来解决其对噪声的脆弱性。Spike-TBR结合了TBR的基于帧的优势和脉冲神经网络的噪声过滤能力，从而创建了更强大的事件流表示。我们评估了Spike-TBR的四个变体，每个变体使用不同的脉冲神经元，跨多个数据集，证明了在受噪声影响的情况下具有卓越的性能，同时提高了在干净数据上的结果。该方法弥合了基于脉冲和基于帧的处理之间的差距，为事件驱动的视觉应用提供了一种简单的抗噪声解决方案。

---

### [Fool the Stoplight: Realistic Adversarial Patch Attacks on Traffic Light Detectors](https://arxiv.org/abs/2506.04823)

**一句话总结:** 本文展示了如何使用打印的补丁成功攻击用于交通信号灯检测的CNN，实现了现实的定向红到绿标签翻转攻击。

**Authors:** Svetlana Pavlitska, Jamie Robb, Nikolai Polley, Melih Yazgan, J. Marius Zöllner
**Categories:** `cs.CV`, `cs.LG`

[**[PDF]**](https://arxiv.org/pdf/2506.04823)

#### 中文摘要 (Abstract in Chinese)

> 到目前为止，已经成功演示了对自动驾驶汽车各种基于摄像头的感知任务的真实对抗性攻击。然而，只有少数作品考虑了对交通信号灯检测器的攻击。这项工作展示了如何使用打印的补丁来攻击用于交通信号灯检测的CNN。我们提出了一种威胁模型，其中每个交通信号灯实例都受到放置在其下方的补丁的攻击，并描述了一种训练策略。我们证明了通用设置中成功的对抗性补丁攻击。我们的实验表明了现实的定向红到绿标签翻转攻击和对象形图分类的攻击。最后，我们使用打印的补丁进行了真实世界的评估，并在实验室设置中使用移动交通信号灯（用于建筑工地）和在测试区域中使用固定交通信号灯进行了攻击。我们的代码可在 https://github.com/KASTEL-MobilityLab/attacks-on-traffic-light-detection 获取。

---

### [DualX-VSR: Dual Axial Spatial$\times$Temporal Transformer for Real-World Video Super-Resolution without Motion Compensation](https://arxiv.org/abs/2506.04830)

**一句话总结:** DualX-VSR提出了一种双轴空间时间注意力机制，用于真实世界视频超分辨率，无需运动补偿，实现了高保真和卓越性能。

**Authors:** Shuo Cao, Yihao Liu, Xiaohui Li. Yuanting Gao. Yu Zhou, Chao Dong
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04830)

#### 中文摘要 (Abstract in Chinese)

> 基于Transformer的模型，如ViViT和TimeSformer，通过有效地建模时空依赖关系，推动了视频理解的发展。Sora和Vidu等最新的视频生成模型进一步突出了transformer在长程特征提取和整体时空建模方面的能力。然而，直接将这些模型应用于真实的视频超分辨率(VSR)具有挑战性，因为VSR需要像素级的精度，而tokenization和连续注意机制可能会降低这种精度。虽然最近的基于transformer的VSR模型试图使用更小的补丁和局部注意来解决这些问题，但它们仍然面临着诸如限制的接受域和依赖于基于光流的对齐等限制，这可能会在实际环境中引入不准确性。为了克服这些问题，我们提出了用于真实世界视频超分辨率的双轴空间时间Transformer (DualX-VSR)，它引入了一种新的双轴空间时间注意机制，该机制沿正交方向整合空间和时间信息。DualX-VSR消除了对运动补偿的需求，提供了一个简化的结构，提供了时空信息的一个有凝聚力的表示。因此，DualX-VSR在真实世界的VSR任务中实现了高保真度和卓越的性能。

---

### [OpenMaskDINO3D : Reasoning 3D Segmentation via Large Language Model](https://arxiv.org/abs/2506.04837)

**一句话总结:** OpenMaskDINO3D是一个用于3D理解和分割的大型语言模型，它通过处理点云数据和文本提示来生成精确的实例分割mask。

**Authors:** Kunshen Zhang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04837)

#### 中文摘要 (Abstract in Chinese)

> 尽管感知系统近年来取得了显著进展，特别是在2D推理分割方面，但这些系统仍然依赖于明确的人工指令或预定义的类别来识别目标对象，然后才能执行视觉识别任务。这类系统已经非常成熟，能够推理和理解二维语境中隐含的用户意图，并根据复杂和隐含的查询文本生成精确的分割mask。然而，目前仍然缺乏一个可比的3D推理分割框架和结构。本文介绍了OpenMaskDINO3D，一个专为全面3D理解和分割而设计的LLM。OpenMaskDINO3D处理点云数据和文本提示，以生成实例分割mask，并在许多3D任务中表现出色。通过引入SEG token和对象标识符，我们实现了高精度的3D分割mask生成，使模型能够直接从自然语言指令中生成精确的点云分割结果。在大型ScanNet数据集上的实验结果验证了我们的OpenMaskDINO3D在各种任务中的有效性。

---

### [Geological Field Restoration through the Lens of Image Inpainting](https://arxiv.org/abs/2506.04869)

**一句话总结:** 该论文提出了一种基于张量补全的重建多维地质场方法，该方法在重建精度上优于普通克里金法。

**Authors:** Vladislav Trifonov, Ivan Oseledets, Ekaterina Muravleva
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04869)

#### 中文摘要 (Abstract in Chinese)

> 我们提出了一种从稀疏观测中重建多维地质场的新视角。借鉴确定性图像修复技术，我们将部分观测到的空间场建模为多维张量，并通过强制执行全局低秩结构来恢复缺失值。我们的方法结合了张量补全和地统计学的思想，提供了一个鲁棒的优化框架。在合成地质场上的实验表明，所使用的张量补全方法在各种观测数据百分比下，重建精度均优于普通克里金法。

---

### [Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking](https://arxiv.org/abs/2506.04879)

**一句话总结:** 本文提出了一种新的后门攻击框架，该框架通过在图像编辑过程中嵌入隐形水印触发器来实现对扩散模型的攻击。

**Authors:** Yu-Feng Chen, Tzuhsuan Huang, Pin-Yen Chiu, Jun-Cheng Chen
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04879)

#### 中文摘要 (Abstract in Chinese)

> 扩散模型在图像生成和编辑方面取得了显著进展。然而，最近的研究表明，它们容易受到后门攻击，即输入中嵌入的特定模式可以操纵模型的行为。目前该领域的大部分研究都集中在图像生成流程的攻击框架上，而对图像编辑中的后门攻击研究较少。在少数针对图像编辑的研究中，大多数使用可见的触发器，这种方法并不实用，因为它们会在编辑前对输入图像进行明显的修改。本文提出了一种新的攻击框架，该框架通过投毒训练数据将隐形触发器嵌入到图像编辑过程中。我们利用现成的深度水印模型将难以察觉的水印编码为后门触发器。我们的目标是使模型在接收到带有水印的输入时产生预定义的后门目标，同时根据给定的提示正常编辑干净的图像。通过对不同水印模型进行的大量实验，该方法实现了有希望的攻击成功率。此外，对水印特征在后门攻击方面的分析结果进一步支持了我们方法的有效性。代码可在https://github.com/aiiu-lab/BackdoorImageEditing获取。

---

### [Learning to Plan via Supervised Contrastive Learning and Strategic Interpolation: A Chess Case Study](https://arxiv.org/abs/2506.04892)

**一句话总结:** 该论文提出了一种基于Transformer编码器的潜在空间规划方法，用于在象棋中进行移动选择，该方法无需深度搜索即可达到较高的 Elo 等级分。

**Authors:** Andrew Hamara, Greg Hamerly, Pablo Rivas, Andrew C. Freeman
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04892)

#### 中文摘要 (Abstract in Chinese)

> 现代象棋引擎通过深度树搜索和回归评估达到超人水平，而人类棋手则依靠直觉选择候选走法，然后进行浅层搜索来验证它们。为了模拟这种直觉驱动的规划过程，我们使用监督对比学习训练了一个Transformer编码器，将棋盘状态嵌入到由位置评估构建的潜在空间中。在这个空间中，距离反映了评估相似性，并且可视化的轨迹显示了游戏状态之间的可解释转换。我们证明了移动选择可以完全在这个嵌入空间内进行，通过朝着有利区域前进，而无需依赖深度搜索。尽管仅使用6步束搜索，我们的模型仍实现了估计的2593 Elo等级分。性能随着模型大小和嵌入维度的增加而提高，这表明潜在规划可能为传统搜索提供可行的替代方案。虽然我们专注于象棋，但所提出的基于嵌入的规划方法可以推广到其他状态评估可学习的完美信息博弈中。所有源代码都可以在https://github.com/andrewhamara/SOLIS找到。

---

### [From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual Grounding in 3D Scenes](https://arxiv.org/abs/2506.04897)

**一句话总结:** 本文提出了Anywhere3D-Bench基准测试，用于评估3D场景中物体以外的指代表达定位，揭示了当前模型在空间和部件级别定位上的不足。

**Authors:** Tianxu Wang, Zhuofan Zhang, Ziyu Zhu, Yue Fan, Jing Xiong, Pengxiang Li, Xiaojian Ma, Qing Li
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04897)

#### 中文摘要 (Abstract in Chinese)

> 3D视觉定位在复杂3D场景中定位物体方面取得了显著进展。然而，对3D场景中物体以外的指代表达进行定位仍未被探索。在本文中，我们介绍了Anywhere3D-Bench，这是一个全面的3D视觉定位基准测试，包含2,632个指代表达式-3D边界框对，涵盖四种不同的定位级别：人类活动区域、物体以外的未占用空间、场景中的物体和细粒度的物体部件。我们评估了一系列最先进的3D视觉定位方法以及大型语言模型（LLM）和多模态LLM（MLLM）在Anywhere3D-Bench上的表现。实验结果表明，空间级别和部件级别的视觉定位提出了最大的挑战：空间级别任务需要更全面的空间推理能力，例如，对3D空间内的距离和空间关系进行建模，而部件级别任务则需要对物体组成进行细粒度的感知。即使是性能最好的模型OpenAI o4-mini，在空间级别任务上也仅达到23.57%的准确率，在部件级别任务上仅达到33.94%，远低于其在区域级别和物体级别任务上的表现。这些发现强调了当前模型在理解和推理超出物体级别语义的3D场景能力方面存在严重不足。

---

### [Generating Synthetic Stereo Datasets using 3D Gaussian Splatting and Expert Knowledge Transfer](https://arxiv.org/abs/2506.04908)

**一句话总结:** 提出了一种基于3D高斯溅射的立体数据集生成流程，可用于快速微调立体匹配模型，并在零样本泛化方面表现出竞争力。

**Authors:** Filip Slezak, Magnus K. Gjerde, Joakim B. Haurum, Ivan Nikolov, Morten S. Laursen, Thomas B. Moeslund
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04908)

#### 中文摘要 (Abstract in Chinese)

> 本文介绍了一种基于3D高斯溅射（3DGS）的立体数据集生成流程，为基于神经辐射场（NeRF）的方法提供了一种有效的替代方案。为了获得有用的几何估计，我们探索了利用来自显式3D表示的重建几何以及来自FoundationStereo模型的深度估计，在一个专家知识转移设置中。我们发现，当在3DGS生成的数据集上微调立体模型时，我们在零样本泛化基准测试中表现出有竞争力的性能。当直接使用重建的几何体时，我们观察到它通常是嘈杂的并且包含伪影，这会将噪声传播到训练后的模型。相比之下，我们发现来自FoundationStereo的视差估计更清晰，因此在零样本泛化基准测试中产生了更好的性能。我们的方法突出了低成本、高保真数据集创建和深度立体模型快速微调的潜力。此外，我们还揭示了虽然最新的基于高斯溅射的方法在已建立的基准测试中取得了优异的性能，但它们的鲁棒性在具有挑战性的实际环境中有所欠缺，需要进一步探索。

---

### [Light and 3D: a methodological exploration of digitisation techniques adapted to a selection of objects from the Mus{é}e d'Arch{é}ologie Nationale](https://arxiv.org/abs/2506.04925)

**一句话总结:** 针对不同文物，应结合其特性和数字孪生用途，选择最合适的3D数字化方法。

**Authors:** Antoine Laurent, Jean Mélou, Catherine Schwab, Rolande Simon-Millot, Sophie Féret, Thomas Sagory, Carole Fritz, Jean-Denis Durou
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04925)

#### 中文摘要 (Abstract in Chinese)

> 数字化遗产物品的需求已被广泛接受。本文介绍了当下非常流行的“数字孪生”概念。文章展示了多种摄影3D数字化方法，但这并非其唯一目的。通过对国立考古博物馆藏品中的一些物品进行分析，文章表明没有单一的方法适用于所有情况。因此，针对特定文物，应该由遗产领域专家和数字领域专家共同协商，选择最合适的数字化方法，因为每件新的物品可能都需要对现有工具进行调整。试图对3D数字化方法进行绝对分类是毫无意义的。相反，我们需要找到最适合每个物体的数字工具，不仅要考虑其特性，还要考虑其数字孪生的未来用途。

---

### [CzechLynx: A Dataset for Individual Identification and Pose Estimation of the Eurasian Lynx](https://arxiv.org/abs/2506.04931)

**一句话总结:** CzechLynx是首个大规模开放获取的欧亚猞猁个体识别、2D姿势估计和实例分割数据集。

**Authors:** Lukas Picek, Elisa Belotti, Michal Bojda, Ludek Bufka, Vojtech Cermak, Martin Dula, Rostislav Dvorak, Luboslav Hrdy, Miroslav Jirik, Vaclav Kocourek, Josefa Krausova, Jirı Labuda, Jakub Straka, Ludek Toman, Vlado Trulık, Martin Vana, Miroslav Kutal
**Categories:** `cs.CV`, `cs.AI`

[**[PDF]**](https://arxiv.org/pdf/2506.04931)

#### 中文摘要 (Abstract in Chinese)

> 我们推出了 CzechLynx，这是首个大规模、开放获取的欧亚猞猁个体识别、2D 姿势估计和实例分割数据集。CzechLynx 包含超过 3 万张相机陷阱图像，这些图像带有分割掩码、身份标签和 20 点骨骼注释，涵盖了两个地理区域 15 年系统监测中的 219 个独特个体：西南波西米亚和西喀尔巴阡山脉。为了增加数据可变性，我们创建了一个补充合成集，其中包含超过 10 万张通过基于 Unity 的管道和扩散驱动的文本到纹理建模生成的逼真图像，涵盖了不同的环境、姿势和毛皮图案变化。为了允许测试跨空间和时间领域的泛化，我们定义了三个量身定制的评估协议/分割：（i）地理感知，（ii）时间感知开放集，以及（iii）时间感知封闭集。

---

### [Time-Lapse Video-Based Embryo Grading via Complementary Spatial-Temporal Pattern Mining](https://arxiv.org/abs/2506.04950)

**一句话总结:** 该论文提出了一个基于完整延时监控视频，预测胚胎学家整体质量评估的互补时空模式挖掘（CoSTeM）框架，用于人工智能辅助胚胎选择。

**Authors:** Yong Sun, Yipeng Wang, Junyu Shi, Zhiyuan Zhang, Yanmei Xiao, Lei Zhu, Manxi Jiang, Qiang Nie
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04950)

#### 中文摘要 (Abstract in Chinese)

> 人工智能最近在用于体外受精（IVF）的自动胚胎选择方面显示出了希望。然而，目前的方法要么只进行部分胚胎评估，缺乏整体质量评估，要么针对不可避免地受到胚胎外因素影响的临床结果，这两者都限制了临床应用。为了弥合这一差距，我们提出了一个新的任务，称为基于视频的胚胎分级——这是第一个直接利用完整延时监控（TLM）视频来预测胚胎学家整体质量评估的范例。为了支持这项任务，我们整理了一个真实的临床数据集，包含超过2500个TLM视频，每个视频都标有一个分级标签，表明胚胎的整体质量。基于临床决策原则，我们提出了一个互补的时空模式挖掘（CoSTeM）框架，该框架在概念上复制了胚胎学家的评估过程。CoSTeM包括两个分支：（1）形态学分支，使用混合交叉注意专家层和时间选择块来选择区分性的局部结构特征，以及（2）一个使用时间Transformer来建模全局发育轨迹的形态动力学分支，协同整合静态和动态决定因素来对胚胎进行分级。大量的实验结果证明了我们设计的优越性。这项工作为人工智能辅助胚胎选择提供了一个有价值的方法框架。数据集和源代码将在接受后公开。

---

### [Robustness as Architecture: Designing IQA Models to Withstand Adversarial Perturbations](https://arxiv.org/abs/2506.04951)

**一句话总结:** 提出了一种鲁棒的图像质量评估（IQA）架构，通过设计而非数据训练来抵抗对抗攻击。

**Authors:** Igor Meleshin, Anna Chistyakova, Anastasia Antsiferova, Dmitriy Vatolin
**Categories:** `cs.CV`, `cs.AI`

[**[PDF]**](https://arxiv.org/pdf/2506.04951)

#### 中文摘要 (Abstract in Chinese)

> 图像质量评估（IQA）模型越来越多地被依赖于评估真实世界系统中的图像质量，例如压缩、增强、生成和流媒体。然而，它们的采用带来了一个根本性的风险：这些模型本质上是不稳定的。对抗性操作很容易欺骗它们，夸大分数并破坏信任。传统上，这种漏洞是通过数据驱动的防御来解决的——对抗性重训练、正则化或输入净化。但如果这是一个错误的视角呢？如果感知模型中的鲁棒性不是需要学习的东西，而是需要设计的东西呢？在这项工作中，我们提出了一个具有启发性的想法：鲁棒性作为一种架构先验。与其训练模型来抵抗扰动，不如重塑其内部结构，从根本上抑制敏感性。我们通过强制正交信息流来实现这一点，将网络限制为保持范数的操作——并通过剪枝和微调进一步稳定系统。结果是一种鲁棒的IQA架构，它可以在不需要对抗训练或对原始模型进行重大更改的情况下抵抗对抗攻击。这种方法提出了一种视角的转变：从通过数据优化鲁棒性到通过设计来实现鲁棒性。

---

### [APVR: Hour-Level Long Video Understanding with Adaptive Pivot Visual Information Retrieval](https://arxiv.org/abs/2506.04953)

**一句话总结:** APVR通过分层视觉信息检索，有效提升了多模态大语言模型对小时级长视频的理解能力。

**Authors:** Hong Gao, Yiming Bao, Xuezhan Tu, Bin Zhong, Minling Zhang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04953)

#### 中文摘要 (Abstract in Chinese)

> 当前的基于视频的多模态大型语言模型在理解小时级视频时面临挑战，这是由于计算资源的限制以及从大量时间序列中提取信息的低效率所致。我们提出了自适应关键视觉信息检索（APVR），这是一个无需训练的框架，通过分层视觉信息检索来解决内存墙的限制。APVR通过两个互补的组件运行：关键帧检索采用语义扩展和多模态置信度评分来识别语义相关的视频帧，而关键Token检索在关键帧内执行查询感知的、由注意力驱动的Token选择。这种双重粒度的方法能够处理长达数小时的视频，同时保持语义的准确性。在LongVideoBench和VideoMME上的实验验证表明，APVR 显著提高了性能，不仅为免训练方法，也为基于训练的方法建立了最先进的结果，同时提供了与现有MLLM架构的即插即用集成能力。

---

### [FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video Generation](https://arxiv.org/abs/2506.04956)

**一句话总结:** FEAT提出了一种全维度高效注意力Transformer，用于高质量动态医疗视频合成，并在效率和性能上均优于现有方法。

**Authors:** Huihan Wang, Zhiwen Yang, Hui Zhang, Dan Zhao, Bingzheng Wei, Yan Xu
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04956)

#### 中文摘要 (Abstract in Chinese)

> 由于需要对空间一致性和时间动态性进行建模，因此合成高质量的动态医疗视频仍然是一个巨大的挑战。现有的基于Transformer的方法面临着关键的限制，包括通道交互不足、自注意力机制带来的高计算复杂性，以及在处理不同噪声水平时，来自时间步嵌入的粗糙的去噪指导。在这项工作中，我们提出了FEAT，一种全维度高效注意力Transformer，它通过三个关键创新来解决这些问题：（1）一个统一的范式，具有连续的空间-时间-通道注意力机制，以捕获所有维度上的全局依赖关系；（2）每个维度上注意力机制的线性复杂度设计，利用加权键值注意力和全局通道注意力；（3）一个残差值引导模块，提供细粒度的像素级引导，以适应不同的噪声水平。我们在标准基准和下游任务上评估了FEAT，结果表明，FEAT-S仅用最先进模型Endora的23%的参数，就实现了相当甚至更优越的性能。此外，FEAT-L在多个数据集上超越了所有比较方法，展示了卓越的有效性和可扩展性。代码可在https://github.com/Yaziwel/FEAT 获取。

---

### [Bringing SAM to new heights: Leveraging elevation data for tree crown segmentation from drone imagery](https://arxiv.org/abs/2506.04970)

**一句话总结:** 本文提出了BalSAM模型，通过整合SAM和DSM信息，提升了树冠实例分割的性能，尤其在种植园环境中表现突出。

**Authors:** Mélisande Teng, Arthur Ouaknine, Etienne Laliberté, Yoshua Bengio, David Rolnick, Hugo Larochelle
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04970)

#### 中文摘要 (Abstract in Chinese)

> 单株树木的信息对于监测森林生态系统和规划森林管理至关重要。目前的监测方法涉及地面测量，需要大量的成本、时间和劳动力。无人机遥感和计算机视觉的进步为从航空影像中大规模绘制单株树木提供了巨大的潜力。大型预训练视觉模型，如Segment Anything Model (SAM)，在标记数据有限的情况下，代表了一个特别引人注目的选择。在这项工作中，我们比较了利用SAM在高分辨率无人机影像中进行自动树冠实例分割的方法，应用于三种用例：1)北方种植园，2)温带森林和3)热带森林。我们还研究了将高程数据以数字表面模型(DSM)信息的形式整合到模型中，这些信息可以很容易地从RGB无人机影像中获得，而无需额外成本。我们提出了BalSAM，一个利用SAM和DSM信息的模型，该模型显示出优于其他方法的潜力，特别是在种植园环境中。我们发现，即使使用精心设计的提示，直接使用SAM的方法也无法胜过定制的Mask R-CNN。然而，高效地端到端调整SAM并整合DSM信息是树冠实例分割模型有希望的发展方向。

---

### [TextVidBench: A Benchmark for Long Video Scene Text Understanding](https://arxiv.org/abs/2506.04983)

**一句话总结:** TextVidBench是一个新的长视频文本问答基准测试，它挑战了现有模型，并提出了一种有效的方法来提高长视频场景文本理解能力。

**Authors:** Yangyang Zhong, Ji Qi, Yuan Yao, Pengxin Luo, Yunfeng Yan, Donglian Qi, Zhiyuan Liu, Tat-Seng Chua
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04983)

#### 中文摘要 (Abstract in Chinese)

> 尽管短视频文本视觉问答（ViteVQA）任务取得了进展，但现有数据集在视频时长和评估范围上存在局限性，难以充分评估多模态大型语言模型的能力。为了解决这些限制，我们推出了TextVidBench，这是第一个专门为长视频文本问答（>3分钟）设计的基准测试。TextVidBench做出了三个关键贡献：1）跨领域长视频覆盖：涵盖9个类别（例如，新闻、体育、游戏），平均视频长度为2306秒，能够更真实地评估长视频理解。2）三阶段评估框架：“文本大海捞针 -> 时间定位 -> 文本动态字幕”。3）高质量的细粒度注释：包含超过5,000个问答对，并带有详细的语义标签。此外，我们提出了一种有效的范例，通过以下方式改进大型模型：（i）引入IT-Rope机制和时间提示工程以增强时间感知，（ii）采用非均匀位置编码以更好地处理长视频序列，以及（iii）对视频文本数据进行轻量级微调。在多个公共数据集以及TextVidBench上的大量实验表明，我们的新基准测试对现有模型提出了重大挑战，而我们提出的方法为提高长视频场景文本理解能力提供了有价值的见解。

---

### [Multi-scale Image Super Resolution with a Single Auto-Regressive Model](https://arxiv.org/abs/2506.04990)

**一句话总结:** 该论文提出了一种新的视觉自回归图像超分辨率模型，该模型在性能上优于现有技术，同时减少了模型大小和对外部数据的依赖。

**Authors:** Enrique Sanchez, Isma Hadji, Adrian Bulat, Christos Tzelepis, Brais Martinez, Georgios Tzimiropoulos
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04990)

#### 中文摘要 (Abstract in Chinese)

> 本文利用视觉自回归 (VAR) 建模的最新进展来解决图像超分辨率 (ISR) 问题。VAR 迭代地估计逐渐增加的图像尺度之间潜在空间中的残差，这一过程称为下一尺度预测。因此，预训练期间学习到的强先验与下游任务 (ISR) 很好地对齐。据我们所知，到目前为止，只有 VARSR 利用了这种协同作用，显示出有希望的结果。然而，由于现有残差量化器的局限性，VARSR 只能在固定分辨率下工作，即它无法将中间输出映射到相应的图像尺度。此外，它依赖于一个 1B Transformer 架构 (VAR-d24)，并利用一个大型私有数据集来实现最先进的结果。我们通过两个新颖的组件来解决这些限制：a) 一种具有多尺度图像标记化的分层图像标记化方法，该方法以不同的尺度逐步表示图像，同时强制跨尺度的标记重叠；b) 一种直接偏好优化 (DPO) 正则化项，它仅依赖于 LR 和 HR 标记化，鼓励 Transformer 生成后者而不是前者。据我们所知，这是首次训练量化器以强制不同尺度的语义一致残差，也是首次使用基于偏好的优化来训练 VAR。使用这两个组件，我们的模型可以在单个前向传递中对 LR 图像进行去噪，并以一半和完整的目标放大因子进行超分辨率处理。此外，我们实现了图像超分辨率任务上的最先进结果，同时使用了小型模型（300M 参数 vs ~1B 参数的 VARSR），并且没有使用外部训练数据。

---

### [PATS: Proficiency-Aware Temporal Sampling for Multi-View Sports Skill Assessment](https://arxiv.org/abs/2506.04996)

**一句话总结:** PATS 是一种自适应时间采样策略，通过在连续片段中保留完整的运动模式，从而提升了自动运动技能评估的准确性。

**Authors:** Edoardo Bianchi, Antonio Liotta
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04996)

#### 中文摘要 (Abstract in Chinese)

> 自动运动技能评估需要捕捉区分专家和新手表现的基本运动模式，但当前的视频采样方法破坏了对熟练程度评估至关重要的时间连续性。为此，我们引入了熟练度感知时间采样 (PATS)，这是一种新颖的采样策略，它在连续的时间段内保留完整的运动模式，用于多视角技能评估。PATS 自适应地分割视频，以确保每个分析部分都包含关键性能组件的完整执行，并在多个片段中重复此过程，以最大限度地提高信息覆盖率，同时保持时间连贯性。在 EgoExo4D 基准上使用 SkillFormer 进行评估，PATS 在所有观看配置中均超过了当前最佳精度（+0.65% 至 +3.05%），并在具有挑战性的领域中实现了显着提升（攀岩 +26.22%，音乐 +2.39%，篮球 +1.13%）。系统分析表明，PATS 成功地适应了不同的活动特征——从动态运动的高频采样到顺序技能的细粒度分割——证明了其作为一种自适应的时间采样方法的有效性，从而推进了现实世界应用中自动技能评估。

---

### [Beyond Cropped Regions: New Benchmark and Corresponding Baseline for Chinese Scene Text Retrieval in Diverse Layouts](https://arxiv.org/abs/2506.04999)

**一句话总结:** CSTR-CLIP模型通过整合全局视觉信息和多粒度对齐训练，显著提升了中文场景文本检索在多样化布局上的性能。

**Authors:** Gengluo Li, Huawen Shen, Yu Zhou
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.04999)

#### 中文摘要 (Abstract in Chinese)

> 中文场景文本检索是一项旨在搜索包含中文查询文本的视觉实例的图像的实际任务。 这项任务极具挑战性，因为中文文本在现实场景中通常具有复杂多样的布局。 目前的努力倾向于沿用英文场景文本检索的解决方案，未能取得令人满意的性能。 在本文中，我们建立了一个多样化布局基准，用于中文街景文本检索 (DL-CSVTR)，该基准专门用于评估各种文本布局（包括垂直、交叉线和部分对齐）的检索性能。 为了解决现有方法的局限性，我们提出了中文场景文本检索 CLIP (CSTR-CLIP)，这是一种新颖的模型，它集成了全局视觉信息与多粒度对齐训练。 CSTR-CLIP 应用两阶段训练过程来克服之前的局限性，例如排除文本区域之外的视觉特征以及依赖于单粒度对齐，从而使模型能够有效地处理各种文本布局。 在现有基准测试中进行的实验表明，CSTR-CLIP 的准确率比之前的最先进模型提高了 18.82%，并且还提供了更快的推理速度。 在 DL-CSVTR 上的进一步分析证实了 CSTR-CLIP 在处理各种文本布局方面的卓越性能。 数据集和代码将公开提供，以促进中文场景文本检索的研究。

---

### [Structure-Aware Radar-Camera Depth Estimation](https://arxiv.org/abs/2506.05008)

**一句话总结:** Depth Anything 擅长提取结构信息，在零样本单目深度估计中表现出领先的性能。

**Authors:** Fuyi Zhang, Zhu Yu, Chunhao Li, Runmin Zhang, Xiaokai Bai, Zili Zhou, Si-Yuan Cao, Wang Wang, Hui-Liang Shen
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05008)

#### 中文摘要 (Abstract in Chinese)

> 单目深度估计旨在从单目相机拍摄的RGB图像中确定每个像素的深度。深度学习的发展通过促进从一些良好注释的数据集中学习深度特征，极大地推动了这一领域。Eigen等人首先引入了一个多尺度融合网络用于深度回归。此后，后续的改进来自于将回归任务重新解释为分类问题，结合额外的先验知识，以及开发更有效的目标函数。尽管取得了这些进展，但泛化到未见领域仍然是一个挑战。最近，一些方法采用了仿射不变损失来实现多数据集联合训练。其中，Depth Anything在零样本单目深度估计中表现出领先的性能。虽然由于缺乏明确的深度线索，它很难估计准确的度量深度，但它擅长从未见图像中提取结构信息，从而产生结构详细的单目深度。

---

### [Point Cloud Segmentation of Agricultural Vehicles using 3D Gaussian Splatting](https://arxiv.org/abs/2506.05009)

**一句话总结:** 该论文提出了一种新的pipeline，利用3DGS和GOF生成逼真的农业车辆合成数据，用于训练点云分割模型，且在特定情况下性能优于真实数据训练的模型。

**Authors:** Alfred T. Christiansen, Andreas H. Højrup, Morten K. Stephansen, Md Ibtihaj A. Sakib, Taman S. Poojary, Filip Slezak, Morten S. Laursen, Thomas B. Moeslund, Joakim B. Haurum
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05009)

#### 中文摘要 (Abstract in Chinese)

> 用于3D点云语义分割等任务的神经网络训练需要大量数据集，但获取和标注真实世界的点云成本高昂且劳动密集。这项工作旨在引入一种新的pipeline来生成逼真的合成数据，通过利用3D高斯溅射（3DGS）和高斯不透明度场（GOF）来生成多种不同农业车辆的3D资产，而不是使用通用模型。这些资产被放置在模拟环境中，在那里使用模拟激光雷达生成点云。这是一种灵活的方法，允许更改激光雷达规格而无需产生额外成本。我们通过仅在合成数据上训练和验证模型，评估了合成数据对PointNet++、Point Transformer V3和OACNN等分割模型的影响。值得注意的是，PTv3模型的mIoU为91.35%，这是一个值得注意的结果，因为该模型既没有在任何真实数据上进行训练，也没有在任何真实数据上进行验证。进一步的研究甚至表明，在某些情况下，仅在合成生成的数据上训练的模型比在真实世界数据上训练的模型表现更好。最后，实验表明，这些模型可以跨语义类别泛化，从而能够对它们从未训练过的网格模型进行准确的预测。

---

### [UAV4D: Dynamic Neural Rendering of Human-Centric UAV Imagery using Gaussian Splatting](https://arxiv.org/abs/2506.05011)

**一句话总结:** 提出了一种名为UAV4D的框架，用于无人机拍摄的动态真实世界场景的照片级真实感渲染，解决了单目视频中多行人动态场景重建的挑战。

**Authors:** Jaehoon Choi, Dongki Jung, Christopher Maxey, Yonghan Lee, Sungmin Eum, Dinesh Manocha, Heesung Kwon
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05011)

#### 中文摘要 (Abstract in Chinese)

> 尽管动态神经渲染取得了显著进展，但现有方法未能解决无人机拍摄场景带来的独特挑战，特别是涉及单目相机设置、俯视视角以及现有数据集中未充分表示的多个小型移动人员。在这项工作中，我们推出了UAV4D，一个为无人机捕获的动态真实世界场景实现照片级真实感渲染的框架。具体来说，我们解决了从单目视频数据重建具有多个移动行人的动态场景的挑战，而无需额外的传感器。我们结合使用3D基础模型和人体网格重建模型来重建场景背景和人体。我们提出了一种新颖的方法来解决场景尺度模糊问题，并通过识别场景中人与物体的接触点将人和场景放置在世界坐标系中。此外，我们利用SMPL模型和背景网格初始化高斯splat，从而实现整体场景渲染。我们在三个复杂的无人机拍摄数据集上评估了我们的方法：VisDrone、Manipal-UAV和Okutama-Action，每个数据集都具有不同的特征，包含10到50个人。我们的结果表明，我们的方法在新视角合成方面优于现有方法，PSNR提高了1.5 dB，并且具有卓越的视觉清晰度。

---

### [Physical Annotation for Automated Optical Inspection: A Concept for In-Situ, Pointer-Based Trainingdata Generation](https://arxiv.org/abs/2506.05026)

**一句话总结:** 本文提出了一种新型物理标注系统，通过直接在物体上进行物理交互，更高效地生成用于自动光学检测的机器学习训练数据。

**Authors:** Oliver Krumpek, Oliver Heimann, Jörg Krüger
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05026)

#### 中文摘要 (Abstract in Chinese)

> 本文介绍了一种新型物理标注系统，旨在为自动光学检测生成训练数据。该系统使用基于指针的原位交互，将训练有素的检验人员的宝贵专业知识直接转移到机器学习 (ML) 训练管道中。与传统的基于屏幕的标注方法不同，我们的系统直接在对象上捕获物理轨迹和轮廓，提供了一种更直观和高效的标注数据方式。核心技术使用经过校准的跟踪指针来准确记录用户输入，并将这些空间交互转换为与开源标注软件兼容的标准化标注格式。此外，一个简单的基于投影仪的界面将视觉引导投影到对象上，以在标注过程中协助用户，确保更高的准确性和一致性。所提出的概念弥合了人类专业知识和自动数据生成之间的差距，使非IT专家能够为ML训练管道做出贡献，并防止有价值的训练样本丢失。初步评估结果证实了捕获详细标注轨迹的可行性，并表明与CVAT的集成简化了后续ML任务的工作流程。本文详细介绍了系统架构、校准程序和界面设计，并讨论了其对未来自动光学检测ML数据生成的潜在贡献。

---

### [FlowDirector: Training-Free Flow Steering for Precise Text-to-Video Editing](https://arxiv.org/abs/2506.05046)

**一句话总结:** FlowDirector是一种无需反演的视频编辑框架，它通过在数据空间中直接演化视频并使用注意力引导的掩蔽机制来实现连贯且可控的编辑。

**Authors:** Guangzhao Li, Yanming Yang, Chenxi Song, Chi Zhang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05046)

#### 中文摘要 (Abstract in Chinese)

> 文本驱动的视频编辑旨在根据自然语言指令修改视频内容。虽然最近的免训练方法通过利用预训练的扩散模型取得了进展，但它们通常依赖于将输入视频映射到潜在空间的反演技术，这通常会导致时间不一致和结构保真度下降。为了解决这个问题，我们提出了FlowDirector，这是一种新颖的无反演视频编辑框架。我们的框架将编辑过程建模为数据空间中的直接演变，通过常微分方程（ODE）引导视频沿着其固有的时空流形平滑过渡，从而保持时间连贯性和结构细节。为了实现局部和可控的编辑，我们引入了一种注意力引导的掩蔽机制，该机制调节ODE速度场，从而在空间和时间上保留非目标区域。此外，为了解决不完整的编辑并增强与编辑指令的语义对齐，我们提出了一种受无分类器指导启发的指导增强编辑策略，该策略利用多个候选流之间的差异信号来引导编辑轨迹朝着更强的语义对齐方向发展，而不会影响结构一致性。在各个基准测试中进行的大量实验表明，FlowDirector在指令遵循度、时间一致性和背景保持方面达到了最先进的性能，为无需反演的高效连贯视频编辑建立了新范例。

---

### [A Survey on Vietnamese Document Analysis and Recognition: Challenges and Future Directions](https://arxiv.org/abs/2506.05061)

**一句话总结:** 本调查综述了越南文档识别技术，强调了现有局限性，并探讨了LLM如何革新该领域。

**Authors:** Anh Le, Thanh Lam, Dung Nguyen
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05061)

#### 中文摘要 (Abstract in Chinese)

> 越南文档分析与识别(DAR)在数字化、信息检索和自动化领域至关重要。尽管OCR和NLP取得了进展，但由于其复杂的变音符号、音调变化以及缺乏大规模带注释的数据集，越南文本识别面临着独特的挑战。传统的OCR方法通常难以处理实际文档的变化，而深度学习方法已经显示出希望，但仍然受到数据稀缺和泛化问题的限制。最近，大型语言模型(llm)和视觉语言模型在文本识别和文档理解方面取得了显著的进步，为越南DAR提供了一个新的方向。然而，领域适应、多模态学习和计算效率等挑战依然存在。本调查对越南文档识别的现有技术进行了全面的回顾，强调了关键的局限性，并探讨了llm如何彻底改变该领域。我们讨论了未来的研究方向，包括数据集开发、模型优化以及多模式方法的集成，以提高文档智能。通过解决这些差距，我们的目标是促进越南DAR的进步，并鼓励社区驱动的解决方案。

---

### [SeedEdit 3.0: Fast and High-Quality Generative Image Editing](https://arxiv.org/abs/2506.05083)

**一句话总结:** SeedEdit 3.0 通过改进的数据管理和联合学习流程，在真实图像编辑方面实现了更高的可用性。

**Authors:** Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, Jianchao Yang
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05083)

#### 中文摘要 (Abstract in Chinese)

> 我们介绍了 SeedEdit 3.0，以及我们的 T2I 模型 Seedream 3.0 [22]，它在编辑指令的遵循和真实图像输入的图像内容（例如，ID/IP）的保留方面，都比我们之前的版本 [27] 有了显著的改进。除了使用 T2I 升级模型外，在本报告中，我们还介绍了几个关键的改进。首先，我们开发了一种增强的数据管理流程，该流程具有元信息范例和元信息嵌入策略，有助于混合来自多个数据源的图像。这使我们能够有效地扩展编辑数据，并且元信息有助于更紧密地将 VLM 与扩散模型连接起来。其次，我们引入了一种用于计算扩散损失和奖励损失的联合学习流程。最后，我们在真实图像编辑的测试基准上评估了 SeedEdit 3.0，它在多个方面实现了最佳的权衡，可用性率高达 56.1%，而 SeedEdit 1.6 为 38.4%，GPT4o 为 37.1%，Gemini 2.0 为 30.3%。

---

### [Interpretable Multimodal Framework for Human-Centered Street Assessment: Integrating Visual-Language Models for Perceptual Urban Diagnostics](https://arxiv.org/abs/2506.05087)

**一句话总结:** 该研究提出了一种融合视觉和语言模型的多模态街道评估框架，以弥补传统城市分析在主观感知方面的不足。

**Authors:** HaoTian Lan
**Categories:** `cs.CV`, `cs.CL`

[**[PDF]**](https://arxiv.org/pdf/2506.05087)

#### 中文摘要 (Abstract in Chinese)

> 虽然从图像或GIS衍生的客观街道指标已成为城市分析的标准，但它们仍然不足以捕捉对包容性城市设计至关重要的主观感知。本研究介绍了一种新颖的多模态街道评估框架（MSEF），该框架融合了视觉转换器（VisualGLM-6B）和大型语言模型（GPT-4），从而能够对街景进行可解释的双重输出评估。利用来自中国哈尔滨的超过15,000张带注释的街景图像，我们使用LoRA和P-Tuning v2对框架进行微调，以实现参数高效的适应。该模型在客观特征上达到了0.84的F1分数，与汇总的居民感知达成89.3%的一致性，并在分层的社会经济地理区域进行了验证。除了分类准确性之外，MSEF还捕捉到了情境相关的矛盾：例如，非正规商业提高了感知活力，同时降低了行人舒适度。它还识别出非线性和语义相关的模式——例如建筑透明度在住宅区和商业区中不同的感知效果——揭示了通用空间启发法的局限性。通过生成基于注意力机制的自然语言理由，该框架将感官数据与社会情感推理联系起来，从而实现与SDG 11相一致的透明诊断。这项工作为城市感知建模提供了方法论创新，并为寻求协调基础设施精确性与生活经验的规划系统提供了实用性。

---

### [FG 2025 TrustFAA: the First Workshop on Towards Trustworthy Facial Affect Analysis: Advancing Insights of Fairness, Explainability, and Safety (TrustFAA)](https://arxiv.org/abs/2506.05095)

**一句话总结:** 本次TrustFAA研讨会旨在汇集研究人员，共同探讨面部情感分析中可信度相关的挑战，以支持FG2025在伦理方面的努力。

**Authors:** Jiaee Cheong, Yang Liu, Harold Soh, Hatice Gunes
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05095)

#### 中文摘要 (Abstract in Chinese)

> 随着情感人工智能驱动的面部情感分析（FAA）工具日益普及和部署，人们对这些系统的信任度越来越关注。首届“迈向可信的面部情感分析：提升公平性、可解释性和安全性洞察（TrustFAA）”研讨会旨在汇集研究人员，他们正在研究与可信度相关的各种挑战，如可解释性、不确定性、偏差和隐私——涵盖各种面部情感分析任务，包括宏/微表情识别、面部动作单元检测，以及疼痛和抑郁检测等其他相应应用，以及人机交互与协作。为了与FG2025对伦理的重视保持一致（今年的投稿要求包含伦理影响声明），本次研讨会通过鼓励对面部情感分析可信度的研究、讨论和对话，来支持FG2025的各项工作。

---

### [Astraea: A GPU-Oriented Token-wise Acceleration Framework for Video Diffusion Transformers](https://arxiv.org/abs/2506.05096)

**一句话总结:** ASTRAEA通过轻量级token选择和稀疏注意力机制，显著加速了视频扩散Transformer的推理过程，同时保持了视频质量。

**Authors:** Haosong Liu, Yuge Cheng, Zihan Liu, Aiyue Chen, Yiwu Yao, Chen Chen, Jingwen Leng, Yu Feng, Minyi Guo
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05096)

#### 中文摘要 (Abstract in Chinese)

> 视频扩散Transformer（vDiT）在文本到视频生成方面取得了显著进展，但它们的高计算需求给实际部署带来了重大挑战。虽然现有的加速方法可以在不同的粒度上减少工作量，但它们通常依赖于启发式方法，限制了它们的适用性。我们介绍了一种名为ASTRAEA的自动框架，该框架搜索基于vDiT的视频生成的近优配置。ASTRAEA的核心是提出了一种轻量级的token选择机制和一种内存高效的GPU并行稀疏注意力策略，从而能够在执行时间上实现线性缩减，同时对生成质量的影响最小。为了确定不同时间步长的最佳token缩减，我们进一步设计了一个搜索框架，该框架利用经典的进化算法来有效地确定token预算的分布。总之，ASTRAEA在单个GPU上实现了高达2.4倍的推理加速，并且具有良好的可扩展性（在8个GPU上实现了高达13.2倍的加速），同时与最先进的方法相比，保持了更好的视频质量（与基线vDiT模型相比，VBench得分损失<0.5%）。

---

### [DIMCIM: A Quantitative Evaluation Framework for Default-mode Diversity and Generalization in Text-to-Image Generative Models](https://arxiv.org/abs/2506.05108)

**一句话总结:** 该论文提出了 DIM-CIM 框架，用于评估文本到图像模型的多样性和泛化能力。

**Authors:** Revant Teotia, Candace Ross, Karen Ullrich, Sumit Chopra, Adriana Romero-Soriano, Melissa Hall, Matthew J. Muckley
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05108)

#### 中文摘要 (Abstract in Chinese)

> 文本到图像 (T2I) 模型在质量和一致性方面取得了显著进展，但牺牲了表示多样性。现有的自动评估方法要么需要参考图像数据集，要么缺乏关于多样性类型的特异性，限制了适应性和可解释性。为了解决这个问题，我们引入了 Does-it/Can-it 框架，即 DIM-CIM，它是一种无需参考的测量默认模式多样性（“模型是否生成具有预期属性的图像？”）和泛化能力（“模型是否可以为特定概念生成不同的属性？”）的方法。我们构建了 COCO-DIMCIM 基准，该基准由 COCO 概念和标题作为种子，并通过大型语言模型进行扩充。通过 COCO-DIMCIM，我们发现广泛使用的模型在从 1.5B 参数扩展到 8.1B 参数时，在泛化方面有所提高，但以默认模式多样性为代价。DIMCIM 还识别出细粒度的失败案例，例如使用通用提示生成的属性，但在明确请求时很少生成。最后，我们使用 DIMCIM 评估 T2I 模型的训练数据，并观察到训练图像的多样性与默认模式多样性之间存在 0.85 的相关性。该论文提供了一个灵活且可解释的框架，用于评估 T2I 模型的多样性和泛化能力，从而更全面地了解模型性能。

---

### [Practical Manipulation Model for Robust Deepfake Detection](https://arxiv.org/abs/2506.05119)

**一句话总结:** 该论文提出了一种新的伪造数据生成方法PMM，并通过增强训练数据的多样性来提高深度伪造检测模型的鲁棒性。

**Authors:** Benedikt Hopf, Radu Timofte
**Categories:** `cs.CV`

[**[PDF]**](https://arxiv.org/pdf/2506.05119)

#### 中文摘要 (Abstract in Chinese)

> 现代深度伪造检测模型即使在具有挑战性的跨数据集任务上也取得了强大的性能。然而，在非理想条件下的检测性能仍然非常不稳定，这限制了在某些基准数据集上的成功，并且很容易规避检测。受到图像超分辨率领域转向更真实的降级模型的启发，我们开发了一种实用的操纵模型（PMM），该模型涵盖了更大范围的可能的伪造品。我们通过使用泊松融合、更多样化的掩码、生成器伪影和干扰物来扩展伪伪造空间。此外，我们通过向训练图像添加强烈的退化来提高检测器的一般性和鲁棒性。我们证明，这些改变不仅显著增强了模型对常见图像退化的鲁棒性，而且提高了在标准基准数据集上的性能。具体来说，我们展示了在DFDC和DFDCP数据集上，超过s-o-t-a LAA主干网络，AUC分别显著提高了3.51\%和6.21\%。此外，我们强调了以前检测器缺乏鲁棒性以及我们在这方面的改进。

---

### [CIVET: Systematic Evaluation of Understanding in VLMs](https://arxiv.org/abs/2506.05146)

**一句话总结:** CIVET框架被提出，用于系统评估视觉语言模型在理解物体属性和关系方面的能力，结果表明现有模型仍有局限性。

**Authors:** Massimo Rizzoli, Simone Alghisi, Olha Khomyn, Gabriel Roccabruna, Seyed Mahed Mousavi, Giuseppe Riccardi
**Categories:** `cs.CV`, `cs.CL`

[**[PDF]**](https://arxiv.org/pdf/2506.05146)

#### 中文摘要 (Abstract in Chinese)

> 虽然视觉语言模型在各种任务中取得了有竞争力的性能，但它们对场景底层结构和语义的理解仍有待研究。为了研究视觉语言模型的理解能力，我们以一种可控和可解释的方式研究了它们在物体属性和关系方面的能力。为此，我们引入了CIVET，这是一个新颖且可扩展的框架，用于通过可控刺激进行系统评估。CIVET 解决了缺乏标准化系统评估来评估视觉语言模型理解能力的问题，使研究人员能够以统计严格性测试假设。通过 CIVET，我们评估了五种最先进的视觉语言模型，这些模型使用了详尽的刺激集，没有注释噪声、数据集特定偏差和不受控制的场景复杂性。我们的发现表明：1) 当前的视觉语言模型只能准确识别有限的一组基本对象属性；2) 它们的性能在很大程度上取决于对象在场景中的位置；3) 他们努力理解对象之间的基本关系。此外，与人类注释者的比较评估表明，视觉语言模型仍然无法达到人类水平的准确性。
