{"id": "2507.02406", "pdf": "https://arxiv.org/pdf/2507.02406", "abs": "https://arxiv.org/abs/2507.02406", "authors": ["Caio Azevedo", "Lina Achaji", "Stefano Sabatini", "Nicola Poerio", "Grzegorz Bartyzel", "Sascha Hornauer", "Fabien Moutarde"], "title": "Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization", "categories": ["cs.LG"], "comment": "Accepted for publication at ITSC 2025", "summary": "Trajectory prediction is an essential step in the pipeline of an autonomous\nvehicle. Inaccurate or inconsistent predictions regarding the movement of\nagents in its surroundings lead to poorly planned maneuvers and potentially\ndangerous situations for the end-user. Current state-of-the-art\ndeep-learning-based trajectory prediction models can achieve excellent accuracy\non public datasets. However, when used in more complex, interactive scenarios,\nthey often fail to capture important interdependencies between agents, leading\nto inconsistent predictions among agents in the traffic scene. Inspired by the\nefficacy of incorporating human preference into large language models, this\nwork fine-tunes trajectory prediction models in multi-agent settings using\npreference optimization. By taking as input automatically calculated preference\nrankings among predicted futures in the fine-tuning process, our\nexperiments--using state-of-the-art models on three separate datasets--show\nthat we are able to significantly improve scene consistency while minimally\nsacrificing trajectory prediction accuracy and without adding any excess\ncomputational requirements at inference time.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper directly addresses vehicle trajectory prediction and explicitly mentions inspiration from incorporating human preference into large language models, using a preference optimization technique. It combines ideas from both fields.", "keywords": ["trajectory prediction", "vehicle trajectory prediction", "large language models", "preference optimization", "multi-agent settings", "scene consistency"]}, "AI": {"tldr": "\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5fae\u8c03\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u7684\u573a\u666f\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u5148\u8fdb\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u5728\u590d\u6742\u3001\u4ea4\u4e92\u573a\u666f\u4e2d\uff0c\u672a\u80fd\u6355\u6349\u5230\u667a\u80fd\u4f53\u4e4b\u95f4\u91cd\u8981\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u4ea4\u901a\u573a\u666f\u4e2d\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u9884\u6d4b\u4e0d\u4e00\u81f4\u3002", "method": "\u4f7f\u7528\u504f\u597d\u4f18\u5316\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5fae\u8c03\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6700\u5c0f\u7a0b\u5ea6\u727a\u7272\u8f68\u8ff9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u573a\u666f\u4e00\u81f4\u6027\u3002", "conclusion": "\u901a\u8fc7\u504f\u597d\u4f18\u5316\u5fae\u8c03\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\uff0c\u80fd\u591f\u5728\u4e0d\u663e\u8457\u727a\u7272\u8f68\u8ff9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u573a\u666f\u4e00\u81f4\u6027\uff0c\u4e14\u4e0d\u4f1a\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002", "summary_zh": "\u8f68\u8ff9\u9884\u6d4b\u662f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6d41\u7a0b\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\u3002\u5bf9\u5468\u56f4\u667a\u80fd\u4f53\u8fd0\u52a8\u7684\u4e0d\u51c6\u786e\u6216\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\u4f1a\u5bfc\u81f4\u89c4\u5212\u4e0d\u4f73\u7684\u64cd\u4f5c\uff0c\u5e76\u53ef\u80fd\u7ed9\u6700\u7ec8\u7528\u6237\u5e26\u6765\u5371\u9669\u3002\u76ee\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u53ef\u4ee5\u5728\u516c\u5171\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u51fa\u8272\u7684\u51c6\u786e\u6027\u3002\u7136\u800c\uff0c\u5f53\u5728\u66f4\u590d\u6742\u3001\u4ea4\u4e92\u7684\u573a\u666f\u4e2d\u4f7f\u7528\u65f6\uff0c\u5b83\u4eec\u901a\u5e38\u65e0\u6cd5\u6355\u6349\u5230\u667a\u80fd\u4f53\u4e4b\u95f4\u91cd\u8981\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u5bfc\u81f4\u4ea4\u901a\u573a\u666f\u4e2d\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u9884\u6d4b\u4e0d\u4e00\u81f4\u3002\u53d7\u5230\u5c06\u4eba\u7c7b\u504f\u597d\u878d\u5165\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u6027\u7684\u542f\u53d1\uff0c\u8fd9\u9879\u5de5\u4f5c\u4f7f\u7528\u504f\u597d\u4f18\u5316\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5fae\u8c03\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u3002\u901a\u8fc7\u5c06\u81ea\u52a8\u8ba1\u7b97\u7684\u9884\u6d4b\u672a\u6765\u504f\u597d\u6392\u5e8f\u4f5c\u4e3a\u5fae\u8c03\u8fc7\u7a0b\u7684\u8f93\u5165\uff0c\u6211\u4eec\u4f7f\u7528\u4e09\u4e2a\u72ec\u7acb\u6570\u636e\u96c6\u4e0a\u7684\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u80fd\u591f\u5728\u6700\u5c0f\u7a0b\u5ea6\u727a\u7272\u8f68\u8ff9\u9884\u6d4b\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u573a\u666f\u4e00\u81f4\u6027\uff0c\u4e14\u4e0d\u4f1a\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u8d1f\u62c5\u3002"}}
{"id": "2507.02029", "pdf": "https://arxiv.org/pdf/2507.02029", "abs": "https://arxiv.org/abs/2507.02029", "authors": ["BAAI RoboBrain Team", "Mingyu Cao", "Huajie Tan", "Yuheng Ji", "Minglan Lin", "Zhiyu Li", "Zhou Cao", "Pengwei Wang", "Enshen Zhou", "Yi Han", "Yingbo Tang", "Xiangqi Xu", "Wei Guo", "Yaoxu Lyu", "Yijie Xu", "Jiayu Shi", "Cheng Chi", "Mengdi Zhao", "Xiaoshuai Hao", "Shanyu Rong", "Zhengliang Cai", "Bolun Zhang", "Shuyi Zhang", "Huaihai Lyu", "Mengfei Du", "Lingfeng Zhang", "Xi Feng", "Xiaodan Liu", "Yance Jiao", "Chenrui He", "Mengsi Lyu", "Zhuo Chen", "Yulong Ao", "Xue Sun", "Zheqi He", "Jingshu Zheng", "Xi Yang", "Donghai Shi", "Kunchang Xie", "Bochao Zhang", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "RoboBrain 2.0 Technical Report", "categories": ["cs.RO"], "comment": null, "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper describes RoboBrain 2.0, an embodied vision-language foundation model. While the primary focus is on embodied AI and reasoning, the abstract explicitly mentions \"trajectory forecasting\" as one of the supported capabilities related to spatial understanding. The mention of a large model (7B and 32B) further contributes to the relevance.", "keywords": ["trajectory forecasting", "large language models", "foundation models", "embodied AI", "planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.02190", "pdf": "https://arxiv.org/pdf/2507.02190", "abs": "https://arxiv.org/abs/2507.02190", "authors": ["Max Argus", "Jelena Bratulic", "Houman Masnavi", "Maxim Velikanov", "Nick Heppert", "Abhinav Valada", "Thomas Brox"], "title": "cVLA: Towards Efficient Camera-Space VLAs", "categories": ["cs.RO", "cs.LG"], "comment": "20 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models offer a compelling framework for tackling\ncomplex robotic manipulation tasks, but they are often expensive to train. In\nthis paper, we propose a novel VLA approach that leverages the competitive\nperformance of Vision Language Models (VLMs) on 2D images to directly infer\nrobot end-effector poses in image frame coordinates. Unlike prior VLA models\nthat output low-level controls, our model predicts trajectory waypoints, making\nit both more efficient to train and robot embodiment agnostic. Despite its\nlightweight design, our next-token prediction architecture effectively learns\nmeaningful and executable robot trajectories. We further explore the\nunderutilized potential of incorporating depth images, inference-time\ntechniques such as decoding strategies, and demonstration-conditioned action\ngeneration. Our model is trained on a simulated dataset and exhibits strong\nsim-to-real transfer capabilities. We evaluate our approach using a combination\nof simulated and real data, demonstrating its effectiveness on a real robotic\nsystem.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u5b83\u4f7f\u7528\u4e86Vision Language Models (VLMs) \u6765\u9884\u6d4b\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u7684\u8f68\u8ff9\u70b9 (trajectory waypoints)\u3002VLM \u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5e7f\u4e49\u7684\u5927\u6a21\u578b\uff0c\u5e76\u4e14\u8bba\u6587\u660e\u786e\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory waypoints", "Vision Language Models", "VLMs", "robot trajectories", "action generation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.02074", "pdf": "https://arxiv.org/pdf/2507.02074", "abs": "https://arxiv.org/abs/2507.02074", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on crash detection in video using Large Language Models. While not directly related to trajectory prediction, it utilizes LLMs for video understanding, which is a related area. The connection to large language models is clear.", "keywords": ["Large Language Models", "LLMs", "video understanding", "foundation models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.01982", "pdf": "https://arxiv.org/pdf/2507.01982", "abs": "https://arxiv.org/abs/2507.01982", "authors": ["Siqing Long", "Xiangzhi Huang", "Jiemin Xie", "Ming Cai"], "title": "DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": "39 pages, 14 figures", "summary": "Accurate traffic demand forecasting enables transportation management\ndepartments to allocate resources more effectively, thereby improving their\nutilization efficiency. However, complex spatiotemporal relationships in\ntraffic systems continue to limit the performance of demand forecasting models.\nTo improve the accuracy of spatiotemporal traffic demand prediction, we propose\na new graph convolutional network structure called DKGCM. Specifically, we\nfirst consider the spatial flow distribution of different traffic nodes and\npropose a novel temporal similarity-based clustering graph convolution method,\nDK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering\nto group traffic nodes and more effectively capture spatial dependencies. On\nthe temporal scale, we integrate the Fast Fourier Transform (FFT) within the\nbidirectional Mamba deep learning framework to capture temporal dependencies in\ntraffic demand. To further optimize model training, we incorporate the GRPO\nreinforcement learning strategy to enhance the loss function feedback\nmechanism. Extensive experiments demonstrate that our model outperforms several\nadvanced methods and achieves strong results on three public datasets.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86Mamba\u6a21\u578b\uff0cMamba\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5e8f\u5217\u5efa\u6a21\u9886\u57df\u7684\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u4e0eTransformer\u67b6\u6784\u5b58\u5728\u7ade\u4e89\u5173\u7cfb\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "traffic flow prediction", "spatio-temporal prediction", "Mamba", "graph convolutional network"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.02250", "pdf": "https://arxiv.org/pdf/2507.02250", "abs": "https://arxiv.org/abs/2507.02250", "authors": ["Jiangxia Chen", "Tongyuan Huang", "Ke Song"], "title": "FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic occupancy prediction plays a pivotal role in autonomous driving.\nHowever, inherent limitations of fewframe images and redundancy in 3D space\ncompromise prediction accuracy for occluded and distant scenes. Existing\nmethods enhance performance by fusing historical frame data, which need\nadditional data and significant computational resources. To address these\nissues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement\noccupancy network with flow matching selective state space model for few-frame\n3D occupancy prediction. Firstly, to generate missing features, we designed a\nfeature refinement module based on a flow matching model, which is called Flow\nMatching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and\nPlane Selective SSM (PS3M), we selectively filter TPV features to reduce the\nimpact of air voxels on non-air voxels, thereby enhancing the overall\nefficiency of the model and prediction capability for distant scenes. Finally,\nwe design the Mask Training (MT) method to enhance the robustness of FMOcc and\naddress the issue of sensor data loss. Experimental results on the\nOcc3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing\nstate-of-theart methods. Our FMOcc with two frame input achieves notable scores\nof 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on\nOpenOcc with 5.4 G inference memory and 330ms inference time.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 3D occupancy prediction for autonomous driving, which is related to trajectory prediction as it involves understanding the environment and predicting future states. While it doesn't directly use large language models, the use of state space models for feature refinement and efficiency enhancement touches upon aspects that could potentially be combined with LLMs in future research. The mention of 'flow matching' also hints at a predictive aspect.", "keywords": ["occupancy prediction", "autonomous driving", "state space model", "flow matching", "3D"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.02085", "pdf": "https://arxiv.org/pdf/2507.02085", "abs": "https://arxiv.org/abs/2507.02085", "authors": ["Wanjia Zhao", "Jiaqi Han", "Siyi Gu", "Mingjian Jiang", "James Zou", "Stefano Ermon"], "title": "GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Geometric diffusion models have shown remarkable success in molecular\ndynamics and structure generation. However, efficiently fine-tuning them for\ndownstream tasks with varying geometric controls remains underexplored. In this\nwork, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables\nflexible and parameter-efficient fine-tuning for controlled generative tasks\nwithout modifying the original model architecture. GeoAda introduces a\nstructured adapter design: control signals are first encoded through coupling\noperators, then processed by a trainable copy of selected pretrained model\nlayers, and finally projected back via decoupling operators followed by an\nequivariant zero-initialized convolution. By fine-tuning only these lightweight\nadapter modules, GeoAda preserves the model's geometric consistency while\nmitigating overfitting and catastrophic forgetting. We theoretically prove that\nthe proposed adapters maintain SE(3)-equivariance, ensuring that the geometric\ninductive biases of the pretrained diffusion model remain intact during\nadaptation. We demonstrate the wide applicability of GeoAda across diverse\ngeometric control types, including frame control, global control, subgraph\ncontrol, and a broad range of application domains such as particle dynamics,\nmolecular dynamics, human motion prediction, and molecule generation. Empirical\nresults show that GeoAda achieves state-of-the-art fine-tuning performance\nwhile preserving original task accuracy, whereas other baselines experience\nsignificant performance degradation due to overfitting and catastrophic\nforgetting.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u51e0\u4f55\u6269\u6563\u6a21\u578b\u7684\u5fae\u8c03\uff0c\u5e76\u5e94\u7528\u4e8e\u591a\u79cd\u51e0\u4f55\u63a7\u5236\u4efb\u52a1\u3002\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\"human motion prediction\"\uff0c\u8fd9\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u51e0\u4f55\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5927\u578b\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u5e76\u4e14\u8be5\u8bba\u6587\u8ba8\u8bba\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u8fd9\u4e0e\u5927\u6a21\u578b\u5fae\u8c03\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["human motion prediction", "diffusion models", "fine-tuning", "geometric control"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.02761", "pdf": "https://arxiv.org/pdf/2507.02761", "abs": "https://arxiv.org/abs/2507.02761", "authors": ["Long Xu", "Choilam Wong", "Mengke Zhang", "Junxiao Lin", "Fei Gao"], "title": "Trajectory Optimization for Differential Drive Mobile Manipulators via Topological Paths Search and Arc Length-Yaw Parameterization", "categories": ["cs.RO"], "comment": "Technical Report", "summary": "We present an efficient hierarchical motion planning pipeline for\ndifferential drive mobile manipulators. Our approach first searches for\nmultiple collisionfree and topologically distinct paths for the mobile base to\nextract the space in which optimal solutions may exist. Further sampling and\noptimization are then conducted in parallel to explore feasible whole-body\ntrajectories. For trajectory optimization, we employ polynomial trajectories\nand arc length-yaw parameterization, enabling efficient handling of the\nnonholonomic dynamics while ensuring optimality.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u79fb\u52a8\u673a\u5668\u4eba\u7684\u8f68\u8ff9\u4f18\u5316\uff0c\u6d89\u53ca\u8f68\u8ff9\u89c4\u5212\u548c\u8def\u5f84\u641c\u7d22\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u3002\u4f46\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e00\u822c\u3002", "keywords": ["trajectory optimization", "motion planning", "trajectory", "paths search"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.02747", "pdf": "https://arxiv.org/pdf/2507.02747", "abs": "https://arxiv.org/abs/2507.02747", "authors": ["Jiawei He", "Danshi Li", "Xinqiang Yu", "Zekun Qi", "Wenyao Zhang", "Jiayi Chen", "Zhaoxiang Zhang", "Zhizheng Zhang", "Li Yi", "He Wang"], "title": "DexVLG: Dexterous Vision-Language-Grasp Model at Scale", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "As large models gain traction, vision-language-action (VLA) systems are\nenabling robots to tackle increasingly complex tasks. However, limited by the\ndifficulty of data collection, progress has mainly focused on controlling\nsimple gripper end-effectors. There is little research on functional grasping\nwith large models for human-like dexterous hands. In this paper, we introduce\nDexVLG, a large Vision-Language-Grasp model for Dexterous grasp pose prediction\naligned with language instructions using single-view RGBD input. To accomplish\nthis, we generate a dataset of 170 million dexterous grasp poses mapped to\nsemantic parts across 174,000 objects in simulation, paired with detailed\npart-level captions. This large-scale dataset, named DexGraspNet 3.0, is used\nto train a VLM and flow-matching-based pose head capable of producing\ninstruction-aligned grasp poses for tabletop objects. To assess DexVLG's\nperformance, we create benchmarks in physics-based simulations and conduct\nreal-world experiments. Extensive testing demonstrates DexVLG's strong\nzero-shot generalization capabilities-achieving over 76% zero-shot execution\nsuccess rate and state-of-the-art part-grasp accuracy in simulation-and\nsuccessful part-aligned grasps on physical objects in real-world scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper describes DexVLG, a large Vision-Language-Grasp model. While the paper focuses on grasp pose prediction, which is related to action prediction and robotic manipulation, it doesn't directly address trajectory prediction. However, it does utilize a large vision-language model (VLM) and leverages large-scale datasets, making it relevant to the 'Large Language Models' theme. The connection to trajectory prediction is weaker but present in the broader context of robotics and action planning.", "keywords": ["large models", "vision-language model", "VLM", "grasp pose prediction", "action"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2507.02771", "pdf": "https://arxiv.org/pdf/2507.02771", "abs": "https://arxiv.org/abs/2507.02771", "authors": ["Melanie Segado", "Felipe Parodi", "Jordan K. Matelsky", "Michael L. Platt", "Eva B. Dyer", "Konrad P. Kording"], "title": "Grounding Intelligence in Movement", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "9 pages, 2 figures", "summary": "Recent advances in machine learning have dramatically improved our ability to\nmodel language, vision, and other high-dimensional data, yet they continue to\nstruggle with one of the most fundamental aspects of biological systems:\nmovement. Across neuroscience, medicine, robotics, and ethology, movement is\nessential for interpreting behavior, predicting intent, and enabling\ninteraction. Despite its core significance in our intelligence, movement is\noften treated as an afterthought rather than as a rich and structured modality\nin its own right. This reflects a deeper fragmentation in how movement data is\ncollected and modeled, often constrained by task-specific goals and\ndomain-specific assumptions. But movement is not domain-bound. It reflects\nshared physical constraints, conserved morphological structures, and purposeful\ndynamics that cut across species and settings. We argue that movement should be\ntreated as a primary modeling target for AI. It is inherently structured and\ngrounded in embodiment and physics. This structure, often allowing for compact,\nlower-dimensional representations (e.g., pose), makes it more interpretable and\ncomputationally tractable to model than raw, high-dimensional sensory inputs.\nDeveloping models that can learn from and generalize across diverse movement\ndata will not only advance core capabilities in generative modeling and\ncontrol, but also create a shared foundation for understanding behavior across\nbiological and artificial systems. Movement is not just an outcome, it is a\nwindow into how intelligent systems engage with the world.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u5f3a\u8c03\u4e86\u8fd0\u52a8\u5728\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8ba4\u4e3a\u5e94\u8be5\u5c06\u5176\u4f5c\u4e3aAI\u5efa\u6a21\u7684\u4e3b\u8981\u76ee\u6807\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u201c\u8f68\u8ff9\u9884\u6d4b\u201d\u6216\u201c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u201d\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cpredicting intent\u201d\u548c\u201cgenerative modeling\u201d\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u610f\u56fe\u9884\u6d4b\u548c\u751f\u6210\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u6b64\u5916\uff0c\u5c06\u8fd0\u52a8\u4f5c\u4e3aAI\u5efa\u6a21\u7684\u4e3b\u8981\u76ee\u6807\uff0c\u4e5f\u4e3a\u672a\u6765\u5c06\u8fd0\u52a8\u6570\u636e\u4e0e\u5927\u578b\u6a21\u578b\u7ed3\u5408\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\u3002", "keywords": ["movement", "predicting intent", "generative modeling", "AI", "modeling"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
