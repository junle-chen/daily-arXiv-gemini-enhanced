{"id": "2511.15992", "pdf": "https://arxiv.org/pdf/2511.15992", "abs": "https://arxiv.org/abs/2511.15992", "authors": ["Shahin Zanbaghi", "Ryan Rostampour", "Farhan Abid", "Salim Al Jarmakani"], "title": "Detecting Sleeper Agents in Large Language Models via Semantic Drift Analysis", "categories": ["cs.AI"], "comment": "7 pages, 3 figures, 1 table", "summary": "Large Language Models (LLMs) can be backdoored to exhibit malicious behavior under specific deployment conditions while appearing safe during training a phenomenon known as \"sleeper agents.\" Recent work by Hubinger et al. demonstrated that these backdoors persist through safety training, yet no practical detection methods exist. We present a novel dual-method detection system combining semantic drift analysis with canary baseline comparison to identify backdoored LLMs in real-time. Our approach uses Sentence-BERT embeddings to measure semantic deviation from safe baselines, complemented by injected canary questions that monitor response consistency. Evaluated on the official Cadenza-Labs dolphin-llama3-8B sleeper agent model, our system achieves 92.5% accuracy with 100% precision (zero false positives) and 85% recall. The combined detection method operates in real-time (<1s per query), requires no model modification, and provides the first practical solution to LLM backdoor detection. Our work addresses a critical security gap in AI deployment and demonstrates that embedding-based detection can effectively identify deceptive model behavior without sacrificing deployment efficiency.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper focuses on detecting backdoors in Large Language Models (LLMs). While it doesn't directly address trajectory prediction, its core subject matter revolves around the security and behavior analysis of LLMs, which is a significant aspect of the 'Large Language Models' theme. The mention of semantic drift analysis and embedding-based detection techniques further aligns with the analysis and understanding of LLMs.", "keywords": ["Large Language Models", "LLMs", "backdoor detection", "semantic drift analysis", "embedding-based detection", "security"]}}
{"id": "2511.16048", "pdf": "https://arxiv.org/pdf/2511.16048", "abs": "https://arxiv.org/abs/2511.16048", "authors": ["Qing Zhang", "Jing Huang", "Mingyang Xu", "Jun Rekimoto"], "title": "Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems", "summary": "While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately \"lo-fi\" approach. We present the \"Semantic Glitch,\" a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a \"physical glitch\" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a \"narrative mind\" that complements the \"weak,\" historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling \"plan to execution\" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper describes an autonomous flying robot that uses a Multimodal Large Language Model for navigation. While it doesn't explicitly focus on trajectory prediction as its primary goal, the autonomous navigation aspect is closely related to trajectory planning and control. The reliance on a large language model directly connects it to the second theme. The paper describes a specific navigation strategy, which is a form of trajectory generation, although not explicitly framed as such.", "keywords": ["Large Language Model", "autonomous navigation", "robotics", "Multimodal Large Language Model"]}}
{"id": "2511.16518", "pdf": "https://arxiv.org/pdf/2511.16518", "abs": "https://arxiv.org/abs/2511.16518", "authors": ["Xiaoshuai Hao", "Lei Zhou", "Zhijian Huang", "Zhiwen Hou", "Yingbo Tang", "Lingfeng Zhang", "Guang Li", "Zheng Lu", "Shuhuai Ren", "Xianhui Meng", "Yuchen Zhang", "Jing Wu", "Jinghui Lu", "Chenxu Dang", "Jiayi Guan", "Jianhua Wu", "Zhiyi Hou", "Hanbing Li", "Shumeng Xia", "Mingliang Zhou", "Yinan Zheng", "Zihao Yue", "Shuhao Gu", "Hao Tian", "Yuannan Shen", "Jianwei Cui", "Wen Zhang", "Shaoqing Xu", "Bing Wang", "Haiyang Sun", "Zeyu Zhu", "Yuncheng Jiang", "Zibin Guo", "Chuhong Gong", "Chaofan Zhang", "Wenbo Ding", "Kun Ma", "Guang Chen", "Rui Cai", "Diyun Xiang", "Heng Qu", "Fuli Luo", "Hangjun Ye", "Long Chen"], "title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report", "categories": ["cs.RO", "cs.CL", "cs.CV"], "comment": "Code: https://github.com/XiaomiMiMo/MiMo-Embodied Model: https://huggingface.co/XiaomiMiMo/MiMo-Embodied-7B", "summary": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper describes MiMo-Embodied, a cross-embodied foundation model that performs well in both Autonomous Driving and Embodied AI. While it doesn't explicitly mention trajectory prediction, \"Driving Planning\" and \"Status Prediction\" within the autonomous driving benchmarks are closely related to trajectory prediction. Also, the use of a foundation model implies the involvement of a large model.", "keywords": ["foundation model", "autonomous driving", "driving planning", "status prediction", "large model"]}}
{"id": "2511.16105", "pdf": "https://arxiv.org/pdf/2511.16105", "abs": "https://arxiv.org/abs/2511.16105", "authors": ["Yuanbo Tang", "Yan Tang", "Zixuan Zhang", "Zihui Zhao", "Yang Li"], "title": "Pathlet Variational Auto-Encoder for Robust Trajectory Generation", "categories": ["cs.LG"], "comment": null, "summary": "Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints.\n  Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\\%$ and $26.3\\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\\%$ of the time and $56.5\\%$ of GPU memory compared to previous approaches.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on trajectory generation using a variational autoencoder, which is related to trajectory prediction. While it utilizes deep learning techniques, it doesn't explicitly involve large language models. However, the mention of generative AI methods and the potential for downstream tasks like trajectory prediction increase its relevance.", "keywords": ["trajectory generation", "trajectory prediction", "variational autoencoder", "deep learning", "generative AI"]}}
{"id": "2511.16227", "pdf": "https://arxiv.org/pdf/2511.16227", "abs": "https://arxiv.org/abs/2511.16227", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Dongming Zhou", "Gangshan Wu", "Jinde Cao"], "title": "SwiTrack: Tri-State Switch for Cross-Modal Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Cross-modal object tracking (CMOT) is an emerging task that maintains target consistency while the video stream switches between different modalities, with only one modality available in each frame, mostly focusing on RGB-Near Infrared (RGB-NIR) tracking. Existing methods typically connect parallel RGB and NIR branches to a shared backbone, which limits the comprehensive extraction of distinctive modality-specific features and fails to address the issue of object drift, especially in the presence of unreliable inputs. In this paper, we propose SwiTrack, a novel state-switching framework that redefines CMOT through the deployment of three specialized streams. Specifically, RGB frames are processed by the visual encoder, while NIR frames undergo refinement via a NIR gated adapter coupled with the visual encoder to progressively calibrate shared latent space features, thereby yielding more robust cross-modal representations. For invalid modalities, a consistency trajectory prediction module leverages spatio-temporal cues to estimate target movement, ensuring robust tracking and mitigating drift. Additionally, we incorporate dynamic template reconstruction to iteratively update template features and employ a similarity alignment loss to reinforce feature consistency. Experimental results on the latest benchmarks demonstrate that our tracker achieves state-of-the-art performance, boosting precision rate and success rate gains by 7.2\\% and 4.3\\%, respectively, while maintaining real-time tracking at 65 frames per second. Code and models are available at https://github.com/xuboyue1999/SwiTrack.git.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8de8\u6a21\u6001\u76ee\u6807\u8ddf\u8e2a\uff0c\u5176\u4e2d\u6d89\u53ca\u4e86\u8f68\u8ff9\u9884\u6d4b\u6a21\u5757\u4ee5\u5904\u7406\u6a21\u6001\u7f3a\u5931\u7684\u60c5\u51b5\u3002\u867d\u7136\u4e0d\u662f\u6838\u5fc3\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u8f68\u8ff9\u9884\u6d4b\u5728\u5176\u4e2d\u8d77\u5230\u4e86\u91cd\u8981\u7684\u4f5c\u7528\u3002\u4f46\u662f\uff0c\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u4efb\u4f55\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5bb9\u3002", "keywords": ["trajectory prediction", "object tracking", "cross-modal object tracking"]}}
{"id": "2511.15914", "pdf": "https://arxiv.org/pdf/2511.15914", "abs": "https://arxiv.org/abs/2511.15914", "authors": ["Debasmita Ghose", "Oz Gitelson", "Ryan Jin", "Grace Abawe", "Marynel Vazquez", "Brian Scassellati"], "title": "I've Changed My Mind: Robots Adapting to Changing Human Goals during Collaboration", "categories": ["cs.RO"], "comment": "Accepted to RA-L", "summary": "For effective human-robot collaboration, a robot must align its actions with human goals, even as they change mid-task. Prior approaches often assume fixed goals, reducing goal prediction to a one-time inference. However, in real-world scenarios, humans frequently shift goals, making it challenging for robots to adapt without explicit communication. We propose a method for detecting goal changes by tracking multiple candidate action sequences and verifying their plausibility against a policy bank. Upon detecting a change, the robot refines its belief in relevant past actions and constructs Receding Horizon Planning (RHP) trees to actively select actions that assist the human while encouraging Differentiating Actions to reveal their updated goal. We evaluate our approach in a collaborative cooking environment with up to 30 unique recipes and compare it to three comparable human goal prediction algorithms. Our method outperforms all baselines, quickly converging to the correct goal after a switch, reducing task completion time, and improving collaboration efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robots adapting to changing human goals during collaboration. While it doesn't directly use Large Language Models, it involves prediction of human actions and goals, which relates to trajectory prediction. The paper uses a policy bank and Receding Horizon Planning, which can be seen as related to predicting future states and actions.", "keywords": ["goal prediction", "action prediction", "human-robot collaboration", "receding horizon planning"]}}
{"id": "2511.15995", "pdf": "https://arxiv.org/pdf/2511.15995", "abs": "https://arxiv.org/abs/2511.15995", "authors": ["Zili Tang", "Ying Zhang", "Meng Guo"], "title": "PushingBots: Collaborative Pushing via Neural Accelerated Combinatorial Hybrid Optimization", "categories": ["cs.RO"], "comment": "20 pages, 24 figures. Accepted to IEEE Transactions on Robotics (T-RO), 2025", "summary": "Many robots are not equipped with a manipulator and many objects are not suitable for prehensile manipulation (such as large boxes and cylinders). In these cases, pushing is a simple yet effective non-prehensile skill for robots to interact with and further change the environment. Existing work often assumes a set of predefined pushing modes and fixed-shape objects. This work tackles the general problem of controlling a robotic fleet to push collaboratively numerous arbitrary objects to respective destinations, within complex environments of cluttered and movable obstacles. It incorporates several characteristic challenges for multi-robot systems such as online task coordination under large uncertainties of cost and duration, and for contact-rich tasks such as hybrid switching among different contact modes, and under-actuation due to constrained contact forces. The proposed method is based on combinatorial hybrid optimization over dynamic task assignments and hybrid execution via sequences of pushing modes and associated forces. It consists of three main components: (I) the decomposition, ordering and rolling assignment of pushing subtasks to robot subgroups; (II) the keyframe guided hybrid search to optimize the sequence of parameterized pushing modes for each subtask; (III) the hybrid control to execute these modes and transit among them. Last but not least, a diffusion-based accelerator is adopted to predict the keyframes and pushing modes that should be prioritized during hybrid search; and further improve planning efficiency. The framework is complete under mild assumptions. Its efficiency and effectiveness under different numbers of robots and general-shaped objects are validated extensively in simulations and hardware experiments, as well as generalizations to heterogeneous robots, planar assembly and 6D pushing.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u673a\u5668\u4eba\u534f\u540c\u63a8\u52a8\u7269\u4f53\uff0c\u5176\u4e2d\u6d89\u53ca\u8def\u5f84\u89c4\u5212\u3001\u52a8\u4f5c\u6267\u884c\u548c\u9884\u6d4b\u7b49\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u7684\u4e3b\u9898\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cdiffusion-based accelerator is adopted to predict the keyframes and pushing modes\u201d\uff0c\u8868\u660e\u4f7f\u7528\u4e86\u6269\u6563\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c\u867d\u7136\u4e0d\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5c5e\u4e8e\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u53ef\u4ee5\u63d0\u5347\u89c4\u5212\u6548\u7387\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8def\u5f84\u89c4\u5212", "\u52a8\u4f5c\u9884\u6d4b", "diffusion-based accelerator", "\u9884\u6d4b", "pushing modes"]}}
{"id": "2511.16200", "pdf": "https://arxiv.org/pdf/2511.16200", "abs": "https://arxiv.org/abs/2511.16200", "authors": ["Kewei Chen", "Yayu Long", "Mingsheng Shang"], "title": "PIPHEN: Physical Interaction Prediction with Hamiltonian Energy Networks", "categories": ["cs.RO"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Multi-robot systems in complex physical collaborations face a \"shared brain dilemma\": transmitting high-dimensional multimedia data (e.g., video streams at ~30MB/s) creates severe bandwidth bottlenecks and decision-making latency. To address this, we propose PIPHEN, an innovative distributed physical cognition-control framework. Its core idea is to replace \"raw data communication\" with \"semantic communication\" by performing \"semantic distillation\" at the robot edge, reconstructing high-dimensional perceptual data into compact, structured physical representations. This idea is primarily realized through two key components: (1) a novel Physical Interaction Prediction Network (PIPN), derived from large model knowledge distillation, to generate this representation; and (2) a Hamiltonian Energy Network (HEN) controller, based on energy conservation, to precisely translate this representation into coordinated actions. Experiments show that, compared to baseline methods, PIPHEN can compress the information representation to less than 5% of the original data volume and reduce collaborative decision-making latency from 315ms to 76ms, while significantly improving task success rates. This work provides a fundamentally efficient paradigm for resolving the \"shared brain dilemma\" in resource-constrained multi-robot systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on physical interaction prediction for multi-robot systems, using a Physical Interaction Prediction Network (PIPN) derived from large model knowledge distillation. While the primary focus is not trajectory prediction in the traditional sense (e.g., predicting future locations), the PIPN component and the mention of large model knowledge distillation suggest a moderate level of relevance. It employs prediction for interaction rather than trajectory, and uses knowledge distillation from a large model.", "keywords": ["large model", "knowledge distillation", "prediction", "multi-robot systems"]}}
{"id": "2511.16223", "pdf": "https://arxiv.org/pdf/2511.16223", "abs": "https://arxiv.org/abs/2511.16223", "authors": ["Vincenzo Pomponi", "Paolo Franceschi", "Stefano Baraldo", "Loris Roveda", "Oliver Avram", "Luca Maria Gambardella", "Anna Valente"], "title": "DynaMimicGen: A Data Generation Framework for Robot Learning of Dynamic Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Learning robust manipulation policies typically requires large and diverse datasets, the collection of which is time-consuming, labor-intensive, and often impractical for dynamic environments. In this work, we introduce DynaMimicGen (D-MG), a scalable dataset generation framework that enables policy training from minimal human supervision while uniquely supporting dynamic task settings. Given only a few human demonstrations, D-MG first segments the demonstrations into meaningful sub-tasks, then leverages Dynamic Movement Primitives (DMPs) to adapt and generalize the demonstrated behaviors to novel and dynamically changing environments. Improving prior methods that rely on static assumptions or simplistic trajectory interpolation, D-MG produces smooth, realistic, and task-consistent Cartesian trajectories that adapt in real time to changes in object poses, robot states, or scene geometry during task execution. Our method supports different scenarios - including scene layouts, object instances, and robot configurations - making it suitable for both static and highly dynamic manipulation tasks. We show that robot agents trained via imitation learning on D-MG-generated data achieve strong performance across long-horizon and contact-rich benchmarks, including tasks like cube stacking and placing mugs in drawers, even under unpredictable environment changes. By eliminating the need for extensive human demonstrations and enabling generalization in dynamic settings, D-MG offers a powerful and efficient alternative to manual data collection, paving the way toward scalable, autonomous robot learning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating data for robot learning of dynamic tasks, specifically using Dynamic Movement Primitives (DMPs) to generate trajectories. While it doesn't directly use Large Language Models, it does involve trajectory generation and prediction in dynamic environments, making it somewhat relevant to trajectory prediction. The use of DMPs for trajectory generalization is a key aspect.", "keywords": ["trajectory generation", "Dynamic Movement Primitives", "robot learning", "dynamic environments", "motion planning"]}}
{"id": "2511.16233", "pdf": "https://arxiv.org/pdf/2511.16233", "abs": "https://arxiv.org/abs/2511.16233", "authors": ["Kewei Chen", "Yayu Long", "Shuai Li", "Mingsheng Shang"], "title": "FT-NCFM: An Influence-Aware Data Distillation Framework for Efficient VLA Models", "categories": ["cs.RO"], "comment": "Accepted at the AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "The powerful generalization of Vision-Language-Action (VLA) models is bottlenecked by their heavy reliance on massive, redundant, and unevenly valued datasets, hindering their widespread application. Existing model-centric optimization paths, such as model compression (which often leads to performance degradation) or policy distillation (whose products are model-dependent and lack generality), fail to fundamentally address this data-level challenge. To this end, this paper introduces FT-NCFM, a fundamentally different, data-centric generative data distillation framework. Our framework employs a self-contained Fact-Tracing (FT) engine that combines causal attribution with programmatic contrastive verification to assess the intrinsic value of samples. Guided by these assessments, an adversarial NCFM process synthesizes a model-agnostic, information-dense, and reusable data asset. Experimental results on several mainstream VLA benchmarks show that models trained on just 5% of our distilled coreset achieve a success rate of 85-90% compared with training on the full dataset, while reducing training time by over 80%. Our work demonstrates that intelligent data distillation is a highly promising new path for building efficient, high-performance VLA models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action (VLA) models and data distillation for improved efficiency. While it doesn't explicitly mention trajectory prediction, the \"Action\" component of VLA models can be related to predicting future actions or movements, potentially including trajectories. The paper also uses large models (VLA models) and aims for efficient training, making it relevant to the large model aspect. The connection to trajectory prediction is somewhat indirect but plausible.", "keywords": ["Vision-Language-Action (VLA) models", "data distillation", "large models", "action"]}}
{"id": "2511.16372", "pdf": "https://arxiv.org/pdf/2511.16372", "abs": "https://arxiv.org/abs/2511.16372", "authors": ["Bowen Xu", "Zexuan Yan", "Minghao Lu", "Xiyu Fan", "Yi Luo", "Youshen Lin", "Zhiqiang Chen", "Yeke Chen", "Qiyuan Qiao", "Peng Lu"], "title": "Flow-Aided Flight Through Dynamic Clutters From Point To Motion", "categories": ["cs.RO"], "comment": "Accepted to IEEE Robotics and Automation Letters (RA-L), November, 2025", "summary": "Challenges in traversing dynamic clutters lie mainly in the efficient perception of the environmental dynamics and the generation of evasive behaviors considering obstacle movement. Previous solutions have made progress in explicitly modeling the dynamic obstacle motion for avoidance, but this key dependency of decision-making is time-consuming and unreliable in highly dynamic scenarios with occlusions. On the contrary, without introducing object detection, tracking, and prediction, we empower the reinforcement learning (RL) with single LiDAR sensing to realize an autonomous flight system directly from point to motion. For exteroception, a depth sensing distance map achieving fixed-shape, low-resolution, and detail-safe is encoded from raw point clouds, and an environment change sensing point flow is adopted as motion features extracted from multi-frame observations. These two are integrated into a lightweight and easy-to-learn representation of complex dynamic environments. For action generation, the behavior of avoiding dynamic threats in advance is implicitly driven by the proposed change-aware sensing representation, where the policy optimization is indicated by the relative motion modulated distance field. With the deployment-friendly sensing simulation and dynamics model-free acceleration control, the proposed system shows a superior success rate and adaptability to alternatives, and the policy derived from the simulator can drive a real-world quadrotor with safe maneuvers.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u98de\u884c\u5668\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u907f\u969c\u98de\u884c\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u4e86\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u9884\u6d4b\u548c\u884c\u4e3a\u751f\u6210\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u5173\u952e\u8bcd\u4e2d\u5305\u542b\u4e86\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u7684\u8fd0\u52a8\u3001\u52a8\u6001\u73af\u5883\u7b49\u3002", "keywords": ["motion", "dynamic environment", "reinforcement learning", "obstacle avoidance", "point flow"]}}
{"id": "2511.16407", "pdf": "https://arxiv.org/pdf/2511.16407", "abs": "https://arxiv.org/abs/2511.16407", "authors": ["Xizhou Bu", "Jiexi Lyu", "Fulei Sun", "Ruichen Yang", "Zhiqiang Ma", "Wei Li"], "title": "LAOF: Robust Latent Action Learning with Optical Flow Constraints", "categories": ["cs.RO"], "comment": "Code can be found at https://github.com/XizoB/LAOF", "summary": "Learning latent actions from large-scale videos is crucial for the pre-training of scalable embodied foundation models, yet existing methods often struggle with action-irrelevant distractors. Although incorporating action supervision can alleviate these distractions, its effectiveness is restricted by the scarcity of available action labels. Optical flow represents pixel-level motion between consecutive frames, naturally suppressing background elements and emphasizing moving objects. Motivated by this, we propose robust Latent Action learning with Optical Flow constraints, called LAOF, a pseudo-supervised framework that leverages the agent's optical flow as an action-driven signal to learn latent action representations robust to distractors. Experimental results show that the latent representations learned by LAOF outperform existing methods on downstream imitation learning and reinforcement learning tasks. This superior performance arises from optical flow constraints, which substantially stabilize training and improve the quality of latent representations under extremely label-scarce conditions, while remaining effective as the proportion of action labels increases to 10 percent. Importantly, even without action supervision, LAOF matches or surpasses action-supervised methods trained with 1 percent of action labels.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on learning latent actions from videos using optical flow constraints. While it mentions 'embodied foundation models' (related to large models), the core contribution revolves around learning action representations for imitation and reinforcement learning, which can be linked to action prediction (a form of trajectory prediction). The primary focus isn't directly trajectory prediction or LLMs, but the 'embodied foundation models' and action learning aspects suggest a moderate level of relevance.", "keywords": ["latent action learning", "optical flow", "embodied foundation models", "imitation learning", "reinforcement learning", "action prediction"]}}
{"id": "2511.16049", "pdf": "https://arxiv.org/pdf/2511.16049", "abs": "https://arxiv.org/abs/2511.16049", "authors": ["Pei Liu", "Songtao Wang", "Lang Zhang", "Xingyue Peng", "Yuandong Lyu", "Jiaxin Deng", "Songxin Lu", "Weiliang Ma", "Xueyang Zhang", "Yifei Zhan", "XianPeng Lang", "Jun Ma"], "title": "LiSTAR: Ray-Centric World Models for 4D LiDAR Sequences in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Synthesizing high-fidelity and controllable 4D LiDAR data is crucial for creating scalable simulation environments for autonomous driving. This task is inherently challenging due to the sensor's unique spherical geometry, the temporal sparsity of point clouds, and the complexity of dynamic scenes. To address these challenges, we present LiSTAR, a novel generative world model that operates directly on the sensor's native geometry. LiSTAR introduces a Hybrid-Cylindrical-Spherical (HCS) representation to preserve data fidelity by mitigating quantization artifacts common in Cartesian grids. To capture complex dynamics from sparse temporal data, it utilizes a Spatio-Temporal Attention with Ray-Centric Transformer (START) that explicitly models feature evolution along individual sensor rays for robust temporal coherence. Furthermore, for controllable synthesis, we propose a novel 4D point cloud-aligned voxel layout for conditioning and a corresponding discrete Masked Generative START (MaskSTART) framework, which learns a compact, tokenized representation of the scene, enabling efficient, high-resolution, and layout-guided compositional generation. Comprehensive experiments validate LiSTAR's state-of-the-art performance across 4D LiDAR reconstruction, prediction, and conditional generation, with substantial quantitative gains: reducing generation MMD by a massive 76%, improving reconstruction IoU by 32%, and lowering prediction L1 Med by 50%. This level of performance provides a powerful new foundation for creating realistic and controllable autonomous systems simulations. Project link: https://ocean-luna.github.io/LiSTAR.gitub.io.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d4D LiDAR\u6570\u636e\u7684\u751f\u6210\u548c\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86Transformer\u67b6\u6784\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528LLM\uff0c\u4f46\u5176\u4f7f\u7528\u7684\u751f\u6210\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\u4e0e\u5927\u6a21\u578b\u9886\u57df\u7684\u6280\u672f\u6709\u4ea4\u53c9\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5e76\u975e\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "autonomous driving", "LiDAR", "Transformer", "prediction", "generation"]}}
{"id": "2511.16651", "pdf": "https://arxiv.org/pdf/2511.16651", "abs": "https://arxiv.org/abs/2511.16651", "authors": ["Yang Tian", "Yuyin Yang", "Yiman Xie", "Zetao Cai", "Xu Shi", "Ning Gao", "Hangxu Liu", "Xuekun Jiang", "Zherui Qiu", "Feng Yuan", "Yaping Li", "Ping Wang", "Junhao Cai", "Jia Zeng", "Hao Dong", "Jiangmiao Pang"], "title": "InternData-A1: Pioneering High-Fidelity Synthetic Data for Pre-training Generalist Policy", "categories": ["cs.RO"], "comment": null, "summary": "Recent works explore how real and synthetic data contribute to Vision-Language-Action (VLA) models' generalization. While current VLA models have shown the strong effectiveness of large-scale real-robot pre-training, synthetic data has not previously demonstrated comparable capability at scale. This paper provides the first evidence that synthetic data alone can match the performance of the strongest $\u03c0$-dataset in pre-training a VLA model, revealing the substantial value of large-scale simulation. The resulting model also exhibits surprisingly zero-shot sim-to-real transfer on several challenging tasks. Our synthetic dataset, InternData-A1, contains over 630k trajectories and 7,433 hours across 4 embodiments, 18 skills, 70 tasks, and 227 scenes, covering rigid, articulated, deformable, and fluid-object manipulation. It is generated through a highly autonomous, fully decoupled, and compositional simulation pipeline that enables long-horizon skill composition, flexible task assembly, and heterogeneous embodiments with minimal manual tuning. Using the same architecture as $\u03c0_0$, we pre-train a model entirely on InternData-A1 and find that it matches the official $\u03c0_0$ across 49 simulation tasks, 5 real-world tasks, and 4 long-horizon dexterous tasks. We release the dataset and will open-source the generation pipeline to broaden access to large-scale robotic data and to lower the barrier to scalable data creation for embodied AI research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating synthetic data for pre-training Vision-Language-Action (VLA) models, which are related to large models. The dataset contains trajectories, indicating a connection to trajectory prediction, although the primary focus is on robotic manipulation and embodied AI rather than trajectory prediction in the traditional sense. The use of 'trajectories' and the mention of 'large-scale' data pre-training contribute to the relevance.", "keywords": ["trajectories", "large-scale", "pre-training", "VLA models", "embodied AI"]}}
{"id": "2511.16043", "pdf": "https://arxiv.org/pdf/2511.16043", "abs": "https://arxiv.org/abs/2511.16043", "authors": ["Peng Xia", "Kaide Zeng", "Jiaqi Liu", "Can Qin", "Fang Wu", "Yiyang Zhou", "Caiming Xiong", "Huaxiu Yao"], "title": "Agent0: Unleashing Self-Evolving Agents from Zero Data via Tool-Integrated Reasoning", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Model (LLM) Agents, often trained with Reinforcement Learning (RL), are constrained by a dependency on human-curated data, limiting scalability and tethering AI to human knowledge. Existing self-evolution frameworks offer an alternative but are typically restricted by the model's inherent capabilities and single-round interactions, hindering the development of complex curricula involving tool use or dynamic reasoning. We introduce Agent0, a fully autonomous framework that evolves high-performing agents without external data through multi-step co-evolution and seamless tool integration. Agent0 establishes a symbiotic competition between two agents initialized from the same base LLM: a curriculum agent that proposes increasingly challenging frontier tasks, and an executor agent that learns to solve them. We integrate external tools to enhance the executor's problem-solving capacity; this improvement, in turn, pressures the curriculum agent to construct more complex, tool-aware tasks. Through this iterative process, Agent0 establishes a self-reinforcing cycle that continuously produces high-quality curricula. Empirically, Agent0 substantially boosts reasoning capabilities, improving the Qwen3-8B-Base model by 18% on mathematical reasoning and 24% on general reasoning benchmarks. Code is available at https://github.com/aiming-lab/Agent0.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Large Language Model (LLM) agents and their self-evolution through tool integration and curriculum learning. While it doesn't directly address trajectory prediction, the concept of agents learning to solve tasks autonomously could potentially be relevant to future applications in areas like autonomous navigation and robotic path planning. The use of LLMs is a strong indicator of relevance to the 'Large Language Models' aspect.", "keywords": ["Large Language Model", "LLM", "Agents", "Reasoning", "Tool Integration", "Self-Evolution"]}}
{"id": "2511.16183", "pdf": "https://arxiv.org/pdf/2511.16183", "abs": "https://arxiv.org/abs/2511.16183", "authors": ["Jeremie Ochin", "Raphael Chekroun", "Bogdan Stanciulescu", "Sotiris Manitsaris"], "title": "FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses trajectory forecasting in the context of soccer player movement and tactical modeling. While it doesn't explicitly mention or utilize large language models, the focus on trajectory forecasting and tactical context provides some relevance. The abstract mentions leveraging tactical knowledge as a prior to support computer-vision-based predictions, which could potentially be enhanced by LLMs in the future.", "keywords": ["trajectory forecasting", "tactical modeling", "action spotting", "multi-agent"]}}
{"id": "2511.16150", "pdf": "https://arxiv.org/pdf/2511.16150", "abs": "https://arxiv.org/abs/2511.16150", "authors": ["Chunxu Liu", "Jiyuan Yang", "Ruopeng Gao", "Yuhan Zhu", "Feng Zhu", "Rui Zhao", "Limin Wang"], "title": "Reasoning Guided Embeddings: Leveraging MLLM Reasoning for Improved Multimodal Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal embeddings are widely used in downstream tasks such as multimodal retrieval, enabling alignment of interleaved modalities in a shared representation space. While recent studies show that Multimodal Large Language Models (MLLMs) can serve as strong embedding extractors, existing approaches treat embedding extraction as a direct encoding step, overlooking the fact that MLLMs possess the generative capability for reasoning that could be leveraged to enhance representation quality. In this work, we explore how to explicitly incorporate reasoning into the embedding process. To this end, we propose Reasoning Guided Embeddings (RGE), which preserves the generative rationale process of MLLMs and couples it with contrastive training. Our method first enables the model to perform structured rationale generation conditioned on the instruction, and then extracts representations after reasoning has unfolded. This simple design enhances the context-conditional inference signals within the embedding, leading to improved multimodal representation quality. Experiments on the MMEB benchmark show that reasoning-guided conditioning improves multimodal retrieval performance by 4.9% over the non-reasoning baseline, confirming that explicit reasoning can effectively enhance embedding quality.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving multimodal retrieval using Multimodal Large Language Models (MLLMs) by incorporating a reasoning process. While it doesn't directly address trajectory prediction, it utilizes large language models and explores embedding techniques, which could potentially be applied to trajectory prediction tasks in the future. The connection is not direct but the underlying technology (MLLMs) is highly relevant.", "keywords": ["Multimodal Large Language Models", "MLLMs", "embeddings", "reasoning", "multimodal retrieval"]}}
{"id": "2511.16160", "pdf": "https://arxiv.org/pdf/2511.16160", "abs": "https://arxiv.org/abs/2511.16160", "authors": ["Yibin Huang", "Wang Xu", "Wanyue Zhang", "Helu Zhi", "Jingjing Huang", "Yangbin Xu", "Yangang Sun", "Conghui Zhu", "Tiejun Zhao"], "title": "Video2Layout: Recall and Reconstruct Metric-Grounded Cognitive Map for Spatial Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Spatial intelligence is a critical frontier for Multimodal Large Language Models (MLLMs), empowering them to comprehend the physical world. Drawing inspiration from human perception mechanisms, existing studies attempt to construct a coherent spatial understanding via grid-based cognitive maps from multi-frame visual inputs. However, current grid-based map methods rely on discretized raster representations, which limit the model's ability in fine-grained spatial reasoning. To overcome this limitation, we propose Video2Layout, a framework for reconstructing metric-grounded spatial layouts from video. The framework employs continuous object boundary coordinates to quantify inter-object physical distances and object size. This empowers the model with quantitative spatial computation capabilities, effectively alleviating the inherent ambiguity when describing spatial relationships in natural language. Specifically, our method comprises two core stages. First, in supervised fine-tuning stage, we construct a high-quality dataset from the AI2THOR simulator, which enables the model to learn the mapping from visual inputs to precise boundary coordinates. Subsequently, a reinforcement fine-tuning stage further enhances the model's real-world generalization capabilities. To systematically evaluate the correlation between cognitive map accuracy and image quantity, as well as how the quantity of image inputs affects spatial reasoning accuracy, we introduce QVS-Bench, a diagnostic benchmark designed to analyze the relevant mechanisms. Evaluated on QVS-Bench and mainstream spatial reasoning benchmarks, our model, V2LO-7B achieves an average improvement of 4.92% over the model trained on grid maps, validating the superiority of our method. Our code is available at https://github.com/ybrrraway/Video2Layout.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLMs)\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u4ece\u89c6\u9891\u4e2d\u91cd\u5efa\u5ea6\u91cf\u5316\u7684\u7a7a\u95f4\u5e03\u5c40\u6765\u63d0\u9ad8\u6a21\u578b\u5bf9\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u57fa\u7840\uff0c\u5e76\u4e14\u8bba\u6587\u660e\u786e\u63d0\u5230\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Multimodal Large Language Models (MLLMs)", "spatial reasoning", "cognitive map", "foundation models"]}}
{"id": "2511.16166", "pdf": "https://arxiv.org/pdf/2511.16166", "abs": "https://arxiv.org/abs/2511.16166", "authors": ["Zeting Liu", "Zida Yang", "Zeyu Zhang", "Hao Tang"], "title": "EvoVLA: Self-Evolving Vision-Language-Action Model", "categories": ["cs.CV"], "comment": null, "summary": "Long-horizon robotic manipulation remains challenging for Vision-Language-Action (VLA) models despite recent progress in zero-shot generalization and simulation-to-real-world transfer. Current VLA models suffer from stage hallucination, where agents exploit coarse evaluation signals to shortcut multi-step tasks, reporting high progress without truly completing them. We present EvoVLA, a self-supervised VLA framework that addresses this issue through three complementary components: Stage-Aligned Reward (SAR), which uses triplet contrastive learning with Gemini-generated hard negatives to prevent visual shortcuts; Pose-Based Object Exploration (POE), which grounds curiosity in relative object-gripper pose instead of raw pixels; and Long-Horizon Memory, which uses selective context retention and gated fusion to stabilize intrinsic shaping during extended rollouts. Extensive evaluations on Discoverse-L, a long-horizon manipulation benchmark with three multi-stage tasks, show that EvoVLA improves average task success by 10.2 percentage points over the strongest baseline (OpenVLA-OFT), reaching 69.2 percent. EvoVLA also achieves one-and-a-half times better sample efficiency and reduces stage hallucination from 38.5 percent to 14.8 percent. Real-world deployment on physical robots reaches an average success rate of 54.6 percent across four manipulation tasks, outperforming OpenVLA-OFT by 11 points, demonstrating effective sim-to-real transfer and strong generalization. Code: https://github.com/AIGeeksGroup/EvoVLA. Website: https://aigeeksgroup.github.io/EvoVLA.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action models for robotic manipulation. While it doesn't directly address trajectory prediction in the traditional sense (e.g., pedestrian or vehicle), the 'action' component and 'long-horizon' aspects suggest a form of trajectory planning or sequential decision-making. The use of Gemini (a large model) is also a relevant factor. Therefore, there's a moderate level of connection to both trajectory prediction and large models.", "keywords": ["Vision-Language-Action Models", "robotic manipulation", "long-horizon", "Gemini", "action", "pose-based object exploration"]}}
{"id": "2511.16175", "pdf": "https://arxiv.org/pdf/2511.16175", "abs": "https://arxiv.org/abs/2511.16175", "authors": ["Yi Yang", "Xueqi Li", "Yiyang Chen", "Jin Song", "Yihan Wang", "Zipeng Xiao", "Jiadi Su", "You Qiaoben", "Pengfei Liu", "Zhijie Deng"], "title": "Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $\u03c0_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53caVision-Language-Action\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u4e86\u89c6\u89c9\u9884\u6d4b\uff08Visual Foresight\uff09\u6765\u9884\u6d4b\u672a\u6765\u7684\u89c6\u89c9\u72b6\u6001\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u6b64\u5916\uff0c\u4f7f\u7528\u4e86Diffusion Transformer (DiT)\uff0c\u867d\u7136\u4e0d\u662f\u4f20\u7edf\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4e5f\u662f\u4e00\u79cd\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002 \u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e0d\u662f\u76f4\u63a5\u76f8\u5173\u3002", "keywords": ["Vision-Language-Action Model", "Visual Foresight", "Diffusion Transformer", "action prediction"]}}
{"id": "2511.16258", "pdf": "https://arxiv.org/pdf/2511.16258", "abs": "https://arxiv.org/abs/2511.16258", "authors": ["Yang Xu", "Zuliang Yang", "Kai Ming Ting"], "title": "GeoPTH: A Lightweight Approach to Category-Based Trajectory Retrieval via Geometric Prototype Trajectory Hashing", "categories": ["cs.LG"], "comment": null, "summary": "Trajectory similarity retrieval is an important part of spatiotemporal data mining, however, existing methods have the following limitations: traditional metrics are computationally expensive, while learning-based methods suffer from substantial training costs and potential instability. This paper addresses these problems by proposing \\textbf{Geo}metric \\textbf{P}rototype \\textbf{T}rajectory \\textbf{H}ashing (GeoPTH), a novel, lightweight, and non-learning framework for efficient category-based trajectory retrieval. GeoPTH constructs data-dependent hash functions by using representative trajectory prototypes, i.e., small point sets preserving geometric characteristics, as anchors. The hashing process is efficient, which involves mapping a new trajectory to its closest prototype via a robust, \\textit{Hausdorff} metric. Extensive experiments show that GeoPTH's retrieval accuracy is highly competitive with both traditional metrics and state-of-the-art learning methods, and it significantly outperforms binary codes generated through simple binarization of the learned embeddings. Critically, GeoPTH consistently outperforms all competitors in terms of efficiency. Our work demonstrates that a lightweight, prototype-centric approach offers a practical and powerful alternative, achieving an exceptional retrieval performance and computational efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory retrieval using geometric hashing. While it deals with trajectories, it doesn't involve trajectory prediction or large language models. The connection is limited to the general domain of trajectory data.", "keywords": ["trajectory retrieval", "spatiotemporal data mining", "trajectory hashing"]}}
{"id": "2511.16264", "pdf": "https://arxiv.org/pdf/2511.16264", "abs": "https://arxiv.org/abs/2511.16264", "authors": ["Sinan Mutlu", "Georgios F. Angelis", "Savas Ozkan", "Paul Wisbey", "Anastasios Drosou", "Mete Ozay"], "title": "Mem-MLP: Real-Time 3D Human Motion Generation from Sparse Inputs", "categories": ["cs.CV"], "comment": null, "summary": "Realistic and smooth full-body tracking is crucial for immersive AR/VR applications. Existing systems primarily track head and hands via Head Mounted Devices (HMDs) and controllers, making the 3D full-body reconstruction in-complete. One potential approach is to generate the full-body motions from sparse inputs collected from limited sensors using a Neural Network (NN) model. In this paper, we propose a novel method based on a multi-layer perceptron (MLP) backbone that is enhanced with residual connections and a novel NN-component called Memory-Block. In particular, Memory-Block represents missing sensor data with trainable code-vectors, which are combined with the sparse signals from previous time instances to improve the temporal consistency. Furthermore, we formulate our solution as a multi-task learning problem, allowing our MLP-backbone to learn robust representations that boost accuracy. Our experiments show that our method outperforms state-of-the-art baselines by substantially reducing prediction errors. Moreover, it achieves 72 FPS on mobile HMDs that ultimately improves the accuracy-running time tradeoff.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on 3D human motion generation from sparse inputs, which is related to trajectory prediction and action prediction. While it uses neural networks (MLP), it doesn't explicitly mention or utilize large language models. The core idea aligns with predicting the movement of a human body given limited sensor data, thus a moderate relevance score.", "keywords": ["motion generation", "3D human motion", "Neural Network", "action prediction"]}}
{"id": "2511.16426", "pdf": "https://arxiv.org/pdf/2511.16426", "abs": "https://arxiv.org/abs/2511.16426", "authors": ["Seyed Mohamad Moghadas", "Bruno Cornelis", "Adrian Munteanu"], "title": "FreqFlow: Long-term forecasting using lightweight flow matching", "categories": ["cs.LG"], "comment": "Accepted at EurIPS, 2025", "summary": "Multivariate time-series (MTS) forecasting is fundamental to applications ranging from urban mobility and resource management to climate modeling. While recent generative models based on denoising diffusion have advanced state-of-the-art performance in capturing complex data distributions, they suffer from significant computational overhead due to iterative stochastic sampling procedures that limit real-time deployment. Moreover, these models can be brittle when handling high-dimensional, non-stationary, and multi-scale periodic patterns characteristic of real-world sensor networks. We introduce FreqFlow, a novel framework that leverages conditional flow matching in the frequency domain for deterministic MTS forecasting. Unlike conventional approaches that operate in the time domain, FreqFlow transforms the forecasting problem into the spectral domain, where it learns to model amplitude and phase shifts through a single complex-valued linear layer. This frequency-domain formulation enables the model to efficiently capture temporal dynamics via complex multiplication, corresponding to scaling and temporal translations. The resulting architecture is exceptionally lightweight with only 89k parameters - an order of magnitude smaller than competing diffusion-based models-while enabling single-pass deterministic sampling through ordinary differential equation (ODE) integration. Our approach decomposes MTS signals into trend, seasonal, and residual components, with the flow matching mechanism specifically designed for residual learning to enhance long-term forecasting accuracy. Extensive experiments on real-world traffic speed, volume, and flow datasets demonstrate that FreqFlow achieves state-of-the-art forecasting performance, on average 7\\% RMSE improvements, while being significantly faster and more parameter-efficient than existing methods", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u7279\u522b\u662f\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u4e2a\u5e94\u7528\u9886\u57df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u91c7\u7528\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u7684flow matching\u6280\u672f\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u6280\u672f\u7684\u4e00\u79cd\u5173\u8054\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "time-series forecasting", "flow matching", "traffic speed", "traffic volume", "traffic flow"]}}
