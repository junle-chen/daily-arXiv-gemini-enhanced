{"id": "2510.12642", "pdf": "https://arxiv.org/pdf/2510.12642", "abs": "https://arxiv.org/abs/2510.12642", "authors": ["Meihui Zhang", "Liming Wang", "Chi Zhang", "Zhaojing Luo"], "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data Analysis", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."}
{"id": "2510.11813", "pdf": "https://arxiv.org/pdf/2510.11813", "abs": "https://arxiv.org/abs/2510.11813", "authors": ["Marcus Emmanuel Barnes", "Taher A. Ghaleb", "Safwat Hassan"], "title": "Task-Aware Reduction for Scalable LLM-Database Systems", "categories": ["cs.SE", "cs.CL", "cs.DB"], "comment": "Preprint. Accepted for presentation at the Workshop on Language\n  Models and Databases (LMD), co-located with CASCON 2025 (IEEE). The final\n  version will appear in IEEE Xplore", "summary": "Large Language Models (LLMs) are increasingly applied to data-intensive\nworkflows, from database querying to developer observability. Yet the\neffectiveness of these systems is constrained by the volume, verbosity, and\nnoise of real-world text-rich data such as logs, telemetry, and monitoring\nstreams. Feeding such data directly into LLMs is costly, environmentally\nunsustainable, and often misaligned with task objectives. Parallel efforts in\nLLM efficiency have focused on model- or architecture-level optimizations, but\nthe challenge of reducing upstream input verbosity remains underexplored. In\nthis paper, we argue for treating the token budget of an LLM as an attention\nbudget and elevating task-aware text reduction as a first-class design\nprinciple for language -- data systems. We position input-side reduction not as\ncompression, but as attention allocation: prioritizing information most\nrelevant to downstream tasks. We outline open research challenges for building\nbenchmarks, designing adaptive reduction pipelines, and integrating\ntoken-budget--aware preprocessing into database and retrieval systems. Our\nvision is to channel scarce attention resources toward meaningful signals in\nnoisy, data-intensive workflows, enabling scalable, accurate, and sustainable\nLLM--data integration."}
{"id": "2510.12280", "pdf": "https://arxiv.org/pdf/2510.12280", "abs": "https://arxiv.org/abs/2510.12280", "authors": ["Yosuke Bando", "Akinobu Mita", "Kazuhiro Hiwada", "Shintaro Sano", "Tomoya Suzuki", "Yu Nakanishi", "Kazutaka Tomida", "Hirotsugu Kajihara", "Akiyuki Kaneko", "Daisuke Taki", "Yukimasa Miyamoto", "Tomokazu Yoshida", "Tatsuo Shiozawa"], "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for In-Memory Indices and Caches in SSD-Based Key-Value Stores", "categories": ["cs.PF", "cs.DB"], "comment": null, "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."}
