{"id": "2511.12769", "pdf": "https://arxiv.org/pdf/2511.12769", "abs": "https://arxiv.org/abs/2511.12769", "authors": ["Luyao Niu", "Zepu Wang", "Shuyi Guan", "Yang Liu", "Peng Sun"], "title": "Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper explicitly uses a Large Language Model to enhance spatio-temporal forecasting, specifically in the context of traffic prediction. It combines LLMs with GNNs and LSTMs, making it highly relevant to both trajectory prediction and large language models.", "keywords": ["trajectory prediction", "spatio-temporal forecasting", "Large Language Model", "GNN", "LSTM", "traffic management"]}}
{"id": "2511.11967", "pdf": "https://arxiv.org/pdf/2511.11967", "abs": "https://arxiv.org/abs/2511.11967", "authors": ["Mani Amani", "Behrad Beheshti", "Reza Akhavian"], "title": "Bootstrapped LLM Semantics for Context-Aware Path Planning", "categories": ["cs.RO"], "comment": null, "summary": "Prompting robots with natural language (NL) has largely been studied as what task to execute (goal selection, skill sequencing) rather than how to execute that task safely and efficiently in semantically rich, human-centric spaces. We address this gap with a framework that turns a large language model (LLM) into a stochastic semantic sensor whose outputs modulate a classical planner. Given a prompt and a semantic map, we draw multiple LLM \"danger\" judgments and apply a Bayesian bootstrap to approximate a posterior over per-class risk. Using statistics from the posterior, we create a potential cost to formulate a path planning problem. Across simulated environments and a BIM-backed digital twin, our method adapts how the robot moves in response to explicit prompts and implicit contextual information. We present qualitative and quantitative results.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper directly combines large language models (LLMs) with path planning, which is a type of trajectory prediction. The LLM is used to generate semantic information that influences the path planning process, indicating a strong connection between the two fields.", "keywords": ["trajectory prediction", "path planning", "large language models", "LLM", "semantic map", "robot", "context-aware"]}}
{"id": "2511.12214", "pdf": "https://arxiv.org/pdf/2511.12214", "abs": "https://arxiv.org/abs/2511.12214", "authors": ["Ruochen Li", "Zhanxing Zhu", "Tanqiu Qiao", "Hubert P. H. Shum"], "title": "ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on pedestrian trajectory prediction using graph neural networks. While it doesn't directly involve large language models, the core topic is trajectory prediction, making it relevant. The use of \"expert router\" could be seen as a connection, albeit a weak one, to Mixture-of-Experts architectures sometimes used in LLMs.", "keywords": ["trajectory prediction", "pedestrian trajectory prediction", "Graph Neural Network", "GNN", "expert router"]}}
{"id": "2511.12061", "pdf": "https://arxiv.org/pdf/2511.12061", "abs": "https://arxiv.org/abs/2511.12061", "authors": ["Zhichen Lai", "Hua Lu", "Huan Li", "Jialiang Li", "Christian S. Jensen"], "title": "MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity", "categories": ["cs.CV", "cs.AI", "cs.DB"], "comment": "8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper", "summary": "Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u76f8\u4f3c\u6027\u8ba1\u7b97\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u76f8\u5173\u9886\u57df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u6210\u679c\u53ef\u4ee5\u5e94\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5e76\u53ef\u80fd\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u4f7f\u7528\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u4e86\u8f68\u8ff9\u7684\u9884\u6d4b\u5e94\u7528\u3002", "keywords": ["trajectory similarity", "trajectory prediction", "movement dynamics", "trajectory semantics"]}}
{"id": "2511.11931", "pdf": "https://arxiv.org/pdf/2511.11931", "abs": "https://arxiv.org/abs/2511.11931", "authors": ["Saida Liu", "Nikolay Atanasov", "Shumon Koga"], "title": "MATT-Diff: Multimodal Active Target Tracking by Diffusion Policy", "categories": ["cs.RO"], "comment": "14 pages, 3 figures. Submitted to L4DC 2026", "summary": "This paper proposes MATT-Diff: Multi-Modal Active Target Tracking by Diffusion Policy, a control policy that captures multiple behavioral modes - exploration, dedicated tracking, and target reacquisition - for active multi-target tracking. The policy enables agent control without prior knowledge of target numbers, states, or dynamics. Effective target tracking demands balancing exploration for undetected or lost targets with following the motion of detected but uncertain ones. We generate a demonstration dataset from three expert planners including frontier-based exploration, an uncertainty-based hybrid planner switching between frontier-based exploration and RRT* tracking based on target uncertainty, and a time-based hybrid planner switching between exploration and tracking based on target detection time. We design a control policy utilizing a vision transformer for egocentric map tokenization and an attention mechanism to integrate variable target estimates represented by Gaussian densities. Trained as a diffusion model, the policy learns to generate multi-modal action sequences through a denoising process. Evaluations demonstrate MATT-Diff's superior tracking performance against expert and behavior cloning baselines across multiple target motions, empirically validating its advantages in target tracking.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u4f7f\u7528\u4e86\u6269\u6563\u6a21\u578b\uff08Diffusion Model\uff09\u6765\u751f\u6210\u591a\u6a21\u6001\u7684\u52a8\u4f5c\u5e8f\u5217\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5927\u578b\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "target tracking", "diffusion model", "multi-modal action sequences", "action prediction"]}}
{"id": "2511.11824", "pdf": "https://arxiv.org/pdf/2511.11824", "abs": "https://arxiv.org/abs/2511.11824", "authors": ["Zhongping Dong", "Pengyang Yu", "Shuangjian Li", "Liming Chen", "Mohand Tahar Kechadi"], "title": "SOTFormer: A Minimal Transformer for Unified Object Tracking and Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Accurate single-object tracking and short-term motion forecasting remain challenging under occlusion, scale variation, and temporal drift, which disrupt the temporal coherence required for real-time perception. We introduce \\textbf{SOTFormer}, a minimal constant-memory temporal transformer that unifies object detection, tracking, and short-horizon trajectory prediction within a single end-to-end framework. Unlike prior models with recurrent or stacked temporal encoders, SOTFormer achieves stable identity propagation through a ground-truth-primed memory and a burn-in anchor loss that explicitly stabilizes initialization. A single lightweight temporal-attention layer refines embeddings across frames, enabling real-time inference with fixed GPU memory. On the Mini-LaSOT (20%) benchmark, SOTFormer attains 76.3 AUC and 53.7 FPS (AMP, 4.3 GB VRAM), outperforming transformer baselines such as TrackFormer and MOTRv2 under fast motion, scale change, and occlusion.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u8f68\u8ff9\u9884\u6d4b\uff08Trajectory Prediction\uff09\u548cTransformer\u67b6\u6784\uff0cTransformer\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u76f8\u5bf9\u8f83\u5c0f\u7684\u6a21\u578b\uff0c\u4f46\u4e5f\u662f\u5927\u6a21\u578b\u7684\u57fa\u7840\u3002\u8bba\u6587\u5c06\u76ee\u6807\u8ddf\u8e2a\u548c\u8f68\u8ff9\u9884\u6d4b\u7edf\u4e00\u5728\u4e00\u4e2a\u6846\u67b6\u4e2d\uff0c\u5e76\u4f7f\u7528Transformer\u8fdb\u884c\u65f6\u95f4\u5efa\u6a21\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "transformer", "object tracking", "motion forecasting"]}}
{"id": "2511.12232", "pdf": "https://arxiv.org/pdf/2511.12232", "abs": "https://arxiv.org/abs/2511.12232", "authors": ["Lingfeng Zhang", "Erjia Xiao", "Xiaoshuai Hao", "Haoxiang Fu", "Zeying Gong", "Long Chen", "Xiaojun Liang", "Renjing Xu", "Hangjun Ye", "Wenbo Ding"], "title": "SocialNav-Map: Dynamic Mapping with Human Trajectory Prediction for Zero-Shot Social Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Social navigation in densely populated dynamic environments poses a significant challenge for autonomous mobile robots, requiring advanced strategies for safe interaction. Existing reinforcement learning (RL)-based methods require over 2000+ hours of extensive training and often struggle to generalize to unfamiliar environments without additional fine-tuning, limiting their practical application in real-world scenarios. To address these limitations, we propose SocialNav-Map, a novel zero-shot social navigation framework that combines dynamic human trajectory prediction with occupancy mapping, enabling safe and efficient navigation without the need for environment-specific training. Specifically, SocialNav-Map first transforms the task goal position into the constructed map coordinate system. Subsequently, it creates a dynamic occupancy map that incorporates predicted human movements as dynamic obstacles. The framework employs two complementary methods for human trajectory prediction: history prediction and orientation prediction. By integrating these predicted trajectories into the occupancy map, the robot can proactively avoid potential collisions with humans while efficiently navigating to its destination. Extensive experiments on the Social-HM3D and Social-MP3D datasets demonstrate that SocialNav-Map significantly outperforms state-of-the-art (SOTA) RL-based methods, which require 2,396 GPU hours of training. Notably, it reduces human collision rates by over 10% without necessitating any training in novel environments. By eliminating the need for environment-specific training, SocialNav-Map achieves superior navigation performance, paving the way for the deployment of social navigation systems in real-world environments characterized by diverse human behaviors. The code is available at: https://github.com/linglingxiansen/SocialNav-Map.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u8f68\u8ff9\u9884\u6d4b\u8fdb\u884c\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u660e\u786e\u63d0\u5230\u4e86 human trajectory prediction\uff0c\u4f46\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5e76\u975e\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "human trajectory prediction", "social navigation", "dynamic mapping"]}}
{"id": "2511.12755", "pdf": "https://arxiv.org/pdf/2511.12755", "abs": "https://arxiv.org/abs/2511.12755", "authors": ["Aleesha Khurram", "Amir Moeini", "Shangtong Zhang", "Rohan Chandra"], "title": "Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u63d0\u793a\u9a71\u52a8\u7684\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5f3a\u5316\u5b66\u4e60\u6765\u6539\u8fdb\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u4e0d\u540c\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002\u867d\u7136\u8bba\u6587\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\u672c\u8d28\u4e0a\u4e0e\u8f68\u8ff9\u89c4\u5212\u548c\u63a7\u5236\u5bc6\u5207\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4f7f\u7528\u4e86LLMs\u548cVLMs\u7684\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u6574\u4f53\u6765\u770b\uff0c\u8bba\u6587\u5c06\u5927\u6a21\u578b\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\u7684\u4efb\u52a1\uff0c\u5177\u5907\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["autonomous driving", "domain adaptation", "LLMs", "VLMs", "in-context learning", "reinforcement learning"]}}
{"id": "2511.12882", "pdf": "https://arxiv.org/pdf/2511.12882", "abs": "https://arxiv.org/abs/2511.12882", "authors": ["Taiyi Su", "Jian Zhu", "Yaxuan Li", "Chong Ma", "Zitai Huang", "Yichen Zhu", "Hanli Wang", "Yi Xu"], "title": "Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos", "categories": ["cs.RO", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5177\u8eab\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u591a\u89c6\u89d2\u8f68\u8ff9\u89c6\u9891\u8fdb\u884c\u7cbe\u786e\u7684\u89c6\u89c9\u8fd0\u52a8\u9884\u6d4b\uff0c\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u3002\u540c\u65f6\uff0c\u8bba\u6587\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u4f46\u6574\u4f53\u4fa7\u91cd\u4e8e\u8f68\u8ff9\u9884\u6d4b\u5728\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u5e94\u7528\uff0c\u800c\u975e\u5927\u6a21\u578b\u672c\u8eab\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "trajectory videos", "multi-view", "large models", "embodied world model", "visuomotor prediction"]}}
{"id": "2511.11616", "pdf": "https://arxiv.org/pdf/2511.11616", "abs": "https://arxiv.org/abs/2511.11616", "authors": ["Rathin Chandra Shit", "Sharmila Subudhi"], "title": "Hierarchical Federated Graph Attention Networks for Scalable and Resilient UAV Collision Avoidance", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted and scheduled for conference presentation", "summary": "The real-time performance, adversarial resiliency, and privacy preservation are the most important metrics that need to be balanced to practice collision avoidance in large-scale multi-UAV (Unmanned Aerial Vehicle) systems. Current frameworks tend to prescribe monolithic solutions that are not only prohibitively computationally complex with a scaling cost of $O(n^2)$ but simply do not offer Byzantine fault tolerance. The proposed hierarchical framework presented in this paper tries to eliminate such trade-offs by stratifying a three-layered architecture. We spread the intelligence into three layers: an immediate collision avoiding local layer running on dense graph attention with latency of $<10 ms$, a regional layer using sparse attention with $O(nk)$ computational complexity and asynchronous federated learning with coordinate-wise trimmed mean aggregation, and lastly, a global layer using a lightweight Hashgraph-inspired protocol. We have proposed an adaptive differential privacy mechanism, wherein the noise level $(\u03b5\\in [0.1, 1.0])$ is dynamically reduced based on an evaluation of the measured real-time threat that in turn maximized the privacy-utility tradeoff. Through the use of Distributed Hash Table (DHT)-based lightweight audit logging instead of heavyweight blockchain consensus, the median cost of getting a $95^{th}$ percentile decision within 50ms is observed across all tested swarm sizes. This architecture provides a scalable scenario of 500 UAVs with a collision rate of $< 2.0\\%$ and the Byzantine fault tolerance of $f < n/3$.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on UAV collision avoidance using graph attention networks and federated learning. While it involves trajectory prediction in the context of avoiding collisions, it doesn't explicitly mention or utilize large language models. The use of graph attention networks and federated learning suggests some relevance to modern AI techniques, but the core focus is on collision avoidance and distributed computing rather than LLMs. The 'UAV collision avoidance' and 'trajectory prediction' aspects contribute to the relevance score.", "keywords": ["trajectory prediction", "collision avoidance", "UAV", "graph attention networks", "federated learning"]}}
{"id": "2511.11885", "pdf": "https://arxiv.org/pdf/2511.11885", "abs": "https://arxiv.org/abs/2511.11885", "authors": ["Kausar Patherya", "Ashutosh Dhekne", "Francisco Romero"], "title": "Flash-Fusion: Enabling Expressive, Low-Latency Queries on IoT Sensor Streams with LLMs", "categories": ["cs.DC", "cs.AI", "cs.DB"], "comment": "12 pages, 5 figures. Under review", "summary": "Smart cities and pervasive IoT deployments have generated interest in IoT data analysis across transportation and urban planning. At the same time, Large Language Models offer a new interface for exploring IoT data - particularly through natural language. Users today face two key challenges when working with IoT data using LLMs: (1) data collection infrastructure is expensive, producing terabytes of low-level sensor readings that are too granular for direct use, and (2) data analysis is slow, requiring iterative effort and technical expertise. Directly feeding all IoT telemetry to LLMs is impractical due to finite context windows, prohibitive token costs at scale, and non-interactive latencies. What is missing is a system that first parses a user's query to identify the analytical task, then selects the relevant data slices, and finally chooses the right representation before invoking an LLM.\n  We present Flash-Fusion, an end-to-end edge-cloud system that reduces the IoT data collection and analysis burden on users. Two principles guide its design: (1) edge-based statistical summarization (achieving 73.5% data reduction) to address data volume, and (2) cloud-based query planning that clusters behavioral data and assembles context-rich prompts to address data interpretation. We deploy Flash-Fusion on a university bus fleet and evaluate it against a baseline that feeds raw data to a state-of-the-art LLM. Flash-Fusion achieves a 95% latency reduction and 98% decrease in token usage and cost while maintaining high-quality responses. It enables personas across disciplines - safety officers, urban planners, fleet managers, and data scientists - to efficiently iterate over IoT data without the burden of manual query authoring or preprocessing.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528LLM\u5904\u7406IoT\u4f20\u611f\u5668\u6570\u636e\uff0c\u867d\u7136\u63d0\u5230\u4e86\u4ea4\u901a\u8fd0\u8f93\uff08university bus fleet\uff09\uff0c\u4f46\u91cd\u70b9\u5728\u4e8e\u6570\u636e\u5904\u7406\u548c\u67e5\u8be2\u6548\u7387\uff0c\u800c\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u3002\u4e0eLLM\u7684\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u76f8\u5173\u6027\u8f83\u4f4e\uff0c\u4f46\u5b58\u5728\u6f5c\u5728\u7684\u5e94\u7528\u573a\u666f\u3002", "keywords": ["Large Language Models", "LLMs", "IoT", "sensor streams", "data analysis"]}}
{"id": "2511.12979", "pdf": "https://arxiv.org/pdf/2511.12979", "abs": "https://arxiv.org/abs/2511.12979", "authors": ["Zhengchao Wang", "Yitao Hu", "Jianing Ye", "Zhuxuan Chang", "Jiazheng Yu", "Youpeng Deng", "Keqiu Li"], "title": "RAGPulse: An Open-Source RAG Workload Trace to Optimize RAG Serving Systems", "categories": ["cs.LG", "cs.DB"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) is a critical paradigm for building reliable, knowledge-intensive Large Language Model (LLM) applications. However, the multi-stage pipeline (retrieve, generate) and unique workload characteristics (e.g., knowledge dependency) of RAG systems pose significant challenges for serving performance optimization. Existing generic LLM inference traces fail to capture these RAG-specific dynamics, creating a significant performance gap between academic research and real-world deployment. To bridge this gap, this paper introduces RAGPulse, an open-source RAG workload trace dataset. This dataset was collected from an university-wide Q&A system serving that has served more than 40,000 students and faculties since April 2024. We detail RAGPulse's system architecture, its privacy-preserving hash-based data format, and provide an in-depth statistical analysis. Our analysis reveals that real-world RAG workloads exhibit significant temporal locality and a highly skewed hot document access pattern. RAGPulse provides a high-fidelity foundation for researchers to develop and validate novel optimization strategies for RAG systems, such as content-aware batching and retrieval caching, ultimately enhancing the efficiency and reliability of RAG services. The code is available at https://github.com/flashserve/RAGPulse.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Retrieval-Augmented Generation (RAG) which is a common technique used with Large Language Models (LLMs). While it doesn't directly address trajectory prediction, it is relevant because it deals with optimizing the performance of LLM applications. The paper discusses the unique workload characteristics of RAG systems and introduces a dataset for optimizing RAG serving systems. Therefore, it has some relevance to the large language model aspect but not the trajectory prediction aspect.", "keywords": ["Large Language Model", "LLM", "Retrieval-Augmented Generation", "RAG", "knowledge-intensive"]}}
{"id": "2511.11740", "pdf": "https://arxiv.org/pdf/2511.11740", "abs": "https://arxiv.org/abs/2511.11740", "authors": ["Haowen Jiang", "Xinyu Huang", "You Lu", "Dingji Wang", "Yuheng Cao", "Chaofeng Sha", "Bihuan Chen", "Keyu Chen", "Xin Peng"], "title": "ExpertAD: Enhancing Autonomous Driving Systems with Mixture of Experts", "categories": ["cs.RO", "cs.AI"], "comment": "The paper has been accepted by the Fortieth AAAI Conference on Artificial Intelligence. AAAI 2026", "summary": "Recent advancements in end-to-end autonomous driving systems (ADSs) underscore their potential for perception and planning capabilities. However, challenges remain. Complex driving scenarios contain rich semantic information, yet ambiguous or noisy semantics can compromise decision reliability, while interference between multiple driving tasks may hinder optimal planning. Furthermore, prolonged inference latency slows decision-making, increasing the risk of unsafe driving behaviors. To address these challenges, we propose ExpertAD, a novel framework that enhances the performance of ADS with Mixture of Experts (MoE) architecture. We introduce a Perception Adapter (PA) to amplify task-critical features, ensuring contextually relevant scene understanding, and a Mixture of Sparse Experts (MoSE) to minimize task interference during prediction, allowing for effective and efficient planning. Our experiments show that ExpertAD reduces average collision rates by up to 20% and inference latency by 25% compared to prior methods. We further evaluate its multi-skill planning capabilities in rare scenarios (e.g., accidents, yielding to emergency vehicles) and demonstrate strong generalization to unseen urban environments. Additionally, we present a case study that illustrates its decision-making process in complex driving scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on enhancing autonomous driving systems using a Mixture of Experts (MoE) architecture. While it doesn't explicitly mention Large Language Models, the use of MoE suggests a connection to large-scale models and efficient task handling, which aligns with the broader theme of large models. The paper also directly addresses planning and prediction within autonomous driving, indicating relevance to trajectory prediction. The connection to trajectory prediction is through the 'prediction' component of planning in autonomous driving. However, the absence of explicit mention of LLMs lowers the relevance score.", "keywords": ["autonomous driving", "Mixture of Experts", "prediction", "planning", "MoE"]}}
{"id": "2511.11777", "pdf": "https://arxiv.org/pdf/2511.11777", "abs": "https://arxiv.org/abs/2511.11777", "authors": ["Vinit Mehta", "Charu Sharma", "Karthick Thiyagarajan"], "title": "Large Language Models and 3D Vision for Intelligent Robotic Perception and Autonomy: A Review", "categories": ["cs.RO", "cs.CV"], "comment": "45 pages, 15 figures, MDPI Sensors Journal", "summary": "With the rapid advancement of artificial intelligence and robotics, the integration of Large Language Models (LLMs) with 3D vision is emerging as a transformative approach to enhancing robotic sensing technologies. This convergence enables machines to perceive, reason and interact with complex environments through natural language and spatial understanding, bridging the gap between linguistic intelligence and spatial perception. This review provides a comprehensive analysis of state-of-the-art methodologies, applications and challenges at the intersection of LLMs and 3D vision, with a focus on next-generation robotic sensing technologies. We first introduce the foundational principles of LLMs and 3D data representations, followed by an in-depth examination of 3D sensing technologies critical for robotics. The review then explores key advancements in scene understanding, text-to-3D generation, object grounding and embodied agents, highlighting cutting-edge techniques such as zero-shot 3D segmentation, dynamic scene synthesis and language-guided manipulation. Furthermore, we discuss multimodal LLMs that integrate 3D data with touch, auditory and thermal inputs, enhancing environmental comprehension and robotic decision-making. To support future research, we catalog benchmark datasets and evaluation metrics tailored for 3D-language and vision tasks. Finally, we identify key challenges and future research directions, including adaptive model architectures, enhanced cross-modal alignment and real-time processing capabilities, which pave the way for more intelligent, context-aware and autonomous robotic sensing systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses the intersection of Large Language Models (LLMs) and 3D vision, which is relevant to the prompt's criteria. While it doesn't explicitly focus on trajectory prediction, the concepts discussed, such as scene understanding, object grounding, and embodied agents, are related to how robots perceive and interact with their environment, which is a prerequisite for trajectory prediction. The connection is not direct, but the underlying technologies are relevant.", "keywords": ["Large Language Models", "LLMs", "3D vision", "robotic perception", "embodied agents", "scene understanding", "object grounding"]}}
{"id": "2511.12022", "pdf": "https://arxiv.org/pdf/2511.12022", "abs": "https://arxiv.org/abs/2511.12022", "authors": ["Anh-Quan Pham", "Kabir Ram Puri", "Shreyas Raorane"], "title": "SBAMP: Sampling Based Adaptive Motion Planning", "categories": ["cs.RO", "eess.SY"], "comment": "8 pages, 13 figures", "summary": "Autonomous robotic systems must navigate complex, dynamic environments in real time, often facing unpredictable obstacles and rapidly changing conditions. Traditional sampling-based methods, such as RRT*, excel at generating collision-free paths but struggle to adapt to sudden changes without extensive replanning. Conversely, learning-based dynamical systems, such as the Stable Estimator of Dynamical Systems (SEDS), offer smooth, adaptive trajectory tracking but typically rely on pre-collected demonstration data, limiting their generalization to novel scenarios. This paper introduces Sampling-Based Adaptive Motion Planning (SBAMP), a novel framework that overcomes these limitations by integrating RRT* for global path planning with a SEDS-based local controller for continuous, adaptive trajectory adjustment. Our approach requires no pre-trained datasets and ensures smooth transitions between planned waypoints, maintaining stability through Lyapunov-based guarantees. We validate SBAMP in both simulated environments and real hardware using the RoboRacer platform, demonstrating superior performance in dynamic obstacle scenarios, rapid recovery from perturbations, and robust handling of sharp turns. Experimental results highlight SBAMP's ability to adapt in real time without sacrificing global path optimality, providing a scalable solution for dynamic, unstructured environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u91c7\u6837\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u89c4\u5212\uff0c\u5c5e\u4e8e\u8def\u5f84\u89c4\u5212\u548c\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u5173\u6ce8\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u89c4\u5212\u4e0e\u8f68\u8ff9\u8c03\u6574\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "motion planning", "adaptive trajectory tracking", "path planning", "dynamic environments"]}}
{"id": "2511.12101", "pdf": "https://arxiv.org/pdf/2511.12101", "abs": "https://arxiv.org/abs/2511.12101", "authors": ["Jian Zhou", "Sihao Lin", "Shuai Fu", "Qi WU"], "title": "Decoupled Action Head: Confining Task Knowledge to Conditioning Layers", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses Behavior Cloning, Diffusion Policy (DP-CNN, DP-Transformer), and robotic manipulation, which are related to trajectory prediction and action generation. While it doesn't explicitly mention large language models, it draws inspiration from scaling laws in language and vision domains. The use of Diffusion Policy and continuous action sequence prediction indicates a connection to trajectory generation/prediction.", "keywords": ["Behavior Cloning", "Diffusion Policy", "action generation", "robotic manipulation", "trajectory"]}}
{"id": "2511.12160", "pdf": "https://arxiv.org/pdf/2511.12160", "abs": "https://arxiv.org/abs/2511.12160", "authors": ["Wenbin Mai", "Minghui Liwang", "Xinlei Yi", "Xiaoyu Xia", "Seyyedali Hosseinalipour", "Xianbin Wang"], "title": "Game-Theoretic Safe Multi-Agent Motion Planning with Reachability Analysis for Dynamic and Uncertain Environments (Extended Version)", "categories": ["cs.RO"], "comment": "12 pages, 9 figures", "summary": "Ensuring safe, robust, and scalable motion planning for multi-agent systems in dynamic and uncertain environments is a persistent challenge, driven by complex inter-agent interactions, stochastic disturbances, and model uncertainties. To overcome these challenges, particularly the computational complexity of coupled decision-making and the need for proactive safety guarantees, we propose a Reachability-Enhanced Dynamic Potential Game (RE-DPG) framework, which integrates game-theoretic coordination into reachability analysis. This approach formulates multi-agent coordination as a dynamic potential game, where the Nash equilibrium (NE) defines optimal control strategies across agents. To enable scalability and decentralized execution, we develop a Neighborhood-Dominated iterative Best Response (ND-iBR) scheme, built upon an iterated $\\varepsilon$-BR (i$\\varepsilon$-BR) process that guarantees finite-step convergence to an $\\varepsilon$-NE. This allows agents to compute strategies based on local interactions while ensuring theoretical convergence guarantees. Furthermore, to ensure safety under uncertainty, we integrate a Multi-Agent Forward Reachable Set (MA-FRS) mechanism into the cost function, explicitly modeling uncertainty propagation and enforcing collision avoidance constraints. Through both simulations and real-world experiments in 2D and 3D environments, we validate the effectiveness of RE-DPG across diverse operational scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u4f7f\u7528\u4e86\u535a\u5f08\u8bba\u548c\u53ef\u8fbe\u6027\u5206\u6790\u6765\u4fdd\u8bc1\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u89e3\u51b3\u7684\u95ee\u9898\uff08\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\uff09\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\uff0c\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u5728\u591a\u667a\u80fd\u4f53\u573a\u666f\u4e0b\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["motion planning", "multi-agent systems", "reachability analysis", "dynamic environments", "uncertain environments"]}}
{"id": "2511.12203", "pdf": "https://arxiv.org/pdf/2511.12203", "abs": "https://arxiv.org/abs/2511.12203", "authors": ["Antony Thomas", "Fulvio Mastrogiovanni", "Marco Baglietto"], "title": "Locally Optimal Solutions to Constraint Displacement Problems via Path-Obstacle Overlaps", "categories": ["cs.RO", "cs.AI"], "comment": "Robotics and Autonomous Systems", "summary": "We present a unified approach for constraint displacement problems in which a robot finds a feasible path by displacing constraints or obstacles. To this end, we propose a two stage process that returns locally optimal obstacle displacements to enable a feasible path for the robot. The first stage proceeds by computing a trajectory through the obstacles while minimizing an appropriate objective function. In the second stage, these obstacles are displaced to make the computed robot trajectory feasible, that is, collision-free. Several examples are provided that successfully demonstrate our approach on two distinct classes of constraint displacement problems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses path planning for robots, which is related to trajectory prediction. It doesn't explicitly mention large language models, but the concept of finding optimal solutions and generating feasible paths could potentially be relevant in the context of using LLMs for trajectory generation or planning. The connection is not direct, hence the moderate relevance score.", "keywords": ["trajectory", "path planning", "obstacle avoidance", "robotics"]}}
{"id": "2511.12618", "pdf": "https://arxiv.org/pdf/2511.12618", "abs": "https://arxiv.org/abs/2511.12618", "authors": ["Jordan Leyva", "Nahim J. Moran Vera", "Yihan Xu", "Adrien Durasno", "Christopher U. Romero", "Tendai Chimuka", "Gabriel O. Huezo Ramirez", "Ziqian Dong", "Roberto Rojas-Cessa"], "title": "EcoFlight: Finding Low-Energy Paths Through Obstacles for Autonomous Sensing Drones", "categories": ["cs.RO"], "comment": "Autonomous drone, A* algorithm, 3D environments, path planning, obstacle avoidance, energy efficiency, MIT Conference", "summary": "Obstacle avoidance path planning for uncrewed aerial vehicles (UAVs), or drones, is rarely addressed in most flight path planning schemes, despite obstacles being a realistic condition. Obstacle avoidance can also be energy-intensive, making it a critical factor in efficient point-to-point drone flights. To address these gaps, we propose EcoFlight, an energy-efficient pathfinding algorithm that determines the lowest-energy route in 3D space with obstacles. The algorithm models energy consumption based on the drone propulsion system and flight dynamics. We conduct extensive evaluations, comparing EcoFlight with direct-flight and shortest-distance schemes. The simulation results across various obstacle densities show that EcoFlight consistently finds paths with lower energy consumption than comparable algorithms, particularly in high-density environments. We also demonstrate that a suitable flying speed can further enhance energy savings.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u7684\u8def\u5f84\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u4f46\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u8bba\u6587\u8ba8\u8bba\u4e86\u80fd\u91cf\u6548\u7387\u548c\u907f\u969c\u95ee\u9898\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u90e8\u5206\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "path planning", "obstacle avoidance"]}}
{"id": "2511.12778", "pdf": "https://arxiv.org/pdf/2511.12778", "abs": "https://arxiv.org/abs/2511.12778", "authors": ["Vignesh Rajagopal", "Kasun Weerakoon Kulathun Mudiyanselage", "Gershom Devake Seneviratne", "Pon Aswin Sankaralingam", "Mohamed Elnoor", "Jing Liang", "Rohan Chandra", "Dinesh Manocha"], "title": "DR. Nav: Semantic-Geometric Representations for Proactive Dead-End Recovery and Navigation", "categories": ["cs.RO"], "comment": null, "summary": "We present DR. Nav (Dead-End Recovery-aware Navigation), a novel approach to autonomous navigation in scenarios where dead-end detection and recovery are critical, particularly in unstructured environments where robots must handle corners, vegetation occlusions, and blocked junctions. DR. Nav introduces a proactive strategy for navigation in unmapped environments without prior assumptions. Our method unifies dead-end prediction and recovery by generating a single, continuous, real-time semantic cost map. Specifically, DR. Nav leverages cross-modal RGB-LiDAR fusion with attention-based filtering to estimate per-cell dead-end likelihoods and recovery points, which are continuously updated through Bayesian inference to enhance robustness. Unlike prior mapping methods that only encode traversability, DR. Nav explicitly incorporates recovery-aware risk into the navigation cost map, enabling robots to anticipate unsafe regions and plan safer alternative trajectories. We evaluate DR. Nav across multiple dense indoor and outdoor scenarios and demonstrate an increase of 83.33% in accuracy in detection, a 52.4% reduction in time-to-goal (path efficiency), compared to state-of-the-art planners such as DWA, MPPI, and Nav2 DWB. Furthermore, the dead-end classifier functions", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autonomous navigation with a focus on dead-end recovery. While it doesn't explicitly mention large language models, the generation of trajectories and planning safer alternative trajectories relates to trajectory prediction. The semantic cost map generation and Bayesian inference also suggest a degree of learning and prediction involved, albeit not using LLMs directly.", "keywords": ["autonomous navigation", "trajectory planning", "dead-end recovery", "semantic cost map"]}}
{"id": "2511.12254", "pdf": "https://arxiv.org/pdf/2511.12254", "abs": "https://arxiv.org/abs/2511.12254", "authors": ["Yuxiang Zhou", "Jichang Li", "Yanhao Zhang", "Haonan Lu", "Guanbin Li"], "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving the performance of mobile agents in long-horizon tasks by using retrieval-augmented generation (RAG) with large language models (MLLMs). While it doesn't directly address trajectory prediction, the concept of long-horizon mobile automation and the need for strategic planning and execution can be related to trajectory planning and control. The use of LLMs is a key aspect of the paper, making it relevant to the query.", "keywords": ["Large Language Models", "MLLMs", "RAG", "multi-agent", "long-horizon", "planning", "execution"]}}
{"id": "2511.12040", "pdf": "https://arxiv.org/pdf/2511.12040", "abs": "https://arxiv.org/abs/2511.12040", "authors": ["Xinyuan Hu", "Changyue Shi", "Chuxiao Yang", "Minghao Chen", "Jiajun Ding", "Tao Wei", "Chen Wei", "Zhou Yu", "Min Tan"], "title": "SRSplat: Feed-Forward Super-Resolution Gaussian Splatting from Sparse Multi-View Images", "categories": ["cs.CV"], "comment": "AAAI2026-Oral. Project Page: https://xinyuanhu66.github.io/SRSplat/", "summary": "Feed-forward 3D reconstruction from sparse, low-resolution (LR) images is a crucial capability for real-world applications, such as autonomous driving and embodied AI. However, existing methods often fail to recover fine texture details. This limitation stems from the inherent lack of high-frequency information in LR inputs. To address this, we propose \\textbf{SRSplat}, a feed-forward framework that reconstructs high-resolution 3D scenes from only a few LR views. Our main insight is to compensate for the deficiency of texture information by jointly leveraging external high-quality reference images and internal texture cues. We first construct a scene-specific reference gallery, generated for each scene using Multimodal Large Language Models (MLLMs) and diffusion models. To integrate this external information, we introduce the \\textit{Reference-Guided Feature Enhancement (RGFE)} module, which aligns and fuses features from the LR input images and their reference twin image. Subsequently, we train a decoder to predict the Gaussian primitives using the multi-view fused feature obtained from \\textit{RGFE}. To further refine predicted Gaussian primitives, we introduce \\textit{Texture-Aware Density Control (TADC)}, which adaptively adjusts Gaussian density based on the internal texture richness of the LR inputs. Extensive experiments demonstrate that our SRSplat outperforms existing methods on various datasets, including RealEstate10K, ACID, and DTU, and exhibits strong cross-dataset and cross-resolution generalization capabilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u8fdb\u884c\u8d85\u5206\u8fa8\u7387\u9ad8\u65af\u6e85\u5c04\u7684\u91cd\u5efa\uff0c\u76ee\u6807\u5e94\u7528\u573a\u666f\u5305\u62ec\u81ea\u52a8\u9a7e\u9a76\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce83D\u91cd\u5efa\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86Multimodal Large Language Models (MLLMs)\uff0c\u5e76\u4e14\u5e94\u7528\u573a\u666f\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff08\u81ea\u52a8\u9a7e\u9a76\uff09\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u4fa7\u91cd\u4e8e\u91cd\u5efa\uff0c\u8f68\u8ff9\u9884\u6d4b\u5e76\u975e\u6838\u5fc3\uff0c\u4f46MLLM\u7684\u4f7f\u7528\u4f7f\u5176\u76f8\u5173\u6027\u9ad8\u4e8e\u7eaf\u7cb9\u76843D\u91cd\u5efa\u3002", "keywords": ["Multimodal Large Language Models", "MLLMs", "autonomous driving"]}}
{"id": "2511.12754", "pdf": "https://arxiv.org/pdf/2511.12754", "abs": "https://arxiv.org/abs/2511.12754", "authors": ["Benjamin Li", "Shuyang Shi", "Lucia Romero", "Huao Li", "Yaqi Xie", "Woojun Kim", "Stefanos Nikolaidis", "Michael Lewis", "Katia Sycara", "Simon Stepputtis"], "title": "Adaptively Coordinating with Novel Partners via Learned Latent Strategies", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": "Accepted to NeurIPS 2025", "summary": "Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on adaptation and coordination in human-agent teams, using agent trajectory data to learn latent strategies. While it doesn't explicitly mention large language models, it does deal with learning from trajectory data and predicting/adapting to partner behavior, which relates to trajectory prediction and behavior modeling. The 'strategy-conditioned cooperator framework' learns to represent and categorize partner strategies from agent trajectory data, indicating a focus on learning from and predicting movement patterns, even if not directly trajectory prediction in the traditional sense. The use of a variational autoencoder is also relevant.", "keywords": ["agent trajectory data", "adaptation", "strategy learning", "behavior modeling", "variational autoencoder"]}}
{"id": "2511.13188", "pdf": "https://arxiv.org/pdf/2511.13188", "abs": "https://arxiv.org/abs/2511.13188", "authors": ["Osama Al Sheikh Ali", "Sotiris Koutsoftas", "Ze Zhang", "Knut Akesson", "Emmanuel Dean"], "title": "Collision-Free Navigation of Mobile Robots via Quadtree-Based Model Predictive Control", "categories": ["cs.RO", "eess.SY"], "comment": "This paper has been accepted by IEEE SII 2026", "summary": "This paper presents an integrated navigation framework for Autonomous Mobile Robots (AMRs) that unifies environment representation, trajectory generation, and Model Predictive Control (MPC). The proposed approach incorporates a quadtree-based method to generate structured, axis-aligned collision-free regions from occupancy maps. These regions serve as both a basis for developing safe corridors and as linear constraints within the MPC formulation, enabling efficient and reliable navigation without requiring direct obstacle encoding. The complete pipeline combines safe-area extraction, connectivity graph construction, trajectory generation, and B-spline smoothing into one coherent system. Experimental results demonstrate consistent success and superior performance compared to baseline approaches across complex environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory generation and collision avoidance for mobile robots using Model Predictive Control (MPC). While it doesn't directly involve Large Language Models, it falls under the broader category of trajectory prediction and path planning. The mention of trajectory generation and MPC contributes to a moderate relevance score.", "keywords": ["trajectory generation", "collision avoidance", "Model Predictive Control", "navigation"]}}
{"id": "2511.13207", "pdf": "https://arxiv.org/pdf/2511.13207", "abs": "https://arxiv.org/abs/2511.13207", "authors": ["Cheng Peng", "Zhenzhe Zhang", "Cheng Chi", "Xiaobao Wei", "Yanhao Zhang", "Heng Wang", "Pengwei Wang", "Zhongyuan Wang", "Jing Liu", "Shanghang Zhang"], "title": "PIGEON: VLM-Driven Object Navigation via Points of Interest Selection", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Navigating to a specified object in an unknown environment is a fundamental yet challenging capability of embodied intelligence. However, current methods struggle to balance decision frequency with intelligence, resulting in decisions lacking foresight or discontinuous actions. In this work, we propose PIGEON: Point of Interest Guided Exploration for Object Navigation with VLM, maintaining a lightweight and semantically aligned snapshot memory during exploration as semantic input for the exploration strategy. We use a large Visual-Language Model (VLM), named PIGEON-VL, to select Points of Interest (PoI) formed during exploration and then employ a lower-level planner for action output, increasing the decision frequency. Additionally, this PoI-based decision-making enables the generation of Reinforcement Learning with Verifiable Reward (RLVR) data suitable for simulators. Experiments on classic object navigation benchmarks demonstrate that our zero-shot transfer method achieves state-of-the-art performance, while RLVR further enhances the model's semantic guidance capabilities, enabling deep reasoning during real-time navigation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses a large Visual-Language Model (VLM) for object navigation, which is related to trajectory prediction. The VLM helps in selecting Points of Interest (PoI) for navigation, thus influencing the planned trajectory. While not directly focusing on trajectory prediction as its primary goal, the navigation aspect and the use of a VLM connects it to both areas.", "keywords": ["VLM", "Large Language Model", "Object Navigation", "Trajectory Prediction", "Points of Interest", "Navigation"]}}
