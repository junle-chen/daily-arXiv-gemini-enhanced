{"id": "2510.19025", "pdf": "https://arxiv.org/pdf/2510.19025", "abs": "https://arxiv.org/abs/2510.19025", "authors": ["Hamed Jelodar", "Samita Bai", "Roozbeh Razavi-Far", "Ali A. Ghorbani"], "title": "FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Dataset availability and quality remain critical challenges in machine\nlearning, especially in domains where data are scarce, expensive to acquire, or\nconstrained by privacy regulations. Fields such as healthcare, biomedical\nresearch, and cybersecurity frequently encounter high data acquisition costs,\nlimited access to annotated data, and the rarity or sensitivity of key events.\nThese issues-collectively referred to as the dataset challenge-hinder the\ndevelopment of accurate and generalizable machine learning models in such\nhigh-stakes domains. To address this, we introduce FlexiDataGen, an adaptive\nlarge language model (LLM) framework designed for dynamic semantic dataset\ngeneration in sensitive domains. FlexiDataGen autonomously synthesizes rich,\nsemantically coherent, and linguistically diverse datasets tailored to\nspecialized fields. The framework integrates four core components: (1)\nsyntactic-semantic analysis, (2) retrieval-augmented generation, (3) dynamic\nelement injection, and (4) iterative paraphrasing with semantic validation.\nTogether, these components ensure the generation of high-quality,\ndomain-relevant data. Experimental results show that FlexiDataGen effectively\nalleviates data shortages and annotation bottlenecks, enabling scalable and\naccurate machine learning model development."}
{"id": "2510.19197", "pdf": "https://arxiv.org/pdf/2510.19197", "abs": "https://arxiv.org/abs/2510.19197", "authors": ["Nofar Carmeli", "Nikolaos Tziavelis"], "title": "Fine-Grained Dichotomies for Conjunctive Queries with Minimum or Maximum", "categories": ["cs.DB", "cs.DS"], "comment": null, "summary": "We investigate the fine-grained complexity of direct access to Conjunctive\nQuery (CQ) answers according to their position, ordered by the minimum (or\nmaximum) value between attributes. We further use the tools we develop to\nexplore a wealth of related tasks. We consider the task of ranked enumeration\nunder min/max orders, as well as tasks concerning CQs with predicates of the\nform x <= min X , where X is a set of variables and x is a single variable:\ncounting, enumeration, direct access, and predicate elimination (i.e.,\ntransforming the pair of query and database to an equivalent pair without\nmin-predicates). For each task, we establish a complete dichotomy for\nself-join-free CQs, precisely identifying the cases that are solvable in\nnear-ideal time, i.e., (quasi)linear preprocessing time followed by constant or\nlogarithmic time per output."}
{"id": "2510.18897", "pdf": "https://arxiv.org/pdf/2510.18897", "abs": "https://arxiv.org/abs/2510.18897", "authors": ["Jacopo Tagliabue"], "title": "AI for Distributed Systems Design: Scalable Cloud Optimization Through Repeated LLMs Sampling And Simulators", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.SE"], "comment": "Pre-print IAAA workshop submission", "summary": "We explore AI-driven distributed-systems policy design by combining\nstochastic code generation from large language models (LLMs) with deterministic\nverification in a domain-specific simulator. Using a Function-as-a-Service\nruntime (Bauplan) and its open-source simulator (Eudoxia) as a case study, we\nframe scheduler design as an iterative generate-and-verify loop: an LLM\nproposes a Python policy, the simulator evaluates it on standardized traces,\nand structured feedback steers subsequent generations. This setup preserves\ninterpretability while enabling targeted search over a large design space. We\ndetail the system architecture and report preliminary results on throughput\nimprovements across multiple models. Beyond early gains, we discuss the limits\nof the current setup and outline next steps; in particular, we conjecture that\nAI will be crucial for scaling this methodology by helping to bootstrap new\nsimulators."}
{"id": "2510.18998", "pdf": "https://arxiv.org/pdf/2510.18998", "abs": "https://arxiv.org/abs/2510.18998", "authors": ["Buang Zhang", "Tung Kieu", "Xiangfei Qiu", "Chenjuan Guo", "Jilin Hu", "Aoying Zhou", "Christian S. Jensen", "Bin Yang"], "title": "An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version", "categories": ["cs.LG", "cs.DB"], "comment": "15 pages. An extended version of \"An Encode-then-Decompose Approach\n  to Unsupervised Time Series Anomaly Detection on Contaminated Training Data\"\n  accepted at ICDE 2026", "summary": "Time series anomaly detection is important in modern large-scale systems and\nis applied in a variety of domains to analyze and monitor the operation of\ndiverse systems. Unsupervised approaches have received widespread interest, as\nthey do not require anomaly labels during training, thus avoiding potentially\nhigh costs and having wider applications. Among these, autoencoders have\nreceived extensive attention. They use reconstruction errors from compressed\nrepresentations to define anomaly scores. However, representations learned by\nautoencoders are sensitive to anomalies in training time series, causing\nreduced accuracy. We propose a novel encode-then-decompose paradigm, where we\ndecompose the encoded representation into stable and auxiliary representations,\nthereby enhancing the robustness when training with contaminated time series.\nIn addition, we propose a novel mutual information based metric to replace the\nreconstruction errors for identifying anomalies. Our proposal demonstrates\ncompetitive or state-of-the-art performance on eight commonly used multi- and\nunivariate time series benchmarks and exhibits robustness to time series with\ndifferent contamination ratios."}
{"id": "2510.19012", "pdf": "https://arxiv.org/pdf/2510.19012", "abs": "https://arxiv.org/abs/2510.19012", "authors": ["Ivan Borodii", "Illia Fedorovych", "Halyna Osukhivska", "Diana Velychko", "Roman Butsii"], "title": "Comparative analysis of large data processing in Apache Spark using Java, Python and Scala", "categories": ["cs.DC", "cs.DB", "cs.PL", "cs.SE"], "comment": "CITI 2025, 3rd International Workshop on Computer Information\n  Technologies in Industry 4.0, June 11-12, 2025, Ternopil, Ukraine. The\n  article includes 10 pages, 5 figures, 9 tables", "summary": "During the study, the results of a comparative analysis of the process of\nhandling large datasets using the Apache Spark platform in Java, Python, and\nScala programming languages were obtained. Although prior works have focused on\nindividual stages, comprehensive comparisons of full ETL workflows across\nprogramming languages using Apache Iceberg remain limited. The analysis was\nperformed by executing several operations, including downloading data from CSV\nfiles, transforming and loading it into an Apache Iceberg analytical table. It\nwas found that the performance of the Spark algorithm varies significantly\ndepending on the amount of data and the programming language used. When\nprocessing a 5-megabyte CSV file, the best result was achieved in Python: 6.71\nseconds, which is superior to Scala's score of 9.13 seconds and Java's time of\n9.62 seconds. For processing a large CSV file of 1.6 gigabytes, all programming\nlanguages demonstrated similar results: the fastest performance was showed in\nPython: 46.34 seconds, while Scala and Java showed results of 47.72 and 50.56\nseconds, respectively. When performing a more complex operation that involved\ncombining two CSV files into a single dataset for further loading into an\nApache Iceberg table, Scala demonstrated the highest performance, at 374.42\nseconds. Java processing was completed in 379.8 seconds, while Python was the\nleast efficient, with a runtime of 398.32 seconds. It follows that the\nprogramming language significantly affects the efficiency of data processing by\nthe Apache Spark algorithm, with Scala and Java being more productive for\nprocessing large amounts of data and complex operations, while Python\ndemonstrates an advantage in working with small amounts of data. The results\nobtained can be useful for optimizing data handling processes depending on\nspecific performance requirements and the amount of information being\nprocessed."}
{"id": "2510.19805", "pdf": "https://arxiv.org/pdf/2510.19805", "abs": "https://arxiv.org/abs/2510.19805", "authors": ["Carl-Johan Fauvelle Munck af Rosensch\"old", "Feras M. Awaysheh", "Ahmad Awad"], "title": "Next Generation Cloud-native In-Memory Stores: From Redis to Valkey and Beyond", "categories": ["cs.DC", "cs.DB"], "comment": "10 pages, 5 figures, 2 algorithms, 4 tables", "summary": "In-memory key-value datastores have become indispensable building blocks of\nmodern cloud-native infrastructures, yet their evolution faces scalability,\ncompatibility, and sustainability constraints. The current literature lacks an\nexperimental evaluation of state-of-the-art tools in the domain. This study\naddressed this timely gap by benchmarking Redis alternatives and systematically\nevaluating Valkey, KeyDB, and Garnet under realistic workloads within\nKubernetes deployments. The results demonstrate clear trade-offs among the\nbenchmarked data systems. Our study presents a comprehensive performance and\nviability assessment of the emerging in-memory key-value stores. Metrics\ninclude throughput, tail latency, CPU and memory efficiency, and migration\ncomplexity. We highlight trade-offs between performance, compatibility, and\nlong-term viability, including project maturity, community support, and\nsustained development."}
