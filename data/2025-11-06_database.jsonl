{"id": "2511.01896", "pdf": "https://arxiv.org/pdf/2511.01896", "abs": "https://arxiv.org/abs/2511.01896", "authors": ["Alessandro Padella", "Francesco Vinci", "Massimiliano de Leoni"], "title": "An Experimental Comparison of Alternative Techniques for Event-Log Augmentation", "categories": ["cs.DB"], "comment": null, "summary": "Process mining analyzes and improves processes by examining transactional\ndata stored in event logs, which record sequences of events with timestamps.\nHowever, the effectiveness of process mining, especially when combined with\nmachine or deep learning, depends on having large event logs. Event log\naugmentation addresses this limitation by generating additional traces that\nsimulate realistic process executions while considering various perspectives\nlike time, control-flow, workflow, resources, and domain-specific attributes.\nAlthough prior research has explored event-log augmentation techniques, there\nhas been no comprehensive comparison of their effectiveness. This paper reports\non an evaluation of seven state-of-the-art augmentation techniques across eight\nevent logs. The results are also compared with those obtained by a baseline\ntechnique based on a stochastic transition system. The comparison has been\ncarried on analyzing four different aspects: similarity, preservation of\npredictive information, information loss/enhancement, and computational times\nrequired. Results show that, considering the different criteria, a technique\nbased on a stochastic transition system combined with resource queue modeling\nwould provide higher quality synthetic event logs. Event-log augmentation\ntechniques are also compared with traditional data-augmentation techniques,\nshowing that the former provide significant benefits, whereas the latter fail\nto consider process constraints."}
{"id": "2511.01942", "pdf": "https://arxiv.org/pdf/2511.01942", "abs": "https://arxiv.org/abs/2511.01942", "authors": ["Khalil Rejiba", "Sang-Hyeok Lee", "Christina Gasper", "Martina Freund", "Sandra Korte-Kerzel", "Ulrich Kerzel"], "title": "Towards Defect Phase Diagrams: From Research Data Management to Automated Workflows", "categories": ["cs.DB", "cond-mat.mtrl-sci", "cs.DL"], "comment": null, "summary": "Defect phase diagrams provide a unified description of crystal defect states\nfor materials design and are central to the scientific objectives of the\nCollaborative Research Centre (CRC) 1394. Their construction requires the\nsystematic integration of heterogeneous experimental and simulation data across\nresearch groups and locations. In this setting, research data management (RDM)\nis a key enabler of new scientific insight by linking distributed research\nactivities and making complex data reproducible and reusable.\n  To address the challenge of heterogeneous data sources and formats, a\ncomprehensive RDM infrastructure has been established that links experiment,\ndata, and analysis in a seamless workflow. The system combines: (1) a joint\nelectronic laboratory notebook and laboratory information management system,\n(2) easy-to-use large-object data storage, (3) automatic metadata extraction\nfrom heterogeneous and proprietary file formats, (4) interactive provenance\ngraphs for data exploration and reuse, and (5) automated reporting and analysis\nworkflows. The two key technological elements are the openBIS electronic\nlaboratory notebook and laboratory information management system, and a newly\ndeveloped companion application that extends openBIS with large-scale data\nhandling, automated metadata capture, and federated access to distributed\nresearch data.\n  This integrated approach reduces friction in data capture and curation,\nenabling traceable and reusable datasets that accelerate the construction of\ndefect phase diagrams across institutions."}
{"id": "2511.02002", "pdf": "https://arxiv.org/pdf/2511.02002", "abs": "https://arxiv.org/abs/2511.02002", "authors": ["Xiangru Jian", "Zhengyuan Dong", "M. Tamer \u00d6zsu"], "title": "InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations", "categories": ["cs.DB", "cs.AI", "cs.IR"], "comment": "Working paper", "summary": "In recent years, querying semantic web data using SPARQL has remained\nchallenging, especially for non-expert users, due to the language's complex\nsyntax and the prerequisite of understanding intricate data structures. To\naddress these challenges, we propose InteracSPARQL, an interactive SPARQL query\ngeneration and refinement system that leverages natural language explanations\n(NLEs) to enhance user comprehension and facilitate iterative query refinement.\nInteracSPARQL integrates LLMs with a rule-based approach to first produce\nstructured explanations directly from SPARQL abstract syntax trees (ASTs),\nfollowed by LLM-based linguistic refinements. Users can interactively refine\nqueries through direct feedback or LLM-driven self-refinement, enabling the\ncorrection of ambiguous or incorrect query components in real time. We evaluate\nInteracSPARQL on standard benchmarks, demonstrating significant improvements in\nquery accuracy, explanation clarity, and overall user satisfaction compared to\nbaseline approaches. Our experiments further highlight the effectiveness of\ncombining rule-based methods with LLM-driven refinements to create more\naccessible and robust SPARQL interfaces."}
{"id": "2511.02062", "pdf": "https://arxiv.org/pdf/2511.02062", "abs": "https://arxiv.org/abs/2511.02062", "authors": ["Yuting Yang", "Tiancheng Yuan", "Jamal Hashim", "Thiago Garrett", "Jeffrey Qian", "Ann Zhang", "Yifan Wang", "Weijia Song", "Ken Birman"], "title": "Vortex: Hosting ML Inference and Knowledge Retrieval Services With Tight Latency and Throughput Requirements", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "There is growing interest in deploying ML inference and knowledge retrieval\nas services that could support both interactive queries by end users and more\ndemanding request flows that arise from AIs integrated into a end-user\napplications and deployed as agents. Our central premise is that these latter\ncases will bring service level latency objectives (SLOs). Existing ML serving\nplatforms use batching to optimize for high throughput, exposing them to\nunpredictable tail latencies. Vortex enables an SLO-first approach. For\nidentical tasks, Vortex's pipelines achieve significantly lower and more stable\nlatencies than TorchServe and Ray Serve over a wide range of workloads, often\nenabling a given SLO target at more than twice the request rate. When RDMA is\navailable, the Vortex advantage is even more significant."}
{"id": "2511.02096", "pdf": "https://arxiv.org/pdf/2511.02096", "abs": "https://arxiv.org/abs/2511.02096", "authors": ["Savo Tomovic"], "title": "Numbering Combinations for Compact Representation of Many-to-Many Relationship Sets", "categories": ["cs.DB", "cs.DM"], "comment": null, "summary": "In this paper we propose an approach to implement specific relation-ship set\nbetween two entities called combinatorial relationship set. For the\ncombinatorial relationship set B between entity sets G and I the mapping\ncardinality is many-to-many. Additionally, entities from G can be uniquely\nencoded with a pair of values (h, k) generated with the procedure for numbering\ncombinations of entities from I. The encoding procedure is based on\ncombinatorial number system that provides a representation of all possible k\n-combinations of a set of n elements by a single number. In general\nmany-to-many relationship sets are represented by a relation or table, while\nthe combinatorial relationship is not physically stored as separate table.\nHowever, all information is encapsulated into a single column added to G. The\nnew column is a candidate key in G. Additional operation named Rank-Join to\nfundamental relational-algebra is presented to combine information from g and i\nassociated with a combinatorial relationship set. Motivation for combinatorial\nrelationship originates from challenges in designing and implementing\nmultivalued dimensions and bridge tables in data-warehouse models."}
{"id": "2511.02611", "pdf": "https://arxiv.org/pdf/2511.02611", "abs": "https://arxiv.org/abs/2511.02611", "authors": ["Andrea D'Ascenzo", "Julian Meffert", "Petra Mutzel", "Fabrizio Rossi"], "title": "Accelerating Graph Similarity Search through Integer Linear Programming", "categories": ["cs.DB", "cs.DS"], "comment": null, "summary": "The Graph Edit Distance (GED) is an important metric for measuring the\nsimilarity between two (labeled) graphs. It is defined as the minimum cost\nrequired to convert one graph into another through a series of (elementary)\nedit operations. Its effectiveness in assessing the similarity of large graphs\nis limited by the complexity of its exact calculation, which is NP-hard\ntheoretically and computationally challenging in practice. The latter can be\nmitigated by switching to the Graph Similarity Search under GED constraints,\nwhich determines whether the edit distance between two graphs is below a given\nthreshold. A popular framework for solving Graph Similarity Search under GED\nconstraints in a graph database for a query graph is the\nfilter-and-verification framework. Filtering discards unpromising graphs, while\nthe verification step certifies the similarity between the filtered graphs and\nthe query graph. To improve the filtering step, we define a lower bound based\non an integer linear programming formulation. We prove that this lower bound\ndominates the effective branch match-based lower bound and can also be computed\nefficiently. Consequently, we propose a graph similarity search algorithm that\nuses a hierarchy of lower bound algorithms and solves a novel integer\nprogramming formulation that exploits the threshold parameter. An extensive\ncomputational experience on a well-assessed test bed shows that our approach\nsignificantly outperforms the state-of-the-art algorithm on most of the\nexamined thresholds."}
{"id": "2511.02674", "pdf": "https://arxiv.org/pdf/2511.02674", "abs": "https://arxiv.org/abs/2511.02674", "authors": ["Tim Otto"], "title": "EasyTUS: A Comprehensive Framework for Fast and Accurate Table Union Search across Data Lakes", "categories": ["cs.DB"], "comment": "Copyright 2025 IEEE. This is the author's version of the work that\n  has been accepted for publication in Proceedings of the IEEE International\n  Conference on Big Data (IEEE BigData 2025). The final version of record is\n  available at: tba", "summary": "Data lakes enable easy maintenance of heterogeneous data in its native form.\nWhile this flexibility can accelerate data ingestion, it shifts the complexity\nof data preparation and query processing to data discovery tasks. One such task\nis Table Union Search (TUS), which identifies tables that can be unioned with a\ngiven input table. In this work, we present EasyTUS, a comprehensive framework\nthat leverages Large Language Models (LLMs) to perform efficient and scalable\nTable Union Search across data lakes. EasyTUS implements the search pipeline as\nthree modular steps: Table Serialization for consistent formatting and\nsampling, Table Representation that utilizes LLMs to generate embeddings, and\nVector Search that leverages approximate nearest neighbor indexing for semantic\nmatching. To enable reproducible and systematic evaluation, in this paper, we\nalso introduce TUSBench, a novel standardized benchmarking environment within\nthe EasyTUS framework. TUSBench supports unified comparisons across approaches\nand data lakes, promoting transparency and progress in the field. Our\nexperiments using TUSBench show that EasyTUS consistently outperforms most of\nthe state-of the-art approaches, achieving improvements in average of up to\n34.3% in Mean Average Precision (MAP), up to 79.2x speedup in data preparation,\nand up to 7.7x faster query processing performance. Furthermore, EasyTUS\nmaintains strong performance even in metadata-absent settings, highlighting its\nrobustness and adaptability across data lakes."}
{"id": "2511.02711", "pdf": "https://arxiv.org/pdf/2511.02711", "abs": "https://arxiv.org/abs/2511.02711", "authors": ["Daren Chao", "Kaiwen Chen", "Naiqing Guan", "Nick Koudas"], "title": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data", "categories": ["cs.DB", "cs.IR"], "comment": null, "summary": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora."}
