# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-10-16

## 目录

- [cs.DB (1)](#cs-db)
- [cs.PF (1)](#cs-pf)
- [cs.SE (1)](#cs-se)

## cs.DB [cs.DB]
### [1] [Aixel: A Unified, Adaptive and Extensible System for AI-powered Data Analysis](https://arxiv.org/abs/2510.12642)
*Meihui Zhang, Liming Wang, Chi Zhang, Zhaojing Luo*

Main category: cs.DB

TL;DR: Aixel是一个统一、自适应和可扩展的AI驱动数据分析系统。


<details>
  <summary>Details</summary>
Motivation: 现有系统在数据库、分析库和调优服务之间存在碎片化，导致用户交互复杂、适应性有限、性能欠佳和组件扩展性差。

Method: Aixel系统在应用、任务、模型和数据四个层面上组织工作。任务层提供声明式接口，优化器编译和调度该计划以满足精度、延迟和成本方面的特定目标。模型层为索引、元数据、张量和模型工件提供版本化存储。数据层提供统一的数据管理功能，包括索引、约束感知发现、任务对齐选择和综合特征管理。

Result: Aixel提供了一个用户友好、自适应、高效和可扩展的系统。

Conclusion: Aixel通过统一的架构和自适应的优化，解决了AI驱动数据分析中现有系统的局限性。

Abstract: 现代数据分析的一个日益增长的趋势是将数据管理与学习相结合，并以准确性、延迟和成本要求为指导。在实践中，应用程序从许多来源提取不同格式的数据。与此同时，目标和预算随时间而变化。现有的系统跨数据库、分析库和调优服务处理这些应用程序。这种碎片化导致复杂的用户交互、有限的适应性、次优的性能以及组件之间较差的可扩展性。为了应对这些挑战，我们提出了Aixel，一个统一的、自适应的、可扩展的AI驱动的数据分析系统。该系统在四个层面上组织工作：应用层、任务层、模型层和数据层。任务层提供了一个声明式接口来捕获用户意图，该接口被解析为可执行的算子计划。优化器编译和调度该计划，以满足在准确性、延迟和成本方面的特定目标。任务层协调数据和模型算子的执行，并内置对重用和缓存的支持，以提高效率。模型层为索引、元数据、张量和模型工件提供版本化的存储。它支持自适应构建、任务对齐的漂移检测以及重用共享组件的安全更新。数据层提供统一的数据管理功能，包括索引、约束感知发现、任务对齐选择和综合特征管理。通过上述设计的层，Aixel提供了一个用户友好、自适应、高效和可扩展的系统。

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.12642) | **Categories:** cs.DB, cs.AI

---


## cs.PF [cs.PF]
### [1] [Analysis and Evaluation of Using Microsecond-Latency Memory for In-Memory Indices and Caches in SSD-Based Key-Value Stores](https://arxiv.org/abs/2510.12280)
*Yosuke Bando, Akinobu Mita, Kazuhiro Hiwada, Shintaro Sano, Tomoya Suzuki, Yu Nakanishi, Kazutaka Tomida, Hirotsugu Kajihara, Akiyuki Kaneko, Daisuke Taki, Yukimasa Miyamoto, Tomokazu Yoshida, Tatsuo Shiozawa*

Main category: cs.PF

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: When key-value (KV) stores use SSDs for storing a large number of items, oftentimes they also require large in-memory data structures including indices and caches to be traversed to reduce IOs. This paper considers offloading most of such data structures from the costly host DRAM to secondary memory whose latency is in the microsecond range, an order of magnitude longer than those of currently available DIMM-mounted or CXL memory devices. While emerging microsecond-latency memory is likely to cost much less than DRAM, it can significantly slow down SSD-based KV stores if naively employed. This paper analyzes and evaluates the impact of microsecond-level memory latency on the KV operation throughput. Our analysis finds that a well-known latency-hiding technique of software prefetching for long-latency memory from user-level threads is effective. The novelty of our analysis lies in modeling how the interplay between prefetching and IO affects performance, from which we derive an equation that well explains the throughput degradation due to long memory latency. The model tells us that the presence of IO significantly enhances the tolerance to memory latency, leading to a finding that SSD-based KV stores can be made latency-tolerant without devising new techniques for microsecond-latency memory. To confirm this, we design a microbenchmark as well as modify existing SSD-based KV stores so that they issue prefetches from user-level threads, and run them while placing most of in-memory data structures on FPGA-based memory with adjustable microsecond latency. The results demonstrate that their KV operation throughputs can be well explained by our model, and the modified KV stores achieve near-DRAM throughputs for up to a memory latency of 5 microseconds. This suggests the possibility that SSD-based KV stores can use microsecond-latency memory as a cost-effective alternative to the host DRAM.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.12280) | **Categories:** cs.PF, cs.DB

---


## cs.SE [cs.SE]
### [1] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes, Taher A. Ghaleb, Safwat Hassan*

Main category: cs.SE

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive workflows, from database querying to developer observability. Yet the effectiveness of these systems is constrained by the volume, verbosity, and noise of real-world text-rich data such as logs, telemetry, and monitoring streams. Feeding such data directly into LLMs is costly, environmentally unsustainable, and often misaligned with task objectives. Parallel efforts in LLM efficiency have focused on model- or architecture-level optimizations, but the challenge of reducing upstream input verbosity remains underexplored. In this paper, we argue for treating the token budget of an LLM as an attention budget and elevating task-aware text reduction as a first-class design principle for language -- data systems. We position input-side reduction not as compression, but as attention allocation: prioritizing information most relevant to downstream tasks. We outline open research challenges for building benchmarks, designing adaptive reduction pipelines, and integrating token-budget--aware preprocessing into database and retrieval systems. Our vision is to channel scarce attention resources toward meaningful signals in noisy, data-intensive workflows, enabling scalable, accurate, and sustainable LLM--data integration.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.11813) | **Categories:** cs.SE, cs.CL, cs.DB

---
