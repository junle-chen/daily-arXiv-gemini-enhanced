{"id": "2511.13753", "pdf": "https://arxiv.org/pdf/2511.13753", "abs": "https://arxiv.org/abs/2511.13753", "authors": ["Feilong Wang", "Fuqiang Liu"], "title": "Robustness of LLM-enabled vehicle trajectory prediction under data security threats", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "20 pages, 2 figures, 11 tables, working paper", "summary": "The integration of large language models (LLMs) into automated driving systems has opened new possibilities for reasoning and decision-making by transforming complex driving contexts into language-understandable representations. Recent studies demonstrate that fine-tuned LLMs can accurately predict vehicle trajectories and lane-change intentions by gathering and transforming data from surrounding vehicles. However, the robustness of such LLM-based prediction models for safety-critical driving systems remains unexplored, despite the increasing concerns about the trustworthiness of LLMs. This study addresses this gap by conducting a systematic vulnerability analysis of LLM-enabled vehicle trajectory prediction. We propose a one-feature differential evolution attack that perturbs a single kinematic feature of surrounding vehicles within the LLM's input prompts under a black-box setting. Experiments on the highD dataset reveal that even minor, physically plausible perturbations can significantly disrupt model outputs, underscoring the susceptibility of LLM-based predictors to adversarial manipulation. Further analyses reveal a trade-off between accuracy and robustness, examine the failure mechanism, and explore potential mitigation solutions. The findings provide the very first insights into adversarial vulnerabilities of LLM-driven automated vehicle models in the context of vehicular interactions and highlight the need for robustness-oriented design in future LLM-based intelligent transportation systems.", "relevance_analysis": {"relevance_score": 1.0, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u6307\u51fa\u5176\u7814\u7a76\u5185\u5bb9\u662f\u5173\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5206\u6790\u4e86\u5728\u6570\u636e\u5b89\u5168\u5a01\u80c1\u4e0b\u7684\u9c81\u68d2\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "large language models", "LLMs", "vehicle trajectory prediction", "automated driving systems", "robustness", "adversarial vulnerability"]}}
{"id": "2511.14120", "pdf": "https://arxiv.org/pdf/2511.14120", "abs": "https://arxiv.org/abs/2511.14120", "authors": ["Hao Zhen", "Yunxiang Yang", "Jidong J. Yang"], "title": "Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning Framework with Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "23 pages, 4 figures, 3 tables", "summary": "Pedestrian-vehicle incidents remain a critical urban safety challenge, with pedestrians accounting for over 20% of global traffic fatalities. Although existing video-based systems can detect when incidents occur, they provide little insight into how these events unfold across the distinct cognitive phases of pedestrian behavior. Recent vision-language models (VLMs) have shown strong potential for video understanding, but they remain limited in that they typically process videos in isolation, without explicit temporal structuring or multi-view integration. This paper introduces Multi-view Phase-aware Pedestrian-Vehicle Incident Reasoning (MP-PVIR), a unified framework that systematically processes multi-view video streams into structured diagnostic reports through four stages: (1) event-triggered multi-view video acquisition, (2) pedestrian behavior phase segmentation, (3) phase-specific multi-view reasoning, and (4) hierarchical synthesis and diagnostic reasoning. The framework operationalizes behavioral theory by automatically segmenting incidents into cognitive phases, performing synchronized multi-view analysis within each phase, and synthesizing results into causal chains with targeted prevention strategies. Particularly, two specialized VLMs underpin the MP-PVIR pipeline: TG-VLM for behavioral phase segmentation (mIoU = 0.4881) and PhaVR-VLM for phase-aware multi-view analysis, achieving a captioning score of 33.063 and up to 64.70% accuracy on question answering. Finally, a designated large language model is used to generate comprehensive reports detailing scene understanding, behavior interpretation, causal reasoning, and prevention recommendations. Evaluation on the Woven Traffic Safety dataset shows that MP-PVIR effectively translates multi-view video data into actionable insights, advancing AI-driven traffic safety analytics for vehicle-infrastructure cooperative systems.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant because it focuses on pedestrian-vehicle incident reasoning, which is related to trajectory prediction. It also leverages vision-language models (VLMs) and a large language model (LLM) to analyze multi-view video data and generate diagnostic reports, thus incorporating both trajectory-related analysis (in the context of incident reasoning) and large language models.", "keywords": ["Pedestrian-vehicle incidents", "Vision-language models (VLMs)", "Large language model (LLM)", "Multi-view video", "Behavioral phase segmentation", "Traffic safety"]}}
{"id": "2511.13881", "pdf": "https://arxiv.org/pdf/2511.13881", "abs": "https://arxiv.org/abs/2511.13881", "authors": ["Xin Hu", "Taotao Jing", "Renran Tian", "Zhengming Ding"], "title": "VLMs Guided Interpretable Decision Making for Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by WACV 2026", "summary": "Recent advancements in autonomous driving (AD) have explored the use of vision-language models (VLMs) within visual question answering (VQA) frameworks for direct driving decision-making. However, these approaches often depend on handcrafted prompts and suffer from inconsistent performance, limiting their robustness and generalization in real-world scenarios. In this work, we evaluate state-of-the-art open-source VLMs on high-level decision-making tasks using ego-view visual inputs and identify critical limitations in their ability to deliver reliable, context-aware decisions. Motivated by these observations, we propose a new approach that shifts the role of VLMs from direct decision generators to semantic enhancers. Specifically, we leverage their strong general scene understanding to enrich existing vision-based benchmarks with structured, linguistically rich scene descriptions. Building on this enriched representation, we introduce a multi-modal interactive architecture that fuses visual and linguistic features for more accurate decision-making and interpretable textual explanations. Furthermore, we design a post-hoc refinement module that utilizes VLMs to enhance prediction reliability. Extensive experiments on two autonomous driving benchmarks demonstrate that our approach achieves state-of-the-art performance, offering a promising direction for integrating VLMs into reliable and interpretable AD systems.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on using Vision-Language Models (VLMs) for decision-making in Autonomous Driving. While it doesn't directly address trajectory prediction, the context of autonomous driving inherently involves trajectory planning and prediction. The use of VLMs aligns with the 'Large Language Models' aspect. Therefore, the paper has a moderate relevance.", "keywords": ["VLMs", "vision-language models", "autonomous driving", "decision-making"]}}
{"id": "2511.14131", "pdf": "https://arxiv.org/pdf/2511.14131", "abs": "https://arxiv.org/abs/2511.14131", "authors": ["Yu Zhong", "Zihao Zhang", "Rui Zhang", "Lingdong Huang", "Haihan Gao", "Shuo Wang", "Da Li", "Ruijian Han", "Jiaming Guo", "Shaohui Peng", "Di Huang", "Yunji Chen"], "title": "Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation", "categories": ["cs.AI"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper utilizes Large Language Models (LLMs) for Vision-and-Language Navigation (VLN). While VLN is not strictly trajectory prediction, it involves navigating through an environment based on instructions, which requires understanding spatial relationships and planning a path, thus having some relevance to trajectory prediction. The core aspect of using LLMs makes it relevant to the large language model theme.", "keywords": ["Large Language Models", "LLMs", "Vision-and-Language Navigation", "VLN", "navigation", "reasoning"]}}
{"id": "2511.14499", "pdf": "https://arxiv.org/pdf/2511.14499", "abs": "https://arxiv.org/abs/2511.14499", "authors": ["Jack Qin", "Zhitao Wang", "Yinan Zheng", "Keyu Chen", "Yang Zhou", "Yuanxin Zhong", "Siyuan Cheng"], "title": "Enhancing End-to-End Autonomous Driving with Risk Semantic Distillaion from VLM", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The autonomous driving (AD) system has exhibited remarkable performance in complex driving scenarios. However, generalization is still a key limitation for the current system, which refers to the ability to handle unseen scenarios or unfamiliar sensor configurations.Related works have explored the use of Vision-Language Models (VLMs) to address few-shot or zero-shot tasks. While promising, these methods introduce a new challenge: the emergence of a hybrid AD system, where two distinct systems are used to plan a trajectory, leading to potential inconsistencies. Alternative research directions have explored Vision-Language-Action (VLA) frameworks that generate control actions from VLM directly. However, these end-to-end solutions demonstrate prohibitive computational demands. To overcome these challenges, we introduce Risk Semantic Distillation (RSD), a novel framework that leverages VLMs to enhance the training of End-to-End (E2E) AD backbones. By providing risk attention for key objects, RSD addresses the issue of generalization. Specifically, we introduce RiskHead, a plug-in module that distills causal risk estimates from Vision-Language Models into Bird's-Eye-View (BEV) features, yielding interpretable risk-attention maps.This approach allows BEV features to learn richer and more nuanced risk attention representations, which directly enhance the model's ability to handle spatial boundaries and risky objects.By focusing on risk attention, RSD aligns better with human-like driving behavior, which is essential to navigate in complex and dynamic environments. Our experiments on the Bench2Drive benchmark demonstrate the effectiveness of RSD in managing complex and unpredictable driving conditions. Due to the enhanced BEV representations enabled by RSD, we observed a significant improvement in both perception and planning capabilities.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u589e\u5f3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u98ce\u9669\u8bed\u4e49\u84b8\u998f\uff08RSD\uff09\u7684\u65b9\u6cd5\uff0c\u5c06VLM\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230BEV\u7279\u5f81\u4e2d\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5904\u7406\u590d\u6742\u548c\u52a8\u6001\u73af\u5883\u7684\u80fd\u529b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u7684\u5927\u6a21\u578b\uff0c\u4f46\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u662f\u5176\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\uff0c\u4e14\u4f7f\u7528\u4e86VLM\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language Models", "VLMs", "autonomous driving", "trajectory planning", "risk attention"]}}
{"id": "2511.14265", "pdf": "https://arxiv.org/pdf/2511.14265", "abs": "https://arxiv.org/abs/2511.14265", "authors": ["Rui Zhang", "Chao Li", "Kezhong Liu", "Chen Wang", "Bolong Zheng", "Hongbo Jiang"], "title": "Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention", "categories": ["cs.LG"], "comment": null, "summary": "Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on vessel trajectory prediction, which falls under the trajectory prediction domain. While it does not directly involve large language models, the use of a Conditional Variational Autoencoder (CVAE) for modeling transient intentions suggests the use of a relatively complex model for prediction. The title and abstract clearly indicate a focus on trajectory prediction in the maritime domain.", "keywords": ["trajectory prediction", "vessel trajectory prediction", "multimodal trajectory prediction", "CVAE", "navigation intention"]}}
{"id": "2511.13961", "pdf": "https://arxiv.org/pdf/2511.13961", "abs": "https://arxiv.org/abs/2511.13961", "authors": ["Jiarui Li", "Alessandro Zanardi", "Runyu Zhang", "Gioele Zardini"], "title": "FICO: Finite-Horizon Closed-Loop Factorization for Unified Multi-Agent Path Finding", "categories": ["cs.RO"], "comment": null, "summary": "Multi-Agent Path Finding is a fundamental problem in robotics and AI, yet most existing formulations treat planning and execution separately and address variants of the problem in an ad hoc manner. This paper presents a system-level framework for MAPF that integrates planning and execution, generalizes across variants, and explicitly models uncertainties. At its core is the MAPF system, a formal model that casts MAPF as a control design problem encompassing classical and uncertainty-aware formulations. To solve it, we introduce Finite-Horizon Closed-Loop Factorization (FICO), a factorization-based algorithm inspired by receding-horizon control that exploits compositional structure for efficient closed-loop operation. FICO enables real-time responses -- commencing execution within milliseconds -- while scaling to thousands of agents and adapting seamlessly to execution-time uncertainties. Extensive case studies demonstrate that it reduces computation time by up to two orders of magnitude compared with open-loop baselines, while delivering significantly higher throughput under stochastic delays and agent arrivals. These results establish a principled foundation for analyzing and advancing MAPF through system-level modeling, factorization, and closed-loop design.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Multi-Agent Path Finding (MAPF), which is related to trajectory prediction for multiple agents. While it doesn't explicitly mention large language models, the problem of planning and execution in MAPF can potentially benefit from leveraging LLMs for decision-making or strategy generation in complex environments. The paper's emphasis on real-time responses and handling uncertainties aligns with the challenges addressed in trajectory prediction, particularly in dynamic environments.", "keywords": ["Multi-Agent Path Finding", "MAPF", "path planning", "trajectory prediction", "uncertainties", "robotics", "AI"]}}
{"id": "2511.13741", "pdf": "https://arxiv.org/pdf/2511.13741", "abs": "https://arxiv.org/abs/2511.13741", "authors": ["Silin Zhou", "Yao Chen", "Shuo Shang", "Lisi Chen", "Bingsheng He", "Ryosuke Shibasaki"], "title": "Blurred Encoding for Trajectory Representation Learning", "categories": ["cs.LG"], "comment": "This paper is accepted by KDD2025(Feb. Cycle)", "summary": "Trajectory representation learning (TRL) maps trajectories to vector embeddings and facilitates tasks such as trajectory classification and similarity search. State-of-the-art (SOTA) TRL methods transform raw GPS trajectories to grid or road trajectories to capture high-level travel semantics, i.e., regions and roads. However, they lose fine-grained spatial-temporal details as multiple GPS points are grouped into a single grid cell or road segment. To tackle this problem, we propose the BLUrred Encoding method, dubbed BLUE, which gradually reduces the precision of GPS coordinates to create hierarchical patches with multiple levels. The low-level patches are small and preserve fine-grained spatial-temporal details, while the high-level patches are large and capture overall travel patterns. To complement different patch levels with each other, our BLUE is an encoder-decoder model with a pyramid structure. At each patch level, a Transformer is used to learn the trajectory embedding at the current level, while pooling prepares inputs for the higher level in the encoder, and up-resolution provides guidance for the lower level in the decoder. BLUE is trained using the trajectory reconstruction task with the MSE loss. We compare BLUE with 8 SOTA TRL methods for 3 downstream tasks, the results show that BLUE consistently achieves higher accuracy than all baselines, outperforming the best-performing baselines by an average of 30.90%. Our code is available at https://github.com/slzhou-xy/BLUE.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u8868\u793a\u5b66\u4e60\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u7684\u9886\u57df\u3002\u867d\u7136\u8bba\u6587\u4e2d\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528Transformer\u8fdb\u884c\u8f68\u8ff9\u7f16\u7801\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u501f\u9274\u4e86\u5927\u6a21\u578b\u4e2d\u7684\u4e00\u4e9b\u601d\u60f3\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory representation learning", "trajectory", "Transformer", "embedding", "GPS trajectories"]}}
{"id": "2511.14148", "pdf": "https://arxiv.org/pdf/2511.14148", "abs": "https://arxiv.org/abs/2511.14148", "authors": ["Yuhua Jiang", "Shuang Cheng", "Yan Ding", "Feifei Gao", "Biqing Qi"], "title": "AsyncVLA: Asynchronous Flow Matching for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Vision-language-action (VLA) models have recently emerged as a powerful paradigm for building generalist robots. However, traditional VLA models that generate actions through flow matching (FM) typically rely on rigid and uniform time schedules, i.e., synchronous FM (SFM). Without action context awareness and asynchronous self-correction, SFM becomes unstable in long-horizon tasks, where a single action error can cascade into failure. In this work, we propose asynchronous flow matching VLA (AsyncVLA), a novel framework that introduces temporal flexibility in asynchronous FM (AFM) and enables self-correction in action generation. AsyncVLA breaks from the vanilla SFM in VLA models by generating the action tokens in a non-uniform time schedule with action context awareness. Besides, our method introduces the confidence rater to extract confidence of the initially generated actions, enabling the model to selectively refine inaccurate action tokens before execution. Moreover, we propose a unified training procedure for SFM and AFM that endows a single model with both modes, improving KV-cache utilization. Extensive experiments on robotic manipulation benchmarks demonstrate that AsyncVLA is data-efficient and exhibits self-correction ability. AsyncVLA achieves state-of-the-art results across general embodied evaluations due to its asynchronous generation in AFM. Our code is available at https://github.com/YuhuaJiang2002/AsyncVLA.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action (VLA) models for robotics, which involves generating actions based on visual and linguistic inputs. While it doesn't explicitly mention trajectory prediction, the generation of actions for a robot inherently involves predicting the future state/trajectory of the robot. The paper also mentions using flow matching, which can be used for trajectory generation. It's related to large models as VLA models are often large. The connection to trajectory prediction is somewhat indirect, but the action generation aspect is relevant.", "keywords": ["Vision-Language-Action Models", "VLA", "flow matching", "action generation", "robotics"]}}
{"id": "2511.13795", "pdf": "https://arxiv.org/pdf/2511.13795", "abs": "https://arxiv.org/abs/2511.13795", "authors": ["Weiying Shen", "Hao Yu", "Yu Dong", "Pan Liu", "Yu Han", "Xin Wen"], "title": "A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "To be presented at TRB 2026 (TRBAM-26-01711) and a revised version will be submitted to Transportation Research Part C: Emerging Technologies", "summary": "Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8e\u8def\u6bb5\u5730\u56fe\u7684\u78b0\u649e\u68c0\u6d4b\uff0c\u907f\u514d\u4e86\u76f4\u63a5\u4f7f\u7528\u8f68\u8ff9\u6570\u636e\u3002\u867d\u7136\u6807\u9898\u63d0\u5230\u4e86\u201cTrajectory-free\u201d\uff0c\u4f46\u6838\u5fc3\u662f\u78b0\u649e\u68c0\u6d4b\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86diffusion model\u7528\u4e8e\u751f\u6210\u8def\u6bb5\u5730\u56fe\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\u5e94\u7528\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5173\u8054\u4e0d\u5927\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory-free", "crash detection", "diffusion model", "segment map generation"]}}
{"id": "2511.14178", "pdf": "https://arxiv.org/pdf/2511.14178", "abs": "https://arxiv.org/abs/2511.14178", "authors": ["Zhuo Li", "Junjia Liu", "Zhipeng Dong", "Tao Teng", "Quentin Rouxel", "Darwin Caldwell", "Fei Chen"], "title": "Towards Deploying VLA without Fine-Tuning: Plug-and-Play Inference-Time VLA Policy Steering via Embodied Evolutionary Diffusion", "categories": ["cs.RO", "cs.AI"], "comment": "9 pages, 8 figures, submitted to IEEE RA-L", "summary": "Vision-Language-Action (VLA) models have demonstrated significant potential in real-world robotic manipulation. However, pre-trained VLA policies still suffer from substantial performance degradation during downstream deployment. Although fine-tuning can mitigate this issue, its reliance on costly demonstration collection and intensive computation makes it impractical in real-world settings. In this work, we introduce VLA-Pilot, a plug-and-play inference-time policy steering method for zero-shot deployment of pre-trained VLA without any additional fine-tuning or data collection. We evaluate VLA-Pilot on six real-world downstream manipulation tasks across two distinct robotic embodiments, encompassing both in-distribution and out-of-distribution scenarios. Experimental results demonstrate that VLA-Pilot substantially boosts the success rates of off-the-shelf pre-trained VLA policies, enabling robust zero-shot generalization to diverse tasks and embodiments. Experimental videos and code are available at: https://rip4kobe.github.io/vla-pilot/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8Vision-Language-Action (VLA)\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u7b56\u7565\u5f15\u5bfc\u65b9\u6cd5\u3002\u867d\u7136VLA\u6a21\u578b\u5c5e\u4e8e\u5927\u6a21\u578b\u8303\u7574\uff0c\u5e76\u4e14\u6d89\u53ca\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u63a7\u5236\uff0c\u4f46\u8bba\u6587\u7684\u6838\u5fc3\u5728\u4e8e\u7b56\u7565\u5f15\u5bfc\u548c\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u76f4\u63a5\u5173\u8054\u8f83\u5f31\uff0c\u4f46\u4e0e\u5927\u6a21\u578b\u5728\u5177\u8eab\u667a\u80fd\u7684\u5e94\u7528\u76f8\u5173\u3002", "keywords": ["Vision-Language-Action (VLA) models", "Large Language Models", "Robotic Manipulation", "Zero-shot generalization"]}}
{"id": "2511.13970", "pdf": "https://arxiv.org/pdf/2511.13970", "abs": "https://arxiv.org/abs/2511.13970", "authors": ["Sanjay Acharjee", "Abir Khan Ratul", "Diego Patino", "Md Nazmus Sakib"], "title": "Scene Graph-Guided Generative AI Framework for Synthesizing and Evaluating Industrial Hazard Scenarios", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Training vision models to detect workplace hazards accurately requires realistic images of unsafe conditions that could lead to accidents. However, acquiring such datasets is difficult because capturing accident-triggering scenarios as they occur is nearly impossible. To overcome this limitation, this study presents a novel scene graph-guided generative AI framework that synthesizes photorealistic images of hazardous scenarios grounded in historical Occupational Safety and Health Administration (OSHA) accident reports. OSHA narratives are analyzed using GPT-4o to extract structured hazard reasoning, which is converted into object-level scene graphs capturing spatial and contextual relationships essential for understanding risk. These graphs guide a text-to-image diffusion model to generate compositionally accurate hazard scenes. To evaluate the realism and semantic fidelity of the generated data, a visual question answering (VQA) framework is introduced. Across four state-of-the-art generative models, the proposed VQA Graph Score outperforms CLIP and BLIP metrics based on entropy-based validation, confirming its higher discriminative sensitivity.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses GPT-4o to extract structured hazard reasoning from OSHA narratives and uses a text-to-image diffusion model to generate images of hazardous scenarios. While it doesn't directly address trajectory prediction, it utilizes a large language model (GPT-4o) as a key component. The generation of scenes could potentially be extended to include dynamic elements and motion, making it relevant to future trajectory prediction research in hazard scenarios.", "keywords": ["GPT-4o", "Large Language Model", "Generative AI", "Diffusion Model"]}}
{"id": "2511.14396", "pdf": "https://arxiv.org/pdf/2511.14396", "abs": "https://arxiv.org/abs/2511.14396", "authors": ["Xiuxiu Qi", "Yu Yang", "Jiannong Cao", "Luyao Bai", "Chongshan Fan", "Chengtai Cao", "Hongpeng Wang"], "title": "Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted at AAAI 2026, the Project website is available at https://qhemu.github.io/CCoL/", "summary": "Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u673a\u5668\u4eba\u52a8\u4f5c\u8f68\u8ff9\u7684\u751f\u6210\uff0c\u5e76\u4f7f\u7528\u8bed\u8a00\u4f5c\u4e3a\u6761\u4ef6\u8fdb\u884c\u63a7\u5236\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u63a2\u7d22\u4e86\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\u7684\u8054\u5408\u5b66\u4e60\uff0c\u8fd9\u4e0e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u7684\u601d\u8def\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u5173\u952e\u8bcd\u8868\u660e\u5176\u4e0e\u52a8\u4f5c\u9884\u6d4b/\u8def\u5f84\u89c4\u5212\u76f8\u5173\uff0c\u4f46\u7f3a\u4e4f\u76f4\u63a5\u7684\u5927\u6a21\u578b\u5e94\u7528\u3002", "keywords": ["behavioral cloning", "action generation", "language-conditioned manipulation", "robot internal states", "execution trajectories"]}}
{"id": "2511.13765", "pdf": "https://arxiv.org/pdf/2511.13765", "abs": "https://arxiv.org/abs/2511.13765", "authors": ["Shengjie Sun", "Jiafei Lyu", "Runze Liu", "Mengbei Yan", "Bo Liu", "Deheng Ye", "Xiu Li"], "title": "PROF: An LLM-based Reward Code Preference Optimization Framework for Offline Imitation Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Offline imitation learning (offline IL) enables training effective policies without requiring explicit reward annotations. Recent approaches attempt to estimate rewards for unlabeled datasets using a small set of expert demonstrations. However, these methods often assume that the similarity between a trajectory and an expert demonstration is positively correlated with the reward, which oversimplifies the underlying reward structure. We propose PROF, a novel framework that leverages large language models (LLMs) to generate and improve executable reward function codes from natural language descriptions and a single expert trajectory. We propose Reward Preference Ranking (RPR), a novel reward function quality assessment and ranking strategy without requiring environment interactions or RL training. RPR calculates the dominance scores of the reward functions, where higher scores indicate better alignment with expert preferences. By alternating between RPR and text-based gradient optimization, PROF fully automates the selection and refinement of optimal reward functions for downstream policy learning. Empirical results on D4RL demonstrate that PROF surpasses or matches recent strong baselines across numerous datasets and domains, highlighting the effectiveness of our approach.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes Large Language Models (LLMs) for offline imitation learning, which can be applied to trajectory prediction problems (e.g., predicting the actions of an agent based on observed trajectories). While the paper itself doesn't directly address trajectory prediction, the techniques described are applicable to that domain.", "keywords": ["Large Language Models", "LLMs", "imitation learning", "offline imitation learning", "reward function"]}}
{"id": "2511.14565", "pdf": "https://arxiv.org/pdf/2511.14565", "abs": "https://arxiv.org/abs/2511.14565", "authors": ["Minyoung Hwang", "Alexandra Forsey-Smerek", "Nathaniel Dennler", "Andreea Bobu"], "title": "Masked IRL: LLM-Guided Reward Disambiguation from Demonstrations and Language", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Robots can adapt to user preferences by learning reward functions from demonstrations, but with limited data, reward models often overfit to spurious correlations and fail to generalize. This happens because demonstrations show robots how to do a task but not what matters for that task, causing the model to focus on irrelevant state details. Natural language can more directly specify what the robot should focus on, and, in principle, disambiguate between many reward functions consistent with the demonstrations. However, existing language-conditioned reward learning methods typically treat instructions as simple conditioning signals, without fully exploiting their potential to resolve ambiguity. Moreover, real instructions are often ambiguous themselves, so naive conditioning is unreliable. Our key insight is that these two input types carry complementary information: demonstrations show how to act, while language specifies what is important. We propose Masked Inverse Reinforcement Learning (Masked IRL), a framework that uses large language models (LLMs) to combine the strengths of both input types. Masked IRL infers state-relevance masks from language instructions and enforces invariance to irrelevant state components. When instructions are ambiguous, it uses LLM reasoning to clarify them in the context of the demonstrations. In simulation and on a real robot, Masked IRL outperforms prior language-conditioned IRL methods by up to 15% while using up to 4.7 times less data, demonstrating improved sample-efficiency, generalization, and robustness to ambiguous language. Project page: https://MIT-CLEAR-Lab.github.io/Masked-IRL and Code: https://github.com/MIT-CLEAR-Lab/Masked-IRL", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses using Large Language Models (LLMs) to guide reward disambiguation in Inverse Reinforcement Learning (IRL). While it doesn't directly focus on trajectory prediction, IRL is often used in robotics and autonomous navigation, which can involve trajectory prediction. The key connection is the use of LLMs to improve robot behavior, which could indirectly affect trajectory prediction tasks. The primary focus is on reward learning, but the application domain is relevant.", "keywords": ["Large Language Models", "LLMs", "Inverse Reinforcement Learning", "Robotics", "Reward Learning"]}}
{"id": "2511.14199", "pdf": "https://arxiv.org/pdf/2511.14199", "abs": "https://arxiv.org/abs/2511.14199", "authors": ["Jiazhuo Tian", "Yachao Yuan"], "title": "HFL-FlowLLM: Large Language Models for Network Traffic Flow Classification in Heterogeneous Federated Learning", "categories": ["cs.AI"], "comment": null, "summary": "In modern communication networks driven by 5G and the Internet of Things (IoT), effective network traffic flow classification is crucial for Quality of Service (QoS) management and security. Traditional centralized machine learning struggles with the distributed data and privacy concerns in these heterogeneous environments, while existing federated learning approaches suffer from high costs and poor generalization. To address these challenges, we propose HFL-FlowLLM, which to our knowledge is the first framework to apply large language models to network traffic flow classification in heterogeneous federated learning. Compared to state-of-the-art heterogeneous federated learning methods for network traffic flow classification, the proposed approach improves the average F1 score by approximately 13%, demonstrating compelling performance and strong robustness. When compared to existing large language models federated learning frameworks, as the number of clients participating in each training round increases, the proposed method achieves up to a 5% improvement in average F1 score while reducing the training costs by about 87%. These findings prove the potential and practical value of HFL-FlowLLM in modern communication networks security.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses the application of large language models (LLMs) in heterogeneous federated learning for network traffic flow classification. While it doesn't directly deal with trajectory prediction, it leverages LLMs, a key component of the prompt, and the classification task can be seen as predicting future network behavior. Therefore, there's a moderate degree of relevance.", "keywords": ["Large Language Models", "LLMs", "Federated Learning", "Classification"]}}
{"id": "2511.14592", "pdf": "https://arxiv.org/pdf/2511.14592", "abs": "https://arxiv.org/abs/2511.14592", "authors": ["Xianhui Meng", "Yuchen Zhang", "Zhijian Huang", "Zheng Lu", "Ziling Ji", "Yaoyao Yin", "Hongyuan Zhang", "Guangfeng Jiang", "Yandan Lin", "Long Chen", "Hangjun Ye", "Li Zhang", "Jun Liu", "Xiaoshuai Hao"], "title": "Is Your VLM for Autonomous Driving Safety-Ready? A Comprehensive Benchmark for Evaluating External and In-Cabin Risks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) show great promise for autonomous driving, but their suitability for safety-critical scenarios is largely unexplored, raising safety concerns. This issue arises from the lack of comprehensive benchmarks that assess both external environmental risks and in-cabin driving behavior safety simultaneously. To bridge this critical gap, we introduce DSBench, the first comprehensive Driving Safety Benchmark designed to assess a VLM's awareness of various safety risks in a unified manner. DSBench encompasses two major categories: external environmental risks and in-cabin driving behavior safety, divided into 10 key categories and a total of 28 sub-categories. This comprehensive evaluation covers a wide range of scenarios, ensuring a thorough assessment of VLMs' performance in safety-critical contexts. Extensive evaluations across various mainstream open-source and closed-source VLMs reveal significant performance degradation under complex safety-critical situations, highlighting urgent safety concerns. To address this, we constructed a large dataset of 98K instances focused on in-cabin and external safety scenarios, showing that fine-tuning on this dataset significantly enhances the safety performance of existing VLMs and paves the way for advancing autonomous driving technology. The benchmark toolkit, code, and model checkpoints will be publicly accessible.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on evaluating Vision-Language Models (VLMs) for autonomous driving safety, assessing their awareness of external and in-cabin risks. While it doesn't directly address trajectory prediction, the context of autonomous driving implies an indirect relationship. The use of VLMs, a type of large model, is a key aspect. It doesn't explicitly combine trajectory prediction and large models, but the application to autonomous driving suggests potential for future integration.", "keywords": ["Vision-Language Models", "VLMs", "autonomous driving", "safety", "benchmark", "large language models"]}}
{"id": "2511.13904", "pdf": "https://arxiv.org/pdf/2511.13904", "abs": "https://arxiv.org/abs/2511.13904", "authors": ["Yuqiang Lin", "Sam Lockyer", "Florian Stanek", "Markus Zarbock", "Adrian Evans", "Wenbin Li", "Nic Zhang"], "title": "SAE-MCVT: A Real-Time and Scalable Multi-Camera Vehicle Tracking Framework Powered by Edge Computing", "categories": ["cs.CV"], "comment": null, "summary": "In modern Intelligent Transportation Systems (ITS), cameras are a key component due to their ability to provide valuable information for multiple stakeholders. A central task is Multi-Camera Vehicle Tracking (MCVT), which generates vehicle trajectories and enables applications such as anomaly detection, traffic density estimation, and suspect vehicle tracking. However, most existing studies on MCVT emphasize accuracy while overlooking real-time performance and scalability. These two aspects are essential for real-world deployment and become increasingly challenging in city-scale applications as the number of cameras grows. To address this issue, we propose SAE-MCVT, the first scalable real-time MCVT framework. The system includes several edge devices that interact with one central workstation separately. On the edge side, live RTSP video streams are serialized and processed through modules including object detection, object tracking, geo-mapping, and feature extraction. Only lightweight metadata -- vehicle locations and deep appearance features -- are transmitted to the central workstation. On the central side, cross-camera association is calculated under the constraint of spatial-temporal relations between adjacent cameras, which are learned through a self-supervised camera link model. Experiments on the RoundaboutHD dataset show that SAE-MCVT maintains real-time operation on 2K 15 FPS video streams and achieves an IDF1 score of 61.2. To the best of our knowledge, this is the first scalable real-time MCVT framework suitable for city-scale deployment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6444\u50cf\u5934\u8f66\u8f86\u8ddf\u8e2a\uff0c\u751f\u6210\u8f66\u8f86\u8f68\u8ff9\uff0c\u5e76\u4fa7\u91cd\u4e8e\u5b9e\u65f6\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u867d\u7136\u6d89\u53ca\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u6ca1\u6709\u63d0\u53ca\u6216\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory prediction", "vehicle tracking", "Multi-Camera Vehicle Tracking", "vehicle trajectories"]}}
{"id": "2511.13909", "pdf": "https://arxiv.org/pdf/2511.13909", "abs": "https://arxiv.org/abs/2511.13909", "authors": ["Chalamalasetti Kranti"], "title": "Mind the Gap: Evaluating LLM Understanding of Human-Taught Road Safety Principles", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Following road safety norms is non-negotiable not only for humans but also for the AI systems that govern autonomous vehicles. In this work, we evaluate how well multi-modal large language models (LLMs) understand road safety concepts, specifically through schematic and illustrative representations. We curate a pilot dataset of images depicting traffic signs and road-safety norms sourced from school text books and use it to evaluate models capabilities in a zero-shot setting. Our preliminary results show that these models struggle with safety reasoning and reveal gaps between human learning and model interpretation. We further provide an analysis of these performance gaps for future research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on evaluating large language models (LLMs) understanding of road safety principles. While it doesn't directly address trajectory prediction, the context of road safety and autonomous vehicles implies a potential connection to trajectory prediction as autonomous vehicles need to predict the trajectories of other agents and themselves. The paper explicitly mentions LLMs and their evaluation, increasing its relevance.", "keywords": ["Large Language Models", "LLMs", "Road Safety", "Autonomous Vehicles"]}}
{"id": "2511.14659", "pdf": "https://arxiv.org/pdf/2511.14659", "abs": "https://arxiv.org/abs/2511.14659", "authors": ["Chia-Yu Hung", "Navonil Majumder", "Haoyuan Deng", "Liu Renhang", "Yankang Ang", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Ziwei Wang", "Soujanya Poria"], "title": "NORA-1.5: A Vision-Language-Action Model Trained using World Model- and Action-based Preference Rewards", "categories": ["cs.RO", "cs.AI"], "comment": "https://declare-lab.github.io/nora-1.5", "summary": "Vision--language--action (VLA) models have recently shown promising performance on a variety of embodied tasks, yet they still fall short in reliability and generalization, especially when deployed across different embodiments or real-world environments. In this work, we introduce NORA-1.5, a VLA model built from the pre-trained NORA backbone by adding to it a flow-matching-based action expert. This architectural enhancement alone yields substantial performance gains, enabling NORA-1.5 to outperform NORA and several state-of-the-art VLA models across both simulated and real-world benchmarks. To further improve robustness and task success, we develop a set of reward models for post-training VLA policies. Our rewards combine (i) an action-conditioned world model (WM) that evaluates whether generated actions lead toward the desired goal, and (ii) a deviation-from-ground-truth heuristic that distinguishes good actions from poor ones. Using these reward signals, we construct preference datasets and adapt NORA-1.5 to target embodiments through direct preference optimization (DPO). Extensive evaluations show that reward-driven post-training consistently improves performance in both simulation and real-robot settings, demonstrating significant VLA model-reliability gains through simple yet effective reward models. Our findings highlight NORA-1.5 and reward-guided post-training as a viable path toward more dependable embodied agents suitable for real-world deployment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Vision-Language-Action (VLA) models for embodied tasks. While it doesn't explicitly mention trajectory prediction, the concept of action planning and achieving goals in an environment is related. The use of a world model (WM) to evaluate actions and improve policy robustness suggests a connection to predicting future states, which is fundamental to trajectory prediction. The paper also touches upon large pre-trained models (NORA backbone) and preference optimization, linking it to the large model domain. However, the primary focus is not directly on trajectory prediction or developing new large language models, hence the moderate relevance score.", "keywords": ["Vision-Language-Action Models", "World Model", "Action-conditioned", "Preference Optimization", "Large Pre-trained Models"]}}
{"id": "2511.14227", "pdf": "https://arxiv.org/pdf/2511.14227", "abs": "https://arxiv.org/abs/2511.14227", "authors": ["Yuxiang Wang", "Siwen Wang", "Haowei Han", "Ao Wang", "Boya Liu", "Yong Zhao", "Chengbo Wu", "Bin Zhu", "Bin Qin", "Xiaokai Zhou", "Xiao Yan", "Jiawei Jiang", "Bo Du"], "title": "DevPiolt: Operation Recommendation for IoT Devices at Xiaomi Home", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Operation recommendation for IoT devices refers to generating personalized device operations for users based on their context, such as historical operations, environment information, and device status. This task is crucial for enhancing user satisfaction and corporate profits. Existing recommendation models struggle with complex operation logic, diverse user preferences, and sensitive to suboptimal suggestions, limiting their applicability to IoT device operations. To address these issues, we propose DevPiolt, a LLM-based recommendation model for IoT device operations. Specifically, we first equip the LLM with fundamental domain knowledge of IoT operations via continual pre-training and multi-task fine-tuning. Then, we employ direct preference optimization to align the fine-tuned LLM with specific user preferences. Finally, we design a confidence-based exposure control mechanism to avoid negative user experiences from low-quality recommendations. Extensive experiments show that DevPiolt significantly outperforms baselines on all datasets, with an average improvement of 69.5% across all metrics. DevPiolt has been practically deployed in Xiaomi Home app for one quarter, providing daily operation recommendations to 255,000 users. Online experiment results indicate a 21.6% increase in unique visitor device coverage and a 29.1% increase in page view acceptance rates.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on operation recommendation for IoT devices using Large Language Models (LLMs). While it doesn't directly address trajectory prediction, it utilizes LLMs, which is one of the core areas of interest. Therefore, the relevance is moderate.", "keywords": ["Large Language Models", "LLM", "recommendation model"]}}
{"id": "2511.14248", "pdf": "https://arxiv.org/pdf/2511.14248", "abs": "https://arxiv.org/abs/2511.14248", "authors": ["Hongju Lee", "Youngjun Park", "Jisun An", "Dongman Lee"], "title": "Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility", "categories": ["cs.AI"], "comment": "Accepted at ASONAM 2025", "summary": "The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses LLM embeddings to enhance time-series forecasting, incorporating human mobility as a contextual factor. While the primary focus is not trajectory prediction itself, the use of human mobility data and LLMs for forecasting connects to the relevant themes. The inclusion of LLMs raises the relevance score.", "keywords": ["Large Language Model (LLM)", "human mobility", "time-series forecasting"]}}
{"id": "2511.13841", "pdf": "https://arxiv.org/pdf/2511.13841", "abs": "https://arxiv.org/abs/2511.13841", "authors": ["Zelei Shao", "Vikranth Srivatsa", "Sanjana Srivastava", "Qingyang Wu", "Alpay Ariyak", "Xiaoxia Wu", "Ameen Patel", "Jue Wang", "Percy Liang", "Tri Dao", "Ce Zhang", "Yiying Zhang", "Ben Athiwaratkun", "Chenfeng Xu", "Junxiong Wang"], "title": "Beat the long tail: Distribution-Aware Speculative Decoding for RL Training", "categories": ["cs.LG"], "comment": null, "summary": "Reinforcement learning(RL) post-training has become essential for aligning large language models (LLMs), yet its efficiency is increasingly constrained by the rollout phase, where long trajectories are generated token by token. We identify a major bottleneck:the long-tail distribution of rollout lengths, where a small fraction of long generations dominates wall clock time and a complementary opportunity; the availability of historical rollouts that reveal stable prompt level patterns across training epochs. Motivated by these observations, we propose DAS, a Distribution Aware Speculative decoding framework that accelerates RL rollouts without altering model outputs. DAS integrates two key ideas: an adaptive, nonparametric drafter built from recent rollouts using an incrementally maintained suffix tree, and a length aware speculation policy that allocates more aggressive draft budgets to long trajectories that dominate makespan. This design exploits rollout history to sustain acceptance while balancing base and token level costs during decoding. Experiments on math and code reasoning tasks show that DAS reduces rollout time up to 50% while preserving identical training curves, demonstrating that distribution-aware speculative decoding can significantly accelerate RL post training without compromising learning quality.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6539\u8fdb\u89e3\u7801\u7b56\u7565\u6765\u52a0\u901f\u751f\u6210\u957f\u5e8f\u5217\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u4f18\u5316\u5e8f\u5217\u751f\u6210\u8fc7\u7a0b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5728\u5e8f\u5217\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u8054\u6027\uff0c\u5e76\u4e14\u660e\u786e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["Large Language Models", "LLMs", "Reinforcement Learning", "Speculative Decoding", "long trajectories"]}}
{"id": "2511.11567", "pdf": "https://arxiv.org/pdf/2511.11567", "abs": "https://arxiv.org/abs/2511.11567", "authors": ["Allen Emmanuel Binny", "Anushri Dixit"], "title": "Who Moved My Distribution? Conformal Prediction for Interactive Multi-Agent Systems", "categories": ["eess.SY", "cs.RO"], "comment": null, "summary": "Uncertainty-aware prediction is essential for safe motion planning, especially when using learned models to forecast the behavior of surrounding agents. Conformal prediction is a statistical tool often used to produce uncertainty-aware prediction regions for machine learning models. Most existing frameworks utilizing conformal prediction-based uncertainty predictions assume that the surrounding agents are non-interactive. This is because in closed-loop, as uncertainty-aware agents change their behavior to account for prediction uncertainty, the surrounding agents respond to this change, leading to a distribution shift which we call endogenous distribution shift. To address this challenge, we introduce an iterative conformal prediction framework that systematically adapts the uncertainty-aware ego-agent controller to the endogenous distribution shift. The proposed method provides probabilistic safety guarantees while adapting to the evolving behavior of reactive, non-ego agents. We establish a model for the endogenous distribution shift and provide the conditions for the iterative conformal prediction pipeline to converge under such a distribution shift. We validate our framework in simulation for 2- and 3- agent interaction scenarios, demonstrating collision avoidance without resulting in overly conservative behavior and an overall improvement in success rates of up to 9.6% compared to other conformal prediction-based baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on uncertainty-aware prediction in multi-agent systems, which is related to trajectory prediction. It uses conformal prediction for forecasting agent behavior. While it doesn't explicitly mention large language models, the concept of learning agent behavior and dealing with distribution shift could potentially be relevant to how LLMs are used in similar contexts. However, the primary focus is on multi-agent interaction and conformal prediction, making the connection to LLMs weaker.", "keywords": ["trajectory prediction", "multi-agent systems", "uncertainty-aware prediction", "conformal prediction", "distribution shift"]}}
{"id": "2511.13993", "pdf": "https://arxiv.org/pdf/2511.13993", "abs": "https://arxiv.org/abs/2511.13993", "authors": ["Kumar Ashutosh", "Kristen Grauman"], "title": "Learning Skill-Attributes for Transferable Assessment in Video", "categories": ["cs.CV"], "comment": "NeurIPS 2025, Project webpage: https://vision.cs.utexas.edu/projects/CrossTrainer/", "summary": "Skill assessment from video entails rating the quality of a person's physical performance and explaining what could be done better. Today's models specialize for an individual sport, and suffer from the high cost and scarcity of expert-level supervision across the long tail of sports. Towards closing that gap, we explore transferable video representations for skill assessment. Our CrossTrainer approach discovers skill-attributes, such as balance, control, and hand positioning -- whose meaning transcends the boundaries of any given sport, then trains a multimodal language model to generate actionable feedback for a novel video, e.g., \"lift hands more to generate more power\" as well as its proficiency level, e.g., early expert. We validate the new model on multiple datasets for both cross-sport (transfer) and intra-sport (in-domain) settings, where it achieves gains up to 60% relative to the state of the art. By abstracting out the shared behaviors indicative of human skill, the proposed video representation generalizes substantially better than an array of existing techniques, enriching today's multimodal large language models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses skill assessment from video using a multimodal language model. While not directly related to trajectory prediction, it utilizes large language models for generating feedback based on observed actions and movements. The connection to trajectory prediction is indirect, as the model analyzes the performance of skills, which could involve analyzing movement trajectories. The paper also explicitly mentions 'multimodal large language models'.", "keywords": ["multimodal language model", "large language models", "skill assessment", "video representation"]}}
{"id": "2511.14670", "pdf": "https://arxiv.org/pdf/2511.14670", "abs": "https://arxiv.org/abs/2511.14670", "authors": ["Ruomeng Ding", "Wei Cheng", "Minglai Shao", "Chen Zhao"], "title": "SkillGen: Learning Domain Skills for In-Context Sequential Decision Making", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly applied to sequential decision-making through in-context learning (ICL), yet their effectiveness is highly sensitive to prompt quality. Effective prompts should meet three principles: focus on decision-critical information, provide step-level granularity, and minimize reliance on expert annotations through label efficiency. However, existing ICL methods often fail to satisfy all three criteria simultaneously. Motivated by these challenges, we introduce SkillGen, a skill-based ICL framework for structured sequential reasoning. It constructs an action-centric, domain-level graph from sampled trajectories, identifies high-utility actions via temporal-difference credit assignment, and retrieves step-wise skills to generate fine-grained, context-aware prompts. We further present a theoretical analysis showing that focusing on high-utility segments supports task identifiability and informs more effective ICL prompt design. Experiments on ALFWorld, BabyAI, and ScienceWorld, using both open-source and proprietary LLMs, show that SkillGen achieves consistent gains, improving progress rate by 5.9%-16.5% on average across models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86 SkillGen \u6846\u67b6\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4f8b\u5982\u8def\u5f84\u89c4\u5212\u53ef\u4ee5\u770b\u4f5c\u662f\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\u3002\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u51b3\u7b56\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002\u6574\u4f53\u6765\u770b\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["Large language models", "LLMs", "sequential decision-making", "in-context learning", "ICL"]}}
{"id": "2511.14100", "pdf": "https://arxiv.org/pdf/2511.14100", "abs": "https://arxiv.org/abs/2511.14100", "authors": ["Yiqing Shen", "Chenjia Li", "Mathias Unberath"], "title": "Text-Driven Reasoning Video Editing via Reinforcement Learning on Digital Twin Representations", "categories": ["cs.CV"], "comment": null, "summary": "Text-driven video editing enables users to modify video content only using text queries. While existing methods can modify video content if explicit descriptions of editing targets with precise spatial locations and temporal boundaries are provided, these requirements become impractical when users attempt to conceptualize edits through implicit queries referencing semantic properties or object relationships. We introduce reasoning video editing, a task where video editing models must interpret implicit queries through multi-hop reasoning to infer editing targets before executing modifications, and a first model attempting to solve this complex task, RIVER (Reasoning-based Implicit Video Editor). RIVER decouples reasoning from generation through digital twin representations of video content that preserve spatial relationships, temporal trajectories, and semantic attributes. A large language model then processes this representation jointly with the implicit query, performing multi-hop reasoning to determine modifications, then outputs structured instructions that guide a diffusion-based editor to execute pixel-level changes. RIVER training uses reinforcement learning with rewards that evaluate reasoning accuracy and generation quality. Finally, we introduce RVEBenchmark, a benchmark of 100 videos with 519 implicit queries spanning three levels and categories of reasoning complexity specifically for reasoning video editing. RIVER demonstrates best performance on the proposed RVEBenchmark and also achieves state-of-the-art performance on two additional video editing benchmarks (VegGIE and FiVE), where it surpasses six baseline methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u63d0\u5230\u4e86\"temporal trajectories\"\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u89c6\u9891\u7f16\u8f91\u7684\u63a8\u7406\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\uff0c\u5c24\u5176\u662fLLM\u90e8\u5206\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u7f16\u8f91\uff0c\u4f46\u5176\u4f7f\u7528\u7684\u6280\u672f\u548c\u65b9\u6cd5\u53ef\u80fd\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u7ed3\u5408\u7684\u5e94\u7528\u6709\u501f\u9274\u610f\u4e49\u3002", "keywords": ["temporal trajectories", "large language model", "LLM"]}}
