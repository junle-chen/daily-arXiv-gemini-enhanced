{"id": "2506.10853", "pdf": "https://arxiv.org/pdf/2506.10853", "abs": "https://arxiv.org/abs/2506.10853", "authors": ["Yu Zhang", "Yang Hu", "De Wang"], "title": "A Study on Individual Spatiotemporal Activity Generation Method Using MCP-Enhanced Chain-of-Thought Large Language Models", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Human spatiotemporal behavior simulation is critical for urban planning\nresearch, yet traditional rule-based and statistical approaches suffer from\nhigh computational costs, limited generalizability, and poor scalability. While\nlarge language models (LLMs) show promise as \"world simulators,\" they face\nchallenges in spatiotemporal reasoning including limited spatial cognition,\nlack of physical constraint understanding, and group homogenization tendencies.\nThis paper introduces a framework integrating chain-of-thought (CoT) reasoning\nwith Model Context Protocol (MCP) to enhance LLMs' capability in simulating\nspatiotemporal behaviors that correspond with validation data patterns. The\nmethodology combines human-like progressive reasoning through a five-stage\ncognitive framework with comprehensive data processing via six specialized MCP\ntool categories: temporal management, spatial navigation, environmental\nperception, personal memory, social collaboration, and experience evaluation.\nExperiments in Shanghai's Lujiazui district validate the framework's\neffectiveness across 1,000 generated samples. Results demonstrate high\nsimilarity with real mobile signaling data, achieving generation quality scores\nof 7.86 to 8.36 across different base models. Parallel processing experiments\nshow efficiency improvements, with generation times decreasing from 1.30 to\n0.17 minutes per sample when scaling from 2 to 12 processes. This work\ncontributes to integrating CoT reasoning with MCP for urban behavior modeling,\nadvancing LLMs applications in urban computing and providing a practical\napproach for synthetic mobility data generation. The framework offers a\nfoundation for smart city planning, transportation forecasting, and\nparticipatory urban design applications.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u751f\u6210\u65f6\u7a7a\u6d3b\u52a8\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u57ce\u5e02\u89c4\u5212\u7814\u7a76\u3002\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\uff08\u901a\u8fc7\u6a21\u62df\u65f6\u7a7a\u884c\u4e3a\uff09\u548c\u5927\u6a21\u578b\u8fd9\u4e24\u4e2a\u4e3b\u9898\u90fd\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u4f7f\u7528\u4e86Chain-of-Thought (CoT) reasoning \u548c Model Context Protocol (MCP) \u6765\u589e\u5f3aLLMs\u5728\u65f6\u7a7a\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8868\u660e\u4e86\u5176\u5728\u6a21\u62df\u79fb\u52a8\u884c\u4e3a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "keywords": ["trajectory prediction", "large language models", "LLMs", "spatiotemporal behavior", "Chain-of-Thought", "CoT", "Model Context Protocol", "MCP", "urban planning", "mobility data generation"]}}
{"id": "2506.10145", "pdf": "https://arxiv.org/pdf/2506.10145", "abs": "https://arxiv.org/abs/2506.10145", "authors": ["Rajeev Yasarla", "Shizhong Han", "Hsin-Pai Cheng", "Litian Liu", "Shweta Mahajan", "Apratim Bhattacharyya", "Yunxiao Shi", "Risheek Garrepalli", "Hong Cai", "Fatih Porikli"], "title": "RoCA: Robust Cross-Domain End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end (E2E) autonomous driving has recently emerged as a new paradigm,\noffering significant potential. However, few studies have looked into the\npractical challenge of deployment across domains (e.g., cities). Although\nseveral works have incorporated Large Language Models (LLMs) to leverage their\nopen-world knowledge, LLMs do not guarantee cross-domain driving performance\nand may incur prohibitive retraining costs during domain adaptation. In this\npaper, we propose RoCA, a novel framework for robust cross-domain E2E\nautonomous driving. RoCA formulates the joint probabilistic distribution over\nthe tokens that encode ego and surrounding vehicle information in the E2E\npipeline. Instantiating with a Gaussian process (GP), RoCA learns a set of\nbasis tokens with corresponding trajectories, which span diverse driving\nscenarios. Then, given any driving scene, it is able to probabilistically infer\nthe future trajectory. By using RoCA together with a base E2E model in\nsource-domain training, we improve the generalizability of the base model,\nwithout requiring extra inference computation. In addition, RoCA enables robust\nadaptation on new target domains, significantly outperforming direct\nfinetuning. We extensively evaluate RoCA on various cross-domain scenarios and\nshow that it achieves strong domain generalization and adaptation performance.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on end-to-end autonomous driving, which inherently involves trajectory prediction. It also explicitly mentions and utilizes Large Language Models (LLMs) for leveraging open-world knowledge. The core of the paper, probabilistic inference of future trajectories, directly relates to trajectory prediction. Therefore, the paper is highly relevant to both trajectory prediction and large language models.", "keywords": ["trajectory prediction", "autonomous driving", "Large Language Models", "LLMs", "end-to-end", "cross-domain", "future trajectory"]}}
{"id": "2506.10756", "pdf": "https://arxiv.org/pdf/2506.10756", "abs": "https://arxiv.org/abs/2506.10756", "authors": ["Yuhang Zhang", "Haosheng Yu", "Jiaping Xiao", "Mir Feroskhan"], "title": "Grounded Vision-Language Navigation for UAVs with Open-Vocabulary Goal Understanding", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-and-language navigation (VLN) is a long-standing challenge in\nautonomous robotics, aiming to empower agents with the ability to follow human\ninstructions while navigating complex environments. Two key bottlenecks remain\nin this field: generalization to out-of-distribution environments and reliance\non fixed discrete action spaces. To address these challenges, we propose\nVision-Language Fly (VLFly), a framework tailored for Unmanned Aerial Vehicles\n(UAVs) to execute language-guided flight. Without the requirement for\nlocalization or active ranging sensors, VLFly outputs continuous velocity\ncommands purely from egocentric observations captured by an onboard monocular\ncamera. The VLFly integrates three modules: an instruction encoder based on a\nlarge language model (LLM) that reformulates high-level language into\nstructured prompts, a goal retriever powered by a vision-language model (VLM)\nthat matches these prompts to goal images via vision-language similarity, and a\nwaypoint planner that generates executable trajectories for real-time UAV\ncontrol. VLFly is evaluated across diverse simulation environments without\nadditional fine-tuning and consistently outperforms all baselines. Moreover,\nreal-world VLN tasks in indoor and outdoor environments under direct and\nindirect instructions demonstrate that VLFly achieves robust open-vocabulary\ngoal understanding and generalized navigation capabilities, even in the\npresence of abstract language input.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5e94\u7528\u4e8e\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4efb\u52a1\uff0c\u5176\u4e2d\u6d89\u53ca\u751f\u6210\u53ef\u6267\u884c\u8f68\u8ff9\u7684\u8def\u5f84\u89c4\u5212\u5668\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u5747\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "large language model", "LLM", "vision-language navigation", "UAV", "waypoint planner"]}}
{"id": "2506.10172", "pdf": "https://arxiv.org/pdf/2506.10172", "abs": "https://arxiv.org/abs/2506.10172", "authors": ["Yicheng Duan", "Kaiyu tang"], "title": "A Navigation Framework Utilizing Vision-Language Models", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) presents a complex challenge in embodied\nAI, requiring agents to interpret natural language instructions and navigate\nthrough visually rich, unfamiliar environments. Recent advances in large\nvision-language models (LVLMs), such as CLIP and Flamingo, have significantly\nimproved multimodal understanding but introduced new challenges related to\ncomputational cost and real-time deployment. In this project, we propose a\nmodular, plug-and-play navigation framework that decouples vision-language\nunderstanding from action planning. By integrating a frozen vision-language\nmodel, Qwen2.5-VL-7B-Instruct, with lightweight planning logic, we aim to\nachieve flexible, fast, and adaptable navigation without extensive model\nfine-tuning. Our framework leverages prompt engineering, structured history\nmanagement, and a two-frame visual input strategy to enhance decision-making\ncontinuity across navigation steps. We evaluate our system on the Room-to-Room\nbenchmark within the VLN-CE setting using the Matterport3D dataset and\nHabitat-Lab simulation environment. Although our initial results reveal\nchallenges in generalizing to unseen environments under strict evaluation\nsettings, our modular approach lays a foundation for scalable and efficient\nnavigation systems, highlighting promising directions for future improvement\nthrough enhanced environmental priors and expanded multimodal input\nintegration.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on Vision-Language Navigation (VLN), which involves navigating through environments based on language instructions and visual input. While not directly trajectory prediction, navigation inherently involves planning and executing a path, which is related. The paper also explicitly uses and discusses Large Vision-Language Models (LVLMs) like CLIP, Flamingo, and Qwen2.5-VL-7B-Instruct, making it relevant to the large language model aspect.", "keywords": ["Vision-Language Navigation", "VLN", "Large Vision-Language Models", "LVLMs", "Qwen2.5-VL-7B-Instruct", "navigation", "action planning"]}}
{"id": "2506.10353", "pdf": "https://arxiv.org/pdf/2506.10353", "abs": "https://arxiv.org/abs/2506.10353", "authors": ["Runqi Ouyang", "Haoyun Li", "Zhenyuan Zhang", "Xiaofeng Wang", "Zheng Zhu", "Guan Huang", "Xingang Wang"], "title": "Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in large language models, especially in natural language\nunderstanding and reasoning, have opened new possibilities for text-to-motion\ngeneration. Although existing approaches have made notable progress in semantic\nalignment and motion synthesis, they often rely on end-to-end mapping\nstrategies that fail to capture deep linguistic structures and logical\nreasoning. Consequently, generated motions tend to lack controllability,\nconsistency, and diversity. To address these limitations, we propose Motion-R1,\na unified motion-language modeling framework that integrates a Chain-of-Thought\nmechanism. By explicitly decomposing complex textual instructions into\nlogically structured action paths, Motion-R1 provides high-level semantic\nguidance for motion generation, significantly enhancing the model's ability to\ninterpret and execute multi-step, long-horizon, and compositionally rich\ncommands. To train our model, we adopt Group Relative Policy Optimization, a\nreinforcement learning algorithm designed for large models, which leverages\nmotion quality feedback to optimize reasoning chains and motion synthesis\njointly. Extensive experiments across multiple benchmark datasets demonstrate\nthat Motion-R1 achieves competitive or superior performance compared to\nstate-of-the-art methods, particularly in scenarios requiring nuanced semantic\nunderstanding and long-term temporal coherence. The code, model and data will\nbe publicly available.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper is relevant because it uses large language models and reinforcement learning for motion generation. While it doesn't explicitly mention trajectory prediction, motion generation can be considered a related area. The paper focuses on generating human motions from text, leveraging LLMs for reasoning and planning.", "keywords": ["large language models", "motion generation", "reinforcement learning", "reasoning", "Chain-of-Thought"]}}
{"id": "2506.10574", "pdf": "https://arxiv.org/pdf/2506.10574", "abs": "https://arxiv.org/abs/2506.10574", "authors": ["Qing Wang", "Xiaohang Yang", "Yilan Dong", "Naveen Raj Govindaraj", "Gregory Slabaugh", "Shanxin Yuan"], "title": "DanceChat: Large Language Model-Guided Music-to-Dance Generation", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "comment": "check demos at https://dancechat.github.io/anon/", "summary": "Music-to-dance generation aims to synthesize human dance motion conditioned\non musical input. Despite recent progress, significant challenges remain due to\nthe semantic gap between music and dance motion, as music offers only abstract\ncues, such as melody, groove, and emotion, without explicitly specifying the\nphysical movements. Moreover, a single piece of music can produce multiple\nplausible dance interpretations. This one-to-many mapping demands additional\nguidance, as music alone provides limited information for generating diverse\ndance movements. The challenge is further amplified by the scarcity of paired\nmusic and dance data, which restricts the model\\^a\\u{A}\\'Zs ability to learn\ndiverse dance patterns. In this paper, we introduce DanceChat, a Large Language\nModel (LLM)-guided music-to-dance generation approach. We use an LLM as a\nchoreographer that provides textual motion instructions, offering explicit,\nhigh-level guidance for dance generation. This approach goes beyond implicit\nlearning from music alone, enabling the model to generate dance that is both\nmore diverse and better aligned with musical styles. Our approach consists of\nthree components: (1) an LLM-based pseudo instruction generation module that\nproduces textual dance guidance based on music style and structure, (2) a\nmulti-modal feature extraction and fusion module that integrates music, rhythm,\nand textual guidance into a shared representation, and (3) a diffusion-based\nmotion synthesis module together with a multi-modal alignment loss, which\nensures that the generated dance is aligned with both musical and textual cues.\nExtensive experiments on AIST++ and human evaluations show that DanceChat\noutperforms state-of-the-art methods both qualitatively and quantitatively.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on music-to-dance generation, which involves generating human motion (a form of trajectory) conditioned on music. It explicitly uses a Large Language Model (LLM) to guide the dance generation process, making it relevant to both trajectory prediction (motion generation) and large language models. Although the generated motion is dance, the underlying principles of motion prediction and control are related to trajectory prediction. The LLM component is directly related to the 'large language models' theme.", "keywords": ["trajectory prediction", "large language model", "LLM", "motion generation", "dance generation"]}}
{"id": "2506.10093", "pdf": "https://arxiv.org/pdf/2506.10093", "abs": "https://arxiv.org/abs/2506.10093", "authors": ["Marcos Abel Zuzu\u00e1rregui", "Stefano Carpin"], "title": "Leveraging LLMs for Mission Planning in Precision Agriculture", "categories": ["cs.RO", "cs.AI"], "comment": "Published in Proceedings of 2025 International Conference on Robotics\n  and Automation (ICRA)", "summary": "Robotics and artificial intelligence hold significant potential for advancing\nprecision agriculture. While robotic systems have been successfully deployed\nfor various tasks, adapting them to perform diverse missions remains\nchallenging, particularly because end users often lack technical expertise. In\nthis paper, we present an end-to-end system that leverages large language\nmodels (LLMs), specifically ChatGPT, to enable users to assign complex data\ncollection tasks to autonomous robots using natural language instructions. To\nenhance reusability, mission plans are encoded using an existing IEEE task\nspecification standard, and are executed on robots via ROS2 nodes that bridge\nhigh-level mission descriptions with existing ROS libraries. Through extensive\nexperiments, we highlight the strengths and limitations of LLMs in this\ncontext, particularly regarding spatial reasoning and solving complex routing\nchallenges, and show how our proposed implementation overcomes them.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u8868\u660e\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\uff0c\u6d89\u53ca\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u548c\u590d\u6742\u8def\u7531\u6311\u6218\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e3b\u8981\u4fa7\u91cd\u4e8e\u4efb\u52a1\u89c4\u5212\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4e0e\u79fb\u52a8\u7269\u4f53\u7684\u8def\u5f84\u89c4\u5212\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["LLMs", "large language models", "mission planning", "routing", "robots"]}}
{"id": "2506.10106", "pdf": "https://arxiv.org/pdf/2506.10106", "abs": "https://arxiv.org/abs/2506.10106", "authors": ["Marcos Abel Zuzu\u00e1rregui", "Mustafa Melih Toslak", "Stefano Carpin"], "title": "One For All: LLM-based Heterogeneous Mission Planning in Precision Agriculture", "categories": ["cs.RO", "cs.AI"], "comment": "Accepted to International Federation of Automatic Control (IFAC)\n  Sensing, Control and Automation Technologies for Agriculture - 8th\n  AGRICONTROL 2025", "summary": "Artificial intelligence is transforming precision agriculture, offering\nfarmers new tools to streamline their daily operations. While these\ntechnological advances promise increased efficiency, they often introduce\nadditional complexity and steep learning curves that are particularly\nchallenging for non-technical users who must balance tech adoption with\nexisting workloads. In this paper, we present a natural language (NL) robotic\nmission planner that enables non-specialists to control heterogeneous robots\nthrough a common interface. By leveraging large language models (LLMs) and\npredefined primitives, our architecture seamlessly translates human language\ninto intermediate descriptions that can be executed by different robotic\nplatforms. With this system, users can formulate complex agricultural missions\nwithout writing any code. In the work presented in this paper, we extend our\nprevious system tailored for wheeled robot mission planning through a new class\nof experiments involving robotic manipulation and computer vision tasks. Our\nresults demonstrate that the architecture is both general enough to support a\ndiverse set of robots and powerful enough to execute complex mission requests.\nThis work represents a significant step toward making robotic automation in\nprecision agriculture more accessible to non-technical users.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528LLM\u8fdb\u884c\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\uff0c\u6d89\u53ca\u63a7\u5236\u673a\u5668\u4eba\u6267\u884c\u4efb\u52a1\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u5e7f\u4e49\u7684\u201c\u8f68\u8ff9\u201d\u89c4\u5212\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8eLLM\u7684\u5e94\u7528\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e2d\u7b49\u3002\u8bba\u6587\u660e\u786e\u63d0\u53caLLM\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u519c\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u3002", "keywords": ["Large Language Models", "LLMs", "robotic mission planning", "robot"]}}
{"id": "2506.10085", "pdf": "https://arxiv.org/pdf/2506.10085", "abs": "https://arxiv.org/abs/2506.10085", "authors": ["Christos Ziakas", "Alessandra Russo"], "title": "Test-Time Adaptation for Generalizable Task Progress Estimation", "categories": ["cs.CV", "cs.AI", "I.2.6; I.2.9; I.2.10"], "comment": "pages, 2 figures, accepted to the 2nd Workshop on Test-Time\n  Adaptation: Putting Updates to the Test (PUT) at 42nd International\n  Conference on Machine Learning (ICML), Vancouver, Canada, 2025", "summary": "We propose a test-time adaptation method that enables a progress estimation\nmodel to adapt online to the visual and temporal context of test trajectories\nby optimizing a learned self-supervised objective. To this end, we introduce a\ngradient-based meta-learning strategy to train the model on expert visual\ntrajectories and their natural language task descriptions, such that test-time\nadaptation improves progress estimation relying on semantic content over\ntemporal order. Our test-time adaptation method generalizes from a single\ntraining environment to diverse out-of-distribution tasks, environments, and\nembodiments, outperforming the state-of-the-art in-context learning approach\nusing autoregressive vision-language models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on task progress estimation based on visual trajectories and natural language task descriptions. It uses a model trained on expert visual trajectories and language descriptions, and mentions outperforming state-of-the-art in-context learning approaches using autoregressive vision-language models. This indicates a connection to both trajectory analysis and large language models, though the primary focus seems to be on adapting to different tasks using trajectory information and language.", "keywords": ["trajectory", "vision-language models", "natural language task descriptions", "test-time adaptation"]}}
{"id": "2506.10100", "pdf": "https://arxiv.org/pdf/2506.10100", "abs": "https://arxiv.org/abs/2506.10100", "authors": ["Yantai Yang", "Yuhao Wang", "Zichen Wen", "Luo Zhongwei", "Chang Zou", "Zhipeng Zhang", "Chuan Wen", "Linfeng Zhang"], "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Vision-Language-Action (VLA) models, which are relevant to trajectory prediction because 'action' can refer to movement and path planning. It also discusses language models and their efficiency. While not directly about trajectory prediction or LLMs, the intersection of vision, language, and action, especially in the context of embodied intelligence, suggests a moderate relevance, particularly if the 'action' component involves predicting trajectories.", "keywords": ["Vision-Language-Action Models", "language module", "diffusion-based action head", "embodied intelligence"]}}
{"id": "2506.10239", "pdf": "https://arxiv.org/pdf/2506.10239", "abs": "https://arxiv.org/abs/2506.10239", "authors": ["Maximilian M\u00fchlbauer", "Freek Stulp", "Sylvain Calinon", "Alin Albu-Sch\u00e4ffer", "Jo\u00e3o Silv\u00e9rio"], "title": "A Unified Framework for Probabilistic Dynamic-, Trajectory- and Vision-based Virtual Fixtures", "categories": ["cs.RO"], "comment": null, "summary": "Probabilistic Virtual Fixtures (VFs) enable the adaptive selection of the\nmost suitable haptic feedback for each phase of a task, based on learned or\nperceived uncertainty. While keeping the human in the loop remains essential,\nfor instance, to ensure high precision, partial automation of certain task\nphases is critical for productivity. We present a unified framework for\nprobabilistic VFs that seamlessly switches between manual fixtures,\nsemi-automated fixtures (with the human handling precise tasks), and full\nautonomy. We introduce a novel probabilistic Dynamical System-based VF for\ncoarse guidance, enabling the robot to autonomously complete certain task\nphases while keeping the human operator in the loop. For tasks requiring\nprecise guidance, we extend probabilistic position-based trajectory fixtures\nwith automation allowing for seamless human interaction as well as\ngeometry-awareness and optimal impedance gains. For manual tasks requiring very\nprecise guidance, we also extend visual servoing fixtures with the same\ngeometry-awareness and impedance behaviour. We validate our approach\nexperimentally on different robots, showcasing multiple operation modes and the\nease of programming fixtures.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses trajectory-based virtual fixtures using dynamical systems for guidance, which falls under trajectory prediction. However, it does not involve Large Language Models. The mention of 'probabilistic' and 'dynamical system' contributes to the relevance, as these concepts are often used in trajectory prediction.", "keywords": ["trajectory", "dynamical system", "guidance"]}}
{"id": "2506.10317", "pdf": "https://arxiv.org/pdf/2506.10317", "abs": "https://arxiv.org/abs/2506.10317", "authors": ["Akshar Tumu", "Henrik I. Christensen", "Marcell Vazquez-Chanlatte", "Chikao Tsuchiya", "Dhaval Bhanderi"], "title": "Using Language and Road Manuals to Inform Map Reconstruction for Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "4 pages, 3 figures, Accepted at RSS 2025 Workshop -\n  RobotEvaluation@RSS2025", "summary": "Lane-topology prediction is a critical component of safe and reliable\nautonomous navigation. An accurate understanding of the road environment aids\nthis task. We observe that this information often follows conventions encoded\nin natural language, through design codes that reflect the road structure and\nroad names that capture the road functionality. We augment this information in\na lightweight manner to SMERF, a map-prior-based online lane-topology\nprediction model, by combining structured road metadata from OSM maps and\nlane-width priors from Road design manuals with the road centerline encodings.\nWe evaluate our method on two geo-diverse complex intersection scenarios. Our\nmethod shows improvement in both lane and traffic element detection and their\nassociation. We report results using four topology-aware metrics to\ncomprehensively assess the model performance. These results demonstrate the\nability of our approach to generalize and scale to diverse topologies and\nconditions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5730\u56fe\u91cd\u5efa\uff0c\u5176\u4e2d\u6d89\u53ca\u8f66\u9053\u62d3\u6251\u9884\u6d4b\uff08lane-topology prediction\uff09\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u6458\u8981\u63d0\u5230\u4e86\u5229\u7528\u81ea\u7136\u8bed\u8a00\uff08natural language\uff09\u548c\u9053\u8def\u8bbe\u8ba1\u624b\u518c\u7684\u4fe1\u606f\uff0c\u4f46\u5e76\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u5e76\u975e\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["lane-topology prediction", "autonomous driving", "map reconstruction"]}}
{"id": "2506.10387", "pdf": "https://arxiv.org/pdf/2506.10387", "abs": "https://arxiv.org/abs/2506.10387", "authors": ["Yuquan Xie", "Zaijing Li", "Rui Shao", "Gongwei Chen", "Kaiwen Zhou", "Yinchuan Li", "Dongmei Jiang", "Liqiang Nie"], "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills", "categories": ["cs.AI"], "comment": "20 pages, 5 figures, 5 tables", "summary": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI\nagents have yielded promising outcomes. However, these agents still struggle\nwith long-horizon tasks in online environments, primarily due to insufficient\nknowledge and the inherent gap between offline and online domains. In this\npaper, inspired by how humans generalize knowledge in open-ended environments,\nwe propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of\ninsufficient knowledge. It progressively abstracts trajectories into execution\nskills, core skills, and ultimately meta-skills, providing a hierarchical\nknowledge structure for long-horizon task planning. To bridge the domain gap,\nwe propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,\nwhich efficiently leverages skills acquired in offline environments to reduce\nthe action search space during online tree exploration. Building on HMS, we\npropose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To\nvalidate the performance of Mirage-1 in real-world long-horizon scenarios, we\nconstructed a new benchmark, AndroidLH. Experimental results show that Mirage-1\noutperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld,\nMobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:\nhttps://cybertronagent.github.io/Mirage-1.github.io/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4f5c\u4e3aGUI\u4ee3\u7406\uff0c\u5e76\u6d89\u53ca\u4e86\u8f68\u8ff9\uff08trajectories\uff09\u7684\u62bd\u8c61\u548c\u5229\u7528\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8GUI\u4ee3\u7406\uff0c\u4f46\u8f68\u8ff9\u7684\u6982\u5ff5\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4e14\u4f7f\u7528\u4e86\u5927\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Multi-modal Large Language Model", "MLLM", "trajectories", "skills"]}}
{"id": "2506.10242", "pdf": "https://arxiv.org/pdf/2506.10242", "abs": "https://arxiv.org/abs/2506.10242", "authors": ["Rajeev Yasarla", "Shizhong Han", "Hong Cai", "Fatih Porikli"], "title": "DySS: Dynamic Queries and State-Space Learning for Efficient 3D Object Detection from Multi-Camera Videos", "categories": ["cs.CV"], "comment": "CVPR 2025 Workshop on Autonomous Driving", "summary": "Camera-based 3D object detection in Bird's Eye View (BEV) is one of the most\nimportant perception tasks in autonomous driving. Earlier methods rely on dense\nBEV features, which are costly to construct. More recent works explore sparse\nquery-based detection. However, they still require a large number of queries\nand can become expensive to run when more video frames are used. In this paper,\nwe propose DySS, a novel method that employs state-space learning and dynamic\nqueries. More specifically, DySS leverages a state-space model (SSM) to\nsequentially process the sampled features over time steps. In order to\nencourage the model to better capture the underlying motion and correspondence\ninformation, we introduce auxiliary tasks of future prediction and masked\nreconstruction to better train the SSM. The state of the SSM then provides an\ninformative yet efficient summarization of the scene. Based on the state-space\nlearned features, we dynamically update the queries via merge, remove, and\nsplit operations, which help maintain a useful, lean set of detection queries\nthroughout the network. Our proposed DySS achieves both superior detection\nperformance and efficient inference. Specifically, on the nuScenes test split,\nDySS achieves 65.31 NDS and 57.4 mAP, outperforming the latest state of the\nart. On the val split, DySS achieves 56.2 NDS and 46.2 mAP, as well as a\nreal-time inference speed of 33 FPS.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 3D object detection from multi-camera videos, which is highly relevant to autonomous driving. While it doesn't directly use large language models, the mention of future prediction and underlying motion makes it somewhat related to trajectory prediction, particularly for vehicles. The state-space model (SSM) also hints at temporal modeling, a common technique in trajectory prediction.", "keywords": ["autonomous driving", "3D object detection", "future prediction", "state-space model", "motion"]}}
{"id": "2506.10966", "pdf": "https://arxiv.org/pdf/2506.10966", "abs": "https://arxiv.org/abs/2506.10966", "authors": ["Ning Gao", "Yilun Chen", "Shuai Yang", "Xinyi Chen", "Yang Tian", "Hao Li", "Haifeng Huang", "Hanqing Wang", "Tai Wang", "Jiangmiao Pang"], "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Robotic manipulation in real-world settings remains challenging, especially\nregarding robust generalization. Existing simulation platforms lack sufficient\nsupport for exploring how policies adapt to varied instructions and scenarios.\nThus, they lag behind the growing interest in instruction-following foundation\nmodels like LLMs, whose adaptability is crucial yet remains underexplored in\nfair comparisons. To bridge this gap, we introduce GenManip, a realistic\ntabletop simulation platform tailored for policy generalization studies. It\nfeatures an automatic pipeline via LLM-driven task-oriented scene graph to\nsynthesize large-scale, diverse tasks using 10K annotated 3D object assets. To\nsystematically assess generalization, we present GenManip-Bench, a benchmark of\n200 scenarios refined via human-in-the-loop corrections. We evaluate two policy\ntypes: (1) modular manipulation systems integrating foundation models for\nperception, reasoning, and planning, and (2) end-to-end policies trained\nthrough scalable data collection. Results show that while data scaling benefits\nend-to-end methods, modular systems enhanced with foundation models generalize\nmore effectively across diverse scenarios. We anticipate this platform to\nfacilitate critical insights for advancing policy generalization in realistic\nconditions. Project Page: https://genmanip.axi404.top/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on robotic manipulation and uses LLMs for scene generation and task planning within a simulation environment. While it doesn't directly address trajectory prediction, it explores how policies, including those potentially related to trajectory planning, can be generalized using LLMs. The connection to trajectory prediction is indirect but plausible, especially considering the planning aspect of manipulation.", "keywords": ["LLM", "Large Language Models", "foundation models", "manipulation", "planning", "generalization"]}}
{"id": "2506.10897", "pdf": "https://arxiv.org/pdf/2506.10897", "abs": "https://arxiv.org/abs/2506.10897", "authors": ["Daniel Borrajo", "Giuseppe Canonaco", "Tom\u00e1s de la Rosa", "Alfredo Garrach\u00f3n", "Sriram Gopalakrishnan", "Simerjot Kaur", "Marianela Morales", "Sunandita Patra", "Alberto Pozanco", "Keshav Ramani", "Charese Smiley", "Pietro Totis", "Manuela Veloso"], "title": "GenPlanX. Generation of Plans and Execution", "categories": ["cs.AI"], "comment": null, "summary": "Classical AI Planning techniques generate sequences of actions for complex\ntasks. However, they lack the ability to understand planning tasks when\nprovided using natural language. The advent of Large Language Models (LLMs) has\nintroduced novel capabilities in human-computer interaction. In the context of\nplanning tasks, LLMs have shown to be particularly good in interpreting human\nintents among other uses. This paper introduces GenPlanX that integrates LLMs\nfor natural language-based description of planning tasks, with a classical AI\nplanning engine, alongside an execution and monitoring framework. We\ndemonstrate the efficacy of GenPlanX in assisting users with office-related\ntasks, highlighting its potential to streamline workflows and enhance\nproductivity through seamless human-AI collaboration.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Large Language Models (LLMs) for natural language understanding in the context of AI planning. While it doesn't directly address trajectory prediction, the core concept of AI planning can be related to path planning for moving objects. The strong presence of LLMs makes it moderately relevant.", "keywords": ["Large Language Models", "LLMs", "AI Planning"]}}
{"id": "2506.10016", "pdf": "https://arxiv.org/pdf/2506.10016", "abs": "https://arxiv.org/abs/2506.10016", "authors": ["Longzhen Han", "Awes Mubarak", "Almas Baimagambetov", "Nikolaos Polatidis", "Thar Baker"], "title": "Multimodal Large Language Models: A Survey", "categories": ["cs.MM", "cs.AI", "cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text\ngeneration, now spanning diverse output modalities including images, music,\nvideo, human motion, and 3D objects, by integrating language with other sensory\nmodalities under unified architectures. This survey categorises six primary\ngenerative modalities and examines how foundational techniques, namely\nSelf-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement\nLearning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting,\nenable cross-modal capabilities. We analyze key models, architectural trends,\nand emergent cross-modal synergies, while highlighting transferable techniques\nand unresolved challenges. Architectural innovations like transformers and\ndiffusion models underpin this convergence, enabling cross-modal transfer and\nmodular specialization. We highlight emerging patterns of synergy, and identify\nopen challenges in evaluation, modularity, and structured reasoning. This\nsurvey offers a unified perspective on MLLM development and identifies critical\npaths toward more general-purpose, adaptive, and interpretable multimodal\nsystems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper surveys Multimodal Large Language Models (MLLMs) and their capabilities in generating various modalities, including human motion. While it doesn't directly focus on trajectory prediction, the mention of human motion suggests a potential connection. The paper heavily focuses on large language models and their multimodal extensions.", "keywords": ["Multimodal Large Language Models", "MLLMs", "Large Language Models", "human motion", "foundation models"]}}
{"id": "2506.10443", "pdf": "https://arxiv.org/pdf/2506.10443", "abs": "https://arxiv.org/abs/2506.10443", "authors": ["Zhaode Wang", "Jingbang Yang", "Xinyu Qian", "Shiwen Xing", "Xiaotang Jiang", "Chengfei Lv", "Shengyu Zhang"], "title": "MNN-LLM: A Generic Inference Engine for Fast Large Language Model Deployment on Mobile Devices", "categories": ["cs.LG"], "comment": "7 pages, 5 figures. Published in the Proceedings of the 6th ACM\n  International Conference on Multimedia in Asia Workshops (MMAsia '24\n  Workshops). The final authenticated version is available at\n  https://dl.acm.org/doi/10.1145/3700410.3702126", "summary": "Large language models (LLMs) have demonstrated exceptional performance across\na variety of tasks. However, their substantial scale leads to significant\ncomputational resource consumption during inference, resulting in high costs.\nConsequently, edge device inference presents a promising solution. The primary\nchallenges of edge inference include memory usage and inference speed. This\npaper introduces MNN-LLM, a framework specifically designed to accelerate the\ndeployment of large language models on mobile devices. MNN-LLM addresses the\nruntime characteristics of LLMs through model quantization and DRAM-Flash\nhybrid storage, effectively reducing memory usage. It rearranges weights and\ninputs based on mobile CPU instruction sets and GPU characteristics while\nemploying strategies such as multicore load balancing, mixed-precision\nfloating-point operations, and geometric computations to enhance performance.\nNotably, MNN-LLM achieves up to a 8.6x speed increase compared to current\nmainstream LLM-specific frameworks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u548c\u52a0\u901f\uff0c\u91cd\u70b9\u662f\u6a21\u578b\u91cf\u5316\u548c\u63a8\u7406\u4f18\u5316\u3002\u867d\u7136\u4e0e\u5927\u6a21\u578b\u76f8\u5173\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u8054\u7cfb\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["Large language models", "LLMs", "inference", "model quantization"]}}
{"id": "2506.10630", "pdf": "https://arxiv.org/pdf/2506.10630", "abs": "https://arxiv.org/abs/2506.10630", "authors": ["Yucong Luo", "Yitong Zhou", "Mingyue Cheng", "Jiahao Wang", "Daoyu Wang", "Tingyue Pan", "Jintao Zhang"], "title": "Time Series Forecasting as Reasoning: A Slow-Thinking Approach with Reinforced LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "To advance time series forecasting (TSF), various methods have been proposed\nto improve prediction accuracy, evolving from statistical techniques to\ndata-driven deep learning architectures. Despite their effectiveness, most\nexisting methods still adhere to a fast thinking paradigm-relying on extracting\nhistorical patterns and mapping them to future values as their core modeling\nphilosophy, lacking an explicit thinking process that incorporates intermediate\ntime series reasoning. Meanwhile, emerging slow-thinking LLMs (e.g., OpenAI-o1)\nhave shown remarkable multi-step reasoning capabilities, offering an\nalternative way to overcome these issues. However, prompt engineering alone\npresents several limitations - including high computational cost, privacy\nrisks, and limited capacity for in-depth domain-specific time series reasoning.\nTo address these limitations, a more promising approach is to train LLMs to\ndevelop slow thinking capabilities and acquire strong time series reasoning\nskills. For this purpose, we propose Time-R1, a two-stage reinforcement\nfine-tuning framework designed to enhance multi-step reasoning ability of LLMs\nfor time series forecasting. Specifically, the first stage conducts supervised\nfine-tuning for warmup adaptation, while the second stage employs reinforcement\nlearning to improve the model's generalization ability. Particularly, we design\na fine-grained multi-objective reward specifically for time series forecasting,\nand then introduce GRIP (group-based relative importance for policy\noptimization), which leverages non-uniform sampling to further encourage and\noptimize the model's exploration of effective reasoning paths. Experiments\ndemonstrate that Time-R1 significantly improves forecast performance across\ndiverse datasets.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on time series forecasting using reinforced LLMs, specifically for multi-step reasoning. While it doesn't directly address trajectory prediction, the underlying techniques and the use of LLMs are relevant. The connection to trajectory prediction is weaker than if it explicitly mentioned trajectory data, but the general methodology is transferable.", "keywords": ["Large Language Models", "LLMs", "time series forecasting", "reinforcement learning", "reasoning", "foundation models"]}}
{"id": "2506.10911", "pdf": "https://arxiv.org/pdf/2506.10911", "abs": "https://arxiv.org/abs/2506.10911", "authors": ["Jari Kolehmainen", "Nikolay Blagoev", "John Donaghy", "O\u011fuzhan Ersoy", "Christopher Nies"], "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models", "categories": ["cs.LG"], "comment": null, "summary": "Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication training methods still employ\na synchronization step for model parameters, which, when performed over all\nmodel replicas, can become costly on a low-bandwidth network.\n  In this work, we propose a novel optimization method, NoLoCo, that does not\nexplicitly synchronize all model parameters during training and, as a result,\ndoes not require any collective communication. NoLoCo implicitly synchronizes\nmodel weights via a novel variant of the Nesterov momentum optimizer by\npartially averaging model weights with a randomly selected other one. We\nprovide both a theoretical convergence analysis for our proposed optimizer as\nwell as empirical results from language model training.\n  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,\nbetween 125M to 6.8B parameters. Our method requires significantly less\ncommunication overhead than fully sharded data parallel training or even widely\nused low communication training method, DiLoCo. The synchronization step itself\nis estimated to be one magnitude faster than the all-reduce used in DiLoCo for\nfew hundred accelerators training over the internet. We also do not have any\nglobal blocking communication that reduces accelerator idling time. Compared to\nDiLoCo, we also observe up to $4\\%$ faster convergence rate with wide range of\nmodel sizes and accelerator counts.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on training large language models (LLMs) with a novel optimization method that reduces communication overhead. While it doesn't directly address trajectory prediction, it is relevant due to its focus on large models, a key component in some trajectory prediction approaches. The connection is indirect, as the paper aims to improve the efficiency of training LLMs, which can then be applied to various downstream tasks, including trajectory prediction.", "keywords": ["Large Language Models", "LLMs", "Large Models", "Optimization", "Training"]}}
{"id": "2506.10778", "pdf": "https://arxiv.org/pdf/2506.10778", "abs": "https://arxiv.org/abs/2506.10778", "authors": ["Jian Li", "Wan Han", "Ning Lin", "Yu-Liang Zhan", "Ruizhi Chengze", "Haining Wang", "Yi Zhang", "Hongsheng Liu", "Zidong Wang", "Fan Yu", "Hao Sun"], "title": "SlotPi: Physics-informed Object-centric Reasoning Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding and reasoning about dynamics governed by physical laws through\nvisual observation, akin to human capabilities in the real world, poses\nsignificant challenges. Currently, object-centric dynamic simulation methods,\nwhich emulate human behavior, have achieved notable progress but overlook two\ncritical aspects: 1) the integration of physical knowledge into models. Humans\ngain physical insights by observing the world and apply this knowledge to\naccurately reason about various dynamic scenarios; 2) the validation of model\nadaptability across diverse scenarios. Real-world dynamics, especially those\ninvolving fluids and objects, demand models that not only capture object\ninteractions but also simulate fluid flow characteristics. To address these\ngaps, we introduce SlotPi, a slot-based physics-informed object-centric\nreasoning model. SlotPi integrates a physical module based on Hamiltonian\nprinciples with a spatio-temporal prediction module for dynamic forecasting.\nOur experiments highlight the model's strengths in tasks such as prediction and\nVisual Question Answering (VQA) on benchmark and fluid datasets. Furthermore,\nwe have created a real-world dataset encompassing object interactions, fluid\ndynamics, and fluid-object interactions, on which we validated our model's\ncapabilities. The model's robust performance across all datasets underscores\nits strong adaptability, laying a foundation for developing more advanced world\nmodels.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on object-centric reasoning models with physics-informed dynamics for prediction. While it doesn't explicitly mention trajectory prediction, the dynamic forecasting aspect is related. It does not mention large language models, but the development of more advanced world models could potentially benefit from or incorporate LLMs in the future. The connection to trajectory prediction is present but not central.", "keywords": ["physics-informed", "object-centric reasoning", "dynamic forecasting", "prediction"]}}
{"id": "2506.10401", "pdf": "https://arxiv.org/pdf/2506.10401", "abs": "https://arxiv.org/abs/2506.10401", "authors": ["Jiaqi Lv", "Xufeng He", "Yanchen Liu", "Xu Dai", "Yang Hu", "Shouyi Yin"], "title": "HPCTransCompile: An AI Compiler Generated Dataset for High-Performance CUDA Transpilation and LLM Preliminary Exploration", "categories": ["cs.DC", "cs.AI"], "comment": null, "summary": "The rapid growth of deep learning has driven exponential increases in model\nparameters and computational demands. NVIDIA GPUs and their CUDA-based software\necosystem provide robust support for parallel computing, significantly\nalleviating computational bottlenecks. Meanwhile, due to the cultivation of\nuser programming habits and the high performance of GPUs, the CUDA ecosystem\nhas established a dominant position in the field of parallel software. This\ndominance requires other hardware platforms to support CUDA-based software with\nperformance portability. However, translating CUDA code to other platforms\nposes significant challenges due to differences in parallel programming\nparadigms and hardware architectures. Existing approaches rely on language\nextensions, domain-specific languages (DSLs), or compilers but face limitations\nin workload coverage and generalizability. Moreover, these methods often incur\nsubstantial development costs. Recently, LLMs have demonstrated extraordinary\npotential in various vertical domains, especially in code-related tasks.\nHowever, the performance of existing LLMs in CUDA transpilation, particularly\nfor high-performance code, remains suboptimal. The main reason for this\nlimitation lies in the lack of high-quality training datasets. To address these\nchallenges, we propose a novel framework for generating high-performance CUDA\nand corresponding platform code pairs, leveraging AI compiler and automatic\noptimization technology. We further enhance the framework with a graph-based\ndata augmentation method and introduce HPCTransEval, a benchmark for evaluating\nLLM performance on CUDA transpilation. We conduct experiments using CUDA-to-CPU\ntranspilation as a case study on leading LLMs. The result demonstrates that our\nframework significantly improves CUDA transpilation, highlighting the potential\nof LLMs to address compatibility challenges within the CUDA ecosystem.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528LLM\u8fdb\u884cCUDA\u4ee3\u7801\u8f6c\u6362\uff0c\u5e76\u751f\u6210\u9ad8\u6027\u80fd\u4ee3\u7801\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u63a2\u7d22\u4e86LLM\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u5e94\u7528\uff0c\u5e76\u4e14\u63d0\u5230\u4e86LLM\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "AI Compiler", "CUDA", "transpilation"]}}
{"id": "2506.10111", "pdf": "https://arxiv.org/pdf/2506.10111", "abs": "https://arxiv.org/abs/2506.10111", "authors": ["Abiodun Ganiyu", "Pranshav Gajjar", "Vijay K Shah"], "title": "AI5GTest: AI-Driven Specification-Aware Automated Testing and Validation of 5G O-RAN Components", "categories": ["cs.NI", "cs.LG"], "comment": null, "summary": "The advent of Open Radio Access Networks (O-RAN) has transformed the\ntelecommunications industry by promoting interoperability, vendor diversity,\nand rapid innovation. However, its disaggregated architecture introduces\ncomplex testing challenges, particularly in validating multi-vendor components\nagainst O-RAN ALLIANCE and 3GPP specifications. Existing frameworks, such as\nthose provided by Open Testing and Integration Centres (OTICs), rely heavily on\nmanual processes, are fragmented and prone to human error, leading to\ninconsistency and scalability issues. To address these limitations, we present\nAI5GTest -- an AI-powered, specification-aware testing framework designed to\nautomate the validation of O-RAN components. AI5GTest leverages a cooperative\nLarge Language Models (LLM) framework consisting of Gen-LLM, Val-LLM, and\nDebug-LLM. Gen-LLM automatically generates expected procedural flows for test\ncases based on 3GPP and O-RAN specifications, while Val-LLM cross-references\nsignaling messages against these flows to validate compliance and detect\ndeviations. If anomalies arise, Debug-LLM performs root cause analysis,\nproviding insight to the failure cause. To enhance transparency and\ntrustworthiness, AI5GTest incorporates a human-in-the-loop mechanism, where the\nGen-LLM presents top-k relevant official specifications to the tester for\napproval before proceeding with validation. Evaluated using a range of test\ncases obtained from O-RAN TIFG and WG5-IOT test specifications, AI5GTest\ndemonstrates a significant reduction in overall test execution time compared to\ntraditional manual methods, while maintaining high validation accuracy.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on automated testing and validation of 5G O-RAN components using Large Language Models (LLMs). While it doesn't directly address trajectory prediction, it heavily utilizes LLMs, which is one of the specified relevant topics. The connection to trajectory prediction is weak or non-existent, but the strong presence of LLMs justifies a moderate relevance score.", "keywords": ["Large Language Models", "LLM"]}}
