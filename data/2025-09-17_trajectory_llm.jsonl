{"id": "2509.10570", "pdf": "https://arxiv.org/pdf/2509.10570", "abs": "https://arxiv.org/abs/2509.10570", "authors": ["Wei Dai", "Shengen Wu", "Wei Wu", "Zhenhao Wang", "Sisuo Lyu", "Haicheng Liao", "Limin Yu", "Weiping Ding", "Runwei Guan", "Yutao Yue"], "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey", "categories": ["cs.RO", "cs.AI"], "comment": "22 pages, 6 figures", "summary": "Trajectory prediction serves as a critical functionality in autonomous\ndriving, enabling the anticipation of future motion paths for traffic\nparticipants such as vehicles and pedestrians, which is essential for driving\nsafety. Although conventional deep learning methods have improved accuracy,\nthey remain hindered by inherent limitations, including lack of\ninterpretability, heavy reliance on large-scale annotated data, and weak\ngeneralization in long-tail scenarios. The rise of Large Foundation Models\n(LFMs) is transforming the research paradigm of trajectory prediction. This\nsurvey offers a systematic review of recent advances in LFMs, particularly\nLarge Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for\ntrajectory prediction. By integrating linguistic and scene semantics, LFMs\nfacilitate interpretable contextual reasoning, significantly enhancing\nprediction safety and generalization in complex environments. The article\nhighlights three core methodologies: trajectory-language mapping, multimodal\nfusion, and constraint-based reasoning. It covers prediction tasks for both\nvehicles and pedestrians, evaluation metrics, and dataset analyses. Key\nchallenges such as computational latency, data scarcity, and real-world\nrobustness are discussed, along with future research directions including\nlow-latency inference, causality-aware modeling, and motion foundation models.", "relevance_analysis": {"relevance_score": 1.0, "explanation": "The paper explicitly focuses on the application of Large Foundation Models (LFMs) and Large Language Models (LLMs) to trajectory prediction in autonomous driving. The title and abstract clearly indicate a high relevance to both trajectory prediction and large language models.", "keywords": ["trajectory prediction", "autonomous driving", "Large Foundation Models", "LFMs", "Large Language Models", "LLMs", "multimodal large language models", "MLLMs", "motion foundation models"]}}
{"id": "2509.11165", "pdf": "https://arxiv.org/pdf/2509.11165", "abs": "https://arxiv.org/abs/2509.11165", "authors": ["Waikit Xiu", "Qiang Lu", "Xiying Li", "Chen Hu", "Shengbo Sun"], "title": "Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic", "categories": ["cs.CV"], "comment": null, "summary": "As intelligent transportation systems advance, traffic video understanding\nplays an increasingly pivotal role in comprehensive scene perception and causal\nanalysis. Yet, existing approaches face notable challenges in accurately\nmodeling spatiotemporal causality and integrating domain-specific knowledge,\nlimiting their effectiveness in complex scenarios. To address these\nlimitations, we propose Traffic-MLLM, a multimodal large language model\ntailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,\nour model leverages high-quality traffic-specific multimodal datasets and uses\nLow-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancing\nits capacity to model continuous spatiotemporal features in video sequences.\nFurthermore, we introduce an innovative knowledge prompting module fusing\nChain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),\nenabling precise injection of detailed traffic regulations and domain knowledge\ninto the inference process. This design markedly boosts the model's logical\nreasoning and knowledge adaptation capabilities. Experimental results on\nTrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-art\nperformance, validating its superior ability to process multimodal traffic\ndata. It also exhibits remarkable zero-shot reasoning and cross-scenario\ngeneralization capabilities.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper is highly relevant because it explicitly combines traffic analysis (which often involves trajectory prediction) with a Multimodal Large Language Model (MLLM). The abstract mentions spatiotemporal modeling, causal inference in traffic, and the use of a large language model (Qwen2.5-VL) fine-tuned for traffic analysis. The use of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) further strengthens the connection to large language models.", "keywords": ["traffic", "spatio-temporal", "MLLM", "multimodal large language model", "causal inference", "Qwen2.5-VL", "RAG", "Retrieval-Augmented Generation", "CoT", "Chain-of-Thought"]}}
{"id": "2509.11197", "pdf": "https://arxiv.org/pdf/2509.11197", "abs": "https://arxiv.org/abs/2509.11197", "authors": ["Yunheng Wang", "Yuetong Fang", "Taowen Wang", "Yixiao Feng", "Yawen Tan", "Shuning Zhang", "Peiran Liu", "Yiding Ji", "Renjing Xu"], "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which\nlinks language instructions to perception and control in the real world, is a\ncore capability of embodied robots. Recently, large-scale pretrained foundation\nmodels have been leveraged as shared priors for perception, reasoning, and\naction, enabling zero-shot VLN without task-specific training. However,\nexisting zero-shot VLN methods depend on costly perception and passive scene\nunderstanding, collapsing control to point-level choices. As a result, they are\nexpensive to deploy, misaligned in action semantics, and short-sighted in\nplanning. To address these issues, we present DreamNav that focuses on the\nfollowing three aspects: (1) for reducing sensory cost, our EgoView Corrector\naligns viewpoints and stabilizes egocentric perception; (2) instead of\npoint-level actions, our Trajectory Predictor favors global trajectory-level\nplanning to better align with instruction semantics; and (3) to enable\nanticipatory and long-horizon planning, we propose an Imagination Predictor to\nendow the agent with proactive thinking capability. On VLN-CE and real-world\ntests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the\nstrongest egocentric baseline with extra information by up to 7.49\\% and\n18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first\nzero-shot VLN method to unify trajectory-level planning and active imagination\nwhile using only egocentric inputs.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u5c06\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08foundation models\uff09\u5e94\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\uff0c\u5e76\u4f7f\u7528\u8f68\u8ff9\u9884\u6d4b\u8fdb\u884c\u5168\u5c40\u89c4\u5212\u3002\u867d\u7136\u4e3b\u8981\u7126\u70b9\u662f\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff0c\u4f46\u5b83\u660e\u786e\u4f7f\u7528\u4e86\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u4e86\u5927\u578b\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "large language models", "foundation models", "vision-and-language navigation", "zero-shot VLN"]}}
{"id": "2509.11719", "pdf": "https://arxiv.org/pdf/2509.11719", "abs": "https://arxiv.org/abs/2509.11719", "authors": ["Bingqing Wei", "Lianmin Chen", "Zhongyu Xia", "Yongtao Wang"], "title": "HeLoFusion: An Efficient and Scalable Encoder for Modeling Heterogeneous and Multi-Scale Interactions in Trajectory Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Multi-agent trajectory prediction in autonomous driving requires a\ncomprehensive understanding of complex social dynamics. Existing methods,\nhowever, often struggle to capture the full richness of these dynamics,\nparticularly the co-existence of multi-scale interactions and the diverse\nbehaviors of heterogeneous agents. To address these challenges, this paper\nintroduces HeLoFusion, an efficient and scalable encoder for modeling\nheterogeneous and multi-scale agent interactions. Instead of relying on global\ncontext, HeLoFusion constructs local, multi-scale graphs centered on each\nagent, allowing it to effectively model both direct pairwise dependencies and\ncomplex group-wise interactions (\\textit{e.g.}, platooning vehicles or\npedestrian crowds). Furthermore, HeLoFusion tackles the critical challenge of\nagent heterogeneity through an aggregation-decomposition message-passing scheme\nand type-specific feature networks, enabling it to learn nuanced,\ntype-dependent interaction patterns. This locality-focused approach enables a\nprincipled representation of multi-level social context, yielding powerful and\nexpressive agent embeddings. On the challenging Waymo Open Motion Dataset,\nHeLoFusion achieves state-of-the-art performance, setting new benchmarks for\nkey metrics including Soft mAP and minADE. Our work demonstrates that a\nlocality-grounded architecture, which explicitly models multi-scale and\nheterogeneous interactions, is a highly effective strategy for advancing motion\nforecasting.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u6307\u51fa\u5176\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\uff0c\u7279\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u7f16\u7801\u5668\uff0c\u7528\u4e8e\u5efa\u6a21\u5f02\u6784\u548c\u591a\u5c3a\u5ea6\u7684\u4ea4\u4e92\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "multi-agent trajectory prediction", "autonomous driving", "motion forecasting", "heterogeneous agents", "multi-scale interactions"]}}
{"id": "2509.10796", "pdf": "https://arxiv.org/pdf/2509.10796", "abs": "https://arxiv.org/abs/2509.10796", "authors": ["Hanjing Ye", "Weixi Situ", "Jianwei Peng", "Yu Zhan", "Bingyi Xia", "Kuanqi Cai", "Hong Zhang"], "title": "Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following", "categories": ["cs.RO"], "comment": "TBD. All code, data, and deployment scripts are publicly available at\n  https://follow-bench.github.io/", "summary": "Robot person following (RPF) -- mobile robots that follow and assist a\nspecific person -- has emerging applications in personal assistance, security\npatrols, eldercare, and logistics. To be effective, such robots must follow the\ntarget while ensuring safety and comfort for both the target and surrounding\npeople. In this work, we present the first end-to-end study of RPF, which (i)\nsurveys representative scenarios, motion-planning methods, and evaluation\nmetrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a\nunified benchmark simulating diverse scenarios, including various target\ntrajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)\nre-implements six popular RPF planners, ensuring that both safety and comfort\nare systematically considered. Moreover, we evaluate the two highest-performing\nplanners from our benchmark on a differential-drive robot to provide insights\ninto real-world deployment. Extensive simulation and real-world experiments\nprovide quantitative insights into the safety-comfort trade-offs of existing\nplanners, while revealing open challenges and future research directions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u8ddf\u968f\u4efb\u52a1\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u6d89\u53ca\u5230\u8f68\u8ff9\u9884\u6d4b\uff08\u76ee\u6807\u8f68\u8ff9\u6a21\u5f0f\uff09\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u6d89\u53ca\u793e\u4f1a\u611f\u77e5\uff0c\u6697\u793a\u4e86\u5bf9\u884c\u4eba\u6216\u5176\u4ed6\u79fb\u52a8\u7269\u4f53\u7684\u884c\u4e3a\u9884\u6d4b\uff0c\u4f46\u91cd\u70b9\u5728\u4e8e\u673a\u5668\u4eba\u81ea\u8eab\u7684\u8def\u5f84\u89c4\u5212\u548c\u8ddf\u968f\u7b56\u7565\u3002", "keywords": ["motion planning", "trajectory patterns", "socially-aware", "robot person following"]}}
{"id": "2509.10884", "pdf": "https://arxiv.org/pdf/2509.10884", "abs": "https://arxiv.org/abs/2509.10884", "authors": ["Qingxiang Liu", "Ting Huang", "Zeyu Zhang", "Hao Tang"], "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5177\u8eab\u5bfc\u822a\uff08embodied navigation\uff09\uff0c\u4f7f\u7528\u4e86\u5927\u578b\u6570\u636e\u96c6\u548c\u57fa\u7840\u6a21\u578b\uff08foundation model\uff09\u8fdb\u884c\u63a8\u7406\u548c\u5bfc\u822a\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5bfc\u822a\u4efb\u52a1\u4e0e\u8def\u5f84\u89c4\u5212\u76f8\u5173\uff0c\u4e14\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["embodied navigation", "foundation model", "large-scale dataset", "reasoning", "navigation"]}}
{"id": "2509.11149", "pdf": "https://arxiv.org/pdf/2509.11149", "abs": "https://arxiv.org/abs/2509.11149", "authors": ["Mintae Kim", "Jiaze Cai", "Koushil Sreenath"], "title": "RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "8 pages", "summary": "Designing robust controllers for precise, arbitrary trajectory tracking with\nquadrotors is challenging due to nonlinear dynamics and underactuation, and\nbecomes harder with flexible cable-suspended payloads that introduce extra\ndegrees of freedom and hybridness. Classical model-based methods offer\nstability guarantees but require extensive tuning and often do not adapt when\nthe configuration changes, such as when a payload is added or removed, or when\nthe payload mass or cable length varies. We present RoVerFly, a unified\nlearning-based control framework in which a reinforcement learning (RL) policy\nserves as a robust and versatile tracking controller for standard quadrotors\nand for cable-suspended payload systems across a range of configurations.\nTrained with task and domain randomization, the controller is resilient to\ndisturbances and varying dynamics. It achieves strong zero-shot generalization\nacross payload settings, including no payload as well as varying mass and cable\nlength, without controller switching or re-tuning, while retaining the\ninterpretability and structure of a feedback tracking controller. Code and\nsupplementary materials are available at\nhttps://github.com/mintaeshkim/roverfly", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on controlling quadrotors for trajectory tracking, particularly with cable-suspended payloads. While it doesn't directly involve Large Language Models, the core problem of trajectory tracking and the use of reinforcement learning for control suggests a moderate level of relevance to trajectory prediction. The focus is on control rather than prediction, hence the score is not higher.", "keywords": ["trajectory tracking", "quadrotor", "reinforcement learning", "control", "payload"]}}
{"id": "2509.10522", "pdf": "https://arxiv.org/pdf/2509.10522", "abs": "https://arxiv.org/abs/2509.10522", "authors": ["Kaizhen Tan"], "title": "Multimodal Deep Learning for ATCO Command Lifecycle Modeling and Workload Prediction", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.AS"], "comment": null, "summary": "Air traffic controllers (ATCOs) issue high-intensity voice commands in dense\nairspace, where accurate workload modeling is critical for safety and\nefficiency. This paper proposes a multimodal deep learning framework that\nintegrates structured data, trajectory sequences, and image features to\nestimate two key parameters in the ATCO command lifecycle: the time offset\nbetween a command and the resulting aircraft maneuver, and the command\nduration. A high-quality dataset was constructed, with maneuver points detected\nusing sliding window and histogram-based methods. A CNN-Transformer ensemble\nmodel was developed for accurate, generalizable, and interpretable predictions.\nBy linking trajectories to voice commands, this work offers the first model of\nits kind to support intelligent command generation and provides practical value\nfor workload assessment, staffing, and scheduling.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on modeling ATCO command lifecycle and workload prediction using multimodal deep learning. It integrates trajectory sequences and uses a CNN-Transformer ensemble model. While it involves trajectory data and uses deep learning architectures, it doesn't explicitly mention or utilize large language models. The connection to trajectory prediction is present, but the absence of LLMs lowers the overall relevance.", "keywords": ["trajectory sequences", "CNN-Transformer", "trajectory prediction", "workload prediction"]}}
{"id": "2509.11240", "pdf": "https://arxiv.org/pdf/2509.11240", "abs": "https://arxiv.org/abs/2509.11240", "authors": ["Yechen Zhang", "Bin Gao", "Gang Wang", "Jian Sun", "Zhuo Li"], "title": "CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "11 pages, 8 figures. Submitted to IEEE/ASME T-MECH. Code available at\n  https://github.com/ChenzycBIT/CORB-planner", "summary": "Reinforcement learning (RL) has shown promise in a large number of robotic\ncontrol tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)\nremains challenging, mainly because of reliance on accurate dynamic models and\nplatform-specific sensing, which hinders cross-platform transfer. This paper\npresents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a\nreal-time, RL-based trajectory planning framework for high-speed autonomous UAV\nflight across heterogeneous platforms. The key idea is to combine B-spline\ntrajectory generation with the RL policy producing successive control points\nwith a compact safe flight corridor (SFC) representation obtained via heuristic\nsearch. The SFC abstracts obstacle information in a low-dimensional form,\nmitigating overfitting to platform-specific details and reducing sensitivity to\nmodel inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard\nprogressive training pipeline in simulation. A value-based soft\ndecomposed-critic Q (SDCQ) algorithm is used to learn effective policies within\napproximately ten minutes of training. Benchmarks in simulation and real-world\ntests demonstrate real-time planning on lightweight onboard hardware and\nsupport maximum flight speeds up to 8.2m/s in dense, cluttered environments\nwithout external positioning. Compatibility with various UAV configurations\n(quadrotors, hexarotors) and modest onboard compute underlines the generality\nand robustness of CORB-Planner for practical deployment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory planning for UAVs using reinforcement learning. While it doesn't directly involve large language models, it falls under the broader category of trajectory prediction/planning for mobile objects. The use of RL for trajectory generation is a relevant aspect.", "keywords": ["trajectory planning", "RL", "reinforcement learning", "UAV", "autonomous flight"]}}
{"id": "2509.11364", "pdf": "https://arxiv.org/pdf/2509.11364", "abs": "https://arxiv.org/abs/2509.11364", "authors": ["Sheng Liu", "Zhe Li", "Weiheng Wang", "Han Sun", "Heng Zhang", "Hongpeng Chen", "Yusen Qin", "Arash Ajoudani", "Yizhao Wang"], "title": "ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation", "categories": ["cs.RO"], "comment": "6D Pose, Diffusion Policy", "summary": "Accurate 6-DoF object pose estimation and tracking are critical for reliable\nrobotic manipulation. However, zero-shot methods often fail under\nviewpoint-induced ambiguities and fixed-camera setups struggle when objects\nmove or become self-occluded. To address these challenges, we propose an active\npose estimation pipeline that combines a Vision-Language Model (VLM) with\n\"robotic imagination\" to dynamically detect and resolve ambiguities in real\ntime. In an offline stage, we render a dense set of views of the CAD model,\ncompute the FoundationPose entropy for each view, and construct a\ngeometric-aware prompt that includes low-entropy (unambiguous) and high-entropy\n(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live\nimage for an ambiguity score; (2) if ambiguity is detected, imagines a discrete\nset of candidate camera poses by rendering virtual views, scores each based on\na weighted combination of VLM ambiguity probability and FoundationPose entropy,\nand then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated\npose estimation. Furthermore, since moving objects may leave the camera's field\nof view, we introduce an active pose tracking module: a diffusion-policy\ntrained via imitation learning, which generates camera trajectories that\npreserve object visibility and minimize pose ambiguity. Experiments in\nsimulation and real-world show that our approach significantly outperforms\nclassical baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 6D object pose estimation and tracking for robotic manipulation. While the core focus is not trajectory prediction in the sense of predicting the future path of an agent (e.g., pedestrian or vehicle), it does involve camera trajectory generation to maintain object visibility. Furthermore, the paper explicitly uses a Vision-Language Model (VLM), which falls under the umbrella of Large Language Models. The active tracking module uses a diffusion policy to generate camera trajectories, which can be seen as a form of trajectory planning. Therefore, there's a moderate relevance to both trajectory prediction and large models.", "keywords": ["Vision-Language Model", "VLM", "diffusion policy", "robotic manipulation", "active pose tracking", "camera trajectories"]}}
{"id": "2509.11058", "pdf": "https://arxiv.org/pdf/2509.11058", "abs": "https://arxiv.org/abs/2509.11058", "authors": ["Canhui Tang", "Sanping Zhou", "Haoyue Shi", "Le Wang"], "title": "Action Hints: Semantic Typicality and Context Uniqueness for Generalizable Skeleton-based Video Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Zero-Shot Video Anomaly Detection (ZS-VAD) requires temporally localizing\nanomalies without target domain training data, which is a crucial task due to\nvarious practical concerns, e.g., data privacy or new surveillance deployments.\nSkeleton-based approach has inherent generalizable advantages in achieving\nZS-VAD as it eliminates domain disparities both in background and human\nappearance. However, existing methods only learn low-level skeleton\nrepresentation and rely on the domain-limited normality boundary, which cannot\ngeneralize well to new scenes with different normal and abnormal behavior\npatterns. In this paper, we propose a novel zero-shot video anomaly detection\nframework, unlocking the potential of skeleton data via action typicality and\nuniqueness learning. Firstly, we introduce a language-guided semantic\ntypicality modeling module that projects skeleton snippets into action semantic\nspace and distills LLM's knowledge of typical normal and abnormal behaviors\nduring training. Secondly, we propose a test-time context uniqueness analysis\nmodule to finely analyze the spatio-temporal differences between skeleton\nsnippets and then derive scene-adaptive boundaries. Without using any training\nsamples from the target domain, our method achieves state-of-the-art results\nagainst skeleton-based methods on four large-scale VAD datasets: ShanghaiTech,\nUBnormal, NWPU, and UCF-Crime, featuring over 100 unseen surveillance scenes.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f46\u5b83\u4f7f\u7528\u9aa8\u9abc\u6570\u636e\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u77e5\u8bc6\u6765\u533a\u5206\u6b63\u5e38\u548c\u5f02\u5e38\u884c\u4e3a\u3002\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u9aa8\u9abc\u52a8\u4f5c\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\uff0c\u5e76\u4e14\u5229\u7528\u4e86LLM\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLM", "skeleton", "action"]}}
{"id": "2509.11071", "pdf": "https://arxiv.org/pdf/2509.11071", "abs": "https://arxiv.org/abs/2509.11071", "authors": ["Jinghan Peng", "Jingwen Wang", "Xing Yu", "Dehui Du"], "title": "The System Description of CPS Team for Track on Driving with Language of CVPR 2024 Autonomous Grand Challenge", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This report outlines our approach using vision language model systems for the\nDriving with Language track of the CVPR 2024 Autonomous Grand Challenge. We\nhave exclusively utilized the DriveLM-nuScenes dataset for training our models.\nOur systems are built on the LLaVA models, which we enhanced through\nfine-tuning with the LoRA and DoRA methods. Additionally, we have integrated\ndepth information from open-source depth estimation models to enrich the\ntraining and inference processes. For inference, particularly with\nmultiple-choice and yes/no questions, we adopted a Chain-of-Thought reasoning\napproach to improve the accuracy of the results. This comprehensive methodology\nenabled us to achieve a top score of 0.7799 on the validation set leaderboard,\nranking 1st on the leaderboard.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses LLaVA, a vision language model, and mentions DriveLM-nuScenes dataset which suggests a driving context. While it doesn't explicitly mention trajectory prediction, the driving context and the use of a vision language model for the CVPR Autonomous Grand Challenge suggest a potential connection to understanding driving scenarios, which might involve trajectory prediction. The Chain-of-Thought reasoning also implies complex scene understanding.", "keywords": ["vision language model", "LLaVA", "DriveLM-nuScenes", "autonomous driving", "Chain-of-Thought", "Large Language Models"]}}
{"id": "2509.11090", "pdf": "https://arxiv.org/pdf/2509.11090", "abs": "https://arxiv.org/abs/2509.11090", "authors": ["Chao Chen", "Shunyu Yao", "Yuanwu He", "Tao Feng", "Ruojing Song", "Yuliang Guo", "Xinyu Huang", "Chenxu Wu", "Ren Liu", "Chen Feng"], "title": "End-to-End Visual Autonomous Parking via Control-Aided Attention", "categories": ["cs.CV"], "comment": null, "summary": "Precise parking requires an end-to-end system where perception adaptively\nprovides policy-relevant details-especially in critical areas where fine\ncontrol decisions are essential. End-to-end learning offers a unified framework\nby directly mapping sensor inputs to control actions, but existing approaches\nlack effective synergy between perception and control. We find that\ntransformer-based self-attention, when used alone, tends to produce unstable\nand temporally inconsistent spatial attention, which undermines the reliability\nof downstream policy decisions over time. Instead, we propose CAA-Policy, an\nend-to-end imitation learning system that allows control signal to guide the\nlearning of visual attention via a novel Control-Aided Attention (CAA)\nmechanism. For the first time, we train such an attention module in a\nself-supervised manner, using backpropagated gradients from the control outputs\ninstead of from the training loss. This strategy encourages the attention to\nfocus on visual features that induce high variance in action outputs, rather\nthan merely minimizing the training loss-a shift we demonstrate leads to a more\nrobust and generalizable policy. To further enhance stability, CAA-Policy\nintegrates short-horizon waypoint prediction as an auxiliary task, and\nintroduces a separately trained motion prediction module to robustly track the\ntarget spot over time. Extensive experiments in the CARLA simulator show that\n\\titlevariable~consistently surpasses both the end-to-end learning baseline and\nthe modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,\nrobustness, and interpretability. Code is released at\nhttps://github.com/Joechencc/CAAPolicy.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u81ea\u4e3b\u6cca\u8f66\uff0c\u5176\u4e2d\u6d89\u53ca\u5230\u63a7\u5236\u4fe1\u53f7\u5f15\u5bfc\u7684\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528\u77ed\u65f6\u7a0b\u822a\u70b9\u9884\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6d89\u53ca\u5230\u4e86\u8f68\u8ff9\u9884\u6d4b\uff08\u822a\u70b9\u9884\u6d4b\uff0cmotion prediction\uff09\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "motion prediction", "attention"]}}
{"id": "2509.11766", "pdf": "https://arxiv.org/pdf/2509.11766", "abs": "https://arxiv.org/abs/2509.11766", "authors": ["Andy Zhai", "Brae Liu", "Bruno Fang", "Chalse Cai", "Ellie Ma", "Ethan Yin", "Hao Wang", "Hugo Zhou", "James Wang", "Lights Shi", "Lucy Liang", "Make Wang", "Qian Wang", "Roy Gan", "Ryan Yu", "Shalfun Li", "Starrick Liu", "Sylas Chen", "Vincent Chen", "Zach Xu"], "title": "Igniting VLMs toward the Embodied Space", "categories": ["cs.RO"], "comment": null, "summary": "While foundation models show remarkable progress in language and vision,\nexisting vision-language models (VLMs) still have limited spatial and\nembodiment understanding. Transferring VLMs to embodied domains reveals\nfundamental mismatches between modalities, pretraining distributions, and\ntraining objectives, leaving action comprehension and generation as a central\nbottleneck on the path to AGI.\n  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages\nlarge-scale multimodal pretraining to achieve (1) embodiment-aware\nvision-language understanding, (2) strong language-action association, and (3)\nrobust manipulation capability.\n  Our approach employs a tightly coupled architecture and multi-strategies\ntraining curriculum that enables Unified Cross-Level CoT-seamlessly unifying\ninstruction reasoning, subgoal decomposition, and fine-grained action synthesis\nwithin a single differentiable framework.\n  Our results show that WALL-OSS attains high success on complex long-horizon\nmanipulations, demonstrates strong instruction-following capabilities, complex\nunderstanding and reasoning, and outperforms strong baselines, thereby\nproviding a reliable and scalable path from VLMs to embodied foundation models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses vision-language models (VLMs) and their application in embodied domains, which involves understanding and generating actions. While it doesn't explicitly mention trajectory prediction, the concept of action generation is related, and the use of large-scale multimodal pretraining aligns with the theme of large models. The connection is not direct but present.", "keywords": ["vision-language models", "VLMs", "foundation models", "large-scale multimodal pretraining", "embodied"]}}
{"id": "2509.11839", "pdf": "https://arxiv.org/pdf/2509.11839", "abs": "https://arxiv.org/abs/2509.11839", "authors": ["Jiacheng Liu", "Pengxiang Ding", "Qihang Zhou", "Yuxuan Wu", "Da Huang", "Zimian Peng", "Wei Xiao", "Weinan Zhang", "Lixin Yang", "Cewu Lu", "Donglin Wang"], "title": "TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Imitation learning (IL) enables efficient skill acquisition from\ndemonstrations but often struggles with long-horizon tasks and high-precision\ncontrol due to compounding errors. Residual policy learning offers a promising,\nmodel-agnostic solution by refining a base policy through closed-loop\ncorrections. However, existing approaches primarily focus on local corrections\nto the base policy, lacking a global understanding of state evolution, which\nlimits robustness and generalization to unseen scenarios. To address this, we\npropose incorporating global dynamics modeling to guide residual policy\nupdates. Specifically, we leverage Koopman operator theory to impose linear\ntime-invariant structure in a learned latent space, enabling reliable state\ntransitions and improved extrapolation for long-horizon prediction and unseen\nenvironments. We introduce KORR (Koopman-guided Online Residual Refinement), a\nsimple yet effective framework that conditions residual corrections on\nKoopman-predicted latent states, enabling globally informed and stable action\nrefinement. We evaluate KORR on long-horizon, fine-grained robotic furniture\nassembly tasks under various perturbations. Results demonstrate consistent\ngains in performance, robustness, and generalization over strong baselines. Our\nfindings further highlight the potential of Koopman-based modeling to bridge\nmodern learning methods with classical control theory. For more details, please\nrefer to https://jiachengliu3.github.io/TrajBooster.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory optimization and control for humanoid manipulation, utilizing trajectory-centric learning and Koopman operator theory for dynamics modeling and long-horizon prediction. While it doesn't explicitly use large language models, the trajectory prediction aspect and the use of dynamics modeling connects it to the broader field of trajectory prediction. It's not highly related, but has some relevance.", "keywords": ["trajectory prediction", "trajectory-centric learning", "dynamics modeling", "long-horizon prediction", "Koopman operator theory"]}}
