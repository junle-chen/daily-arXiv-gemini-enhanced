{"id": "2511.04769", "pdf": "https://arxiv.org/pdf/2511.04769", "abs": "https://arxiv.org/abs/2511.04769", "authors": ["Phat Nguyen", "Tsun-Hsuan Wang", "Zhang-Wei Hong", "Erfan Aasi", "Andrew Silva", "Guy Rosman", "Sertac Karaman", "Daniela Rus"], "title": "ReGen: Generative Robot Simulation via Inverse Design", "categories": ["cs.RO"], "comment": null, "summary": "Simulation plays a key role in scaling robot learning and validating\npolicies, but constructing simulations remains a labor-intensive process. This\npaper introduces ReGen, a generative simulation framework that automates\nsimulation design via inverse design. Given a robot's behavior -- such as a\nmotion trajectory or an objective function -- and its textual description,\nReGen infers plausible scenarios and environments that could have caused the\nbehavior. ReGen leverages large language models to synthesize scenarios by\nexpanding a directed graph that encodes cause-and-effect relationships,\nrelevant entities, and their properties. This structured graph is then\ntranslated into a symbolic program, which configures and executes a robot\nsimulation environment. Our framework supports (i) augmenting simulations based\non ego-agent behaviors, (ii) controllable, counterfactual scenario generation,\n(iii) reasoning about agent cognition and mental states, and (iv) reasoning\nwith distinct sensing modalities, such as braking due to faulty GPS signals. We\ndemonstrate ReGen in autonomous driving and robot manipulation tasks,\ngenerating more diverse, complex simulated environments compared to existing\nsimulations with high success rates, and enabling controllable generation for\ncorner cases. This approach enhances the validation of robot policies and\nsupports data or simulation augmentation, advancing scalable robot learning for\nimproved generalization and robustness. We provide code and example videos at:\nhttps://regen-sim.github.io/", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper discusses generative robot simulation, particularly for autonomous driving, which is relevant to trajectory prediction. It also leverages large language models (LLMs) for scenario generation, connecting it to the LLM theme. The focus is more on simulation design than directly on trajectory prediction algorithms, but the application to autonomous driving makes it relevant.", "keywords": ["trajectory", "autonomous driving", "large language models", "simulation", "robot learning", "scenario generation"]}}
{"id": "2511.05124", "pdf": "https://arxiv.org/pdf/2511.05124", "abs": "https://arxiv.org/abs/2511.05124", "authors": ["Felix Divo", "Maurice Kraus", "Anh Q. Nguyen", "Hao Xue", "Imran Razzak", "Flora D. Salim", "Kristian Kersting", "Devendra Singh Dhami"], "title": "QuAnTS: Question Answering on Time Series", "categories": ["cs.LG", "I.2.6; I.2.7"], "comment": null, "summary": "Text offers intuitive access to information. This can, in particular,\ncomplement the density of numerical time series, thereby allowing improved\ninteractions with time series models to enhance accessibility and\ndecision-making. While the creation of question-answering datasets and models\nhas recently seen remarkable growth, most research focuses on question\nanswering (QA) on vision and text, with time series receiving minute attention.\nTo bridge this gap, we propose a challenging novel time series QA (TSQA)\ndataset, QuAnTS, for Question Answering on Time Series data. Specifically, we\npose a wide variety of questions and answers about human motion in the form of\ntracked skeleton trajectories. We verify that the large-scale QuAnTS dataset is\nwell-formed and comprehensive through extensive experiments. Thoroughly\nevaluating existing and newly proposed baselines then lays the groundwork for a\ndeeper exploration of TSQA using QuAnTS. Additionally, we provide human\nperformances as a key reference for gauging the practical usability of such\nmodels. We hope to encourage future research on interacting with time series\nmodels through text, enabling better decision-making and more transparent\nsystems.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper introduces a question answering dataset (QuAnTS) for time series data, specifically focusing on human motion tracked as skeleton trajectories. This strongly relates to trajectory prediction as it involves analyzing and understanding patterns in movement data. While the abstract doesn't explicitly mention large language models, the concept of question answering suggests potential applications of LLMs for interpreting and interacting with trajectory data. The connection to trajectory prediction is clear, and the potential for LLM integration exists, hence the score.", "keywords": ["trajectory prediction", "time series", "human motion", "skeleton trajectories", "question answering"]}}
{"id": "2511.05369", "pdf": "https://arxiv.org/pdf/2511.05369", "abs": "https://arxiv.org/abs/2511.05369", "authors": ["Shiyao Xu", "Benedetta Liberatori", "G\u00fcl Varol", "Paolo Rota"], "title": "Dense Motion Captioning", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "comment": "12 pages, 5 figures, accepted to 3DV 2026", "summary": "Recent advances in 3D human motion and language integration have primarily\nfocused on text-to-motion generation, leaving the task of motion understanding\nrelatively unexplored. We introduce Dense Motion Captioning, a novel task that\naims to temporally localize and caption actions within 3D human motion\nsequences. Current datasets fall short in providing detailed temporal\nannotations and predominantly consist of short sequences featuring few actions.\nTo overcome these limitations, we present the Complex Motion Dataset (CompMo),\nthe first large-scale dataset featuring richly annotated, complex motion\nsequences with precise temporal boundaries. Built through a carefully designed\ndata generation pipeline, CompMo includes 60,000 motion sequences, each\ncomposed of multiple actions ranging from at least two to ten, accurately\nannotated with their temporal extents. We further present DEMO, a model that\nintegrates a large language model with a simple motion adapter, trained to\ngenerate dense, temporally grounded captions. Our experiments show that DEMO\nsubstantially outperforms existing methods on CompMo as well as on adapted\nbenchmarks, establishing a robust baseline for future research in 3D motion\nunderstanding and captioning.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on captioning 3D human motion sequences, which involves understanding and predicting the temporal extent of actions. This relates to action prediction, which is a form of trajectory prediction. The paper also explicitly uses a large language model (LLM) integrated with a motion adapter. Therefore, it connects both trajectory prediction and large language models.", "keywords": ["motion captioning", "3D human motion", "action prediction", "large language model", "temporal localization"]}}
{"id": "2511.04898", "pdf": "https://arxiv.org/pdf/2511.04898", "abs": "https://arxiv.org/abs/2511.04898", "authors": ["Yule Wen", "Yixin Ye", "Yanzhe Zhang", "Diyi Yang", "Hao Zhu"], "title": "Real-Time Reasoning Agents in Evolving Environments", "categories": ["cs.AI"], "comment": "30 pages", "summary": "Agents in the real world must make not only logical but also timely\njudgments. This requires continuous awareness of the dynamic environment:\nhazards emerge, opportunities arise, and other agents act, while the agent's\nreasoning is still unfolding. Despite advances in language model reasoning,\nexisting approaches fail to account for this dynamic nature. We introduce\nreal-time reasoning as a new problem formulation for agents in evolving\nenvironments and build Real-Time Reasoning Gym to demonstrate it. We study two\nparadigms for deploying language models in agents: (1) reactive agents, which\nemploy language models with bounded reasoning computation for rapid responses,\nand (2) planning agents, which allow extended reasoning computation for complex\nproblems. Our experiments show that even state-of-the-art models struggle with\nmaking logical and timely judgments in either paradigm. To address this\nlimitation, we propose AgileThinker, which simultaneously engages both\nreasoning paradigms. AgileThinker consistently outperforms agents engaging only\none reasoning paradigm as the task difficulty and time pressure rise,\neffectively balancing reasoning depth and response latency. Our work\nestablishes real-time reasoning as a critical testbed for developing practical\nagents and provides a foundation for research in temporally constrained AI\nsystems, highlighting a path toward real-time capable agents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u65f6\u63a8\u7406\u7684\u667a\u80fd\u4f53\uff0c\u5e76\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u667a\u80fd\u4f53\u7684\u6838\u5fc3\u7ec4\u6210\u90e8\u5206\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u662f\u5176\u7814\u7a76\u7684\u52a8\u6001\u73af\u5883\u548c\u667a\u80fd\u4f53\u884c\u4e3a\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\uff0c\u5c24\u5176\u662f\u5728\u8003\u8651\u5176\u4ed6\u667a\u80fd\u4f53\u884c\u4e3a\u65f6\u3002\u8bba\u6587\u91cd\u70b9\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u65f6\u63a8\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u548c\u5927\u6a21\u578b\u7684\u76f8\u5173\u6027\u8f83\u9ad8\u3002", "keywords": ["Large Language Models", "LLMs", "real-time reasoning", "agents", "dynamic environment"]}}
{"id": "2511.04835", "pdf": "https://arxiv.org/pdf/2511.04835", "abs": "https://arxiv.org/abs/2511.04835", "authors": ["Shubham Natraj", "Bruno Sinopoli", "Yiannis Kantaros"], "title": "Conformalized Non-uniform Sampling Strategies for Accelerated Sampling-based Motion Planning", "categories": ["cs.RO"], "comment": null, "summary": "Sampling-based motion planners (SBMPs) are widely used to compute dynamically\nfeasible robot paths. However, their reliance on uniform sampling often leads\nto poor efficiency and slow planning in complex environments. We introduce a\nnovel non-uniform sampling strategy that integrates into existing SBMPs by\nbiasing sampling toward `certified' regions. These regions are constructed by\n(i) generating an initial, possibly infeasible, path using any heuristic path\npredictor (e.g., A* or vision-language models) and (ii) applying conformal\nprediction to quantify the predictor's uncertainty. This process yields\nprediction sets around the initial-guess path that are guaranteed, with\nuser-specified probability, to contain the optimal solution. To our knowledge,\nthis is the first non-uniform sampling approach for SBMPs that provides such\nprobabilistically correct guarantees on the sampling regions. Extensive\nevaluations demonstrate that our method consistently finds feasible paths\nfaster and generalizes better to unseen environments than existing baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses motion planning, which is related to trajectory prediction. It also mentions the use of vision-language models as a heuristic path predictor, connecting it to large models. However, the main focus is on sampling strategies for motion planning, not directly on trajectory prediction using large language models. The connection to large models is relatively weak.", "keywords": ["motion planning", "path prediction", "vision-language models"]}}
{"id": "2511.04976", "pdf": "https://arxiv.org/pdf/2511.04976", "abs": "https://arxiv.org/abs/2511.04976", "authors": ["Xin Nie", "Zhiyuan Cheng", "Yuan Zhang", "Chao Ji", "Jiajia Wu", "Yuhan Zhang", "Jia Pan"], "title": "iFlyBot-VLM Technical Report", "categories": ["cs.RO"], "comment": null, "summary": "We introduce iFlyBot-VLM, a general-purpose Vision-Language Model (VLM) used\nto improve the domain of Embodied Intelligence. The central objective of\niFlyBot-VLM is to bridge the cross-modal semantic gap between high-dimensional\nenvironmental perception and low-level robotic motion control. To this end, the\nmodel abstracts complex visual and spatial information into a body-agnostic and\ntransferable Operational Language, thereby enabling seamless perception-action\nclosed-loop coordination across diverse robotic platforms. The architecture of\niFlyBot-VLM is systematically designed to realize four key functional\ncapabilities essential for embodied intelligence: 1) Spatial Understanding and\nMetric Reasoning; 2) Interactive Target Grounding; 3) Action Abstraction and\nControl Parameter Generation; 4) Task Planning and Skill Sequencing. We\nenvision iFlyBot-VLM as a scalable and generalizable foundation model for\nembodied AI, facilitating the progression from specialized task-oriented\nsystems toward generalist, cognitively capable agents. We conducted evaluations\non 10 current mainstream embodied intelligence-related VLM benchmark datasets,\nsuch as Blink and Where2Place, and achieved optimal performance while\npreserving the model's general capabilities. We will publicly release both the\ntraining data and model weights to foster further research and development in\nthe field of Embodied Intelligence.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper describes a Vision-Language Model (VLM) for embodied intelligence. While it doesn't explicitly mention trajectory prediction, the concepts of robotic motion control, spatial understanding, action abstraction, and task planning are relevant to the broader field of robot navigation and potentially trajectory generation. The use of a large VLM also aligns with the large language model aspect. However, the primary focus is on embodied AI rather than specifically trajectory prediction.", "keywords": ["Vision-Language Model", "VLM", "Embodied Intelligence", "robotic motion control", "spatial understanding", "action abstraction", "task planning", "foundation model"]}}
{"id": "2511.05311", "pdf": "https://arxiv.org/pdf/2511.05311", "abs": "https://arxiv.org/abs/2511.05311", "authors": ["Valeriu Dimidov", "Faisal Hawlader", "Sasan Jafarnejad", "Rapha\u00ebl Frank"], "title": "Cleaning Maintenance Logs with LLM Agents for Improved Predictive Maintenance", "categories": ["cs.AI", "cs.LG", "cs.RO", "cs.SE"], "comment": null, "summary": "Economic constraints, limited availability of datasets for reproducibility\nand shortages of specialized expertise have long been recognized as key\nchallenges to the adoption and advancement of predictive maintenance (PdM) in\nthe automotive sector. Recent progress in large language models (LLMs) presents\nan opportunity to overcome these barriers and speed up the transition of PdM\nfrom research to industrial practice. Under these conditions, we explore the\npotential of LLM-based agents to support PdM cleaning pipelines. Specifically,\nwe focus on maintenance logs, a critical data source for training\nwell-performing machine learning (ML) models, but one often affected by errors\nsuch as typos, missing fields, near-duplicate entries, and incorrect dates. We\nevaluate LLM agents on cleaning tasks involving six distinct types of noise.\nOur findings show that LLMs are effective at handling generic cleaning tasks\nand offer a promising foundation for future industrial applications. While\ndomain-specific errors remain challenging, these results highlight the\npotential for further improvements through specialized training and enhanced\nagentic capabilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses using LLM agents for cleaning maintenance logs to improve predictive maintenance (PdM). While it focuses on predictive maintenance, which is related to predicting future states, it doesn't directly address trajectory prediction. The core focus is on leveraging LLMs for data cleaning in the context of PdM. Therefore, the relevance is moderate because it involves large language models and a prediction-related task (maintenance), but lacks a direct connection to trajectory prediction.", "keywords": ["Large Language Models", "LLMs", "predictive maintenance", "PdM"]}}
{"id": "2511.05158", "pdf": "https://arxiv.org/pdf/2511.05158", "abs": "https://arxiv.org/abs/2511.05158", "authors": ["Sahar Salimpour", "Iacopo Catalano", "Tomi Westerlund", "Mohsen Falahi", "Jorge Pe\u00f1a Queralta"], "title": "Follow-Me in Micro-Mobility with End-to-End Imitation Learning", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Autonomous micro-mobility platforms face challenges from the perspective of\nthe typical deployment environment: large indoor spaces or urban areas that are\npotentially crowded and highly dynamic. While social navigation algorithms have\nprogressed significantly, optimizing user comfort and overall user experience\nover other typical metrics in robotics (e.g., time or distance traveled) is\nunderstudied. Specifically, these metrics are critical in commercial\napplications. In this paper, we show how imitation learning delivers smoother\nand overall better controllers, versus previously used manually-tuned\ncontrollers. We demonstrate how DAAV's autonomous wheelchair achieves\nstate-of-the-art comfort in follow-me mode, in which it follows a human\noperator assisting persons with reduced mobility (PRM). This paper analyzes\ndifferent neural network architectures for end-to-end control and demonstrates\ntheir usability in real-world production-level deployments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autonomous navigation and control of a wheelchair in a 'follow-me' mode, which falls under the domain of trajectory prediction and path planning for mobile robots. While it doesn't explicitly mention Large Language Models, the use of neural networks for end-to-end control indicates a potential link to modern AI techniques. The 'follow-me' aspect also implies predicting the human's trajectory.", "keywords": ["trajectory prediction", "imitation learning", "autonomous navigation", "path planning", "neural networks"]}}
{"id": "2511.05199", "pdf": "https://arxiv.org/pdf/2511.05199", "abs": "https://arxiv.org/abs/2511.05199", "authors": ["Yichen Zhu", "Feifei Feng"], "title": "Let Me Show You: Learning by Retrieving from Egocentric Video for Robotic Manipulation", "categories": ["cs.RO"], "comment": "Accepted by IROS 2025", "summary": "Robots operating in complex and uncertain environments face considerable\nchallenges. Advanced robotic systems often rely on extensive datasets to learn\nmanipulation tasks. In contrast, when humans are faced with unfamiliar tasks,\nsuch as assembling a chair, a common approach is to learn by watching video\ndemonstrations. In this paper, we propose a novel method for learning robot\npolicies by Retrieving-from-Video (RfV), using analogies from human\ndemonstrations to address manipulation tasks. Our system constructs a video\nbank comprising recordings of humans performing diverse daily tasks. To enrich\nthe knowledge from these videos, we extract mid-level information, such as\nobject affordance masks and hand motion trajectories, which serve as additional\ninputs to enhance the robot model's learning and generalization capabilities.\nWe further feature a dual-component system: a video retriever that taps into an\nexternal video bank to fetch task-relevant video based on task specification,\nand a policy generator that integrates this retrieved knowledge into the\nlearning cycle. This approach enables robots to craft adaptive responses to\nvarious scenarios and generalize to tasks beyond those in the training data.\nThrough rigorous testing in multiple simulated and real-world settings, our\nsystem demonstrates a marked improvement in performance over conventional\nrobotic systems, showcasing a significant breakthrough in the field of\nrobotics.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper describes a robotic manipulation system that learns from egocentric video by retrieving relevant demonstrations. It extracts hand motion trajectories, which is related to trajectory prediction. However, it doesn't explicitly use large language models. The connection is weak but present due to the trajectory aspect.", "keywords": ["trajectory", "hand motion trajectories", "motion"]}}
{"id": "2511.05038", "pdf": "https://arxiv.org/pdf/2511.05038", "abs": "https://arxiv.org/abs/2511.05038", "authors": ["Zhengxuan Li", "Qinhui Yang", "Yiyu Zhuang", "Chuan Guo", "Xinxin Zuo", "Xiaoxiao Long", "Yao Yao", "Xun Cao", "Qiu Shen", "Hao Zhu"], "title": "Pressure2Motion: Hierarchical Motion Synthesis from Ground Pressure with Text Guidance", "categories": ["cs.CV"], "comment": null, "summary": "We present Pressure2Motion, a novel motion capture algorithm that synthesizes\nhuman motion from a ground pressure sequence and text prompt. It eliminates the\nneed for specialized lighting setups, cameras, or wearable devices, making it\nsuitable for privacy-preserving, low-light, and low-cost motion capture\nscenarios. Such a task is severely ill-posed due to the indeterminate nature of\nthe pressure signals to full-body motion. To address this issue, we introduce\nPressure2Motion, a generative model that leverages pressure features as input\nand utilizes a text prompt as a high-level guiding constraint. Specifically,\nour model utilizes a dual-level feature extractor that accurately interprets\npressure data, followed by a hierarchical diffusion model that discerns\nbroad-scale movement trajectories and subtle posture adjustments. Both the\nphysical cues gained from the pressure sequence and the semantic guidance\nderived from descriptive texts are leveraged to guide the motion generation\nwith precision. To the best of our knowledge, Pressure2Motion is a pioneering\nwork in leveraging both pressure data and linguistic priors for motion\ngeneration, and the established MPL benchmark is the first benchmark for this\ntask. Experiments show our method generates high-fidelity, physically plausible\nmotions, establishing a new state-of-the-art for this task. The codes and\nbenchmarks will be publicly released upon publication.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on motion synthesis from ground pressure with text guidance. While it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting future locations of moving agents), it does involve synthesizing motion trajectories. The use of text guidance suggests a potential link to large language models, even though the abstract doesn't explicitly mention them. The hierarchical diffusion model could potentially be informed or augmented by LLMs in future work, making it moderately relevant.", "keywords": ["motion synthesis", "motion capture", "diffusion model", "text guidance", "motion generation"]}}
{"id": "2511.05355", "pdf": "https://arxiv.org/pdf/2511.05355", "abs": "https://arxiv.org/abs/2511.05355", "authors": ["Tzu-Yuan Huang", "Armin Lederer", "Dai-Jie Wu", "Xiaobing Dai", "Sihua Zhang", "Stefan Sosnowski", "Shao-Hua Sun", "Sandra Hirche"], "title": "SAD-Flower: Flow Matching for Safe, Admissible, and Dynamically Consistent Planning", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Flow matching (FM) has shown promising results in data-driven planning.\nHowever, it inherently lacks formal guarantees for ensuring state and action\nconstraints, whose satisfaction is a fundamental and crucial requirement for\nthe safety and admissibility of planned trajectories on various systems.\nMoreover, existing FM planners do not ensure the dynamical consistency, which\npotentially renders trajectories inexecutable. We address these shortcomings by\nproposing SAD-Flower, a novel framework for generating Safe, Admissible, and\nDynamically consistent trajectories. Our approach relies on an augmentation of\nthe flow with a virtual control input. Thereby, principled guidance can be\nderived using techniques from nonlinear control theory, providing formal\nguarantees for state constraints, action constraints, and dynamic consistency.\nCrucially, SAD-Flower operates without retraining, enabling test-time\nsatisfaction of unseen constraints. Through extensive experiments across\nseveral tasks, we demonstrate that SAD-Flower outperforms various\ngenerative-model-based baselines in ensuring constraint satisfaction.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u9a71\u52a8\u7684\u89c4\u5212\uff0c\u7279\u522b\u662fflow matching\u5728\u8f68\u8ff9\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u81f4\u529b\u4e8e\u89e3\u51b3\u5b89\u5168\u3001\u53ef\u91c7\u7eb3\u6027\u548c\u52a8\u6001\u4e00\u81f4\u6027\u95ee\u9898\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46flow matching\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u751f\u6210\u6a21\u578b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["flow matching", "trajectory planning", "data-driven planning", "dynamic consistency", "state constraints", "action constraints"]}}
