{"id": "2509.12610", "pdf": "https://arxiv.org/pdf/2509.12610", "abs": "https://arxiv.org/abs/2509.12610", "authors": ["Hengrui Zhang", "Yulong Hui", "Yihao Liu", "Huanchen Zhang"], "title": "ScaleDoc: Scaling LLM-based Predicates over Large Document Collections", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": null, "summary": "Predicates are foundational components in data analysis systems. However,\nmodern workloads increasingly involve unstructured documents, which demands\nsemantic understanding, beyond traditional value-based predicates. Given\nenormous documents and ad-hoc queries, while Large Language Models (LLMs)\ndemonstrate powerful zero-shot capabilities, their high inference cost leads to\nunacceptable overhead. Therefore, we introduce \\textsc{ScaleDoc}, a novel\nsystem that addresses this by decoupling predicate execution into an offline\nrepresentation phase and an optimized online filtering phase. In the offline\nphase, \\textsc{ScaleDoc} leverages a LLM to generate semantic representations\nfor each document. Online, for each query, it trains a lightweight proxy model\non these representations to filter the majority of documents, forwarding only\nthe ambiguous cases to the LLM for final decision. Furthermore,\n\\textsc{ScaleDoc} proposes two core innovations to achieve significant\nefficiency: (1) a contrastive-learning-based framework that trains the proxy\nmodel to generate reliable predicating decision scores; (2) an adaptive cascade\nmechanism that determines the effective filtering policy while meeting specific\naccuracy targets. Our evaluations across three datasets demonstrate that\n\\textsc{ScaleDoc} achieves over a 2$\\times$ end-to-end speedup and reduces\nexpensive LLM invocations by up to 85\\%, making large-scale semantic analysis\npractical and efficient."}
{"id": "2509.12384", "pdf": "https://arxiv.org/pdf/2509.12384", "abs": "https://arxiv.org/abs/2509.12384", "authors": ["Seth Ockerman", "Amal Gueroudji", "Song Young Oh", "Robert Underwood", "Nicholas Chia", "Kyle Chard", "Robert Ross", "Shivaram Venkataraman"], "title": "Exploring Distributed Vector Databases Performance on HPC Platforms: A Study with Qdrant", "categories": ["cs.DC", "cs.DB"], "comment": "To appear in the SC'25 Workshop Frontiers in Generative AI for HPC\n  Science and Engineering: Foundations, Challenges, and Opportunities", "summary": "Vector databases have rapidly grown in popularity, enabling efficient\nsimilarity search over data such as text, images, and video. They now play a\ncentral role in modern AI workflows, aiding large language models by grounding\nmodel outputs in external literature through retrieval-augmented generation.\nDespite their importance, little is known about the performance characteristics\nof vector databases in high-performance computing (HPC) systems that drive\nlarge-scale science. This work presents an empirical study of distributed\nvector database performance on the Polaris supercomputer in the Argonne\nLeadership Computing Facility. We construct a realistic biological-text\nworkload from BV-BRC and generate embeddings from the peS2o corpus using\nQwen3-Embedding-4B. We select Qdrant to evaluate insertion, index construction,\nand query latency with up to 32 workers. Informed by practical lessons from our\nexperience, this work takes a first step toward characterizing vector database\nperformance on HPC platforms to guide future research and optimization."}
