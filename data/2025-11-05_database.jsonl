{"id": "2511.00290", "pdf": "https://arxiv.org/pdf/2511.00290", "abs": "https://arxiv.org/abs/2511.00290", "authors": ["Ashwin Gerard Colaco", "Sharad Mehrotra", "Michael J De Lucia", "Kevin Hamlen", "Murat Kantarcioglu", "Latifur Khan", "Ananthram Swami", "Bhavani Thuraisingham"], "title": "NOMAD - Navigating Optimal Model Application to Datastreams", "categories": ["cs.DB"], "comment": null, "summary": "NOMAD (Navigating Optimal Model Application for Datastreams) is an\nintelligent framework for data enrichment during ingestion that optimizes\nrealtime multiclass classification by dynamically constructing model chains,\ni.e ,sequences of machine learning models with varying cost-quality tradeoffs,\nselected via a utilitybased criterion. Inspired by predicate ordering\ntechniques from database query processing, NOMAD leverages cheaper models as\ninitial filters, proceeding to more expensive models only when necessary, while\nguaranteeing classification quality remains comparable to a designated role\nmodel through a formal chain safety mechanism. It employs a dynamic belief\nupdate strategy to adapt model selection based on per event predictions and\nshifting data distributions, and extends to scenarios with dependent models\nsuch as earlyexit DNNs and stacking ensembles. Evaluation across multiple\ndatasets demonstrates that NOMAD achieves significant computational savings\ncompared to static and naive approaches while maintaining classification\nquality comparable to that achieved by the most accurate (and often the most\nexpensive) model."}
{"id": "2511.00414", "pdf": "https://arxiv.org/pdf/2511.00414", "abs": "https://arxiv.org/abs/2511.00414", "authors": ["Sirintra Vaiwsri", "Thilina Ranbaduge"], "title": "Embedding based Encoding Scheme for Privacy Preserving Record Linkage", "categories": ["cs.DB"], "comment": "12 pages", "summary": "To discover new insights from data, there is a growing need to share\ninformation that is often held by different organisations. One key task in data\nintegration is the calculation of similarities between records in different\ndatabases to identify pairs or sets of records that correspond to the same\nreal-world entities. Due to privacy and confidentiality concerns, however, the\nowners of sensitive databases are often not allowed or willing to exchange or\nshare their data with other organisations to allow such similarity\ncalculations. Privacy-preserving record linkage (PPRL) is the process of\nmatching records that refer to the same entity across sensitive databases held\nby different organisations while ensuring no information about the entities is\nrevealed to the participating parties. In this paper, we study how embedding\nbased encoding techniques can be applied in the PPRL context to ensure the\nprivacy of the entities that are being linked. We first convert individual\nq-grams into the embedded space and then convert the embedding of a set of\nq-grams of a given record into a binary representation. The final binary\nrepresentations can be used to link records into matches and non-matches. We\nempirically evaluate our proposed encoding technique against different\nreal-world datasets. The results suggest that our proposed encoding approach\ncan provide better linkage accuracy and protect the privacy of individuals\nagainst attack compared to state-of-the-art techniques for short record values."}
{"id": "2511.00693", "pdf": "https://arxiv.org/pdf/2511.00693", "abs": "https://arxiv.org/abs/2511.00693", "authors": ["Saba Latif", "Huma Latif", "Muhammad Rameez Ur Rahman"], "title": "Object-Centric Analysis of XES Event Logs: Integrating OCED Modeling with SPARQL Queries", "categories": ["cs.DB", "cs.IR"], "comment": "12 pages, 4 figures, PROFES2025 conference", "summary": "Object Centric Event Data (OCED) has gained attention in recent years within\nthe field of process mining. However, there are still many challenges, such as\nconnecting the XES format to object-centric approaches to enable more\ninsightful analysis. It is important for a process miner to understand the\ninsights and dependencies of events in the event log to see what is going on in\nour processes. In previous standards, the dependencies of event logs are only\nused to show events, but not their dependencies among each other and actions in\ndetail as described in OCEDO. There is more information in the event log when\nit is revealed using the OCEDO model. It becomes more understandable and easier\nto grasp the concepts and deal with the processes. This paper proposes the use\nof Object-Centric Event Data Ontology (OCEDO) to overcome the limitations of\nthe XES standard in event logs for process mining. We demonstrate how the OCEDO\napproach, integrated with SPARQL queries, can be applied to the BPIC 2013\ndataset to make the relationships between events and objects more explicit. It\ndescribes dealing with the meta descriptions of the OCEDO model on a business\nprocess challenge as an event log. It improves the completeness and readability\nof process data, suggesting that object-centric modeling allows for richer\nanalyses than traditional approaches."}
{"id": "2511.00748", "pdf": "https://arxiv.org/pdf/2511.00748", "abs": "https://arxiv.org/abs/2511.00748", "authors": ["Yi Yang", "Jian Pei", "Jun Yang", "Jichun Xie"], "title": "Finding Non-Redundant Simpson's Paradox from Multidimensional Data", "categories": ["cs.DB"], "comment": "20 pages, 7 figures", "summary": "Simpson's paradox, a long-standing statistical phenomenon, describes the\nreversal of an observed association when data are disaggregated into\nsub-populations. It has critical implications across statistics, epidemiology,\neconomics, and causal inference. Existing methods for detecting Simpson's\nparadox overlook a key issue: many paradoxes are redundant, arising from\nequivalent selections of data subsets, identical partitioning of\nsub-populations, and correlated outcome variables, which obscure essential\npatterns and inflate computational cost. In this paper, we present the first\nframework for discovering non-redundant Simpson's paradoxes. We formalize three\ntypes of redundancy - sibling child, separator, and statistic equivalence - and\nshow that redundancy forms an equivalence relation. Leveraging this insight, we\npropose a concise representation framework for systematically organizing\nredundant paradoxes and design efficient algorithms that integrate depth-first\nmaterialization of the base table with redundancy-aware paradox discovery.\nExperiments on real-world datasets and synthetic benchmarks show that redundant\nparadoxes are widespread, on some real datasets constituting over 40% of all\nparadoxes, while our algorithms scale to millions of records, reduce run time\nby up to 60%, and discover paradoxes that are structurally robust under data\nperturbation. These results demonstrate that Simpson's paradoxes can be\nefficiently identified, concisely summarized, and meaningfully interpreted in\nlarge multidimensional datasets."}
{"id": "2511.00772", "pdf": "https://arxiv.org/pdf/2511.00772", "abs": "https://arxiv.org/abs/2511.00772", "authors": ["Raymond M. Xiong", "Panyu Chen", "Tianze Dong", "Jian Lu", "Benjamin Goldstein", "Danyang Zhuo", "Anru R. Zhang"], "title": "Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints", "categories": ["cs.DB", "cs.LG", "stat.AP"], "comment": null, "summary": "Electronic health records (EHRs) are central to modern healthcare delivery\nand research; yet, many researchers lack the database expertise necessary to\nwrite complex SQL queries or generate effective visualizations, limiting\nefficient data use and scientific discovery. To address this barrier, we\nintroduce CELEC, a large language model (LLM)-powered framework for automated\nEHR data extraction and analytics. CELEC translates natural language queries\ninto SQL using a prompting strategy that integrates schema information,\nfew-shot demonstrations, and chain-of-thought reasoning, which together improve\naccuracy and robustness. On a subset of the EHRSQL benchmark, CELEC achieves\nexecution accuracy comparable to prior systems while maintaining low latency,\ncost efficiency, and strict privacy by exposing only database metadata to the\nLLM. CELEC also adheres to strict privacy protocols: the LLM accesses only\ndatabase metadata (e.g., table and column names), while all query execution\noccurs securely within the institutional environment, ensuring that no\npatient-level data is ever transmitted to or shared with the LLM. Ablation\nstudies confirm that each component of the SQL generation pipeline,\nparticularly the few-shot demonstrations, plays a critical role in performance.\nBy lowering technical barriers and enabling medical researchers to query EHR\ndatabases directly, CELEC streamlines research workflows and accelerates\nbiomedical discovery."}
{"id": "2511.00826", "pdf": "https://arxiv.org/pdf/2511.00826", "abs": "https://arxiv.org/abs/2511.00826", "authors": ["Shatha Algarni", "Boris Glavic", "Seokki Lee", "Adriane Chapman"], "title": "Efficient Query Repair for Aggregate Constraints", "categories": ["cs.DB"], "comment": "19 pages, 63 figures", "summary": "In many real-world scenarios, query results must satisfy domain-specific\nconstraints. For instance, a minimum percentage of interview candidates\nselected based on their qualifications should be female. These requirements can\nbe expressed as constraints over an arithmetic combination of aggregates\nevaluated on the result of the query. In this work, we study how to repair a\nquery to fulfill such constraints by modifying the filter predicates of the\nquery. We introduce a novel query repair technique that leverages bounds on\nsets of candidate solutions and interval arithmetic to efficiently prune the\nsearch space. We demonstrate experimentally, that our technique significantly\noutperforms baselines that consider a single candidate at a time."}
{"id": "2511.00855", "pdf": "https://arxiv.org/pdf/2511.00855", "abs": "https://arxiv.org/abs/2511.00855", "authors": ["Zhonggen Li", "Yougen Li", "Yifan Zhu", "Zhaoqiang Chen", "Yunjun Gao"], "title": "All-in-one Graph-based Indexing for Hybrid Search on GPUs", "categories": ["cs.DB"], "comment": null, "summary": "Hybrid search has emerged as a promising paradigm to overcome the limitations\nof single-path retrieval, enhancing accuracy for applications like\nrecommendations, information retrieval, and Retrieval-Augmented Generation.\nHowever, existing methods are constrained by a trilemma: they sacrifice\nflexibility for efficiency, suffer from accuracy degradation due to separate\nretrievals, or incur prohibitive storage overhead for flexible combinations of\nretrieval paths. This paper introduces Allan-Poe, a novel All-in-one graph\nindex accelerated by GPUs for efficient hybrid search. We first analyze the\nlimitations of existing retrieval paradigms and distill key design principles\nfor an effective hybrid search index. Guided by these principles, we architect\na unified graph-based index that flexibly integrates four retrieval paths-dense\nvector, sparse vector, full-text, and knowledge graph-within a single, cohesive\nstructure. To enable efficient construction, we design a GPU-accelerated\npipeline featuring a warp-level hybrid distance kernel, RNG-IP joint pruning,\nand keyword-aware neighbor recycling. For query processing, we introduce a\ndynamic fusion framework that supports any combination of retrieval paths and\nweights without index reconstruction, leveraging logical edges from the\nknowledge graph to resolve complex multi-hop queries. Extensive experiments on\n6 real-world datasets demonstrate that Allan-Poe achieves superior end-to-end\nquery accuracy and outperforms state-of-the-art methods by 1.5-186.4x in\nthroughput, while significantly reducing storage overhead."}
{"id": "2511.00865", "pdf": "https://arxiv.org/pdf/2511.00865", "abs": "https://arxiv.org/abs/2511.00865", "authors": ["Hangdong Zhao", "Zhenghong Yu", "Srinag Rao", "Simon Frisk", "Zhiwei Fan", "Paraschos Koutris"], "title": "FlowLog: Efficient and Extensible Datalog via Incrementality", "categories": ["cs.DB", "cs.PL"], "comment": "Accepted to VLDB 2026", "summary": "Datalog-based languages are regaining popularity as a powerful abstraction\nfor expressing recursive computations in domains such as program analysis and\ngraph processing. However, existing systems often face a trade-off between\nefficiency and extensibility. Engines like Souffle achieve high efficiency\nthrough domain-specific designs, but lack general-purpose flexibility. Others,\nlike RecStep, offer modularity by layering Datalog on traditional databases,\nbut struggle to integrate Datalog-specific optimizations.\n  This paper bridges this gap by presenting FlowLog, a new Datalog engine that\nuses an explicit relational IR per-rule to cleanly separate recursive control\n(e.g., semi-naive execution) from each rule's logical plan. This boundary lets\nus retain fine-grained, Datalog-aware optimizations at the logical layer, but\nalso reuse off-the-shelf database primitives at execution. At the logical level\n(i.e. IR), we apply proven SQL optimizations, such as logic fusion and subplan\nreuse. To address high volatility in recursive workloads, we adopt a\nrobustness-first approach that pairs a structural optimizer (avoiding\nworst-case joins) with sideways information passing (early filtering). Built\natop Differential Dataflow--a mature framework for streaming analytics--FlowLog\nsupports both batch and incremental Datalog and adds novel recursion-aware\noptimizations called Boolean (or algebraic) specialization. Our evaluation\nshows that FlowLog outperforms state-of-the-art Datalog engines and modern\ndatabases across a broad range of recursive workloads, achieving superior\nscalability while preserving a simple and extensible architecture."}
{"id": "2511.00985", "pdf": "https://arxiv.org/pdf/2511.00985", "abs": "https://arxiv.org/abs/2511.00985", "authors": ["Yiwen Jiao", "Tonghui Ren", "Yuche Gao", "Zhenying He", "Yinan Jing", "Kai Zhang", "X. Sean Wang"], "title": "ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": "16 pages, 4 figures, preprint", "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ntranslating natural language to SQL, but a significant semantic gap persists\nbetween their general knowledge and domain-specific semantics of databases.\nHistorical translation logs constitute a rich source of this missing in-domain\nknowledge, where SQL queries inherently encapsulate real-world usage patterns\nof database schema. Existing methods primarily enhance the reasoning process\nfor individual translations but fail to accumulate in-domain knowledge from\npast translations. We introduce ORANGE, an online self-evolutionary framework\nthat constructs database-specific knowledge bases by parsing SQL queries from\ntranslation logs. By accumulating in-domain knowledge that contains schema and\ndata semantics, ORANGE progressively reduces the semantic gap and enhances the\naccuracy of subsequent SQL translations. To ensure reliability, we propose a\nnovel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic\ntracking, which reduces semantic errors during knowledge generation.\nExperiments on multiple benchmarks confirm the practicality of ORANGE,\ndemonstrating its effectiveness for real-world Text-to-SQL deployment,\nparticularly in handling complex and domain-specific queries."}
{"id": "2511.00995", "pdf": "https://arxiv.org/pdf/2511.00995", "abs": "https://arxiv.org/abs/2511.00995", "authors": ["Tianming Wu", "Dixin Tang"], "title": "PathFinder: Efficiently Supporting Conjunctions and Disjunctions for Filtered Approximate Nearest Neighbor Search", "categories": ["cs.DB"], "comment": null, "summary": "Filtered approximate nearest neighbor search (ANNS) restricts the search to\ndata objects whose attributes satisfy a given filter and retrieves the top-$K$\nobjects that are most semantically similar to the query object. Many\ngraph-based ANNS indexes are proposed to enable efficient filtered ANNS but\nremain limited in applicability or performance: indexes optimized for a\nspecific attribute achieve high efficiency for filters on that attribute but\nfail to support complex filters with arbitrary conjunctions and disjunctions\nover multiple attributes. Inspired by the design of relational databases, this\npaper presents PathFinder, a new indexing framework that allows users to\nselectively create ANNS indexes optimized for filters on specific attributes\nand employs a cost-based optimizer to efficiently utilize them for processing\ncomplex filters. PathFinder includes three novel techniques: 1) a new\noptimization metric that captures the tradeoff between query execution time and\naccuracy, 2) a two-phase optimization for handling filters with conjunctions\nand disjunctions, and 3) an index borrowing optimization that uses an\nattribute-specific index to process filters on another attribute. Experiments\non four real-world datasets show that PathFinder outperforms the best baseline\nby up to 9.8x in query throughput at recall 0.95."}
{"id": "2511.01025", "pdf": "https://arxiv.org/pdf/2511.01025", "abs": "https://arxiv.org/abs/2511.01025", "authors": ["Huihui Yang", "Pingpeng Yuan"], "title": "Fast Answering Pattern-Constrained Reachability Queries with Two-Dimensional Reachability Index", "categories": ["cs.DB", "cs.DS"], "comment": null, "summary": "Reachability queries ask whether there exists a path from the source vertex\nto the target vertex on a graph. Recently, several powerful reachability\nqueries, such as Label-Constrained Reachability (LCR) queries and Regular Path\nQueries (RPQ), have been proposed for emerging complex edge-labeled digraphs.\nHowever, they cannot allow users to describe complex query requirements by\ncomposing query patterns. Here, we introduce composite patterns, a logical\nexpression of patterns that can express complex constraints on the set of\nlabels. Based on pattern, we propose pattern-constrained reachability queries\n(PCR queries). However, answering PCR queries is NP-hard. Thus, to improve the\nperformance to answer PCR queries, we build a two-dimensional reachability (TDR\nfor short) index which consists of a multi-way index (horizontal dimension) and\na path index (vertical dimension). Because the number of combinations of both\nlabels and vertices is exponential, it is very expensive to build full indices\nthat contain all the reachability information. Thus, the reachable vertices of\na vertex are decomposed into blocks, each of which is hashed into the\nhorizontal dimension index and the vertical dimension index, respectively. The\nindices in the horizontal dimension and the vertical dimension serve as a\nglobal filter and a local filter, respectively, to prune the search space.\nExperimental results demonstrate that our index size and indexing time\noutperform the state-of-the-art label-constrained reachability indexing\ntechnique on 16 real datasets. TDR can efficiently answer pattern-constrained\nreachability queries, including label-constrained reachability queries."}
{"id": "2511.01602", "pdf": "https://arxiv.org/pdf/2511.01602", "abs": "https://arxiv.org/abs/2511.01602", "authors": ["Xinyue Yang", "Chen Zheng", "Yaoyang Hou", "Renhao Zhang", "Yiyan Zhang", "Yanjun Wu", "Heng Zhang"], "title": "L2T-Tune:LLM-Guided Hybrid Database Tuning with LHS and TD3", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Configuration tuning is critical for database performance. Although recent\nadvancements in database tuning have shown promising results in throughput and\nlatency improvement, challenges remain. First, the vast knob space makes direct\noptimization unstable and slow to converge. Second, reinforcement learning\npipelines often lack effective warm-start guidance and require long offline\ntraining. Third, transferability is limited: when hardware or workloads change,\nexisting models typically require substantial retraining to recover\nperformance.\n  To address these limitations, we propose L2T-Tune, a new LLM-guided hybrid\ndatabase tuning framework that features a three-stage pipeline: Stage one\nperforms a warm start that simultaneously generates uniform samples across the\nknob space and logs them into a shared pool; Stage two leverages a large\nlanguage model to mine and prioritize tuning hints from manuals and community\ndocuments for rapid convergence. Stage three uses the warm-start sample pool to\nreduce the dimensionality of knobs and state features, then fine-tunes the\nconfiguration with the Twin Delayed Deep Deterministic Policy Gradient\nalgorithm.\n  We conduct experiments on L2T-Tune and the state-of-the-art models. Compared\nwith the best-performing alternative, our approach improves performance by an\naverage of 37.1% across all workloads, and by up to 73% on TPC-C. Compared with\nmodels trained with reinforcement learning, it achieves rapid convergence in\nthe offline tuning stage on a single server. Moreover, during the online tuning\nstage, it only takes 30 steps to achieve best results."}
{"id": "2511.01625", "pdf": "https://arxiv.org/pdf/2511.01625", "abs": "https://arxiv.org/abs/2511.01625", "authors": ["Han Weng", "Zhou Liu", "Yuanfeng Song", "Xiaoming Yin", "Xing Chen", "Wentao Zhang"], "title": "UniDataBench: Evaluating Data Analytics Agents Across Structured and Unstructured Data", "categories": ["cs.DB"], "comment": null, "summary": "In the real business world, data is stored in a variety of sources, including\nstructured relational databases, unstructured databases (e.g., NoSQL\ndatabases), or even CSV/excel files. The ability to extract reasonable insights\nacross these diverse source is vital for business success. Existing benchmarks,\nhowever, are limited in assessing agents' capabilities across these diverse\ndata types. To address this gap, we introduce UniDataBench, a comprehensive\nbenchmark designed to evaluate the performance of data analytics agents in\nhandling diverse data sources. Specifically, UniDataBench is originating from\nreal-life industry analysis report and we then propose a pipeline to remove the\nprivacy and sensitive information. It encompasses a wide array of datasets,\nincluding relational databases, CSV files to NoSQL data, reflecting real-world\nbusiness scenarios, and provides unified framework to assess how effectively\nagents can explore multiple data formats, extract valuable insights, and\ngenerate meaningful summaries and recommendations. Based on UniDataBench, we\npropose a novel LLM-based agent named ReActInsight, an autonomous agent that\nperforms end-to-end analysis over diverse data sources by automatically\ndiscovering cross-source linkages, decomposing goals, and generating robust,\nself-correcting code to extract actionable insights. Our benchmark and agent\ntogether provide a powerful framework for advancing the capabilities of data\nanalytics agents in real-world applications."}
{"id": "2511.01716", "pdf": "https://arxiv.org/pdf/2511.01716", "abs": "https://arxiv.org/abs/2511.01716", "authors": ["Jiale Lao", "Andreas Zimmerer", "Olga Ovcharenko", "Tianji Cong", "Matthew Russo", "Gerardo Vitagliano", "Michael Cochez", "Fatma \u00d6zcan", "Gautam Gupta", "Thibaud Hottelier", "H. V. Jagadish", "Kris Kissel", "Sebastian Schelter", "Andreas Kipf", "Immanuel Trummer"], "title": "SemBench: A Benchmark for Semantic Query Processing Engines", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "We present a benchmark targeting a novel class of systems: semantic query\nprocessing engines. Those systems rely inherently on generative and reasoning\ncapabilities of state-of-the-art large language models (LLMs). They extend SQL\nwith semantic operators, configured by natural language instructions, that are\nevaluated via LLMs and enable users to perform various operations on multimodal\ndata.\n  Our benchmark introduces diversity across three key dimensions: scenarios,\nmodalities, and operators. Included are scenarios ranging from movie review\nanalysis to medical question-answering. Within these scenarios, we cover\ndifferent data modalities, including images, audio, and text. Finally, the\nqueries involve a diverse set of operators, including semantic filters, joins,\nmappings, ranking, and classification operators.\n  We evaluated our benchmark on three academic systems (LOTUS, Palimpzest, and\nThalamusDB) and one industrial system, Google BigQuery. Although these results\nreflect a snapshot of systems under continuous development, our study offers\ncrucial insights into their current strengths and weaknesses, illuminating\npromising directions for future research."}
{"id": "2511.00078", "pdf": "https://arxiv.org/pdf/2511.00078", "abs": "https://arxiv.org/abs/2511.00078", "authors": ["Chen-Wei Chang", "Yu-Chieh Cheng", "Yun-En Tsai", "Fanglan Chen", "Chang-Tien Lu"], "title": "RailEstate: An Interactive System for Metro Linked Property Trends", "categories": ["cs.CY", "cs.AI", "cs.DB"], "comment": null, "summary": "Access to metro systems plays a critical role in shaping urban housing\nmarkets by enhancing neighborhood accessibility and driving property demand. We\npresent RailEstate, a novel web based system that integrates spatial analytics,\nnatural language interfaces, and interactive forecasting to analyze how\nproximity to metro stations influences residential property prices in the\nWashington metropolitan area. Unlike static mapping tools or generic listing\nplatforms, RailEstate combines 25 years of historical housing data with transit\ninfrastructure to support low latency geospatial queries, time series\nvisualizations, and predictive modeling. Users can interactively explore ZIP\ncode level price patterns, investigate long term trends, and forecast future\nhousing values around any metro station. A key innovation is our natural\nlanguage chatbot, which translates plain-English questions e.g., What is the\nhighest price in Falls Church in the year 2000? into executable SQL over a\nspatial database. This unified and interactive platform empowers urban\nplanners, investors, and residents to derive actionable insights from metro\nlinked housing data without requiring technical expertise."}
{"id": "2511.01376", "pdf": "https://arxiv.org/pdf/2511.01376", "abs": "https://arxiv.org/abs/2511.01376", "authors": ["Jialong Zhou", "Ben Bals", "Matei Tinca", "Ai Guan", "Panagiotis Charalampopoulos", "Grigorios Loukides", "Solon P. Pissis"], "title": "Subtree Mode and Applications", "categories": ["cs.DS", "cs.DB"], "comment": "For reproduction, code available at\n  https://github.com/JialongZhou666/subtree-mode-mining", "summary": "The mode of a collection of values (i.e., the most frequent value in the\ncollection) is a key summary statistic. Finding the mode in a given range of an\narray of values is thus of great importance, and constructing a data structure\nto solve this problem is in fact the well-known Range Mode problem. In this\nwork, we introduce the Subtree Mode (SM) problem, the analogous problem in a\nleaf-colored tree, where the task is to compute the most frequent color in the\nleaves of the subtree of a given node. SM is motivated by several applications\nin domains such as text analytics and biology, where the data are hierarchical\nand can thus be represented as a (leaf-colored) tree. Our central contribution\nis a time-optimal algorithm for SM that computes the answer for every node of\nan input $N$-node tree in $O(N)$ time. We further show how our solution can be\nadapted for node-colored trees, or for computing the $k$ most frequent colors,\nin the optimal $O(N)$ time, for any given $k=O(1)$. Moreover, we prove that a\nsimilarly fast solution for when the input is a sink-colored directed acyclic\ngraph instead of a leaf-colored tree is highly unlikely. Our experiments on\nreal datasets with trees of up to 7.3 billion nodes demonstrate that our\nalgorithm is faster than baselines by at least one order of magnitude and much\nmore space efficient. Last, we present case studies showing the effectiveness\nof our approach in pattern mining and sequence-to-database search applications."}
{"id": "2511.01843", "pdf": "https://arxiv.org/pdf/2511.01843", "abs": "https://arxiv.org/abs/2511.01843", "authors": ["Andrew Goodng", "Kevin Porter", "Thomas Lopatic", "Ashish Shinde", "Sunil Sayyaparaju", "Srinivasan Seshadri", "V. Srinivasan"], "title": "LARK - Linearizability Algorithms for Replicated Keys in Aerospike", "categories": ["cs.DC", "cs.DB"], "comment": "Submitted to Industry Track of a Database Conference", "summary": "We present LARK (Linearizability Algorithms for Replicated Keys), a\nsynchronous replication protocol that achieves linearizability while minimizing\nlatency and infrastructure cost, at significantly higher availability than\ntraditional quorum-log consensus. LARK introduces Partition Availability\nConditions (PAC) that reason over the entire database cluster rather than fixed\nreplica sets, improving partition availability under independent failures by\nroughly 3x when tolerating one failure and 10x when tolerating two. Unlike\nRaft, Paxos, and Viewstamped Replication, LARK eliminates ordered logs,\nenabling immediate partition readiness after leader changes -- with at most a\nper-key duplicate-resolution round trip when the new leader lacks the latest\ncopy. Under equal storage budgets -- where both systems maintain only f+1 data\ncopies to tolerate f failures -- LARK continues committing through data-node\nfailures while log-based protocols must pause commits for replica rebuilding.\nThese properties also enable zero-downtime rolling restarts even when\nmaintaining only two copies. We provide formal safety arguments and a TLA+\nspecification, and we demonstrate through analysis and experiments that LARK\nachieves significant availability gains."}
