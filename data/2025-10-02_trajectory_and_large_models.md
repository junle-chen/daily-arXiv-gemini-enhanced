# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-10-02

## 目录

- [人工智能 (Artificial Intelligence) (3)](#cs-ai)
- [计算机视觉 (Computer Vision) (4)](#cs-cv)
- [机器学习 (Machine Learning) (1)](#cs-lg)
- [机器人学 (Robotics) (12)](#cs-ro)

## 人工智能 (Artificial Intelligence) [cs.AI]
### [1] [ATLAS: Constraints-Aware Multi-Agent Collaboration for Real-World Travel Planning](https://arxiv.org/abs/2509.25586)
*Jihye Choi, Jinsung Yoon, Jiefeng Chen, Somesh Jha, Tomas Pfister*

Main category: cs.AI

TL;DR: 该论文提出了ATLAS框架，通过动态约束管理、迭代计划评估和自适应交错搜索，有效解决了现实世界旅行规划中复杂约束感知的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在复杂约束下生成最优方案时存在不足，尤其是在现实世界的旅行规划中，需要处理显式、隐式以及随环境和用户需求变化的约束。

Method: 论文提出了ATLAS框架，包含动态约束管理、迭代计划评估和自适应交错搜索等机制，以解决约束感知规划的根本挑战。

Result: ATLAS在TravelPlanner基准测试中取得了最佳性能，最终通过率从23.3%提高到44.4%，并在真实旅行规划任务中实现了84%的最终通过率，显著优于ReAct (59%) 和单体Agent (27%)。

Conclusion: ATLAS框架在现实世界旅行规划任务中表现出卓越的整体规划性能，证明了其在处理复杂约束方面的有效性。

Abstract: 尽管大型语言模型在推理和工具使用方面取得了显著进展，但它们在复杂约束下生成最优、可行的解决方案时常常失败。现实世界的旅行规划就是一个典型的例子，它评估了Agent处理约束的能力，这些约束包括显式约束、隐式约束，甚至是在与动态环境和用户需求交互时不断演变的约束。在本文中，我们提出了ATLAS，一个通用的多Agent框架，旨在有效地处理现实世界旅行规划任务中这种复杂性质的约束感知。ATLAS引入了一种原则性的方法，通过动态约束管理、迭代计划评估和自适应交错搜索等专用机制，来解决约束感知规划的根本挑战。ATLAS在TravelPlanner基准测试中表现出了最先进的性能，将其最佳替代方案的最终通过率从23.3%提高到44.4%。更重要的是，我们的工作首次展示了在具有实时信息搜索和多轮反馈的真实世界旅行规划任务中的定量有效性。在这种真实的设置中，ATLAS展示了其卓越的整体规划性能，实现了84%的最终通过率，显著优于包括ReAct (59%) 和单体Agent (27%)在内的基线。

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25586) | **Categories:** cs.AI, cs.CL

---

### [2] [SynthPert: Enhancing LLM Biological Reasoning via Synthetic Reasoning Traces for Cellular Perturbation Prediction](https://arxiv.org/abs/2509.25346)
*Lawrence Phillips, Marc Boubnovski Martell, Aditya Misra, Josefa Lia Stoisser, Cesar A. Prada-Medina, Rory Donovan-Maiye, Kaspar Märtens*

Main category: cs.AI

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Predicting cellular responses to genetic perturbations represents a fundamental challenge in systems biology, critical for advancing therapeutic discovery and virtual cell modeling. While large language models (LLMs) show promise for biological reasoning, their application to perturbation prediction remains underexplored due to challenges in adapting them to structured experimental data. We present SynthPert, a novel method that enhances LLM performance through supervised fine-tuning on synthetic reasoning traces generated by frontier models. Using the PerturbQA benchmark, we demonstrate that our approach not only achieves state-of-the-art performance but surpasses the capabilities of the frontier model that generated the training data. Our results reveal three key insights: (1) Synthetic reasoning traces effectively distill biological knowledge even when partially inaccurate, (2) This approach enables cross-cell-type generalization with 87% accuracy on unseen RPE1 cells, and (3) Performance gains persist despite using only 2% of quality-filtered training data. This work shows the effectiveness of synthetic reasoning distillation for enhancing domain-specific reasoning in LLMs.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25346) | **Categories:** cs.AI, cs.LG, q-bio.CB, q-bio.GN

---

### [3] [Building the EHR Foundation Model via Next Event Prediction](https://arxiv.org/abs/2509.25591)
*Zekai Chen, Arda Pekis, Kevin Brown*

Main category: cs.AI

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Electronic Health Records (EHRs) contain rich temporal dynamics that conventional encoding approaches fail to adequately capture. While Large Language Models (LLMs) show promise for EHR modeling, they struggle to reason about sequential clinical events and temporal dependencies. We propose Next Event Prediction (NEP), a framework that enhances LLMs' temporal reasoning through autoregressive fine-tuning on clinical event sequences. By reformulating EHRs as timestamped event chains and predicting future medical events, NEP explicitly models disease progression patterns and causal relationships. Extensive evaluations across oncology survival prediction and clinical diagnosis tasks demonstrate NEP's superiority, outperforming specialized EHR models by 4.6% AUROC and general-purpose LLMs by 7.2% C-index in temporal reasoning tasks. Our analyses reveal dual benefits: state-of-the-art prediction accuracy combined with clinically interpretable attention patterns that align with known disease pathways.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25591) | **Categories:** cs.AI, cs.CL, q-bio.OT

---


## 计算机视觉 (Computer Vision) [cs.CV]
### [1] [Point-It-Out: Benchmarking Embodied Reasoning for Vision Language Models in Multi-Stage Visual Grounding](https://arxiv.org/abs/2509.25794)
*Haotian Xue, Yunhao Ge, Yu Zeng, Zhaoshuo Li, Ming-Yu Liu, Yongxin Chen, Jiaojiao Fan*

Main category: cs.CV

TL;DR: 本文提出了Point-It-Out (PIO) 基准，用于系统评估视觉语言模型通过精确视觉定位进行具身推理的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要通过基于图像注释的多项选择题评估视觉语言模型的具身推理能力，缺乏通过精确视觉定位进行系统评估的基准。

Method: 提出了一个分层评估协议，包括三个阶段：S1（参考对象定位）、S2（任务驱动指向）和S3（视觉轨迹预测），数据来自室内、厨房、驾驶和机器人操作等关键领域。

Result: 对十多个最先进的视觉语言模型进行了广泛的实验，发现GPT-4o等通用模型在许多基准测试中表现出色，但在精确视觉定位方面不如一些开源模型；MoLMO等模型在S1和S2中表现良好，但在需要定位和视觉轨迹规划的S3中表现不佳。

Conclusion: Point-It-Out基准可以有效评估视觉语言模型通过精确视觉定位进行具身推理的能力，并揭示了现有模型在不同阶段的优缺点。

Abstract: 视觉语言模型（VLM）在各种任务中展现出了令人印象深刻的世界知识，使其成为具身推理应用的有希望的候选者。然而，现有的基准主要通过基于图像注释的多项选择题来评估VLM的具身推理能力——例如，选择哪个轨迹更好地描述了图像中的事件。在这项工作中，我们引入了Point-It-Out（PIO）基准，这是一个新的基准，旨在通过精确的视觉定位来系统地评估VLM的具身推理能力。我们提出了一个分层评估协议，包括三个阶段（S1：参考对象定位，S2：任务驱动指向，S3：视觉轨迹预测），数据来自具身智能的关键领域，包括室内、厨房、驾驶和机器人操作场景。对十多个最先进的VLM进行了广泛的实验，揭示了几个有趣的发现。例如，GPT-4o等强大的通用模型虽然在许多基准测试（例如，语言、感知和推理）中表现出色，但在精确的视觉定位方面不如一些开源模型；MoLMO等模型在S1和S2中表现良好，但在S3中表现不佳，而S3需要定位和视觉轨迹规划。

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25794) | **Categories:** cs.CV, cs.AI

---

### [2] [LUMA: Low-Dimension Unified Motion Alignment with Dual-Path Anchoring for Text-to-Motion Diffusion Model](https://arxiv.org/abs/2509.25304)
*Haozhe Jia, Wenshuo Chen, Yuqi Lin, Yang Yang, Lei Wang, Mang Ning, Bowen Tian, Songning Lai, Nanqian Jia, Yifan Chen, Yutao Yue*

Main category: cs.CV

TL;DR: LUMA提出了一种双路径锚定的文本到动作扩散模型，通过在时域和频域中引入对齐信号，显著提升了语义对齐并加速了收敛。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的文本到动作生成方法存在语义不对齐和运动伪影问题，其根本原因是网络深层梯度衰减导致高层特征学习不足。

Method: LUMA模型采用双路径锚定，一条路径利用轻量级MoCLIP模型在时域提供语义监督，另一条路径从低频DCT分量中提取频域对齐信号，并通过时间调制机制自适应融合。

Result: 在HumanML3D和KIT-ML数据集上，LUMA取得了state-of-the-art的性能，FID分数分别为0.035和0.123，并且比基线模型加速了1.4倍的收敛。

Conclusion: LUMA模型通过双路径锚定有效解决了文本到动作生成中的语义对齐问题，并提高了生成效率，为高保真文本到动作生成提供了一个高效且可扩展的解决方案。

Abstract: 目前基于扩散的模型（通常建立在U-Net架构上）在文本到动作生成任务中显示出有希望的结果，但它们仍然受到语义不对齐和运动伪影的影响。通过分析，我们发现网络深层中严重的梯度衰减是关键瓶颈，导致高层特征学习不足。为了解决这个问题，我们提出了LUMA（低维度统一运动对齐），这是一种文本到动作的扩散模型，它结合了双路径锚定来增强语义对齐。第一条路径结合了一个轻量级的MoCLIP模型，该模型通过对比学习进行训练，无需依赖外部数据，从而在时域中提供语义监督。第二条路径在频域中引入了互补的对齐信号，这些信号是从以其丰富的语义内容而闻名的低频DCT分量中提取的。这两个锚通过时间调制机制自适应地融合，使模型能够在整个去噪过程中逐步从粗略对齐过渡到细粒度语义细化。在HumanML3D和KIT-ML上的实验结果表明，LUMA实现了最先进的性能，FID分数分别为0.035和0.123。此外，与基线相比，LUMA将收敛速度提高了1.4倍，使其成为高保真文本到运动生成的高效且可扩展的解决方案。

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25304) | **Categories:** cs.CV

---

### [3] [Infrastructure Sensor-enabled Vehicle Data Generation using Multi-Sensor Fusion for Proactive Safety Applications at Work Zone](https://arxiv.org/abs/2509.25452)
*Suhala Rabab Saba, Sakib Khan, Minhaj Uddin Ahmad, Jiahe Cao, Mizanur Rahman, Li Zhao, Nathan Huynh, Eren Erman Ozguven*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Infrastructure-based sensing and real-time trajectory generation show promise for improving safety in high-risk roadway segments such as work zones, yet practical deployments are hindered by perspective distortion, complex geometry, occlusions, and costs. This study tackles these barriers by integrating roadside camera and LiDAR sensors into a cosimulation environment to develop a scalable, cost-effective vehicle detection and localization framework, and employing a Kalman Filter-based late fusion strategy to enhance trajectory consistency and accuracy. In simulation, the fusion algorithm reduced longitudinal error by up to 70 percent compared to individual sensors while preserving lateral accuracy within 1 to 3 meters. Field validation in an active work zone, using LiDAR, a radar-camera rig, and RTK-GPS as ground truth, demonstrated that the fused trajectories closely match real vehicle paths, even when single-sensor data are intermittent or degraded. These results confirm that KF based sensor fusion can reliably compensate for individual sensor limitations, providing precise and robust vehicle tracking capabilities. Our approach thus offers a practical pathway to deploy infrastructure-enabled multi-sensor systems for proactive safety measures in complex traffic environments.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25452) | **Categories:** cs.CV, cs.RO

---

### [4] [LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models](https://arxiv.org/abs/2509.25528)
*Pranav Saxena, Avigyan Bhattacharya, Ji Zhang, Wenshan Wang*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Referential grounding in outdoor driving scenes is challenging due to large scene variability, many visually similar objects, and dynamic elements that complicate resolving natural-language references (e.g., "the black car on the right"). We propose LLM-RG, a hybrid pipeline that combines off-the-shelf vision-language models for fine-grained attribute extraction with large language models for symbolic reasoning. LLM-RG processes an image and a free-form referring expression by using an LLM to extract relevant object types and attributes, detecting candidate regions, generating rich visual descriptors with a VLM, and then combining these descriptors with spatial metadata into natural-language prompts that are input to an LLM for chain-of-thought reasoning to identify the referent's bounding box. Evaluated on the Talk2Car benchmark, LLM-RG yields substantial gains over both LLM and VLM-based baselines. Additionally, our ablations show that adding 3D spatial cues further improves grounding. Our results demonstrate the complementary strengths of VLMs and LLMs, applied in a zero-shot manner, for robust outdoor referential grounding.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25528) | **Categories:** cs.CV, cs.AI, cs.RO

---


## 机器学习 (Machine Learning) [cs.LG]
### [1] [AMLA: MUL by ADD in FlashAttention Rescaling](https://arxiv.org/abs/2509.25224)
*Qichen Liao, Chengqiu Hu, Fangzheng Miao, Bao Li, Yiyang Liu, Junlong Lyu, Lirui Jiang, Jun Wang, Lingchao Zheng, Jun Li, Yuwei Fan*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage in Large Language Models while introducing substantial computational overhead and intermediate variable expansion. This poses challenges for efficient hardware implementation -- especially during the decode phase. This paper introduces Ascend MLA (AMLA), a high-performance kernel specifically optimized for Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel FlashAttention-based algorithm that replaces floating-point multiplications with integer additions for output block rescaling, leveraging binary correspondence between FP32 and INT32 representations; (2) A Preload Pipeline strategy with hierarchical tiling that maximizes FLOPS utilization: the Preload Pipeline achieves Cube-bound performance, while hierarchical tiling overlaps data movement and computation within the Cube core. Experiments show that on Ascend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS, reaching 86.8% of the theoretical maximum FLOPS, outperforming the state-of-the-art open-source FlashMLA implementation, whose FLOPS utilization is up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into Huawei's CANN and will be released soon.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25224) | **Categories:** cs.LG

---


## 机器人学 (Robotics) [cs.RO]
### [1] [BEV-VLM: Trajectory Planning via Unified BEV Abstraction](https://arxiv.org/abs/2509.25249)
*Guancheng Chen, Sheng Yang, Tong Zhan, Jian Wang*

Main category: cs.RO

TL;DR: BEV-VLM 通过结合视觉语言模型与鸟瞰图特征，显著提升了自动驾驶轨迹规划的准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 传统自动驾驶轨迹规划依赖原始视觉数据，缺乏几何一致性和场景丰富性。

Method: 提出 BEV-VLM 框架，利用融合多模传感器数据和高清地图的鸟瞰图特征作为视觉语言模型的输入。

Result: 在 nuScenes 数据集上，规划准确率提升 44.8%，并实现了完全的碰撞避免。

Conclusion: 视觉语言模型可以有效解释经过处理的视觉表征，扩展其在轨迹规划中超越原始图像的应用。

Abstract: 本文介绍了一种名为 BEV-VLM 的新型框架，用于自动驾驶中的轨迹规划。该框架利用视觉语言模型（VLM），并将鸟瞰图（BEV）特征图作为视觉输入。与仅依赖原始视觉数据（如相机图像）的传统方法不同，我们的方法利用高度压缩且信息丰富的 BEV 表征，这些表征通过融合多模传感器数据（例如，相机和激光雷达）并将它们与高清地图对齐来生成。这种统一的 BEV-HD 地图格式提供了几何上一致且丰富的场景描述，使 VLM 能够执行准确的轨迹规划。在 nuScenes 数据集上的实验结果表明，规划准确性提高了 44.8%，并完全避免了碰撞。我们的工作强调，VLM 可以有效地解释经过处理的视觉表征（如 BEV 特征），从而扩大了它们在轨迹规划中超越原始图像的适用性。

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25249) | **Categories:** cs.RO, cs.AI

---

### [2] [LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration and Search](https://arxiv.org/abs/2509.26324)
*Ruiyang Wang, Haolun Tsu, David Hunt, Shaocheng Luo, Jiwoo Kim, Miroslav Pajic*

Main category: cs.RO

TL;DR: 该论文提出了一种基于大型语言模型（LLM）的多机器人协同探索和搜索框架LLM-MCoX，能有效提升自主探索和目标搜索效率。


<details>
  <summary>Details</summary>
Motivation: 传统多机器人系统在未知室内环境中的自主探索和目标搜索面临挑战，现有方法通常依赖贪婪策略，缺乏有效的机器人间协作。

Method: 该方法结合了实时激光雷达扫描处理、多模态LLM推理（如GPT-4o）以及共享环境地图和机器人状态，以生成协同航路点分配。

Result: 实验结果表明，LLM-MCoX相比于贪婪和Voronoi规划器，在大型环境中拥有6个机器人时，探索速度提高了22.7%，搜索效率提高了50%。

Conclusion: LLM-MCoX 实现了基于自然语言的目标搜索能力，允许人类操作员提供高级语义指导，这是传统算法无法解释的。

Abstract: 多机器人系统在未知室内环境中的自主探索和目标搜索仍然具有挑战性。传统方法通常依赖于贪婪的前沿分配策略，机器人之间的协同有限。在这项工作中，我们介绍了一种基于LLM的多机器人协同探索和搜索框架LLM-MCoX，该框架利用大型语言模型（LLM）来智能地协调同构和异构机器人团队，以实现高效的探索和目标对象搜索。我们的方法结合了用于前沿聚类提取和门检测的实时激光雷达扫描处理与多模态LLM推理（例如GPT-4o），以基于共享环境地图和机器人状态生成协调的航路点分配。LLM-MCoX与现有方法（包括基于贪婪和Voronoi的规划器）相比，表现出卓越的性能，在具有6个机器人的大型环境中，探索时间缩短了22.7％，搜索效率提高了50％。值得注意的是，LLM-MCoX 实现了基于自然语言的目标搜索能力，从而允许人类操作员提供传统算法无法解释的高级语义指导。

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.26324) | **Categories:** cs.RO, cs.AI, cs.MA

---

### [3] [dVLA: Diffusion Vision-Language-Action Model with Multimodal Chain-of-Thought](https://arxiv.org/abs/2509.25681)
*Junjie Wen, Minjie Zhu, Jiaming Liu, Zhiyuan Liu, Yicun Yang, Linfeng Zhang, Shanghang Zhang, Yichen Zhu, Yi Xu*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies, a prefix attention mask and KV caching, yielding up to around times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark, it achieves state-of-the-art performance with a 96.4% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25681) | **Categories:** cs.RO, cs.CV

---

### [4] [Hierarchical Diffusion Motion Planning with Task-Conditioned Uncertainty-Aware Priors](https://arxiv.org/abs/2509.25685)
*Amelie Minji Kim, Anqi Wu, Ye Zhao*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: We propose a novel hierarchical diffusion planner that embeds task and motion structure directly in the noise model. Unlike standard diffusion-based planners that use zero-mean, isotropic Gaussian noise, we employ a family of task-conditioned structured Gaussians whose means and covariances are derived from Gaussian Process Motion Planning (GPMP): sparse, task-centric key states or their associated timings (or both) are treated as noisy observations to produce a prior instance. We first generalize the standard diffusion process to biased, non-isotropic corruption with closed-form forward and posterior expressions. Building on this, our hierarchy separates prior instantiation from trajectory denoising: the upper level instantiates a task-conditioned structured Gaussian (mean and covariance), and the lower level denoises the full trajectory under that fixed prior. Experiments on Maze2D goal-reaching and KUKA block stacking show improved success rates, smoother trajectories, and stronger task alignment compared to isotropic baselines. Ablation studies indicate that explicitly structuring the corruption process offers benefits beyond simply conditioning the neural network. Overall, our method concentrates probability mass of prior near feasible, smooth, and semantically meaningful trajectories while maintaining tractability. Our project page is available at https://hta-diffusion.github.io.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25685) | **Categories:** cs.RO

---

### [5] [OmniNav: A Unified Framework for Prospective Exploration and Visual-Language Navigation](https://arxiv.org/abs/2509.25687)
*Xinda Xue, Junjun Hu, Minghua Luo, Xie Shichao, Jintao Chen, Zixun Xie, Quan Kuichen, Guo Wei, Mu Xu, Zedong Chu*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Embodied navigation presents a core challenge for intelligent robots, requiring the comprehension of visual environments, natural language instructions, and autonomous exploration. Existing models often fall short in offering a unified solution across diverse navigation paradigms, resulting in low success rates and limited generalization. We introduce OmniNav, a unified framework addressing instruct-goal, object-goal, point-goal navigation, and frontier-based exploration within a single architecture. Our approach features a lightweight, low-latency policy that accurately predicts continuous-space waypoints (coordinates and orientations). This policy surpasses action-chunk methods in precision and supports real-world deployment at control frequencies up to 5 Hz. Architecturally, OmniNav employs a fast-slow system design: a fast module generates waypoints using short-horizon visual context and subtasks, while a slow module performs deliberative planning with long-horizon observations and candidate frontiers to select subsequent subgoals and subtasks. This collaboration enhances path efficiency and maintains trajectory coherence, particularly in exploration and memory-intensive scenarios. Crucially, we identify that the primary bottleneck isn't merely navigation policy learning, but a robust understanding of general instructions and objects. To boost generalization, OmniNav integrates large-scale, general-purpose training datasets, including those for image captioning and visual recognition, into a joint multi-task regimen. This significantly improves success rates and robustness. Extensive experiments confirm OmniNav's state-of-the-art performance across various navigation benchmarks, with real-world deployment further validating its efficacy. OmniNav provides practical insights for embodied navigation, charting a scalable path towards versatile, highly generalizable robotic intelligence.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25687) | **Categories:** cs.RO

---

### [6] [Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies](https://arxiv.org/abs/2509.25822)
*Jing Wang, Weiting Peng, Jing Tang, Zeyu Gong, Xihua Wang, Bo Tao, Li Cheng*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Existing imitation learning methods decouple perception and action, which overlooks the causal reciprocity between sensory representations and action execution that humans naturally leverage for adaptive behaviors. To bridge this gap, we introduce Action--Guided Diffusion Policy (DP--AG), a unified representation learning that explicitly models a dynamic interplay between perception and action through probabilistic latent dynamics. DP--AG encodes latent observations into a Gaussian posterior via variational inference and evolves them using an action-guided SDE, where the Vector-Jacobian Product (VJP) of the diffusion policy's noise predictions serves as a structured stochastic force driving latent updates. To promote bidirectional learning between perception and action, we introduce a cycle--consistent contrastive loss that organizes the gradient flow of the noise predictor into a coherent perception--action loop, enforcing mutually consistent transitions in both latent updates and action refinements. Theoretically, we derive a variational lower bound for the action-guided SDE, and prove that the contrastive objective enhances continuity in both latent and action trajectories. Empirically, DP--AG significantly outperforms state--of--the--art methods across simulation benchmarks and real-world UR5 manipulation tasks. As a result, our DP--AG offers a promising step toward bridging biological adaptability and artificial policy learning.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25822) | **Categories:** cs.RO

---

### [7] [MUVLA: Learning to Explore Object Navigation via Map Understanding](https://arxiv.org/abs/2509.25966)
*Peilong Han, Fan Jia, Min Zhang, Yutao Qiu, Hongyao Tang, Yan Zheng, Tiancai Wang, Jianye Hao*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: In this paper, we present MUVLA, a Map Understanding Vision-Language-Action model tailored for object navigation. It leverages semantic map abstractions to unify and structure historical information, encoding spatial context in a compact and consistent form. MUVLA takes the current and history observations, as well as the semantic map, as inputs and predicts the action sequence based on the description of goal object. Furthermore, it amplifies supervision through reward-guided return modeling based on dense short-horizon progress signals, enabling the model to develop a detailed understanding of action value for reward maximization. MUVLA employs a three-stage training pipeline: learning map-level spatial understanding, imitating behaviors from mixed-quality demonstrations, and reward amplification. This strategy allows MUVLA to unify diverse demonstrations into a robust spatial representation and generate more rational exploration strategies. Experiments on HM3D and Gibson benchmarks demonstrate that MUVLA achieves great generalization and learns effective exploration behaviors even from low-quality or partially successful trajectories.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.25966) | **Categories:** cs.RO

---

### [8] [SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task Planning](https://arxiv.org/abs/2509.26375)
*Zichao Shen, Chen Gao, Jiaqi Yuan, Tianchen Zhu, Xingcheng Fu, Qingyun Sun*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Embodied task planning requires agents to produce executable actions in a close-loop manner within the environment. With progressively improving capabilities of LLMs in task decomposition, planning, and generalization, current embodied task planning methods adopt LLM-based architecture.However, existing LLM-based planners remain limited in three aspects, i.e., fixed planning paradigms, lack of action sequence constraints, and error-agnostic. In this work, we propose SDA-PLANNER, enabling an adaptive planning paradigm, state-dependency aware and error-aware mechanisms for comprehensive embodied task planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to explicitly model action preconditions and effects, guiding the dynamic revision. To handle execution error, it employs an error-adaptive replanning strategy consisting of Error Backtrack and Diagnosis and Adaptive Action SubTree Generation, which locally reconstructs the affected portion of the plan based on the current environment state. Experiments demonstrate that SDA-PLANNER consistently outperforms baselines in success rate and goal completion, particularly under diverse error conditions.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.26375) | **Categories:** cs.RO, cs.AI, cs.CV

---

### [9] [Real-time Velocity Profile Optimization for Time-Optimal Maneuvering with Generic Acceleration Constraints](https://arxiv.org/abs/2509.26428)
*Mattia Piazza, Mattia Piccinini, Sebastiano Taddei, Francesco Biral, Enrico Bertolazzi*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: The computation of time-optimal velocity profiles along prescribed paths, subject to generic acceleration constraints, is a crucial problem in robot trajectory planning, with particular relevance to autonomous racing. However, the existing methods either support arbitrary acceleration constraints at high computational cost or use conservative box constraints for computational efficiency. We propose FBGA, a new \underline{F}orward-\underline{B}ackward algorithm with \underline{G}eneric \underline{A}cceleration constraints, which achieves both high accuracy and low computation time. FBGA operates forward and backward passes to maximize the velocity profile in short, discretized path segments, while satisfying user-defined performance limits. Tested on five racetracks and two vehicle classes, FBGA handles complex, non-convex acceleration constraints with custom formulations. Its maneuvers and lap times closely match optimal control baselines (within $0.11\%$-$0.36\%$), while being up to three orders of magnitude faster. FBGA maintains high accuracy even with coarse discretization, making it well-suited for online multi-query trajectory planning. Our open-source \texttt{C++} implementation is available at: https://anonymous.4open.science/r/FB_public_RAL.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.26428) | **Categories:** cs.RO, cs.SY, eess.SY, math.OC

---

### [10] [Analytic Conditions for Differentiable Collision Detection in Trajectory Optimization](https://arxiv.org/abs/2509.26459)
*Akshay Jaitly, Devesh K. Jha, Kei Ota, Yuki Shirai*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Optimization-based methods are widely used for computing fast, diverse solutions for complex tasks such as collision-free movement or planning in the presence of contacts. However, most of these methods require enforcing non-penetration constraints between objects, resulting in a non-trivial and computationally expensive problem. This makes the use of optimization-based methods for planning and control challenging. In this paper, we present a method to efficiently enforce non-penetration of sets while performing optimization over their configuration, which is directly applicable to problems like collision-aware trajectory optimization. We introduce novel differentiable conditions with analytic expressions to achieve this. To enforce non-collision between non-smooth bodies using these conditions, we introduce a method to approximate polytopes as smooth semi-algebraic sets. We present several numerical experiments to demonstrate the performance of the proposed method and compare the performance with other baseline methods recently proposed in the literature.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.26459) | **Categories:** cs.RO, cs.CG

---

### [11] [Learning from Hallucinating Critical Points for Navigation in Dynamic Environments](https://arxiv.org/abs/2509.26513)
*Saad Abdul Ghani, Kameron Lee, Xuesu Xiao*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Generating large and diverse obstacle datasets to learn motion planning in environments with dynamic obstacles is challenging due to the vast space of possible obstacle trajectories. Inspired by hallucination-based data synthesis approaches, we propose Learning from Hallucinating Critical Points (LfH-CP), a self-supervised framework for creating rich dynamic obstacle datasets based on existing optimal motion plans without requiring expensive expert demonstrations or trial-and-error exploration. LfH-CP factorizes hallucination into two stages: first identifying when and where obstacles must appear in order to result in an optimal motion plan, i.e., the critical points, and then procedurally generating diverse trajectories that pass through these points while avoiding collisions. This factorization avoids generative failures such as mode collapse and ensures coverage of diverse dynamic behaviors. We further introduce a diversity metric to quantify dataset richness and show that LfH-CP produces substantially more varied training data than existing baselines. Experiments in simulation demonstrate that planners trained on LfH-CP datasets achieves higher success rates compared to a prior hallucination method.

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.26513) | **Categories:** cs.RO

---

### [12] [MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation](https://arxiv.org/abs/2509.26642)
*Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, Zhengping Che, Jian Tang, Shanghang Zhang*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLA's understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla

</details>

[**[PDF]**](https://arxiv.org/pdf/2509.26642) | **Categories:** cs.RO

---
