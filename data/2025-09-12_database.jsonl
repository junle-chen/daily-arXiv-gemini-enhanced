{"id": "2509.08014", "pdf": "https://arxiv.org/pdf/2509.08014", "abs": "https://arxiv.org/abs/2509.08014", "authors": ["Festim Halili", "Anila Nuhiji", "Diellza Mustafai Veliu"], "title": "Polyglot Persistence in Microservices: Managing Data Diversity in Distributed Systems", "categories": ["cs.DB"], "comment": null, "summary": "Microservices architectures have become the foundation for developing\nscalable and modern software systems, but they also bring significant\nchallenges in managing heterogeneous and distributed data. The pragmatic\nsolution is polyglot persistence, the deliberate use of several different\ndatabase technologies adapted to a given microservice requirement - is one such\nstrategy. This paper examines polyglot persistence in microservice based\nsystems. This paper brings together theoretical concepts with evidence from\npractical implementations and comparative benchmarks of standard database\nplatforms. A comparative framework is applied to relational, document,\nkey-value, column-family and graph databases to assess scalability,\nconsistency, query expressiveness, operational overhead and integration ease.\nEmpirical data drawn from industry case studies such as Netflix, Uber, and\nShopify, and survey data illustrate real-life adoption trends and challenges.\nThese findings demonstrate that polyglot persistence increases adaptability ,\nperformance , domain alignment but also governance or operational complexity.\nTo cope with such trade-offs, architectural patterns such as saga workflows,\nevent sourcing, and outbox integration are discussed."}
{"id": "2509.08387", "pdf": "https://arxiv.org/pdf/2509.08387", "abs": "https://arxiv.org/abs/2509.08387", "authors": ["Leilei Du", "Peng Cheng", "Lei Chen", "Heng Tao Shen", "Xuemin Lin", "Wei Xi"], "title": "Infinite Stream Estimation under Personalized $w$-Event Privacy", "categories": ["cs.DB"], "comment": "15 pages", "summary": "Streaming data collection is indispensable for stream data analysis, such as\nevent monitoring. However, publishing these data directly leads to privacy\nleaks. $w$-event privacy is a valuable tool to protect individual privacy\nwithin a given time window while maintaining high accuracy in data collection.\nMost existing $w$-event privacy studies on infinite data stream only focus on\nhomogeneous privacy requirements for all users. In this paper, we propose\npersonalized $w$-event privacy protection that allows different users to have\ndifferent privacy requirements in private data stream estimation. Specifically,\nwe design a mechanism that allows users to maintain constant privacy\nrequirements at each time slot, namely Personalized Window Size Mechanism\n(PWSM). Then, we propose two solutions to accurately estimate stream data\nstatistics while achieving $w$-event level $\\epsilon$ personalized differential\nprivacy ( ($w$, $\\epsilon$)-EPDP), namely Personalized Budget Distribution\n(PBD) and Peronalized Budget Absorption (PBA). PBD always provides at least the\nsame privacy budget for the next time step as the amount consumed in the\nprevious release. PBA fully absorbs the privacy budget from the previous $k$\ntime slots, while also borrowing from the privacy budget of the next $k$ time\nslots, to increase the privacy budget for the current time slot. We prove that\nboth PBD and PBA outperform the state-of-the-art private stream estimation\nmethods while satisfying the privacy requirements of all users. We demonstrate\nthe efficiency and effectiveness of our PBD and PBA on both real and synthetic\ndata sets, compared with the recent uniformity $w$-event approaches, Budget\nDistribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error\nthan BD on average on real data sets. Besides, our PBA achieves 24.9% less\nerror than BA on average on synthetic data sets."}
{"id": "2509.08395", "pdf": "https://arxiv.org/pdf/2509.08395", "abs": "https://arxiv.org/abs/2509.08395", "authors": ["Ruoxuan Li", "Xiaoyao Zhong", "Jiabao Jin", "Peng Cheng", "Wangze Ni", "Lei Chen", "Zhitao Shen", "Wei Jia", "Xiangyu Wang", "Xuemin Lin", "Heng Tao Shen", "Jingkuan Song"], "title": "SINDI: an Efficient Index for Approximate Maximum Inner Product Search on Sparse Vectors", "categories": ["cs.DB"], "comment": "13 pages, submitted to VLDB 2026", "summary": "Sparse vector Maximum Inner Product Search (MIPS) is crucial in multi-path\nretrieval for Retrieval-Augmented Generation (RAG). Recent inverted index-based\nand graph-based algorithms have achieved high search accuracy with practical\nefficiency. However, their performance in production environments is often\nlimited by redundant distance computations and frequent random memory accesses.\nFurthermore, the compressed storage format of sparse vectors hinders the use of\nSIMD acceleration. In this paper, we propose the sparse inverted non-redundant\ndistance index (SINDI), which incorporates three key optimizations: (i)\nEfficient Inner Product Computation: SINDI leverages SIMD acceleration and\neliminates redundant identifier lookups, enabling batched inner product\ncomputation; (ii) Memory-Friendly Design: SINDI replaces random memory accesses\nto original vectors with sequential accesses to inverted lists, substantially\nreducing memory-bound latency. (iii) Vector Pruning: SINDI retains only the\nhigh-magnitude non-zero entries of vectors, improving query throughput while\nmaintaining accuracy. We evaluate SINDI on multiple real-world datasets.\nExperimental results show that SINDI achieves state-of-the-art performance\nacross datasets of varying scales, languages, and models. On the MsMarco\ndataset, when Recall@50 exceeds 99%, SINDI delivers single-thread\nquery-per-second (QPS) improvements ranging from 4.2 to 26.4 times compared\nwith SEISMIC and PyANNs. Notably, SINDI has been integrated into Ant Group's\nopen-source vector search library, VSAG."}
{"id": "2509.08433", "pdf": "https://arxiv.org/pdf/2509.08433", "abs": "https://arxiv.org/abs/2509.08433", "authors": ["Jos\u00e9-Luis Vilchis Medina"], "title": "Un cadre paraconsistant pour l'{\u00e9}valuation de similarit{\u00e9} dans les bases de connaissances", "categories": ["cs.DB", "cs.IT", "cs.LO", "cs.SC", "math.CT", "math.IT"], "comment": "in French language, 19{\\`e}mes Journ{\\'e}es d'Intelligence\n  Artificielle Fondamentale et 20{\\`e}mes Journ{\\'e}es Francophones sur la\n  Planification, la D{\\'e}cision et l'Apprentissage pour la conduite de\n  syst{\\`e}mes, JIAF-JFPDA 2025, Coll{\\`e}ge Repr{\\'e}sentation et Raisonnement\n  de l'AFIA, Jul 2025, Dijon, France", "summary": "This article proposes a paraconsistent framework for evaluating similarity in\nknowledge bases. Unlike classical approaches, this framework explicitly\nintegrates contradictions, enabling a more robust and interpretable similarity\nmeasure. A new measure $ S^* $ is introduced, which penalizes inconsistencies\nwhile rewarding shared properties. Paraconsistent super-categories $ \\Xi_K^* $\nare defined to hierarchically organize knowledge entities. The model also\nincludes a contradiction extractor $ E $ and a repair mechanism, ensuring\nconsistency in the evaluations. Theoretical results guarantee reflexivity,\nsymmetry, and boundedness of $ S^* $. This approach offers a promising solution\nfor managing conflicting knowledge, with perspectives in multi-agent systems."}
{"id": "2509.08575", "pdf": "https://arxiv.org/pdf/2509.08575", "abs": "https://arxiv.org/abs/2509.08575", "authors": ["Jie Jiang", "Siqi Shen", "Haining Xie", "Yang Li", "Yu Shen", "Danqing Huang", "Bo Qian", "Yinjun Wu", "Wentao Zhang", "Bin Cui", "Peng Chen"], "title": "SQLGovernor: An LLM-powered SQL Toolkit for Real World Application", "categories": ["cs.DB"], "comment": null, "summary": "SQL queries in real world analytical environments, whether written by humans\nor generated automatically often suffer from syntax errors, inefficiency, or\nsemantic misalignment, especially in complex OLAP scenarios. To address these\nchallenges, we propose SQLGovernor, an LLM powered SQL toolkit that unifies\nmultiple functionalities, including syntax correction, query rewriting, query\nmodification, and consistency verification within a structured framework\nenhanced by knowledge management. SQLGovernor introduces a fragment wise\nprocessing strategy to enable fine grained rewriting and localized error\ncorrection, significantly reducing the cognitive load on the LLM. It further\nincorporates a hybrid self learning mechanism guided by expert feedback,\nallowing the system to continuously improve through DBMS output analysis and\nrule validation. Experiments on benchmarks such as BIRD and BIRD CRITIC, as\nwell as industrial datasets, show that SQLGovernor consistently boosts the\nperformance of base models by up to 10%, while minimizing reliance on manual\nexpertise. Deployed in production environments, SQLGovernor demonstrates strong\npractical utility and effective performance."}
{"id": "2509.08740", "pdf": "https://arxiv.org/pdf/2509.08740", "abs": "https://arxiv.org/abs/2509.08740", "authors": ["Sam Kumar", "Samyukta Yagati", "Conor Power", "David E. Culler", "Raluca Ada Popa"], "title": "Membrane: A Cryptographic Access Control System for Data Lakes", "categories": ["cs.CR", "cs.DB"], "comment": "28 pages, 25 figures", "summary": "Organizations use data lakes to store and analyze sensitive data. But hackers\nmay compromise data lake storage to bypass access controls and access sensitive\ndata. To address this, we propose Membrane, a system that (1) cryptographically\nenforces data-dependent access control views over a data lake, (2) without\nrestricting the analytical queries data scientists can run. We observe that\ndata lakes, unlike DBMSes, disaggregate computation and storage into separate\ntrust domains, making at-rest encryption sufficient to defend against remote\nattackers targeting data lake storage, even when running analytical queries in\nplaintext. This leads to a new system design for Membrane that combines\nencryption at rest with SQL-aware encryption. Using block ciphers, a fast\nsymmetric-key primitive with hardware acceleration in CPUs, we develop a new\nSQL-aware encryption protocol well-suited to at-rest encryption. Membrane adds\noverhead only at the start of an interactive session due to decrypting views,\ndelaying the first query result by up to $\\approx 20\\times$; subsequent queries\nprocess decrypted data in plaintext, resulting in low amortized overhead."}
{"id": "2509.08817", "pdf": "https://arxiv.org/pdf/2509.08817", "abs": "https://arxiv.org/abs/2509.08817", "authors": ["Tobias Winker", "Jinghua Groppe", "Sven Groppe"], "title": "QCardEst/QCardCorr: Quantum Cardinality Estimation and Correction", "categories": ["quant-ph", "cs.AI", "cs.DB", "cs.LG"], "comment": "7 pages", "summary": "Cardinality estimation is an important part of query optimization in DBMS. We\ndevelop a Quantum Cardinality Estimation (QCardEst) approach using Quantum\nMachine Learning with a Hybrid Quantum-Classical Network. We define a compact\nencoding for turning SQL queries into a quantum state, which requires only\nqubits equal to the number of tables in the query. This allows the processing\nof a complete query with a single variational quantum circuit (VQC) on current\nhardware. In addition, we compare multiple classical post-processing layers to\nturn the probability vector output of VQC into a cardinality value. We\nintroduce Quantum Cardinality Correction QCardCorr, which improves classical\ncardinality estimators by multiplying the output with a factor generated by a\nVQC to improve the cardinality estimation. With QCardCorr, we have an\nimprovement over the standard PostgreSQL optimizer of 6.37 times for JOB-light\nand 8.66 times for STATS. For JOB-light we even outperform MSCN by a factor of\n3.47."}
