{"id": "2506.13599", "pdf": "https://arxiv.org/pdf/2506.13599", "abs": "https://arxiv.org/abs/2506.13599", "authors": ["Yuwei Du", "Jie Feng", "Jian Yuan", "Yong Li"], "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility Simulation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose \\textbf{C}ityGPT-Powered\n\\textbf{A}gentic framework for \\textbf{M}obility \\textbf{S}imulation\n(\\textbf{CAMS}), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. \\textbf{CAMS}\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that \\textbf{CAMS} achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, \\textbf{CAMS} generates more realistic and\nplausible trajectories. In general, \\textbf{CAMS} establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u53ca\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08CityGPT\uff09\u8fdb\u884c\u57ce\u5e02\u4eba\u7c7b\u79fb\u52a8\u6a21\u62df\uff0c\u5373\u8f68\u8ff9\u9884\u6d4b\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "human mobility simulation", "large language models", "LLMs", "CityGPT", "agentic framework"]}, "AI": {"tldr": "CAMS \u5229\u7528\u57ce\u5e02\u77e5\u8bc6 LLM \u548c Agentic \u6846\u67b6\uff0c\u4e3a\u4eba\u7c7b\u79fb\u52a8\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5e38\u8bc6\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u6765\u52a0\u901f\u4eba\u7c7b\u79fb\u52a8\u6a21\u62df\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u57ce\u5e02\u7a7a\u95f4\u5efa\u6a21\u65b9\u9762\u4e0d\u8db3\uff0c\u5e76\u4e14\u4e0e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u548c\u96c6\u4f53\u79fb\u52a8\u5206\u5e03\u7684\u6574\u5408\u4e0d\u4f73\u3002", "method": "CAMS \u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aMobExtractor \u7528\u4e8e\u63d0\u53d6\u6a21\u677f\u79fb\u52a8\u6a21\u5f0f\u5e76\u6839\u636e\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u5408\u6210\u65b0\u7684\u6a21\u5f0f\uff1bGeoGenerator \u7528\u4e8e\u751f\u6210\u8003\u8651\u96c6\u4f53\u77e5\u8bc6\u7684\u951a\u70b9\uff0c\u5e76\u4f7f\u7528\u589e\u5f3a\u7248\u7684 CityGPT \u751f\u6210\u5019\u9009\u57ce\u5e02\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\uff1bTrajEnhancer \u7528\u4e8e\u68c0\u7d22\u57fa\u4e8e\u79fb\u52a8\u6a21\u5f0f\u7684\u7a7a\u95f4\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7 DPO \u751f\u6210\u5177\u6709\u771f\u5b9e\u8f68\u8ff9\u504f\u597d\u5bf9\u9f50\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAMS \u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u63d0\u4f9b\u7684\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u901a\u8fc7\u5168\u9762\u5730\u5efa\u6a21\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u548c\u96c6\u4f53\u79fb\u52a8\u7ea6\u675f\uff0cCAMS \u751f\u6210\u4e86\u66f4\u771f\u5b9e\u548c\u5408\u7406\u7684\u8f68\u8ff9\u3002", "conclusion": "CAMS \u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u5c06 Agentic \u6846\u67b6\u4e0e\u5177\u6709\u57ce\u5e02\u77e5\u8bc6\u7684 LLM \u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u4eba\u7c7b\u79fb\u52a8\u6a21\u62df\u3002", "summary_zh": "\u4eba\u7c7b\u79fb\u52a8\u6a21\u62df\u5728\u5404\u79cd\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u6700\u8fd1\uff0c\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u5e38\u8bc6\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u6765\u52a0\u901f\u4eba\u7c7b\u79fb\u52a8\u6a21\u62df\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5b58\u5728\u51e0\u4e2a\u5173\u952e\u7f3a\u9677\uff0c\u5305\u62ec\u57ce\u5e02\u7a7a\u95f4\u5efa\u6a21\u4e0d\u8db3\uff0c\u4ee5\u53ca\u4e0e\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u548c\u96c6\u4f53\u79fb\u52a8\u5206\u5e03\u7684\u6574\u5408\u4e0d\u4f73\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 CAMS\uff0c\u8fd9\u662f\u4e00\u4e2a Agentic \u6846\u67b6\uff0c\u5b83\u5229\u7528\u57fa\u4e8e\u8bed\u8a00\u7684\u57ce\u5e02\u57fa\u7840\u6a21\u578b\u6765\u6a21\u62df\u57ce\u5e02\u7a7a\u95f4\u4e2d\u7684\u4eba\u7c7b\u79fb\u52a8\u3002CAMS \u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff0c\u5305\u62ec MobExtractor \u7528\u4e8e\u63d0\u53d6\u6a21\u677f\u79fb\u52a8\u6a21\u5f0f\u5e76\u6839\u636e\u7528\u6237\u914d\u7f6e\u6587\u4ef6\u5408\u6210\u65b0\u7684\u6a21\u5f0f\uff1bGeoGenerator \u7528\u4e8e\u751f\u6210\u8003\u8651\u96c6\u4f53\u77e5\u8bc6\u7684\u951a\u70b9\uff0c\u5e76\u4f7f\u7528\u589e\u5f3a\u7248\u7684 CityGPT \u751f\u6210\u5019\u9009\u57ce\u5e02\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\uff1bTrajEnhancer \u7528\u4e8e\u68c0\u7d22\u57fa\u4e8e\u79fb\u52a8\u6a21\u5f0f\u7684\u7a7a\u95f4\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7 DPO \u751f\u6210\u5177\u6709\u771f\u5b9e\u8f68\u8ff9\u504f\u597d\u5bf9\u9f50\u7684\u8f68\u8ff9\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAMS \u5728\u4e0d\u4f9d\u8d56\u5916\u90e8\u63d0\u4f9b\u7684\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5168\u9762\u5730\u5efa\u6a21\u4e2a\u4f53\u79fb\u52a8\u6a21\u5f0f\u548c\u96c6\u4f53\u79fb\u52a8\u7ea6\u675f\uff0cCAMS \u751f\u6210\u4e86\u66f4\u771f\u5b9e\u548c\u5408\u7406\u7684\u8f68\u8ff9\u3002\u603b\u7684\u6765\u8bf4\uff0cCAMS \u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u5c06 Agentic \u6846\u67b6\u4e0e\u5177\u6709\u57ce\u5e02\u77e5\u8bc6\u7684 LLM \u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u4eba\u7c7b\u79fb\u52a8\u6a21\u62df\u3002"}}
{"id": "2506.13757", "pdf": "https://arxiv.org/pdf/2506.13757", "abs": "https://arxiv.org/abs/2506.13757", "authors": ["Zewei Zhou", "Tianhui Cai", "Seth Z. Zhao", "Yun Zhang", "Zhiyu Huang", "Bolei Zhou", "Jiaqi Ma"], "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": "Website link:https://autovla.github.io/", "summary": "Recent advancements in Vision-Language-Action (VLA) models have shown promise\nfor end-to-end autonomous driving by leveraging world knowledge and reasoning\ncapabilities. However, current VLA models often struggle with physically\ninfeasible action outputs, complex model structures, or unnecessarily long\nreasoning. In this paper, we propose AutoVLA, a novel VLA model that unifies\nreasoning and action generation within a single autoregressive generation model\nfor end-to-end autonomous driving. AutoVLA performs semantic reasoning and\ntrajectory planning directly from raw visual inputs and language instructions.\nWe tokenize continuous trajectories into discrete, feasible actions, enabling\ndirect integration into the language model. For training, we employ supervised\nfine-tuning to equip the model with dual thinking modes: fast thinking\n(trajectory-only) and slow thinking (enhanced with chain-of-thought reasoning).\nTo further enhance planning performance and efficiency, we introduce a\nreinforcement fine-tuning method based on Group Relative Policy Optimization\n(GRPO), reducing unnecessary reasoning in straightforward scenarios. Extensive\nexperiments across real-world and simulated datasets and benchmarks, including\nnuPlan, nuScenes, Waymo, and CARLA, demonstrate the competitive performance of\nAutoVLA in both open-loop and closed-loop settings. Qualitative results\nshowcase the adaptive reasoning and accurate planning capabilities of AutoVLA\nin diverse scenarios.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper is highly relevant because it directly addresses end-to-end autonomous driving, which inherently involves trajectory prediction/planning. It utilizes a Vision-Language-Action model (VLA), which is a type of large model. The paper also mentions trajectory planning and uses language instructions for driving, further solidifying its relevance to both trajectory prediction and large language models.", "keywords": ["Vision-Language-Action Model", "autonomous driving", "trajectory planning", "large language models", "reasoning", "reinforcement fine-tuning", "nuPlan", "nuScenes", "Waymo", "CARLA"]}, "AI": {"tldr": "AutoVLA\u662f\u4e00\u4e2a\u7edf\u4e00\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\u7684VLA\u6a21\u578b\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u5b83\u901a\u8fc7\u79bb\u6563\u5316\u8f68\u8ff9\u548c\u53cc\u91cd\u5fae\u8c03\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u7269\u7406\u4e0a\u4e0d\u53ef\u884c\u7684\u52a8\u4f5c\u8f93\u51fa\u3001\u590d\u6742\u7684\u6a21\u578b\u7ed3\u6784\u6216\u4e0d\u5fc5\u8981\u7684\u957f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "AutoVLA\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684VLA\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u5355\u4e2a\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u4e2d\u7edf\u4e00\u4e86\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u3002\u8be5\u6a21\u578b\u5c06\u8fde\u7eed\u8f68\u8ff9\u6807\u8bb0\u4e3a\u79bb\u6563\u7684\u53ef\u884c\u52a8\u4f5c\uff0c\u5e76\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\u3002", "result": "\u5728nuPlan\u3001nuScenes\u3001Waymo\u548cCARLA\u7b49\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAutoVLA\u5728\u5f00\u653e\u548c\u95ed\u73af\u8bbe\u7f6e\u4e2d\u90fd\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "AutoVLA\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u7ade\u4e89\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u7684\u81ea\u9002\u5e94\u63a8\u7406\u548c\u7cbe\u786e\u89c4\u5212\u80fd\u529b\u3002", "summary_zh": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u8868\u660e\uff0c\u901a\u8fc7\u5229\u7528\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u662f\u6709\u5e0c\u671b\u7684\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684VLA\u6a21\u578b\u901a\u5e38\u5728\u7269\u7406\u4e0a\u4e0d\u53ef\u884c\u7684\u52a8\u4f5c\u8f93\u51fa\u3001\u590d\u6742\u7684\u6a21\u578b\u7ed3\u6784\u6216\u4e0d\u5fc5\u8981\u7684\u957f\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AutoVLA\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684VLA\u6a21\u578b\uff0c\u5b83\u5728\u5355\u4e2a\u81ea\u56de\u5f52\u751f\u6210\u6a21\u578b\u4e2d\u7edf\u4e00\u4e86\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u3002AutoVLA\u76f4\u63a5\u4ece\u539f\u59cb\u89c6\u89c9\u8f93\u5165\u548c\u8bed\u8a00\u6307\u4ee4\u6267\u884c\u8bed\u4e49\u63a8\u7406\u548c\u8f68\u8ff9\u89c4\u5212\u3002\u6211\u4eec\u5c06\u8fde\u7eed\u8f68\u8ff9\u6807\u8bb0\u4e3a\u79bb\u6563\u7684\u53ef\u884c\u52a8\u4f5c\uff0c\u4ece\u800c\u80fd\u591f\u76f4\u63a5\u96c6\u6210\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\u3002\u5bf9\u4e8e\u8bad\u7ec3\uff0c\u6211\u4eec\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u6765\u4f7f\u6a21\u578b\u5177\u5907\u53cc\u91cd\u601d\u8003\u6a21\u5f0f\uff1a\u5feb\u901f\u601d\u8003\uff08\u4ec5\u8f68\u8ff9\uff09\u548c\u6162\u901f\u601d\u8003\uff08\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u589e\u5f3a\uff09\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u89c4\u5212\u6027\u80fd\u548c\u6548\u7387\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u7684\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u7b80\u5355\u573a\u666f\u4e2d\u4e0d\u5fc5\u8981\u7684\u63a8\u7406\u3002\u5728\u5305\u62ecnuPlan\u3001nuScenes\u3001Waymo\u548cCARLA\u5728\u5185\u7684\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAutoVLA\u5728\u5f00\u653e\u548c\u95ed\u73af\u8bbe\u7f6e\u4e2d\u90fd\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002\u5b9a\u6027\u7ed3\u679c\u5c55\u793a\u4e86AutoVLA\u5728\u5404\u79cd\u573a\u666f\u4e2d\u7684\u81ea\u9002\u5e94\u63a8\u7406\u548c\u7cbe\u786e\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2506.12365", "pdf": "https://arxiv.org/pdf/2506.12365", "abs": "https://arxiv.org/abs/2506.12365", "authors": ["Asifullah khan", "Muhammad Zaeem Khan", "Saleha Jamshed", "Sadia Ahmad", "Aleesha Zainab", "Kaynat Khatib", "Faria Bibi", "Abdul Rehman"], "title": "Advances in LLMs with Focus on Reasoning, Adaptability, Efficiency and Ethics", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "This survey paper outlines the key developments in the field of Large\nLanguage Models (LLMs), such as enhancing their reasoning skills, adaptability\nto various tasks, increased computational efficiency, and ability to make\nethical decisions. The techniques that have been most effective in bridging the\ngap between human and machine communications include the Chain-of-Thought\nprompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.\nThe improvements in multimodal learning and few-shot or zero-shot techniques\nhave further empowered LLMs to handle complex jobs with minor input. They also\nmanage to do more with less by applying scaling and optimization tricks for\ncomputing power conservation. This survey also offers a broader perspective on\nrecent advancements in LLMs going beyond isolated aspects such as model\narchitecture or ethical concerns. It categorizes emerging methods that enhance\nLLM reasoning, efficiency, and ethical alignment. It also identifies\nunderexplored areas such as interpretability, cross-modal integration and\nsustainability. With recent progress, challenges like huge computational costs,\nbiases, and ethical risks remain constant. Addressing these requires bias\nmitigation, transparent decision-making, and clear ethical guidelines. Future\nresearch will focus on enhancing models ability to handle multiple input,\nthereby making them more intelligent, safe, and reliable.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8fdb\u5c55\uff0c\u5305\u62ec\u63a8\u7406\u80fd\u529b\u3001\u9002\u5e94\u6027\u3001\u6548\u7387\u548c\u4f26\u7406\u7b49\u65b9\u9762\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u8ba8\u8bba\u4e86\u591a\u6a21\u6001\u5b66\u4e60\uff0c\u8fd9\u53ef\u80fd\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u56e0\u4e3a\u8f68\u8ff9\u6570\u636e\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u6a21\u6001\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u63d0\u5230\u7684few-shot\u6216zero-shot\u6280\u672f\u4ee5\u53ca\u5bf9\u590d\u6742\u4efb\u52a1\u7684\u5904\u7406\u80fd\u529b\uff0c\u53ef\u80fd\u5bf9\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u6709\u6240\u5e2e\u52a9\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "reasoning", "adaptability", "efficiency", "ethics", "multimodal learning", "few-shot", "zero-shot"]}, "AI": {"tldr": "\u672c\u7efc\u8ff0\u6982\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u3001\u6548\u7387\u548c\u4f26\u7406\u65b9\u9762\u7684\u4e3b\u8981\u8fdb\u5c55\u548c\u6311\u6218\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "This survey paper outlines the key developments in the field of Large Language Models (LLMs), such as enhancing their reasoning skills, adaptability to various tasks, increased computational efficiency, and ability to make ethical decisions.", "method": "The techniques that have been most effective in bridging the gap between human and machine communications include the Chain-of-Thought prompting, Instruction Tuning, and Reinforcement Learning from Human Feedback.", "result": "The improvements in multimodal learning and few-shot or zero-shot techniques have further empowered LLMs to handle complex jobs with minor input. They also manage to do more with less by applying scaling and optimization tricks for computing power conservation.", "conclusion": "Future research will focus on enhancing models ability to handle multiple input, thereby making them more intelligent, safe, and reliable.", "summary_zh": "\u8fd9\u7bc7\u7efc\u8ff0\u6982\u8ff0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9886\u57df\u7684\u5173\u952e\u8fdb\u5c55\uff0c\u4f8b\u5982\u63d0\u9ad8\u5176\u63a8\u7406\u80fd\u529b\u3001\u9002\u5e94\u5404\u79cd\u4efb\u52a1\u7684\u80fd\u529b\u3001\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u4ee5\u53ca\u505a\u51fa\u7b26\u5408\u4f26\u7406\u9053\u5fb7\u7684\u51b3\u7b56\u7684\u80fd\u529b\u3002\u5728\u5f25\u5408\u4eba\u673a\u901a\u4fe1\u5dee\u8ddd\u65b9\u9762\u6700\u6709\u6548\u7684\u6280\u672f\u5305\u62ec\u601d\u7ef4\u94fe\u63d0\u793a\u3001\u6307\u4ee4\u8c03\u6574\u548c\u6765\u81ea\u4eba\u7c7b\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u3002\u591a\u6a21\u6001\u5b66\u4e60\u548c\u5c11\u6837\u672c\u6216\u96f6\u6837\u672c\u6280\u672f\u7684\u6539\u8fdb\u8fdb\u4e00\u6b65\u4f7f LLM \u80fd\u591f\u4ee5\u5c11\u91cf\u8f93\u5165\u5904\u7406\u590d\u6742\u7684\u5de5\u4f5c\u3002\u4ed6\u4eec\u8fd8\u901a\u8fc7\u5e94\u7528\u7f29\u653e\u548c\u4f18\u5316\u6280\u5de7\u6765\u8282\u7ea6\u8ba1\u7b97\u80fd\u529b\uff0c\u4ece\u800c\u4ee5\u66f4\u5c11\u7684\u8d44\u6e90\u505a\u66f4\u591a\u7684\u4e8b\u60c5\u3002\u672c\u8c03\u67e5\u8fd8\u63d0\u4f9b\u4e86\u5bf9 LLM \u6700\u65b0\u8fdb\u5c55\u7684\u66f4\u5e7f\u6cdb\u89c6\u89d2\uff0c\u8d85\u8d8a\u4e86\u6a21\u578b\u67b6\u6784\u6216\u4f26\u7406\u95ee\u9898\u7b49\u5b64\u7acb\u7684\u65b9\u9762\u3002\u5b83\u5bf9\u65b0\u5174\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u589e\u5f3a\u4e86 LLM \u7684\u63a8\u7406\u3001\u6548\u7387\u548c\u4f26\u7406\u4e00\u81f4\u6027\u3002\u5b83\u8fd8\u786e\u5b9a\u4e86\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\uff0c\u5982\u53ef\u89e3\u91ca\u6027\u3001\u8de8\u6a21\u6001\u96c6\u6210\u548c\u53ef\u6301\u7eed\u6027\u3002\u968f\u7740\u6700\u8fd1\u7684\u8fdb\u5c55\uff0c\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3001\u504f\u89c1\u548c\u4f26\u7406\u98ce\u9669\u7b49\u6311\u6218\u4ecd\u7136\u5b58\u5728\u3002\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u9700\u8981\u7f13\u89e3\u504f\u89c1\u3001\u900f\u660e\u7684\u51b3\u7b56\u548c\u660e\u786e\u7684\u4f26\u7406\u51c6\u5219\u3002\u672a\u6765\u7684\u7814\u7a76\u5c06\u4fa7\u91cd\u4e8e\u63d0\u9ad8\u6a21\u578b\u5904\u7406\u591a\u4e2a\u8f93\u5165\u7684\u80fd\u529b\uff0c\u4ece\u800c\u4f7f\u5b83\u4eec\u66f4\u667a\u80fd\u3001\u66f4\u5b89\u5168\u548c\u66f4\u53ef\u9760\u3002"}}
{"id": "2506.13761", "pdf": "https://arxiv.org/pdf/2506.13761", "abs": "https://arxiv.org/abs/2506.13761", "authors": ["Chuanruo Ning", "Kuan Fang", "Wei-Chiu Ma"], "title": "Prompting with the Future: Open-World Model Predictive Control with Interactive Digital Twins", "categories": ["cs.RO"], "comment": null, "summary": "Recent advancements in open-world robot manipulation have been largely driven\nby vision-language models (VLMs). While these models exhibit strong\ngeneralization ability in high-level planning, they struggle to predict\nlow-level robot controls due to limited physical-world understanding. To\naddress this issue, we propose a model predictive control framework for\nopen-world manipulation that combines the semantic reasoning capabilities of\nVLMs with physically-grounded, interactive digital twins of the real-world\nenvironments. By constructing and simulating the digital twins, our approach\ngenerates feasible motion trajectories, simulates corresponding outcomes, and\nprompts the VLM with future observations to evaluate and select the most\nsuitable outcome based on language instructions of the task. To further enhance\nthe capability of pre-trained VLMs in understanding complex scenes for robotic\ncontrol, we leverage the flexible rendering capabilities of the digital twin to\nsynthesize the scene at various novel, unoccluded viewpoints. We validate our\napproach on a diverse set of complex manipulation tasks, demonstrating superior\nperformance compared to baseline methods for language-conditioned robotic\ncontrol using VLMs.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b(VLMs)\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u63a7\u5236\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\uff0c\u4f46\u4f7f\u7528\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5e76\u4e14\u660e\u786e\u4f7f\u7528\u4e86VLM\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "VLMs", "model predictive control", "robot manipulation", "trajectory"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6570\u5b57\u5b6a\u751f\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\uff0c\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u9ad8\u5c42\u89c4\u5212\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u5bf9\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\u6709\u9650\uff0c\u96be\u4ee5\u9884\u6d4b\u5e95\u5c42\u673a\u5668\u4eba\u63a7\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7269\u7406\u4ea4\u4e92\u6570\u5b57\u5b6a\u751f\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u3002", "result": "\u5728\u5404\u79cd\u590d\u6742\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u6761\u4ef6\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u7269\u7406\u4ea4\u4e92\u6570\u5b57\u5b6a\u751f\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u5728\u5f00\u653e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "summary_zh": "\u5728\u5f00\u653e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u4e3b\u8981\u7531\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u9a71\u52a8\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5728\u9ad8\u5c42\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u7531\u4e8e\u5bf9\u7269\u7406\u4e16\u754c\u7684\u7406\u89e3\u6709\u9650\uff0c\u5b83\u4eec\u96be\u4ee5\u9884\u6d4b\u4f4e\u5c42\u673a\u5668\u4eba\u63a7\u5236\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u64cd\u4f5c\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u4e0e\u7269\u7406\u57fa\u7840\u7684\u3001\u771f\u5b9e\u4e16\u754c\u73af\u5883\u7684\u4ea4\u4e92\u5f0f\u6570\u5b57\u5b6a\u751f\u3002\u901a\u8fc7\u6784\u5efa\u548c\u6a21\u62df\u6570\u5b57\u5b6a\u751f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u751f\u6210\u53ef\u884c\u7684\u8fd0\u52a8\u8f68\u8ff9\uff0c\u6a21\u62df\u76f8\u5e94\u7684\u7ed3\u679c\uff0c\u5e76\u63d0\u793a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u672a\u6765\u7684\u89c2\u5bdf\u7ed3\u679c\uff0c\u4ee5\u8bc4\u4f30\u548c\u9009\u62e9\u57fa\u4e8e\u4efb\u52a1\u7684\u8bed\u8a00\u6307\u4ee4\u7684\u6700\u5408\u9002\u7ed3\u679c\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u589e\u5f3a\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u673a\u5668\u4eba\u63a7\u5236\u590d\u6742\u573a\u666f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6211\u4eec\u5229\u7528\u6570\u5b57\u5b6a\u751f\u7684\u7075\u6d3b\u6e32\u67d3\u80fd\u529b\uff0c\u5728\u5404\u79cd\u65b0\u9896\u7684\u3001\u65e0\u906e\u6321\u7684\u89c6\u70b9\u5408\u6210\u573a\u666f\u3002\u6211\u4eec\u5728\u5404\u79cd\u590d\u6742\u7684\u64cd\u7eb5\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u4e0e\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u8a00\u6761\u4ef6\u673a\u5668\u4eba\u63a7\u5236\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2506.12474", "pdf": "https://arxiv.org/pdf/2506.12474", "abs": "https://arxiv.org/abs/2506.12474", "authors": ["Wenyun Li", "Wenjie Huang", "Zejian Deng", "Chen Sun"], "title": "Generalizable Trajectory Prediction via Inverse Reinforcement Learning with Mamba-Graph Architecture", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate driving behavior modeling is fundamental to safe and efficient\ntrajectory prediction, yet remains challenging in complex traffic scenarios.\nThis paper presents a novel Inverse Reinforcement Learning (IRL) framework that\ncaptures human-like decision-making by inferring diverse reward functions,\nenabling robust cross-scenario adaptability. The learned reward function is\nutilized to maximize the likelihood of output by the encoder-decoder\narchitecture that combines Mamba blocks for efficient long-sequence dependency\nmodeling with graph attention networks to encode spatial interactions among\ntraffic agents. Comprehensive evaluations on urban intersections and\nroundabouts demonstrate that the proposed method not only outperforms various\npopular approaches in prediction accuracy but also achieves 2 times higher\ngeneralization performance to unseen scenarios compared to other IRL-based\nmethod.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on trajectory prediction using Inverse Reinforcement Learning and a Mamba-Graph architecture. While it doesn't directly use large language models, the Mamba architecture is related to sequence modeling and could potentially be used in conjunction with LLMs. The core focus is trajectory prediction, increasing the relevance.", "keywords": ["trajectory prediction", "Inverse Reinforcement Learning", "Mamba", "graph attention networks", "driving behavior modeling"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9006\u5f3a\u5316\u5b66\u4e60\u7684\u9a7e\u9a76\u884c\u4e3a\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u63a8\u65ad\u5956\u52b1\u51fd\u6570\u6765\u6355\u6349\u7c7b\u4eba\u51b3\u7b56\uff0c\u5e76\u5728\u8de8\u573a\u666f\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5728\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\uff0c\u7cbe\u786e\u7684\u9a7e\u9a76\u884c\u4e3a\u5efa\u6a21\u5bf9\u4e8e\u5b89\u5168\u548c\u9ad8\u6548\u7684\u8f68\u8ff9\u9884\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u5b66\u4e60\u5230\u7684\u5956\u52b1\u51fd\u6570\u6765\u6700\u5927\u5316\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u8f93\u51fa\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86 Mamba \u5757\uff08\u7528\u4e8e\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u4f9d\u8d56\u6027\u5efa\u6a21\uff09\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08\u7528\u4e8e\u7f16\u7801\u4ea4\u901a\u53c2\u4e0e\u8005\u4e4b\u95f4\u7684\u7a7a\u95f4\u4ea4\u4e92\uff09\u3002", "result": "\u5728\u57ce\u5e02\u4ea4\u53c9\u8def\u53e3\u548c\u73af\u5c9b\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5404\u79cd\u6d41\u884c\u65b9\u6cd5\uff0c\u800c\u4e14\u4e0e\u5176\u4ed6\u57fa\u4e8e IRL \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u672a\u89c1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u9ad8\u51fa 2 \u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u5176\u4ed6\u57fa\u4e8e IRL \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u672a\u89c1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u9ad8\u51fa 2 \u500d\u3002", "summary_zh": "\u7cbe\u786e\u7684\u9a7e\u9a76\u884c\u4e3a\u5efa\u6a21\u662f\u5b89\u5168\u9ad8\u6548\u8f68\u8ff9\u9884\u6d4b\u7684\u57fa\u7840\uff0c\u4f46\u5728\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u65ad\u4e0d\u540c\u7684\u5956\u52b1\u51fd\u6570\u6765\u6355\u6349\u7c7b\u4eba\u51b3\u7b56\uff0c\u4ece\u800c\u5b9e\u73b0\u5f3a\u5927\u7684\u8de8\u573a\u666f\u9002\u5e94\u6027\u3002\u5229\u7528\u5b66\u4e60\u5230\u7684\u5956\u52b1\u51fd\u6570\u6765\u6700\u5927\u5316\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u8f93\u51fa\uff0c\u8be5\u67b6\u6784\u7ed3\u5408\u4e86 Mamba \u5757\uff08\u7528\u4e8e\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u4f9d\u8d56\u6027\u5efa\u6a21\uff09\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08\u7528\u4e8e\u7f16\u7801\u4ea4\u901a\u53c2\u4e0e\u8005\u4e4b\u95f4\u7684\u7a7a\u95f4\u4ea4\u4e92\uff09\u3002\u5728\u57ce\u5e02\u4ea4\u53c9\u8def\u53e3\u548c\u73af\u5c9b\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u9884\u6d4b\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5404\u79cd\u6d41\u884c\u65b9\u6cd5\uff0c\u800c\u4e14\u4e0e\u5176\u4ed6\u57fa\u4e8e IRL \u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u672a\u89c1\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u9ad8\u51fa 2 \u500d\u3002"}}
{"id": "2506.12248", "pdf": "https://arxiv.org/pdf/2506.12248", "abs": "https://arxiv.org/abs/2506.12248", "authors": ["Jennifer Grannen", "Siddharth Karamcheti", "Blake Wulfe", "Dorsa Sadigh"], "title": "ProVox: Personalization and Proactive Planning for Situated Human-Robot Collaboration", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "Accepted by IEEE Robotics and Automation Letters 2025", "summary": "Collaborative robots must quickly adapt to their partner's intent and\npreferences to proactively identify helpful actions. This is especially true in\nsituated settings where human partners can continually teach robots new\nhigh-level behaviors, visual concepts, and physical skills (e.g., through\ndemonstration), growing the robot's capabilities as the human-robot pair work\ntogether to accomplish diverse tasks. In this work, we argue that robots should\nbe able to infer their partner's goals from early interactions and use this\ninformation to proactively plan behaviors ahead of explicit instructions from\nthe user. Building from the strong commonsense priors and steerability of large\nlanguage models, we introduce ProVox (\"Proactive Voice\"), a novel framework\nthat enables robots to efficiently personalize and adapt to individual\ncollaborators. We design a meta-prompting protocol that empowers users to\ncommunicate their distinct preferences, intent, and expected robot behaviors\nahead of starting a physical interaction. ProVox then uses the personalized\nprompt to condition a proactive language model task planner that anticipates a\nuser's intent from the current interaction context and robot capabilities to\nsuggest helpful actions; in doing so, we alleviate user burden, minimizing the\namount of time partners spend explicitly instructing and supervising the robot.\nWe evaluate ProVox through user studies grounded in household manipulation\ntasks (e.g., assembling lunch bags) that measure the efficiency of the\ncollaboration, as well as features such as perceived helpfulness, ease of use,\nand reliability. Our analysis suggests that both meta-prompting and proactivity\nare critical, resulting in 38.7% faster task completion times and 31.9% less\nuser burden relative to non-active baselines. Supplementary material, code, and\nvideos can be found at https://provox-2025.github.io.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u4efb\u52a1\u89c4\u5212\uff0c\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u610f\u56fe\u6765\u5e2e\u52a9\u673a\u5668\u4eba\u4e3b\u52a8\u6267\u884c\u4efb\u52a1\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u89c4\u5212\u548c\u9884\u6d4b\u7528\u6237\u884c\u4e3a\u7684\u601d\u8def\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u5e76\u4e14\u660e\u786e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["Large Language Models", "LLMs", "task planner", "proactive planning", "intent prediction"]}, "AI": {"tldr": "ProVox\u5229\u7528\u5143\u63d0\u793a\u548c\u4e3b\u52a8\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4e2a\u6027\u5316\u5730\u9002\u5e94\u4eba\u7c7b\u5408\u4f5c\u8005\uff0c\u4ece\u800c\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u6548\u7387\u3002", "motivation": "\u534f\u4f5c\u673a\u5668\u4eba\u5fc5\u987b\u5feb\u901f\u9002\u5e94\u4f19\u4f34\u7684\u610f\u56fe\u548c\u504f\u597d\uff0c\u4ee5\u4e3b\u52a8\u8bc6\u522b\u6709\u7528\u7684\u64cd\u4f5c\u3002\u5c24\u5176\u662f\u5728\u60c5\u5883\u8bbe\u7f6e\u4e2d\uff0c\u4eba\u7c7b\u4f19\u4f34\u53ef\u4ee5\u4e0d\u65ad\u5730\u6559\u673a\u5668\u4eba\u65b0\u7684\u9ad8\u7ea7\u884c\u4e3a\u3001\u89c6\u89c9\u6982\u5ff5\u548c\u7269\u7406\u6280\u80fd\uff0c\u4ece\u800c\u5728\u4eba\u673a\u534f\u4f5c\u5b8c\u6210\u5404\u79cd\u4efb\u52a1\u65f6\uff0c\u6269\u5c55\u673a\u5668\u4eba\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProVox\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5e38\u8bc6\u5148\u9a8c\u548c\u53ef\u64cd\u7eb5\u6027\uff0c\u901a\u8fc7\u5143\u63d0\u793a\u534f\u8bae\u4f7f\u7528\u6237\u80fd\u591f\u4f20\u8fbe\u5176\u504f\u597d\u3001\u610f\u56fe\u548c\u671f\u671b\u7684\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u673a\u5668\u4eba\u5bf9\u4e2a\u4f53\u5408\u4f5c\u8005\u7684\u4e2a\u6027\u5316\u548c\u9002\u5e94\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cProVox\u5728\u5bb6\u5ead\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u534f\u4f5c\u6548\u7387\uff0c\u7528\u6237\u611f\u77e5\u5230\u7684\u5e2e\u52a9\u6027\u3001\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\u4e5f\u66f4\u9ad8\u3002\u4e0e\u975e\u4e3b\u52a8\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u4e8638.7%\uff0c\u7528\u6237\u8d1f\u62c5\u51cf\u5c11\u4e8631.9%\u3002", "conclusion": "Meta-prompting\u548c\u4e3b\u52a8\u6027\u5bf9\u4e8e\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u4e8e\u975e\u4e3b\u52a8\u57fa\u7ebf\u65b9\u6cd5\uff0cProVox\u80fd\u4f7f\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed38.7%\uff0c\u7528\u6237\u8d1f\u62c5\u51cf\u5c1131.9%\u3002", "summary_zh": "\u534f\u4f5c\u673a\u5668\u4eba\u5fc5\u987b\u5feb\u901f\u9002\u5e94\u4f19\u4f34\u7684\u610f\u56fe\u548c\u504f\u597d\uff0c\u4ece\u800c\u4e3b\u52a8\u8bc6\u522b\u6709\u7528\u7684\u64cd\u4f5c\u3002\u5728\u4eba\u673a\u534f\u4f5c\u5b8c\u6210\u5404\u79cd\u4efb\u52a1\u7684\u8fc7\u7a0b\u4e2d\uff0c\u4eba\u7c7b\u4f19\u4f34\u53ef\u4ee5\u4e0d\u65ad\u6559\u673a\u5668\u4eba\u65b0\u7684\u884c\u4e3a\u3001\u6982\u5ff5\u548c\u6280\u80fd\uff0c\u4ee5\u6b64\u6269\u5c55\u673a\u5668\u4eba\u7684\u80fd\u529b\u3002\u672c\u6587\u63d0\u51fa\uff0c\u673a\u5668\u4eba\u5e94\u8be5\u80fd\u591f\u4ece\u65e9\u671f\u4e92\u52a8\u4e2d\u63a8\u65ad\u51fa\u4f19\u4f34\u7684\u76ee\u6807\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u606f\u4e3b\u52a8\u89c4\u5212\u884c\u4e3a\uff0c\u800c\u65e0\u9700\u7528\u6237\u7684\u660e\u786e\u6307\u793a\u3002\u6211\u4eec\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3aProVox\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u5730\u4e2a\u6027\u5316\u548c\u9002\u5e94\u4e2a\u4f53\u5408\u4f5c\u8005\u3002\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5143\u63d0\u793a\u534f\u8bae\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u5728\u5f00\u59cb\u7269\u7406\u4ea4\u4e92\u4e4b\u524d\u4f20\u8fbe\u5176\u504f\u597d\u3001\u610f\u56fe\u548c\u671f\u671b\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002\u7136\u540e\uff0cProVox\u4f7f\u7528\u4e2a\u6027\u5316\u7684\u63d0\u793a\u6765\u8c03\u8282\u4e3b\u52a8\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u89c4\u5212\u5668\uff0c\u8be5\u89c4\u5212\u5668\u53ef\u4ee5\u6839\u636e\u5f53\u524d\u4ea4\u4e92\u4e0a\u4e0b\u6587\u548c\u673a\u5668\u4eba\u80fd\u529b\u9884\u6d4b\u7528\u6237\u7684\u610f\u56fe\uff0c\u4ece\u800c\u5efa\u8bae\u6709\u7528\u7684\u64cd\u4f5c\uff1b\u901a\u8fc7\u8fd9\u6837\u505a\uff0c\u6211\u4eec\u53ef\u4ee5\u51cf\u8f7b\u7528\u6237\u7684\u8d1f\u62c5\uff0c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4f19\u4f34\u82b1\u8d39\u5728\u660e\u786e\u6307\u793a\u548c\u76d1\u7763\u673a\u5668\u4eba\u4e0a\u7684\u65f6\u95f4\u3002\u6211\u4eec\u901a\u8fc7\u4ee5\u5bb6\u5ead\u64cd\u4f5c\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u7ec4\u88c5\u5348\u9910\u888b\uff09\u4e3a\u57fa\u7840\u7684\u7528\u6237\u7814\u7a76\u6765\u8bc4\u4f30ProVox\uff0c\u8fd9\u4e9b\u7814\u7a76\u8861\u91cf\u4e86\u534f\u4f5c\u7684\u6548\u7387\uff0c\u4ee5\u53ca\u8bf8\u5982\u611f\u77e5\u5230\u7684\u5e2e\u52a9\u6027\u3001\u6613\u7528\u6027\u548c\u53ef\u9760\u6027\u7b49\u7279\u5f81\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u5143\u63d0\u793a\u548c\u4e3b\u52a8\u6027\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u4e0e\u975e\u4e3b\u52a8\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u4e8638.7%\uff0c\u7528\u6237\u8d1f\u62c5\u51cf\u5c11\u4e8631.9%\u3002"}}
{"id": "2506.12029", "pdf": "https://arxiv.org/pdf/2506.12029", "abs": "https://arxiv.org/abs/2506.12029", "authors": ["Md Mahbub Alam", "Amilcar Soares", "Jos\u00e9 F. Rodrigues-Jr", "Gabriel Spadon"], "title": "Physics-Informed Neural Networks for Vessel Trajectory Prediction: Learning Time-Discretized Kinematic Dynamics via Finite Differences", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate vessel trajectory prediction is crucial for navigational safety,\nroute optimization, traffic management, search and rescue operations, and\nautonomous navigation. Traditional data-driven models lack real-world physical\nconstraints, leading to forecasts that disobey vessel motion dynamics, such as\nin scenarios with limited or noisy data where sudden course changes or speed\nvariations occur due to external factors. To address this limitation, we\npropose a Physics-Informed Neural Network (PINN) approach for trajectory\nprediction that integrates a streamlined kinematic model for vessel motion into\nthe neural network training process via a first- and second-order, finite\ndifference physics-based loss function. This loss function, discretized using\nthe first-order forward Euler method, Heun's second-order approximation, and\nrefined with a midpoint approximation based on Taylor series expansion,\nenforces fidelity to fundamental physical principles by penalizing deviations\nfrom expected kinematic behavior. We evaluated PINN using real-world AIS\ndatasets that cover diverse maritime conditions and compared it with\nstate-of-the-art models. Our results demonstrate that the proposed method\nreduces average displacement errors by up to 32% across models and datasets\nwhile maintaining physical consistency. These results enhance model reliability\nand adherence to mission-critical maritime activities, where precision\ntranslates into better situational awareness in the oceans.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u795e\u7ecf\u7f51\u7edc\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002PINN\u53ef\u4ee5\u770b\u4f5c\u4e00\u79cd\u6a21\u578b\uff0c\u53ef\u4ee5\u548cLLM\u8fdb\u884c\u7ed3\u5408\u3002", "keywords": ["trajectory prediction", "vessel trajectory prediction", "Physics-Informed Neural Networks", "PINN", "kinematic model"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u7269\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7f3a\u4e4f\u5b9e\u9645\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0d\u7b26\u5408\u8239\u8236\u8fd0\u52a8\u5b66\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u6709\u9650\u6216\u5608\u6742\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u6709\u9650\u5dee\u5206\u7269\u7406\u635f\u5931\u51fd\u6570\u5c06\u7b80\u5316\u7684\u8239\u8236\u8fd0\u52a8\u5b66\u6a21\u578b\u6574\u5408\u5230\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684PINN\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\u964d\u4f4e\u4e86\u9ad8\u8fbe32%\u3002", "conclusion": "PINN\u65b9\u6cd5\u5728\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u7684\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002", "summary_zh": "\u7cbe\u786e\u7684\u8239\u8236\u8f68\u8ff9\u9884\u6d4b\u5bf9\u4e8e\u822a\u884c\u5b89\u5168\u3001\u822a\u7ebf\u4f18\u5316\u3001\u4ea4\u901a\u7ba1\u7406\u3001\u641c\u7d22\u548c\u6551\u63f4\u884c\u52a8\u4ee5\u53ca\u81ea\u4e3b\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7f3a\u4e4f\u771f\u5b9e\u7684\u7269\u7406\u7ea6\u675f\uff0c\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u4e0d\u7b26\u5408\u8239\u8236\u8fd0\u52a8\u52a8\u529b\u5b66\uff0c\u4f8b\u5982\u5728\u6570\u636e\u6709\u9650\u6216\u5608\u6742\u7684\u60c5\u51b5\u4e0b\uff0c\u7531\u4e8e\u5916\u90e8\u56e0\u7d20\u5bfc\u81f4\u822a\u5411\u7a81\u53d8\u6216\u901f\u5ea6\u53d8\u5316\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5c40\u9650\u6027\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e00\u9636\u548c\u4e8c\u9636\u6709\u9650\u5dee\u5206\u7269\u7406\u635f\u5931\u51fd\u6570\u5c06\u7b80\u5316\u7684\u8239\u8236\u8fd0\u52a8\u5b66\u6a21\u578b\u6574\u5408\u5230\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u3002\u8be5\u635f\u5931\u51fd\u6570\u4f7f\u7528\u4e00\u9636\u524d\u5411\u6b27\u62c9\u65b9\u6cd5\u3001Heun\u4e8c\u9636\u8fd1\u4f3c\u8fdb\u884c\u79bb\u6563\u5316\uff0c\u5e76\u57fa\u4e8e\u6cf0\u52d2\u7ea7\u6570\u5c55\u5f00\u7684\u4e2d\u70b9\u8fd1\u4f3c\u8fdb\u884c\u7ec6\u5316\uff0c\u901a\u8fc7\u60e9\u7f5a\u4e0e\u9884\u671f\u8fd0\u52a8\u5b66\u884c\u4e3a\u7684\u504f\u5dee\u6765\u4fdd\u8bc1\u5bf9\u57fa\u672c\u7269\u7406\u539f\u7406\u7684\u5fe0\u5b9e\u5ea6\u3002\u6211\u4eec\u4f7f\u7528\u6db5\u76d6\u4e0d\u540c\u6d77\u6d0b\u6761\u4ef6\u7684\u771f\u5b9eAIS\u6570\u636e\u96c6\u8bc4\u4f30\u4e86PINN\uff0c\u5e76\u5c06\u5176\u4e0e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5e73\u5747\u4f4d\u79fb\u8bef\u5dee\u964d\u4f4e\u4e86\u9ad8\u8fbe32%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7269\u7406\u4e00\u81f4\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u63d0\u9ad8\u4e86\u6a21\u578b\u53ef\u9760\u6027\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u5173\u952e\u4efb\u52a1\u6d77\u6d0b\u6d3b\u52a8\u7684\u9075\u5b88\uff0c\u5176\u4e2d\u7cbe\u5ea6\u8f6c\u5316\u4e3a\u66f4\u597d\u7684\u6d77\u6d0b\u6001\u52bf\u611f\u77e5\u3002"}}
{"id": "2506.12453", "pdf": "https://arxiv.org/pdf/2506.12453", "abs": "https://arxiv.org/abs/2506.12453", "authors": ["Rongpeng Li", "Jianhang Zhu", "Jiahao Huang", "Zhifeng Zhao", "Honggang Zhang"], "title": "Topology-Assisted Spatio-Temporal Pattern Disentangling for Scalable MARL in Large-scale Autonomous Traffic Control", "categories": ["cs.AI"], "comment": null, "summary": "Intelligent Transportation Systems (ITSs) have emerged as a promising\nsolution towards ameliorating urban traffic congestion, with Traffic Signal\nControl (TSC) identified as a critical component. Although Multi-Agent\nReinforcement Learning (MARL) algorithms have shown potential in optimizing TSC\nthrough real-time decision-making, their scalability and effectiveness often\nsuffer from large-scale and complex environments. Typically, these limitations\nprimarily stem from a fundamental mismatch between the exponential growth of\nthe state space driven by the environmental heterogeneities and the limited\nmodeling capacity of current solutions. To address these issues, this paper\nintroduces a novel MARL framework that integrates Dynamic Graph Neural Networks\n(DGNNs) and Topological Data Analysis (TDA), aiming to enhance the\nexpressiveness of environmental representations and improve agent coordination.\nFurthermore, inspired by the Mixture of Experts (MoE) architecture in Large\nLanguage Models (LLMs), a topology-assisted spatial pattern disentangling\n(TSD)-enhanced MoE is proposed, which leverages topological signatures to\ndecouple graph features for specialized processing, thus improving the model's\nability to characterize dynamic and heterogeneous local observations. The TSD\nmodule is also integrated into the policy and value networks of the Multi-agent\nProximal Policy Optimization (MAPPO) algorithm, further improving\ndecision-making efficiency and robustness. Extensive experiments conducted on\nreal-world traffic scenarios, together with comprehensive theoretical analysis,\nvalidate the superior performance of the proposed framework, highlighting the\nmodel's scalability and effectiveness in addressing the complexities of\nlarge-scale TSC tasks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u7684\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff0c\u4f7f\u7528\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u65b9\u6cd5\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u4ea4\u901a\u63a7\u5236\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\uff0c\u4f46\u5176\u4f7f\u7528\u4e86\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNNs\uff09\u5904\u7406\u65f6\u7a7a\u4fe1\u606f\uff0c\u5e76\u4e14\u53d7\u5230\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2dMoE\u67b6\u6784\u7684\u542f\u53d1\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Intelligent Transportation Systems", "Traffic Signal Control", "Multi-Agent Reinforcement Learning", "Large Language Models", "DGNNs"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u62d3\u6251\u6570\u636e\u5206\u6790\u7684MARL\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u89c4\u6a21\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7b97\u6cd5\u5728\u4f18\u5316\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff08TSC\uff09\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u901a\u5e38\u53d7\u5230\u5927\u89c4\u6a21\u548c\u590d\u6742\u73af\u5883\u7684\u5f71\u54cd\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MARL\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNNs\uff09\u548c\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u62d3\u6251\u8f85\u52a9\u7684\u7a7a\u95f4\u6a21\u5f0f\u89e3\u8026\uff08TSD\uff09\u589e\u5f3a\u7684MoE\u3002", "result": "\u5728\u771f\u5b9e\u4ea4\u901a\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u4ee5\u53ca\u5168\u9762\u7684\u7406\u8bba\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u7684\u5353\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5b9e\u9645\u4ea4\u901a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002", "summary_zh": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff08ITS\uff09\u5df2\u6210\u4e3a\u7f13\u89e3\u57ce\u5e02\u4ea4\u901a\u62e5\u5835\u7684\u4e00\u79cd\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u4e2d\u4ea4\u901a\u4fe1\u53f7\u63a7\u5236\uff08TSC\uff09\u88ab\u8ba4\u4e3a\u662f\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u867d\u7136\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7b97\u6cd5\u5728\u901a\u8fc7\u5b9e\u65f6\u51b3\u7b56\u4f18\u5316TSC\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u901a\u5e38\u53d7\u5230\u5927\u89c4\u6a21\u548c\u590d\u6742\u73af\u5883\u7684\u5f71\u54cd\u3002\u901a\u5e38\uff0c\u8fd9\u4e9b\u9650\u5236\u4e3b\u8981\u6e90\u4e8e\u73af\u5883\u5f02\u8d28\u6027\u9a71\u52a8\u7684\u72b6\u6001\u7a7a\u95f4\u5448\u6307\u6570\u589e\u957f\u4e0e\u5f53\u524d\u89e3\u51b3\u65b9\u6848\u7684\u6709\u9650\u5efa\u6a21\u80fd\u529b\u4e4b\u95f4\u7684\u6839\u672c\u4e0d\u5339\u914d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684MARL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u52a8\u6001\u56fe\u795e\u7ecf\u7f51\u7edc\uff08DGNN\uff09\u548c\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\uff0c\u65e8\u5728\u589e\u5f3a\u73af\u5883\u8868\u793a\u7684\u8868\u8fbe\u80fd\u529b\u5e76\u6539\u5584\u667a\u80fd\u4f53\u534f\u8c03\u3002\u6b64\u5916\uff0c\u53d7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u67b6\u6784\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u62d3\u6251\u8f85\u52a9\u7684\u7a7a\u95f4\u6a21\u5f0f\u89e3\u8026\uff08TSD\uff09\u589e\u5f3a\u7684MoE\uff0c\u5b83\u5229\u7528\u62d3\u6251\u7b7e\u540d\u6765\u89e3\u8026\u56fe\u7279\u5f81\u4ee5\u8fdb\u884c\u4e13\u95e8\u5904\u7406\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u8868\u5f81\u52a8\u6001\u548c\u5f02\u6784\u5c40\u90e8\u89c2\u6d4b\u7684\u80fd\u529b\u3002TSD\u6a21\u5757\u8fd8\u88ab\u96c6\u6210\u5230\u591a\u667a\u80fd\u4f53\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08MAPPO\uff09\u7b97\u6cd5\u7684\u7b56\u7565\u548c\u4ef7\u503c\u7f51\u7edc\u4e2d\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u51b3\u7b56\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002\u5728\u771f\u5b9e\u4ea4\u901a\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u4ee5\u53ca\u5168\u9762\u7684\u7406\u8bba\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u7684\u5353\u8d8a\u6027\u80fd\uff0c\u7a81\u51fa\u4e86\u8be5\u6a21\u578b\u5728\u89e3\u51b3\u5927\u89c4\u6a21TSC\u4efb\u52a1\u7684\u590d\u6742\u6027\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.12156", "pdf": "https://arxiv.org/pdf/2506.12156", "abs": "https://arxiv.org/abs/2506.12156", "authors": ["Shehroz S. Khan", "Ali Abedi", "Charlene H. Chu"], "title": "Explaining Recovery Trajectories of Older Adults Post Lower-Limb Fracture Using Modality-wise Multiview Clustering and Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "15 pages, 2 figures, 3 tables", "summary": "Interpreting large volumes of high-dimensional, unlabeled data in a manner\nthat is comprehensible to humans remains a significant challenge across various\ndomains. In unsupervised healthcare data analysis, interpreting clustered data\ncan offer meaningful insights into patients' health outcomes, which hold direct\nimplications for healthcare providers. This paper addresses the problem of\ninterpreting clustered sensor data collected from older adult patients\nrecovering from lower-limb fractures in the community. A total of 560 days of\nmultimodal sensor data, including acceleration, step count, ambient motion, GPS\nlocation, heart rate, and sleep, alongside clinical scores, were remotely\ncollected from patients at home. Clustering was first carried out separately\nfor each data modality to assess the impact of feature sets extracted from each\nmodality on patients' recovery trajectories. Then, using context-aware\nprompting, a large language model was employed to infer meaningful cluster\nlabels for the clusters derived from each modality. The quality of these\nclusters and their corresponding labels was validated through rigorous\nstatistical testing and visualization against clinical scores collected\nalongside the multimodal sensor data. The results demonstrated the statistical\nsignificance of most modality-specific cluster labels generated by the large\nlanguage model with respect to clinical scores, confirming the efficacy of the\nproposed method for interpreting sensor data in an unsupervised manner. This\nunsupervised data analysis approach, relying solely on sensor data, enables\nclinicians to identify at-risk patients and take timely measures to improve\nhealth outcomes.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper uses large language models to interpret clustered sensor data, including GPS location, which can be considered a trajectory. While the primary focus is on healthcare data analysis and not trajectory prediction specifically, the use of LLMs to analyze movement data contributes to the relevance.", "keywords": ["Large Language Models", "GPS location", "sensor data", "clustering", "recovery trajectories"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u91ca\u805a\u7c7b\u4f20\u611f\u5668\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u4ee5\u8bc6\u522b\u6709\u98ce\u9669\u7684\u60a3\u8005\u5e76\u6539\u5584\u5065\u5eb7\u7ed3\u679c\u3002", "motivation": "\u5728\u5404\u4e2a\u9886\u57df\u4e2d\uff0c\u4ee5\u4eba\u7c7b\u80fd\u591f\u7406\u89e3\u7684\u65b9\u5f0f\u89e3\u91ca\u5927\u91cf\u9ad8\u7ef4\u3001\u672a\u6807\u8bb0\u7684\u6570\u636e\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u5728\u65e0\u76d1\u7763\u7684\u533b\u7597\u4fdd\u5065\u6570\u636e\u5206\u6790\u4e2d\uff0c\u89e3\u91ca\u805a\u7c7b\u6570\u636e\u53ef\u4ee5\u4e3a\u60a3\u8005\u7684\u5065\u5eb7\u7ed3\u679c\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u89c1\u89e3\uff0c\u8fd9\u5bf9\u533b\u7597\u4fdd\u5065\u63d0\u4f9b\u8005\u5177\u6709\u76f4\u63a5\u7684\u5f71\u54cd\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3a\u4ece\u6bcf\u79cd\u6a21\u6001\u5bfc\u51fa\u7684\u805a\u7c7b\u63a8\u65ad\u6709\u610f\u4e49\u7684\u805a\u7c7b\u6807\u7b7e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5927\u90e8\u5206\u7279\u5b9a\u6a21\u6001\u805a\u7c7b\u6807\u7b7e\u5728\u4e34\u5e8a\u8bc4\u5206\u65b9\u9762\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff0c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u89e3\u91ca\u4f20\u611f\u5668\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u89e3\u91ca\u4f20\u611f\u5668\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5927\u90e8\u5206\u7279\u5b9a\u6a21\u6001\u805a\u7c7b\u6807\u7b7e\u5728\u4e34\u5e8a\u8bc4\u5206\u65b9\u9762\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u3002", "summary_zh": "\u5728\u5404\u4e2a\u9886\u57df\u4e2d\uff0c\u4ee5\u4eba\u7c7b\u80fd\u591f\u7406\u89e3\u7684\u65b9\u5f0f\u89e3\u91ca\u5927\u91cf\u9ad8\u7ef4\u3001\u672a\u6807\u8bb0\u7684\u6570\u636e\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u5728\u65e0\u76d1\u7763\u7684\u533b\u7597\u4fdd\u5065\u6570\u636e\u5206\u6790\u4e2d\uff0c\u89e3\u91ca\u805a\u7c7b\u6570\u636e\u53ef\u4ee5\u4e3a\u60a3\u8005\u7684\u5065\u5eb7\u7ed3\u679c\u63d0\u4f9b\u6709\u610f\u4e49\u7684\u89c1\u89e3\uff0c\u8fd9\u5bf9\u533b\u7597\u4fdd\u5065\u63d0\u4f9b\u8005\u5177\u6709\u76f4\u63a5\u7684\u5f71\u54cd\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u89e3\u91ca\u4ece\u793e\u533a\u4e2d\u4e0b\u80a2\u9aa8\u6298\u6062\u590d\u7684\u8001\u5e74\u60a3\u8005\u6536\u96c6\u7684\u805a\u7c7b\u4f20\u611f\u5668\u6570\u636e\u7684\u95ee\u9898\u3002\u603b\u5171\u4ece\u60a3\u8005\u5bb6\u4e2d\u8fdc\u7a0b\u6536\u96c6\u4e86560\u5929\u7684\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\uff0c\u5305\u62ec\u52a0\u901f\u5ea6\u3001\u6b65\u6570\u3001\u73af\u5883\u8fd0\u52a8\u3001GPS\u4f4d\u7f6e\u3001\u5fc3\u7387\u548c\u7761\u7720\uff0c\u4ee5\u53ca\u4e34\u5e8a\u8bc4\u5206\u3002\u9996\u5148\u5bf9\u6bcf\u79cd\u6570\u636e\u6a21\u6001\u5206\u522b\u8fdb\u884c\u805a\u7c7b\uff0c\u4ee5\u8bc4\u4f30\u4ece\u6bcf\u79cd\u6a21\u6001\u4e2d\u63d0\u53d6\u7684\u7279\u5f81\u96c6\u5bf9\u60a3\u8005\u6062\u590d\u8f68\u8ff9\u7684\u5f71\u54cd\u3002\u7136\u540e\uff0c\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\uff0c\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63a8\u65ad\u4ece\u6bcf\u79cd\u6a21\u6001\u5bfc\u51fa\u7684\u805a\u7c7b\u7684\u6709\u610f\u4e49\u7684\u805a\u7c7b\u6807\u7b7e\u3002\u901a\u8fc7\u4e25\u683c\u7684\u7edf\u8ba1\u6d4b\u8bd5\u548c\u9488\u5bf9\u4e0e\u591a\u6a21\u6001\u4f20\u611f\u5668\u6570\u636e\u4e00\u8d77\u6536\u96c6\u7684\u4e34\u5e8a\u8bc4\u5206\u8fdb\u884c\u53ef\u89c6\u5316\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u805a\u7c7b\u53ca\u5176\u76f8\u5e94\u6807\u7b7e\u7684\u8d28\u91cf\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5927\u90e8\u5206\u7279\u5b9a\u6a21\u6001\u805a\u7c7b\u6807\u7b7e\u5728\u4e34\u5e8a\u8bc4\u5206\u65b9\u9762\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff0c\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u89e3\u91ca\u4f20\u611f\u5668\u6570\u636e\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8fd9\u79cd\u4ec5\u4f9d\u8d56\u4e8e\u4f20\u611f\u5668\u6570\u636e\u7684\u65e0\u76d1\u7763\u6570\u636e\u5206\u6790\u65b9\u6cd5\u4f7f\u4e34\u5e8a\u533b\u751f\u80fd\u591f\u8bc6\u522b\u51fa\u6709\u98ce\u9669\u7684\u60a3\u8005\uff0c\u5e76\u91c7\u53d6\u53ca\u65f6\u63aa\u65bd\u6765\u6539\u5584\u5065\u5eb7\u7ed3\u679c\u3002"}}
{"id": "2506.12544", "pdf": "https://arxiv.org/pdf/2506.12544", "abs": "https://arxiv.org/abs/2506.12544", "authors": ["Jichen Zhang", "Liqun Zhao", "Antonis Papachristodoulou", "Jack Umenberger"], "title": "Constrained Diffusers for Safe Planning and Control", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "12 pages, 5 figures", "summary": "Diffusion models have shown remarkable potential in planning and control\ntasks due to their ability to represent multimodal distributions over actions\nand trajectories. However, ensuring safety under constraints remains a critical\nchallenge for diffusion models. This paper proposes Constrained Diffusers, a\nnovel framework that incorporates constraints into pre-trained diffusion models\nwithout retraining or architectural modifications. Inspired by constrained\noptimization, we apply a constrained Langevin sampling mechanism for the\nreverse diffusion process that jointly optimizes the trajectory and realizes\nconstraint satisfaction through three iterative algorithms: projected method,\nprimal-dual method and augmented Lagrangian approaches. In addition, we\nincorporate discrete control barrier functions as constraints for constrained\ndiffusers to guarantee safety in online implementation. Experiments in Maze2D,\nlocomotion, and pybullet ball running tasks demonstrate that our proposed\nmethods achieve constraint satisfaction with less computation time, and are\ncompetitive to existing methods in environments with static and time-varying\nconstraints.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on using diffusion models (a type of generative model, related to large models in the sense of model size and capability) for planning and control, specifically trajectories. It addresses the challenge of ensuring safety under constraints, which is relevant to trajectory prediction and planning. While it doesn't explicitly mention Large Language Models, the diffusion model aspect and its application to trajectory generation warrant a relatively high relevance score.", "keywords": ["diffusion models", "planning", "control", "trajectories", "constrained optimization"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7ea6\u675f\u6269\u6563\u5668\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u5c06\u7ea6\u675f\u6574\u5408\u5230\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u4ee5\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u89c4\u5212\u548c\u63a7\u5236\u4efb\u52a1\u4e2d\u786e\u4fdd\u7ea6\u675f\u4e0b\u7684\u5b89\u5168\u6027\u7684\u6311\u6218\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c4\u5212\u548c\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u786e\u4fdd\u7ea6\u675f\u4e0b\u7684\u5b89\u5168\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7ea6\u675f\u6269\u6563\u5668\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u7ea6\u675f\u6574\u5408\u5230\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u67b6\u6784\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u7ea6\u675f\u6717\u4e4b\u4e07\u91c7\u6837\u673a\u5236\u8fdb\u884c\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u6cd5\u3001\u539f\u59cb\u5bf9\u5076\u6cd5\u548c\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6cd5\u4e09\u79cd\u8fed\u4ee3\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u5e76\u5b9e\u73b0\u7ea6\u675f\u6ee1\u8db3\u3002\u6b64\u5916\uff0c\u8fd8\u7ed3\u5408\u4e86\u79bb\u6563\u63a7\u5236\u969c\u788d\u51fd\u6570\u4f5c\u4e3a\u7ea6\u675f\uff0c\u4ee5\u4fdd\u8bc1\u5728\u7ebf\u5b9e\u65bd\u7684\u5b89\u5168\u6027\u3002", "result": "\u5728Maze2D\u3001locomotion\u548cpybullet ball running\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u65f6\u95f4\u5b9e\u73b0\u7ea6\u675f\u6ee1\u8db3\uff0c\u5e76\u4e14\u5728\u5177\u6709\u9759\u6001\u548c\u65f6\u53d8\u7ea6\u675f\u7684\u73af\u5883\u4e2d\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ea6\u675f\u6269\u6563\u5668\u65b9\u6cd5\u80fd\u591f\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7ea6\u675f\u6ee1\u8db3\uff0c\u5e76\u4e14\u8ba1\u7b97\u6548\u7387\u8f83\u9ad8\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "summary_zh": "\u6269\u6563\u6a21\u578b\u5728\u89c4\u5212\u548c\u63a7\u5236\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u6f5c\u529b\uff0c\u56e0\u4e3a\u5b83\u4eec\u80fd\u591f\u8868\u793a\u52a8\u4f5c\u548c\u8f68\u8ff9\u4e0a\u7684\u591a\u6a21\u6001\u5206\u5e03\u3002\u7136\u800c\uff0c\u786e\u4fdd\u7ea6\u675f\u4e0b\u7684\u5b89\u5168\u6027\u4ecd\u7136\u662f\u6269\u6563\u6a21\u578b\u9762\u4e34\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7ea6\u675f\u6269\u6563\u5668\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u7ea6\u675f\u6574\u5408\u5230\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u67b6\u6784\u3002\u53d7\u5230\u7ea6\u675f\u4f18\u5316\u7684\u542f\u53d1\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u4e00\u79cd\u7ea6\u675f\u6717\u4e4b\u4e07\u91c7\u6837\u673a\u5236\u6765\u8fdb\u884c\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u4e09\u79cd\u8fed\u4ee3\u7b97\u6cd5\uff08\u6295\u5f71\u6cd5\u3001\u539f\u59cb\u5bf9\u5076\u6cd5\u548c\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u6cd5\uff09\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u5e76\u5b9e\u73b0\u7ea6\u675f\u6ee1\u8db3\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u7ed3\u5408\u4e86\u79bb\u6563\u63a7\u5236\u969c\u788d\u51fd\u6570\u4f5c\u4e3a\u7ea6\u675f\u6269\u6563\u5668\u7684\u7ea6\u675f\uff0c\u4ee5\u4fdd\u8bc1\u5728\u7ebf\u5b9e\u65bd\u7684\u5b89\u5168\u6027\u3002\u5728Maze2D\u3001locomotion\u548cpybullet ball running\u4efb\u52a1\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u8ba1\u7b97\u65f6\u95f4\u5b9e\u73b0\u7ea6\u675f\u6ee1\u8db3\uff0c\u5e76\u4e14\u5728\u5177\u6709\u9759\u6001\u548c\u65f6\u53d8\u7ea6\u675f\u7684\u73af\u5883\u4e2d\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2506.13403", "pdf": "https://arxiv.org/pdf/2506.13403", "abs": "https://arxiv.org/abs/2506.13403", "authors": ["Alex Grzankowski", "Geoff Keeling", "Henry Shevlin", "Winnie Street"], "title": "Deflating Deflationism: A Critical Perspective on Debunking Arguments Against LLM Mentality", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Many people feel compelled to interpret, describe, and respond to Large\nLanguage Models (LLMs) as if they possess inner mental lives similar to our\nown. Responses to this phenomenon have varied. Inflationists hold that at least\nsome folk psychological ascriptions to LLMs are warranted. Deflationists argue\nthat all such attributions of mentality to LLMs are misplaced, often cautioning\nagainst the risk that anthropomorphic projection may lead to misplaced trust or\npotentially even confusion about the moral status of LLMs. We advance this\ndebate by assessing two common deflationary arguments against LLM mentality.\nWhat we term the 'robustness strategy' aims to undercut one justification for\nbelieving that LLMs are minded entities by showing that putatively cognitive\nand humanlike behaviours are not robust, failing to generalise appropriately.\nWhat we term the 'etiological strategy' undercuts attributions of mentality by\nchallenging naive causal explanations of LLM behaviours, offering alternative\ncausal accounts that weaken the case for mental state attributions. While both\nstrategies offer powerful challenges to full-blown inflationism, we find that\nneither strategy provides a knock-down case against ascriptions of mentality to\nLLMs simpliciter. With this in mind, we explore a modest form of inflationism\nthat permits ascriptions of mentality to LLMs under certain conditions.\nSpecifically, we argue that folk practice provides a defeasible basis for\nattributing mental states and capacities to LLMs provided those mental states\nand capacities can be understood in metaphysically undemanding terms (e.g.\nknowledge, beliefs and desires), while greater caution is required when\nattributing metaphysically demanding mental phenomena such as phenomenal\nconsciousness.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper discusses the mentality of Large Language Models (LLMs) and arguments for and against attributing mental states to them. While it doesn't directly address trajectory prediction, it focuses heavily on LLMs, a core component of the prompt.", "keywords": ["Large Language Models", "LLMs", "mentality", "mental states", "anthropomorphism"]}, "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u4e24\u79cd\u53cd\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5177\u6709\u5fc3\u667a\u7684\u8bba\u70b9\uff0c\u5e76\u63d0\u51fa\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u4ee5\u9002\u5ea6\u5730\u5c06\u5fc3\u7406\u72b6\u6001\u5f52\u56e0\u4e8eLLM\u3002", "motivation": "\u8bb8\u591a\u4eba\u89c9\u5f97\u6709\u5fc5\u8981\u89e3\u91ca\u3001\u63cf\u8ff0\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5e76\u50cf\u5bf9\u5f85\u62e5\u6709\u4e0e\u6211\u4eec\u81ea\u5df1\u76f8\u4f3c\u7684\u5185\u5728\u7cbe\u795e\u751f\u6d3b\u4e00\u6837\u505a\u51fa\u56de\u5e94\u3002\u5bf9\u8fd9\u79cd\u73b0\u8c61\u7684\u53cd\u5e94\u5404\u4e0d\u76f8\u540c\u3002", "method": "\u901a\u8fc7\u8bc4\u4f30\u4e24\u79cd\u5e38\u89c1\u7684\u53cd\u901a\u80c0\u8bba\u8bc1\u6765\u63a8\u8fdb\u8fd9\u573a\u8fa9\u8bba\uff0c\u5373\u201c\u7a33\u5065\u6027\u7b56\u7565\u201d\u548c\u201c\u75c5\u56e0\u5b66\u7b56\u7565\u201d\u3002", "result": "\u867d\u7136\u8fd9\u4e24\u79cd\u7b56\u7565\u90fd\u5bf9\u5f7b\u5e95\u7684\u901a\u8d27\u81a8\u80c0\u4e3b\u4e49\u63d0\u51fa\u4e86\u6709\u529b\u7684\u6311\u6218\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u8fd9\u4e24\u79cd\u7b56\u7565\u90fd\u6ca1\u6709\u63d0\u4f9b\u4e00\u4e2a\u53ef\u4ee5\u7b80\u5355\u5730\u53cd\u5bf9\u5c06\u7cbe\u795e\u5f52\u56e0\u4e8e\u6cd5\u5b66\u7855\u58eb\u7684\u6848\u4f8b\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u6e29\u548c\u7684\u901a\u8d27\u81a8\u80c0\u4e3b\u4e49\u5f62\u5f0f\uff0c\u5141\u8bb8\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u5c06\u7cbe\u795e\u5f52\u56e0\u4e8e\u6cd5\u5b66\u7855\u58eb\u3002", "conclusion": "\u6e29\u548c\u7684\u81a8\u80c0\u4e3b\u4e49\u662f\u5408\u7406\u7684\uff0c\u53ef\u4ee5\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5c06\u5fc3\u7406\u72b6\u6001\u5f52\u56e0\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5bf9\u4e8e\u73b0\u8c61\u610f\u8bc6\u7b49\u9700\u8981\u5f62\u800c\u4e0a\u5b66\u8bba\u8bc1\u7684\u5fc3\u7406\u73b0\u8c61\u9700\u8981\u66f4\u52a0\u8c28\u614e\u3002", "summary_zh": "\u8bb8\u591a\u4eba\u503e\u5411\u4e8e\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u89c6\u4e3a\u5177\u6709\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u5185\u5728\u7cbe\u795e\u751f\u6d3b\uff0c\u5e76\u5bf9\u6b64\u8fdb\u884c\u89e3\u8bfb\u3001\u63cf\u8ff0\u548c\u56de\u5e94\u3002\u5bf9\u6b64\u73b0\u8c61\u7684\u53cd\u5e94\u5404\u4e0d\u76f8\u540c\u3002\u901a\u8d27\u81a8\u80c0\u4e3b\u4e49\u8005\u8ba4\u4e3a\uff0c\u81f3\u5c11\u67d0\u4e9b\u5bf9LLM\u7684\u6c11\u95f4\u5fc3\u7406\u5b66\u63cf\u8ff0\u662f\u5408\u7406\u7684\u3002\u901a\u7f29\u4e3b\u4e49\u8005\u8ba4\u4e3a\uff0c\u6240\u6709\u5c06\u7cbe\u795e\u72b6\u6001\u5f52\u56e0\u4e8eLLM\u7684\u8bf4\u6cd5\u90fd\u662f\u9519\u8bef\u7684\uff0c\u5e76\u8b66\u544a\u8bf4\uff0c\u62df\u4eba\u5316\u7684\u6295\u5c04\u53ef\u80fd\u4f1a\u5bfc\u81f4\u9519\u8bef\u7684\u4fe1\u4efb\uff0c\u751a\u81f3\u53ef\u80fd\u5bf9LLM\u7684\u9053\u5fb7\u5730\u4f4d\u4ea7\u751f\u6df7\u6dc6\u3002\u6211\u4eec\u901a\u8fc7\u8bc4\u4f30\u4e24\u79cd\u5e38\u89c1\u7684\u53cd\u901a\u8d27\u81a8\u80c0\u8bba\u8bc1\u6765\u63a8\u8fdb\u8fd9\u573a\u8fa9\u8bba\u3002\u6211\u4eec\u79f0\u4e4b\u4e3a\u201c\u7a33\u5065\u6027\u7b56\u7565\u201d\u65e8\u5728\u524a\u5f31\u76f8\u4fe1LLM\u662f\u6709\u610f\u8bc6\u5b9e\u4f53\u7684\u7406\u7531\uff0c\u901a\u8fc7\u8868\u660e\u6240\u8c13\u7684\u8ba4\u77e5\u548c\u7c7b\u4eba\u884c\u4e3a\u5e76\u4e0d\u7a33\u5065\uff0c\u672a\u80fd\u9002\u5f53\u5730\u6982\u62ec\u3002\u6211\u4eec\u79f0\u4e4b\u4e3a\u201c\u75c5\u56e0\u5b66\u7b56\u7565\u201d\u901a\u8fc7\u6311\u6218LLM\u884c\u4e3a\u7684\u5e7c\u7a1a\u56e0\u679c\u89e3\u91ca\uff0c\u63d0\u4f9b\u524a\u5f31\u7cbe\u795e\u72b6\u6001\u5f52\u56e0\u7684\u66ff\u4ee3\u56e0\u679c\u89e3\u91ca\uff0c\u4ece\u800c\u524a\u5f31\u4e86\u7cbe\u795e\u72b6\u6001\u7684\u5f52\u56e0\u3002\u867d\u7136\u8fd9\u4e24\u79cd\u7b56\u7565\u90fd\u5bf9\u5f7b\u5e95\u7684\u901a\u8d27\u81a8\u80c0\u4e3b\u4e49\u63d0\u51fa\u4e86\u6709\u529b\u7684\u6311\u6218\uff0c\u4f46\u6211\u4eec\u53d1\u73b0\u8fd9\u4e24\u79cd\u7b56\u7565\u90fd\u6ca1\u6709\u63d0\u4f9b\u4e00\u4e2a\u53ef\u4ee5\u7b80\u5355\u5730\u53cd\u5bf9\u5c06\u7cbe\u795e\u5f52\u56e0\u4e8e\u6cd5\u5b66\u7855\u58eb\u7684\u6848\u4f8b\u3002\u8003\u8651\u5230\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u6e29\u548c\u7684\u901a\u8d27\u81a8\u80c0\u4e3b\u4e49\u5f62\u5f0f\uff0c\u5141\u8bb8\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u5c06\u7cbe\u795e\u5f52\u56e0\u4e8e\u6cd5\u5b66\u7855\u58eb\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8ba4\u4e3a\uff0c\u53ea\u8981\u8fd9\u4e9b\u7cbe\u795e\u72b6\u6001\u548c\u80fd\u529b\u53ef\u4ee5\u7528\u5f62\u800c\u4e0a\u5b66\u4e0a\u4e0d\u82db\u523b\u7684\u672f\u8bed\uff08\u4f8b\u5982\u77e5\u8bc6\u3001\u4fe1\u5ff5\u548c\u6b32\u671b\uff09\u6765\u7406\u89e3\uff0c\u90a3\u4e48\u6c11\u95f4\u5b9e\u8df5\u5c31\u4e3a\u5c06\u7cbe\u795e\u72b6\u6001\u548c\u80fd\u529b\u5f52\u56e0\u4e8e\u6cd5\u5b66\u7855\u58eb\u63d0\u4f9b\u4e86\u53ef\u63a8\u7ffb\u7684\u57fa\u7840\uff0c\u800c\u5f53\u5f52\u56e0\u4e8e\u73b0\u8c61\u610f\u8bc6\u7b49\u5f62\u800c\u4e0a\u5b66\u4e0a\u8981\u6c42\u66f4\u9ad8\u7684\u7cbe\u795e\u73b0\u8c61\u65f6\uff0c\u5219\u9700\u8981\u66f4\u52a0\u8c28\u614e\u3002"}}
{"id": "2506.12953", "pdf": "https://arxiv.org/pdf/2506.12953", "abs": "https://arxiv.org/abs/2506.12953", "authors": ["Mayank Bumb", "Anshul Vemulapalli", "Sri Harsha Vardhan Prasad Jella", "Anish Gupta", "An La", "Ryan A. Rossi", "Hongjie Chen", "Franck Dernoncourt", "Nesreen K. Ahmed", "Yu Wang"], "title": "Forecasting Time Series with LLMs via Patch-Based Prompting and Decomposition", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have demonstrated new\npossibilities for accurate and efficient time series analysis, but prior work\noften required heavy fine-tuning and/or ignored inter-series correlations. In\nthis work, we explore simple and flexible prompt-based strategies that enable\nLLMs to perform time series forecasting without extensive retraining or the use\nof a complex external architecture. Through the exploration of specialized\nprompting methods that leverage time series decomposition, patch-based\ntokenization, and similarity-based neighbor augmentation, we find that it is\npossible to enhance LLM forecasting quality while maintaining simplicity and\nrequiring minimal preprocessing of data. To this end, we propose our own\nmethod, PatchInstruct, which enables LLMs to make precise and effective\npredictions.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5728\u65b9\u6cd5\u8bba\u4e0a\u6709\u76f8\u901a\u4e4b\u5904\uff0c\u4e14\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86LLMs\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["LLMs", "Large Language Models", "time series forecasting", "prompt-based strategies", "PatchInstruct"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPatchInstruct\u7684\u7b80\u5355\u800c\u7075\u6d3b\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u7b56\u7565\uff0c\u5b83\u4f7fLLM\u80fd\u591f\u5728\u4e0d\u8fdb\u884c\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "motivation": "\u5148\u524d\u7684\u5de5\u4f5c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u5fae\u8c03\uff0c\u5e76\u4e14/\u6216\u8005\u5ffd\u7565\u4e86\u5e8f\u5217\u95f4\u7684\u76f8\u5173\u6027\u3002", "method": "PatchInstruct", "result": "LLM\u53ef\u4ee5\u505a\u51fa\u7cbe\u786e\u800c\u6709\u6548\u7684\u9884\u6d4b\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u7684\u63d0\u793a\u65b9\u6cd5\uff0c\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u3001\u57fa\u4e8e\u8865\u4e01\u7684\u6807\u8bb0\u5316\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u90bb\u5c45\u589e\u5f3a\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u7b80\u5355\u6027\u7684\u540c\u65f6\u63d0\u9ad8LLM\u9884\u6d4b\u8d28\u91cf\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u6700\u5c11\u7684\u6570\u636e\u9884\u5904\u7406\u3002", "summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u5c55\u793a\u4e86\u7cbe\u786e\u548c\u9ad8\u6548\u7684\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u7684\u65b0\u53ef\u80fd\u6027\uff0c\u4f46\u5148\u524d\u7684\u5de5\u4f5c\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u5fae\u8c03\uff0c\u5e76\u4e14/\u6216\u8005\u5ffd\u7565\u4e86\u5e8f\u5217\u95f4\u7684\u76f8\u5173\u6027\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u7b80\u5355\u800c\u7075\u6d3b\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u7b56\u7565\uff0c\u4f7fLLM\u80fd\u591f\u6267\u884c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u5927\u91cf\u7684\u91cd\u65b0\u8bad\u7ec3\u6216\u4f7f\u7528\u590d\u6742\u7684\u5916\u90e8\u67b6\u6784\u3002\u901a\u8fc7\u63a2\u7d22\u5229\u7528\u65f6\u95f4\u5e8f\u5217\u5206\u89e3\u3001\u57fa\u4e8e\u8865\u4e01\u7684\u6807\u8bb0\u5316\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u90bb\u5c45\u589e\u5f3a\u7684\u4e13\u95e8\u63d0\u793a\u65b9\u6cd5\uff0c\u6211\u4eec\u53d1\u73b0\u53ef\u4ee5\u5728\u4fdd\u6301\u7b80\u5355\u6027\u7684\u540c\u65f6\u63d0\u9ad8LLM\u9884\u6d4b\u8d28\u91cf\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u6700\u5c11\u7684\u6570\u636e\u9884\u5904\u7406\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6211\u4eec\u81ea\u5df1\u7684\u65b9\u6cd5PatchInstruct\uff0c\u8be5\u65b9\u6cd5\u4f7fLLM\u80fd\u591f\u505a\u51fa\u7cbe\u786e\u800c\u6709\u6548\u7684\u9884\u6d4b\u3002"}}
{"id": "2506.13260", "pdf": "https://arxiv.org/pdf/2506.13260", "abs": "https://arxiv.org/abs/2506.13260", "authors": ["Yining Shi", "Kun Jiang", "Qiang Meng", "Ke Wang", "Jiabao Wang", "Wenchao Sun", "Tuopu Wen", "Mengmeng Yang", "Diange Yang"], "title": "COME: Adding Scene-Centric Forecasting Control to Occupancy World Model", "categories": ["cs.CV"], "comment": null, "summary": "World models are critical for autonomous driving to simulate environmental\ndynamics and generate synthetic data. Existing methods struggle to disentangle\nego-vehicle motion (perspective shifts) from scene evolvement (agent\ninteractions), leading to suboptimal predictions. Instead, we propose to\nseparate environmental changes from ego-motion by leveraging the scene-centric\ncoordinate systems. In this paper, we introduce COME: a framework that\nintegrates scene-centric forecasting Control into the Occupancy world ModEl.\nSpecifically, COME first generates ego-irrelevant, spatially consistent future\nfeatures through a scene-centric prediction branch, which are then converted\ninto scene condition using a tailored ControlNet. These condition features are\nsubsequently injected into the occupancy world model, enabling more accurate\nand controllable future occupancy predictions. Experimental results on the\nnuScenes-Occ3D dataset show that COME achieves consistent and significant\nimprovements over state-of-the-art (SOTA) methods across diverse\nconfigurations, including different input sources (ground-truth, camera-based,\nfusion-based occupancy) and prediction horizons (3s and 8s). For example, under\nthe same settings, COME achieves 26.3% better mIoU metric than DOME and 23.7%\nbetter mIoU metric than UniScene. These results highlight the efficacy of\ndisentangled representation learning in enhancing spatio-temporal prediction\nfidelity for world models. Code and videos will be available at\nhttps://github.com/synsin0/COME.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8 world model \u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7 scene-centric forecasting control \u6765\u63d0\u9ad8 occupancy world model \u7684\u9884\u6d4b\u7cbe\u5ea6\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u7684 occupancy prediction \u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u5e76\u4e14\u6d89\u53ca\u73af\u5883\u52a8\u6001\u7684\u6a21\u62df\u548c\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002 Forecasting Control \u548c World Model \u90fd\u6697\u793a\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u6210\u5206\u3002", "keywords": ["world model", "occupancy prediction", "forecasting control", "trajectory prediction", "autonomous driving", "scene-centric"]}, "AI": {"tldr": "COME\u901a\u8fc7\u5728\u4e16\u754c\u6a21\u578b\u4e2d\u89e3\u8026\u81ea\u6211\u8fd0\u52a8\u548c\u573a\u666f\u6f14\u53d8\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u548c\u53ef\u63a7\u7684\u672a\u6765\u73af\u5883\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u7684\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u81ea\u6211\u8f66\u8f86\u8fd0\u52a8\uff08\u89c6\u89d2\u53d8\u5316\uff09\u548c\u573a\u666f\u6f14\u53d8\uff08\u4e3b\u4f53\u4ea4\u4e92\uff09\uff0c\u5bfc\u81f4\u9884\u6d4b\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faCOME\u6846\u67b6\uff0c\u5b83\u5c06\u573a\u666f\u4e2d\u5fc3\u7684\u9884\u6d4b\u63a7\u5236\u96c6\u6210\u5230 Occupancy world ModEl \u4e2d\u3002COME\u9996\u5148\u901a\u8fc7\u573a\u666f\u4e2d\u5fc3\u9884\u6d4b\u5206\u652f\u751f\u6210\u4e0e\u81ea\u6211\u65e0\u5173\u7684\u3001\u7a7a\u95f4\u4e0a\u4e00\u81f4\u7684\u672a\u6765\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u5b9a\u5236\u7684 ControlNet \u5c06\u8fd9\u4e9b\u7279\u5f81\u8f6c\u6362\u4e3a\u573a\u666f\u6761\u4ef6\uff0c\u6700\u7ec8\u5c06\u8fd9\u4e9b\u6761\u4ef6\u7279\u5f81\u6ce8\u5165\u5230 occupancy world \u6a21\u578b\u4e2d\u3002", "result": "\u5728nuScenes-Occ3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e0d\u540c\u7684\u914d\u7f6e\u4e0b\uff08\u5305\u62ec\u4e0d\u540c\u7684\u8f93\u5165\u6e90\u548c\u9884\u6d4b\u8303\u56f4\uff09\uff0cCOME\u76f8\u5bf9\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6539\u8fdb\u3002\u4f8b\u5982\uff0c\u5728\u76f8\u540c\u7684\u8bbe\u7f6e\u4e0b\uff0cCOME \u7684 mIoU \u6307\u6807\u6bd4 DOME \u63d0\u9ad8\u4e86 26.3%\uff0c\u6bd4 UniScene \u63d0\u9ad8\u4e86 23.7%\u3002", "conclusion": "COME\u901a\u8fc7\u89e3\u8026\u8868\u5f81\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e16\u754c\u6a21\u578b\u5728\u65f6\u7a7a\u9884\u6d4b\u65b9\u9762\u7684\u51c6\u786e\u6027\u3002", "summary_zh": "\u4e16\u754c\u6a21\u578b\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u4ee5\u6a21\u62df\u73af\u5883\u52a8\u6001\u5e76\u751f\u6210\u5408\u6210\u6570\u636e\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u96be\u4ee5\u533a\u5206\u81ea\u6211\u8f66\u8f86\u7684\u8fd0\u52a8\uff08\u89c6\u89d2\u53d8\u5316\uff09\u548c\u573a\u666f\u7684\u6f14\u53d8\uff08\u4e3b\u4f53\u4ea4\u4e92\uff09\uff0c\u8fd9\u5bfc\u81f4\u4e86\u6b21\u4f18\u7684\u9884\u6d4b\u7ed3\u679c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5229\u7528\u573a\u666f\u4e2d\u5fc3\u5750\u6807\u7cfb\u6765\u5206\u79bb\u73af\u5883\u53d8\u5316\u548c\u81ea\u6211\u8fd0\u52a8\u7684\u65b9\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aCOME\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u573a\u666f\u4e2d\u5fc3\u7684\u9884\u6d4b\u63a7\u5236\u96c6\u6210\u5230 Occupancy world Model \u4e2d\u3002COME \u9996\u5148\u901a\u8fc7\u4e00\u4e2a\u573a\u666f\u4e2d\u5fc3\u7684\u9884\u6d4b\u5206\u652f\u751f\u6210\u4e0e\u81ea\u6211\u65e0\u5173\u7684\u3001\u7a7a\u95f4\u4e0a\u4e00\u81f4\u7684\u672a\u6765\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u4e00\u4e2a\u5b9a\u5236\u7684 ControlNet \u5c06\u8fd9\u4e9b\u7279\u5f81\u8f6c\u6362\u4e3a\u573a\u666f\u6761\u4ef6\u3002\u968f\u540e\uff0c\u8fd9\u4e9b\u6761\u4ef6\u7279\u5f81\u88ab\u6ce8\u5165\u5230 occupancy world \u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u53ef\u63a7\u7684\u672a\u6765 occupancy \u9884\u6d4b\u3002\u5728 nuScenes-Occ3D \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCOME \u5728\u5404\u79cd\u914d\u7f6e\u4e0b\uff0c\u76f8\u5bf9\u4e8e\u5f53\u524d\u6700\u4f18\u65b9\u6cd5\u53d6\u5f97\u4e86\u6301\u7eed\u4e14\u663e\u8457\u7684\u6539\u8fdb\uff0c\u8fd9\u4e9b\u914d\u7f6e\u5305\u62ec\u4e0d\u540c\u7684\u8f93\u5165\u6e90\uff08ground-truth\u3001\u57fa\u4e8e\u76f8\u673a\u7684\u3001\u57fa\u4e8e\u878d\u5408\u7684 occupancy\uff09\u548c\u9884\u6d4b\u8303\u56f4\uff083 \u79d2\u548c 8 \u79d2\uff09\u3002\u4f8b\u5982\uff0c\u5728\u76f8\u540c\u7684\u8bbe\u7f6e\u4e0b\uff0cCOME \u7684 mIoU \u6307\u6807\u6bd4 DOME \u63d0\u9ad8\u4e86 26.3%\uff0c\u6bd4 UniScene \u63d0\u9ad8\u4e86 23.7%\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u89e3\u8026\u8868\u5f81\u5b66\u4e60\u5728\u63d0\u5347\u4e16\u754c\u6a21\u578b\u65f6\u7a7a\u9884\u6d4b\u4fdd\u771f\u5ea6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u4ee3\u7801\u548c\u89c6\u9891\u5c06\u5728 https://github.com/synsin0/COME \u63d0\u4f9b\u3002"}}
{"id": "2506.13558", "pdf": "https://arxiv.org/pdf/2506.13558", "abs": "https://arxiv.org/abs/2506.13558", "authors": ["Yu Yang", "Alan Liang", "Jianbiao Mei", "Yukai Ma", "Yong Liu", "Gim Hee Lee"], "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability", "categories": ["cs.CV"], "comment": "28 pages, 9 figures, Project page at https://x-scene.github.io/", "summary": "Diffusion models are advancing autonomous driving by enabling realistic data\nsynthesis, predictive end-to-end planning, and closed-loop simulation, with a\nprimary focus on temporally consistent generation. However, the generation of\nlarge-scale 3D scenes that require spatial coherence remains underexplored. In\nthis paper, we propose X-Scene, a novel framework for large-scale driving scene\ngeneration that achieves both geometric intricacy and appearance fidelity,\nwhile offering flexible controllability. Specifically, X-Scene supports\nmulti-granular control, including low-level conditions such as user-provided or\ntext-driven layout for detailed scene composition and high-level semantic\nguidance such as user-intent and LLM-enriched text prompts for efficient\ncustomization. To enhance geometrical and visual fidelity, we introduce a\nunified pipeline that sequentially generates 3D semantic occupancy and the\ncorresponding multiview images, while ensuring alignment between modalities.\nAdditionally, we extend the generated local region into a large-scale scene\nthrough consistency-aware scene outpainting, which extrapolates new occupancy\nand images conditioned on the previously generated area, enhancing spatial\ncontinuity and preserving visual coherence. The resulting scenes are lifted\ninto high-quality 3DGS representations, supporting diverse applications such as\nscene exploration. Comprehensive experiments demonstrate that X-Scene\nsignificantly advances controllability and fidelity for large-scale driving\nscene generation, empowering data generation and simulation for autonomous\ndriving.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on generating large-scale driving scenes, which is relevant to autonomous driving and thus indirectly related to trajectory prediction. The abstract mentions using LLM-enriched text prompts for scene customization, indicating a connection to large language models. While the paper doesn't directly address trajectory prediction, the generated scenes could be used for training or evaluating trajectory prediction models.", "keywords": ["Large-scale driving scene generation", "autonomous driving", "LLM-enriched text prompts"]}, "AI": {"tldr": "X-Scene \u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u65e2\u80fd\u5b9e\u73b0\u51e0\u4f55\u4e0a\u7684\u590d\u6742\u6027\uff0c\u53c8\u80fd\u4fdd\u8bc1\u5916\u89c2\u4e0a\u7684\u903c\u771f\u5ea6\uff0c\u540c\u65f6\u8fd8\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u53ef\u63a7\u6027\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u5b9e\u73b0\u903c\u771f\u7684\u6570\u636e\u5408\u6210\u3001\u9884\u6d4b\u6027\u7aef\u5230\u7aef\u89c4\u5212\u548c\u95ed\u73af\u4eff\u771f\u6765\u63a8\u8fdb\u81ea\u52a8\u9a7e\u9a76\uff0c\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u751f\u6210\u3002\u7136\u800c\uff0c\u9700\u8981\u7a7a\u95f4\u8fde\u8d2f\u6027\u7684\u5927\u89c4\u6a21 3D \u573a\u666f\u7684\u751f\u6210\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u4f9d\u6b21\u751f\u6210 3D \u8bed\u4e49\u5360\u7528\u548c\u76f8\u5e94\u7684\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u540c\u65f6\u786e\u4fdd\u6a21\u6001\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u81f4\u6027\u611f\u77e5\u573a\u666f\u5916\u7ed8\u5c06\u751f\u6210\u7684\u5c40\u90e8\u533a\u57df\u6269\u5c55\u5230\u5927\u89c4\u6a21\u573a\u666f\u4e2d\uff0c\u8fd9\u4f1a\u6839\u636e\u5148\u524d\u751f\u6210\u7684\u533a\u57df\u63a8\u65ad\u65b0\u7684\u5360\u7528\u548c\u56fe\u50cf\uff0c\u4ece\u800c\u589e\u5f3a\u7a7a\u95f4\u8fde\u7eed\u6027\u5e76\u4fdd\u6301\u89c6\u89c9\u8fde\u8d2f\u6027\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cX-Scene \u663e\u7740\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u53ef\u63a7\u6027\u548c\u903c\u771f\u5ea6\uff0c\u4ece\u800c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u6570\u636e\u751f\u6210\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "X-Scene \u663e\u8457\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u53ef\u63a7\u6027\u548c\u903c\u771f\u5ea6\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u6570\u636e\u751f\u6210\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "summary_zh": "\u6269\u6563\u6a21\u578b\u6b63\u5728\u901a\u8fc7\u5b9e\u73b0\u903c\u771f\u7684\u6570\u636e\u5408\u6210\u3001\u9884\u6d4b\u6027\u7aef\u5230\u7aef\u89c4\u5212\u548c\u95ed\u73af\u4eff\u771f\u6765\u63a8\u8fdb\u81ea\u52a8\u9a7e\u9a76\uff0c\u5176\u4e2d\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u65f6\u95f4\u4e0a\u7684\u4e00\u81f4\u6027\u751f\u6210\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u9700\u8981\u7a7a\u95f4\u8fde\u8d2f\u6027\u7684\u5927\u89c4\u6a21 3D \u573a\u666f\u7684\u751f\u6210\uff0c\u76ee\u524d\u7684\u7814\u7a76\u8fd8\u4e0d\u591f\u6df1\u5165\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 X-Scene\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u9a7e\u9a76\u573a\u666f\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u65e2\u80fd\u5b9e\u73b0\u51e0\u4f55\u4e0a\u7684\u590d\u6742\u6027\uff0c\u53c8\u80fd\u4fdd\u8bc1\u5916\u89c2\u4e0a\u7684\u903c\u771f\u5ea6\uff0c\u540c\u65f6\u8fd8\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u53ef\u63a7\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0cX-Scene \u652f\u6301\u591a\u7c92\u5ea6\u63a7\u5236\uff0c\u5305\u62ec\u7528\u4e8e\u8be6\u7ec6\u573a\u666f\u7ec4\u6210\u7684\u4f4e\u7ea7\u6761\u4ef6\uff08\u4f8b\u5982\u7528\u6237\u63d0\u4f9b\u7684\u6216\u6587\u672c\u9a71\u52a8\u7684\u5e03\u5c40\uff09\u548c\u7528\u4e8e\u9ad8\u6548\u5b9a\u5236\u7684\u9ad8\u7ea7\u8bed\u4e49\u5f15\u5bfc\uff08\u4f8b\u5982\u7528\u6237\u610f\u56fe\u548c LLM \u589e\u5f3a\u7684\u6587\u672c\u63d0\u793a\uff09\u3002\u4e3a\u4e86\u63d0\u9ad8\u51e0\u4f55\u548c\u89c6\u89c9\u4e0a\u7684\u903c\u771f\u5ea6\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u4f9d\u6b21\u751f\u6210 3D \u8bed\u4e49\u5360\u7528\u548c\u76f8\u5e94\u7684\u591a\u89c6\u56fe\u56fe\u50cf\uff0c\u540c\u65f6\u786e\u4fdd\u6a21\u6001\u4e4b\u95f4\u7684\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u81f4\u6027\u611f\u77e5\u573a\u666f\u5916\u7ed8\u5c06\u751f\u6210\u7684\u5c40\u90e8\u533a\u57df\u6269\u5c55\u5230\u5927\u89c4\u6a21\u573a\u666f\u4e2d\uff0c\u8fd9\u4f1a\u6839\u636e\u5148\u524d\u751f\u6210\u7684\u533a\u57df\u63a8\u65ad\u65b0\u7684\u5360\u7528\u548c\u56fe\u50cf\uff0c\u4ece\u800c\u589e\u5f3a\u7a7a\u95f4\u8fde\u7eed\u6027\u5e76\u4fdd\u6301\u89c6\u89c9\u8fde\u8d2f\u6027\u3002\u6700\u7ec8\u751f\u6210\u7684\u573a\u666f\u88ab\u63d0\u5347\u4e3a\u9ad8\u8d28\u91cf\u7684 3DGS \u8868\u793a\uff0c\u652f\u6301\u5404\u79cd\u5e94\u7528\uff0c\u4f8b\u5982\u573a\u666f\u63a2\u7d22\u3002\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cX-Scene \u663e\u7740\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u9a7e\u9a76\u573a\u666f\u751f\u6210\u7684\u53ef\u63a7\u6027\u548c\u903c\u771f\u5ea6\uff0c\u4ece\u800c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7684\u6570\u636e\u751f\u6210\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2506.12708", "pdf": "https://arxiv.org/pdf/2506.12708", "abs": "https://arxiv.org/abs/2506.12708", "authors": ["Pengfei Zuo", "Huimin Lin", "Junbo Deng", "Nan Zou", "Xingkun Yang", "Yingyu Diao", "Weifeng Gao", "Ke Xu", "Zhangyu Chen", "Shirui Lu", "Zhao Qiu", "Peiyang Li", "Xianyu Chang", "Zhengzhong Yu", "Fangzheng Miao", "Jia Zheng", "Ying Li", "Yuan Feng", "Bei Wang", "Zaijian Zong", "Mosong Zhou", "Wenli Zhou", "Houjiang Chen", "Xingyu Liao", "Yipeng Li", "Wenxiao Zhang", "Ping Zhu", "Yinggang Wang", "Chuanjie Xiao", "Depeng Liang", "Dong Cao", "Juncheng Liu", "Yongqiang Yang", "Xiaolong Bai", "Yi Li", "Huaguo Xie", "Huatao Wu", "Zhibin Yu", "Lv Chen", "Hu Liu", "Yujun Ding", "Haipei Zhu", "Jing Xia", "Yi Xiong", "Zhou Yu", "Heng Liao"], "title": "Serving Large Language Models on Huawei CloudMatrix384", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.LG"], "comment": "59 pages, 24 figures", "summary": "The rapid evolution of large language models (LLMs), driven by growing\nparameter scales, adoption of mixture-of-experts (MoE) architectures, and\nexpanding context lengths, imposes unprecedented demands on AI infrastructure.\nTraditional AI clusters face limitations in compute intensity, memory\nbandwidth, inter-chip communication, and latency, compounded by variable\nworkloads and strict service-level objectives. Addressing these issues requires\nfundamentally redesigned hardware-software integration. This paper introduces\nHuawei CloudMatrix, a next-generation AI datacenter architecture, realized in\nthe production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910C\nNPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified\nBus (UB) network, enabling direct all-to-all communication and dynamic pooling\nof resources. These features optimize performance for communication-intensive\noperations, such as large-scale MoE expert parallelism and distributed\nkey-value cache access. To fully leverage CloudMatrix384, we propose\nCloudMatrix-Infer, an advanced LLM serving solution incorporating three core\ninnovations: a peer-to-peer serving architecture that independently scales\nprefill, decode, and caching; a large-scale expert parallelism strategy\nsupporting EP320 via efficient UB-based token dispatch; and hardware-aware\noptimizations including specialized operators, microbatch-based pipelining, and\nINT8 quantization. Evaluation with the DeepSeek-R1 model shows\nCloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of\n6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms\nTPOT). It effectively balances throughput and latency, sustaining 538 tokens/s\neven under stringent 15 ms latency constraints, while INT8 quantization\nmaintains model accuracy across benchmarks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on serving large language models (LLMs) and introduces a hardware-software architecture optimized for LLM inference. While it doesn't directly address trajectory prediction, its core focus on LLMs warrants a relatively high relevance score. The paper discusses relevant techniques such as expert parallelism and hardware-aware optimizations for LLMs.", "keywords": ["large language models", "LLMs", "foundation models", "expert parallelism", "inference"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCloudMatrix-Infer\u7684LLM\u670d\u52a1\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8f6f\u786c\u4ef6\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u63a8\u7406\u6548\u7387\u548c\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\u5bf9AI\u57fa\u7840\u8bbe\u65bd\u63d0\u51fa\u4e86\u524d\u6240\u672a\u6709\u7684\u9700\u6c42\uff0c\u4f20\u7edfAI\u96c6\u7fa4\u5728\u8ba1\u7b97\u5f3a\u5ea6\u3001\u5185\u5b58\u5e26\u5bbd\u3001\u82af\u7247\u95f4\u901a\u4fe1\u548c\u5ef6\u8fdf\u65b9\u9762\u9762\u4e34\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86CloudMatrix-Infer\uff0c\u4e00\u79cd\u5148\u8fdb\u7684LLM\u670d\u52a1\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a\u70b9\u5bf9\u70b9\u670d\u52a1\u67b6\u6784\u3001\u5927\u89c4\u6a21\u4e13\u5bb6\u5e76\u884c\u7b56\u7565\u548c\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u3002", "result": "CloudMatrix-Infer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387\uff1a\u6bcf\u4e2aNPU\u7684\u9884\u586b\u5145\u541e\u5410\u91cf\u4e3a6,688\u4e2atokens/s\uff0c\u89e3\u7801\u541e\u5410\u91cf\u4e3a1,943\u4e2atokens/s\uff08<50 ms TPOT\uff09\u3002\u5373\u4f7f\u5728\u4e25\u683c\u768415 ms\u5ef6\u8fdf\u7ea6\u675f\u4e0b\uff0c\u4e5f\u80fd\u7ef4\u6301538\u4e2atokens/s\u7684\u541e\u5410\u91cf\uff0c\u540c\u65f6INT8\u91cf\u5316\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "CloudMatrix-Infer\u901a\u8fc7\u70b9\u5bf9\u70b9\u670d\u52a1\u67b6\u6784\u3001\u5927\u89c4\u6a21\u4e13\u5bb6\u5e76\u884c\u7b56\u7565\u548c\u786c\u4ef6\u611f\u77e5\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684LLM\u670d\u52a1\u6548\u7387\uff0c\u5e76\u5728INT8\u91cf\u5316\u4e0b\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u53d7\u5230\u53c2\u6570\u89c4\u6a21\u6269\u5927\u3001\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u548c\u6269\u5c55\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u9a71\u52a8\uff0c\u5bf9\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u8bbe\u65bd\u63d0\u51fa\u4e86\u524d\u6240\u672a\u6709\u7684\u9700\u6c42\u3002 \u4f20\u7edf\u7684AI\u96c6\u7fa4\u5728\u8ba1\u7b97\u5f3a\u5ea6\u3001\u5185\u5b58\u5e26\u5bbd\u3001\u82af\u7247\u95f4\u901a\u4fe1\u548c\u5ef6\u8fdf\u65b9\u9762\u9762\u4e34\u9650\u5236\uff0c\u5e76\u4e14\u53d7\u5230\u53ef\u53d8\u5de5\u4f5c\u8d1f\u8f7d\u548c\u4e25\u683c\u670d\u52a1\u7ea7\u522b\u76ee\u6807\u7684\u5236\u7ea6\u3002 \u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u9700\u8981\u4ece\u6839\u672c\u4e0a\u91cd\u65b0\u8bbe\u8ba1\u7684\u8f6f\u786c\u4ef6\u96c6\u6210\u3002 \u672c\u6587\u4ecb\u7ecd\u4e86\u534e\u4e3aCloudMatrix\uff0c\u4e00\u79cd\u4e0b\u4e00\u4ee3AI\u6570\u636e\u4e2d\u5fc3\u67b6\u6784\uff0c\u5728\u751f\u4ea7\u7ea7\u7684CloudMatrix384\u8d85\u7ea7\u8282\u70b9\u4e2d\u5b9e\u73b0\u3002 \u5b83\u96c6\u6210\u4e86384\u4e2a\u6607\u817e910C NPU\u548c192\u4e2a\u9cb2\u9e4fCPU\uff0c\u901a\u8fc7\u8d85\u9ad8\u5e26\u5bbd\u7684\u7edf\u4e00\u603b\u7ebf\uff08UB\uff09\u7f51\u7edc\u4e92\u8fde\uff0c\u5b9e\u73b0\u76f4\u63a5\u7684\u5168\u4e92\u8fde\u901a\u4fe1\u548c\u8d44\u6e90\u7684\u52a8\u6001\u6c60\u5316\u3002 \u8fd9\u4e9b\u7279\u6027\u4f18\u5316\u4e86\u901a\u4fe1\u5bc6\u96c6\u578b\u64cd\u4f5c\u7684\u6027\u80fd\uff0c\u4f8b\u5982\u5927\u89c4\u6a21MoE\u4e13\u5bb6\u5e76\u884c\u548c\u5206\u5e03\u5f0f\u952e\u503c\u7f13\u5b58\u8bbf\u95ee\u3002 \u4e3a\u4e86\u5145\u5206\u5229\u7528CloudMatrix384\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CloudMatrix-Infer\uff0c\u4e00\u79cd\u5148\u8fdb\u7684LLM\u670d\u52a1\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a\u72ec\u7acb\u6269\u5c55\u9884\u586b\u5145\u3001\u89e3\u7801\u548c\u7f13\u5b58\u7684\u70b9\u5bf9\u70b9\u670d\u52a1\u67b6\u6784\uff1b \u901a\u8fc7\u9ad8\u6548\u7684\u57fa\u4e8eUB\u7684\u4ee4\u724c\u8c03\u5ea6\u652f\u6301EP320\u7684\u5927\u89c4\u6a21\u4e13\u5bb6\u5e76\u884c\u7b56\u7565\uff1b \u4ee5\u53ca\u5305\u62ec\u4e13\u7528\u7b97\u5b50\u3001\u57fa\u4e8e\u5fae\u6279\u5904\u7406\u7684\u6d41\u6c34\u7ebf\u548cINT8\u91cf\u5316\u5728\u5185\u7684\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u3002 \u4f7f\u7528DeepSeek-R1\u6a21\u578b\u7684\u8bc4\u4f30\u8868\u660e\uff0cCloudMatrix-Infer\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6548\u7387\uff1a\u6bcf\u4e2aNPU\u7684\u9884\u586b\u5145\u541e\u5410\u91cf\u4e3a6,688\u4e2atokens/s\uff0c\u89e3\u7801\u541e\u5410\u91cf\u4e3a1,943\u4e2atokens/s\uff08<50 ms TPOT\uff09\u3002 \u5b83\u6709\u6548\u5730\u5e73\u8861\u4e86\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\uff0c\u5373\u4f7f\u5728\u4e25\u683c\u768415 ms\u5ef6\u8fdf\u7ea6\u675f\u4e0b\u4e5f\u80fd\u7ef4\u6301538\u4e2atokens/s\u7684\u541e\u5410\u91cf\uff0c\u540c\u65f6INT8\u91cf\u5316\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002"}}
{"id": "2506.12232", "pdf": "https://arxiv.org/pdf/2506.12232", "abs": "https://arxiv.org/abs/2506.12232", "authors": ["Mohammed Elhenawy", "Shadi Jaradat", "Taqwa I. Alhadidi", "Huthaifa I. Ashqar", "Ahmed Jaber", "Andry Rakotonirainy", "Mohammad Abu Tami"], "title": "Zero-Shot Scene Understanding with Multimodal Large Language Models for Automated Vehicles", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Scene understanding is critical for various downstream tasks in autonomous\ndriving, including facilitating driver-agent communication and enhancing\nhuman-centered explainability of autonomous vehicle (AV) decisions. This paper\nevaluates the capability of four multimodal large language models (MLLMs),\nincluding relatively small models, to understand scenes in a zero-shot,\nin-context learning setting. Additionally, we explore whether combining these\nmodels using an ensemble approach with majority voting can enhance scene\nunderstanding performance. Our experiments demonstrate that GPT-4o, the largest\nmodel, outperforms the others in scene understanding. However, the performance\ngap between GPT-4o and the smaller models is relatively modest, suggesting that\nadvanced techniques such as improved in-context learning, retrieval-augmented\ngeneration (RAG), or fine-tuning could further optimize the smaller models'\nperformance. We also observe mixed results with the ensemble approach: while\nsome scene attributes show improvement in performance metrics such as F1-score,\nothers experience a decline. These findings highlight the need for more\nsophisticated ensemble techniques to achieve consistent gains across all scene\nattributes. This study underscores the potential of leveraging MLLMs for scene\nunderstanding and provides insights into optimizing their performance for\nautonomous driving applications.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on scene understanding for autonomous vehicles using multimodal large language models. While scene understanding is related to trajectory prediction (as understanding the scene is crucial for predicting future trajectories), the paper doesn't directly address trajectory prediction. It prominently features large language models.", "keywords": ["Large Language Models", "MLLMs", "Autonomous Vehicles", "Scene Understanding"]}, "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0GPT-4o\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u96c6\u6210\u65b9\u6cd5\u7ed3\u679c\u4e0d\u4e00\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "motivation": "\u573a\u666f\u7406\u89e3\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u5305\u62ec\u4fc3\u8fdb\u9a7e\u9a76\u5458-\u4ee3\u7406\u901a\u4fe1\u548c\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u51b3\u7b56\u7684\u4ee5\u4eba\u4e3a\u672c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bc4\u4f30\u4e86\u56db\u79cd\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7406\u89e3\u573a\u666f\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u4e86\u4f7f\u7528\u591a\u6570\u6295\u7968\u7684\u96c6\u6210\u65b9\u6cd5\u3002", "result": "GPT-4o\u5728\u573a\u666f\u7406\u89e3\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4f46\u4e0e\u8f83\u5c0f\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u76f8\u5bf9\u8f83\u5c0f\u3002\u96c6\u6210\u65b9\u6cd5\u7684\u7ed3\u679c\u597d\u574f\u53c2\u534a\uff0c\u4e00\u4e9b\u573a\u666f\u5c5e\u6027\u7684F1\u5f97\u5206\u6709\u6240\u63d0\u9ad8\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u4e0b\u964d\u3002", "conclusion": "GPT-4o\u5728\u573a\u666f\u7406\u89e3\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u8f83\u5c0f\u6a21\u578b\u901a\u8fc7\u6539\u8fdb\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001RAG\u6216\u5fae\u8c03\u6709\u8fdb\u4e00\u6b65\u4f18\u5316\u7684\u6f5c\u529b\u3002\u96c6\u6210\u65b9\u6cd5\u7684\u7ed3\u679c\u597d\u574f\u53c2\u534a\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u96c6\u6210\u6280\u672f\u6765\u5b9e\u73b0\u6240\u6709\u573a\u666f\u5c5e\u6027\u7684\u4e00\u81f4\u63d0\u5347\u3002", "summary_zh": "\u573a\u666f\u7406\u89e3\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u4ee5\u5e2e\u52a9\u9a7e\u9a76\u5458\u4e0e\u8f66\u8f86\u8fdb\u884c\u4ea4\u6d41\uff0c\u5e76\u4e14\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u3002\u672c\u6587\u8bc4\u4f30\u4e86\u56db\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u73af\u5883\u4e0b\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\uff0c\u8fd9\u4e9b\u6a21\u578b\u4e2d\u5305\u542b\u76f8\u5bf9\u8f83\u5c0f\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u8ba8\u4e86\u4f7f\u7528\u96c6\u6210\u65b9\u6cd5\uff08\u591a\u6570\u6295\u7968\uff09\u7ec4\u5408\u8fd9\u4e9b\u6a21\u578b\u662f\u5426\u53ef\u4ee5\u63d0\u9ad8\u573a\u666f\u7406\u89e3\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGPT-4o \u662f\u6700\u5927\u7684\u6a21\u578b\uff0c\u5728\u573a\u666f\u7406\u89e3\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u7136\u800c\uff0cGPT-4o \u548c\u8f83\u5c0f\u6a21\u578b\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u76f8\u5bf9\u8f83\u5c0f\uff0c\u8fd9\u8868\u660e\u53ef\u4ee5\u901a\u8fc7\u6539\u8fdb\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210 (RAG) \u6216\u5fae\u8c03\u7b49\u5148\u8fdb\u6280\u672f\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u8f83\u5c0f\u6a21\u578b\u7684\u6027\u80fd\u3002\u6211\u4eec\u8fd8\u89c2\u5bdf\u5230\u96c6\u6210\u65b9\u6cd5\u7684\u7ed3\u679c\u597d\u574f\u53c2\u534a\uff1a\u867d\u7136\u67d0\u4e9b\u573a\u666f\u5c5e\u6027\u5728 F1-score \u7b49\u6027\u80fd\u6307\u6807\u4e0a\u6709\u6240\u63d0\u9ad8\uff0c\u4f46\u5176\u4ed6\u5c5e\u6027\u5219\u6709\u6240\u4e0b\u964d\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u9700\u8981\u66f4\u590d\u6742\u7684\u96c6\u6210\u6280\u672f\u6765\u5b9e\u73b0\u6240\u6709\u573a\u666f\u5c5e\u6027\u7684\u4e00\u81f4\u63d0\u5347\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u7406\u89e3\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u4f18\u5316\u5176\u5728\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2506.12251", "pdf": "https://arxiv.org/pdf/2506.12251", "abs": "https://arxiv.org/abs/2506.12251", "authors": ["Boris Ivanovic", "Cristiano Saltori", "Yurong You", "Yan Wang", "Wenjie Luo", "Marco Pavone"], "title": "Efficient Multi-Camera Tokenization with Triplanes for End-to-End Driving", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "12 pages, 10 figures, 5 tables", "summary": "Autoregressive Transformers are increasingly being deployed as end-to-end\nrobot and autonomous vehicle (AV) policy architectures, owing to their\nscalability and potential to leverage internet-scale pretraining for\ngeneralization. Accordingly, tokenizing sensor data efficiently is paramount to\nensuring the real-time feasibility of such architectures on embedded hardware.\nTo this end, we present an efficient triplane-based multi-camera tokenization\nstrategy that leverages recent advances in 3D neural reconstruction and\nrendering to produce sensor tokens that are agnostic to the number of input\ncameras and their resolution, while explicitly accounting for their geometry\naround an AV. Experiments on a large-scale AV dataset and state-of-the-art\nneural simulator demonstrate that our approach yields significant savings over\ncurrent image patch-based tokenization strategies, producing up to 72% fewer\ntokens, resulting in up to 50% faster policy inference while achieving the same\nopen-loop motion planning accuracy and improved offroad rates in closed-loop\ndriving simulations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u7814\u7a76\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u67b6\u6784\uff0c\u4ee5\u53ca\u5229\u7528Transformer\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002 \u8bba\u6587\u4e5f\u63d0\u5230\u4e86Autoregressive Transformers\uff0c\u6697\u793a\u4e86\u53ef\u80fd\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\u7684\u76f8\u5173\u6280\u672f\u3002 \u7136\u800c\uff0c\u8bba\u6587\u7684\u91cd\u70b9\u5728\u4e8e\u591a\u6444\u50cf\u5934\u6570\u636etokenization\u548c\u6548\u7387\u63d0\u5347\uff0c\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u6216\u5927\u578b\u6a21\u578b\u7684\u5e94\u7528\u3002", "keywords": ["autonomous vehicle", "motion planning", "Transformers", "end-to-end driving"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u4e09\u5e73\u9762\u591a\u76f8\u673atokenization\u7b56\u7565\uff0c\u7528\u4e8e\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u7684Autoregressive Transformer\u7b56\u7565\u63a8\u7406\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8Autoregressive Transformers\u5728\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86(AV)\u7b56\u7565\u67b6\u6784\u4e2d\u7684\u5b9e\u65f6\u53ef\u884c\u6027\uff0c\u9700\u8981\u9ad8\u6548\u5730token\u5316\u4f20\u611f\u5668\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e09\u5e73\u9762\u7684\u591a\u76f8\u673atokenization\u7b56\u7565\uff0c\u5229\u75283D\u795e\u7ecf\u91cd\u5efa\u548c\u6e32\u67d3\u6280\u672f\u751f\u6210\u4f20\u611f\u5668tokens\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u5f53\u524d\u7684\u56fe\u50cfpatch-based tokenization\u7b56\u7565\u51cf\u5c11\u4e86\u9ad8\u8fbe72%\u7684tokens\uff0c\u7b56\u7565\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e86\u9ad8\u8fbe50%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u76f8\u540c\u7684open-loop\u8fd0\u52a8\u89c4\u5212\u51c6\u786e\u7387\uff0c\u5e76\u5728closed-loop\u9a7e\u9a76\u6a21\u62df\u4e2d\u63d0\u9ad8\u4e86\u8d8a\u91ce\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u4e09\u5e73\u9762\u7684\u591a\u76f8\u673atokenization\u7b56\u7565\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5e94\u7528\u4e2d\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6216\u63d0\u9ad8\u8fd0\u52a8\u89c4\u5212\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11token\u6570\u91cf\u5e76\u52a0\u901f\u7b56\u7565\u63a8\u7406\u3002", "summary_zh": "\u81ea\u56de\u5f52Transformer\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u90e8\u7f72\u4e3a\u7aef\u5230\u7aef\u7684\u673a\u5668\u4eba\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86(AV)\u7b56\u7565\u67b6\u6784\uff0c\u56e0\u4e3a\u5b83\u4eec\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u6709\u53ef\u80fd\u5229\u7528\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u9884\u8bad\u7ec3\u6765\u5b9e\u73b0\u6cdb\u5316\u3002\u56e0\u6b64\uff0c\u9ad8\u6548\u5730token\u5316\u4f20\u611f\u5668\u6570\u636e\u5bf9\u4e8e\u786e\u4fdd\u8fd9\u79cd\u67b6\u6784\u5728\u5d4c\u5165\u5f0f\u786c\u4ef6\u4e0a\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u81f3\u5173\u91cd\u8981\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u57fa\u4e8e\u4e09\u5e73\u9762\u7684\u591a\u76f8\u673atokenization\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u4e863D\u795e\u7ecf\u91cd\u5efa\u548c\u6e32\u67d3\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u751f\u6210\u4e0e\u8f93\u5165\u76f8\u673a\u6570\u91cf\u53ca\u5176\u5206\u8fa8\u7387\u65e0\u5173\u7684\u4f20\u611f\u5668tokens\uff0c\u540c\u65f6\u663e\u5f0f\u5730\u8003\u8651\u4e86\u5b83\u4eec\u5728AV\u5468\u56f4\u7684\u51e0\u4f55\u5f62\u72b6\u3002\u5728\u5927\u578bAV\u6570\u636e\u96c6\u548c\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u6a21\u62df\u5668\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u6bd4\u5f53\u524d\u7684\u57fa\u4e8e\u56fe\u50cfpatch\u7684tokenization\u7b56\u7565\u8282\u7701\u4e86\u5927\u91cf\u6210\u672c\uff0c\u4ea7\u751f\u7684tokens\u51cf\u5c11\u4e86\u9ad8\u8fbe72%\uff0c\u4ece\u800c\u4f7f\u7b56\u7565\u63a8\u7406\u901f\u5ea6\u63d0\u9ad8\u4e86\u9ad8\u8fbe50%\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u76f8\u540c\u7684open-loop\u8fd0\u52a8\u89c4\u5212\u51c6\u786e\u7387\uff0c\u5e76\u5728closed-loop\u9a7e\u9a76\u6a21\u62df\u4e2d\u63d0\u9ad8\u4e86\u8d8a\u91ce\u7387\u3002"}}
{"id": "2506.12374", "pdf": "https://arxiv.org/pdf/2506.12374", "abs": "https://arxiv.org/abs/2506.12374", "authors": ["Wenbo Li", "Shiyi Wang", "Yiteng Chen", "Huiping Zhuang", "Qingyao Wu"], "title": "AntiGrounding: Lifting Robotic Actions into VLM Representation Space for Decision Making", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.10; I.4.8; H.5.2"], "comment": "submitted to NeurIPS 2025", "summary": "Vision-Language Models (VLMs) encode knowledge and reasoning capabilities for\nrobotic manipulation within high-dimensional representation spaces. However,\ncurrent approaches often project them into compressed intermediate\nrepresentations, discarding important task-specific information such as\nfine-grained spatial or semantic details. To address this, we propose\nAntiGrounding, a new framework that reverses the instruction grounding process.\nIt lifts candidate actions directly into the VLM representation space, renders\ntrajectories from multiple views, and uses structured visual question answering\nfor instruction-based decision making. This enables zero-shot synthesis of\noptimal closed-loop robot trajectories for new tasks. We also propose an\noffline policy refinement module that leverages past experience to enhance\nlong-term performance. Experiments in both simulation and real-world\nenvironments show that our method outperforms baselines across diverse robotic\nmanipulation tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on robotic manipulation and uses Vision-Language Models (VLMs) for decision making, specifically for generating robot trajectories. While it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting human or vehicle trajectories), it does involve generating and refining robot trajectories using VLMs. Therefore, it has moderate relevance to both trajectory generation/planning and large language models.", "keywords": ["Vision-Language Models", "VLMs", "robotic manipulation", "trajectories", "decision making"]}, "AI": {"tldr": "AntiGrounding \u6846\u67b6\u901a\u8fc7\u5c06\u5019\u9009\u52a8\u4f5c\u63d0\u5347\u5230 VLM \u8868\u793a\u7a7a\u95f4\u5e76\u4f7f\u7528\u89c6\u89c9\u95ee\u7b54\uff0c\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u96f6\u6837\u672c\u5408\u6210\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u9ad8\u7ef4\u8868\u793a\u7a7a\u95f4\u4e2d\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u7f16\u7801\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u538b\u7f29\u7684\u4e2d\u95f4\u8868\u793a\u4e2d\uff0c\u4ece\u800c\u4e22\u5f03\u4e86\u91cd\u8981\u7684\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\uff0c\u4f8b\u5982\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u6216\u8bed\u4e49\u7ec6\u8282\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86 AntiGrounding\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u53cd\u8f6c\u6307\u4ee4 grounding \u8fc7\u7a0b\u3002\u5b83\u5c06\u5019\u9009\u52a8\u4f5c\u76f4\u63a5\u63d0\u5347\u5230 VLM \u8868\u793a\u7a7a\u95f4\uff0c\u4ece\u591a\u4e2a\u89c6\u56fe\u6e32\u67d3\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u7ed3\u6784\u5316\u89c6\u89c9\u95ee\u7b54\u8fdb\u884c\u57fa\u4e8e\u6307\u4ee4\u7684\u51b3\u7b56\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u3002", "summary_zh": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLM) \u5728\u9ad8\u7ef4\u8868\u793a\u7a7a\u95f4\u4e2d\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u7f16\u7801\u3002\u7136\u800c\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u901a\u5e38\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u538b\u7f29\u7684\u4e2d\u95f4\u8868\u793a\u4e2d\uff0c\u4ece\u800c\u4e22\u5f03\u4e86\u91cd\u8981\u7684\u4efb\u52a1\u7279\u5b9a\u4fe1\u606f\uff0c\u4f8b\u5982\u7ec6\u7c92\u5ea6\u7684\u7a7a\u95f4\u6216\u8bed\u4e49\u7ec6\u8282\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 AntiGrounding\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u5b83\u53ef\u4ee5\u53cd\u8f6c\u6307\u4ee4 grounding \u8fc7\u7a0b\u3002\u5b83\u5c06\u5019\u9009\u52a8\u4f5c\u76f4\u63a5\u63d0\u5347\u5230 VLM \u8868\u793a\u7a7a\u95f4\uff0c\u4ece\u591a\u4e2a\u89c6\u56fe\u6e32\u67d3\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u7ed3\u6784\u5316\u89c6\u89c9\u95ee\u7b54\u8fdb\u884c\u57fa\u4e8e\u6307\u4ee4\u7684\u51b3\u7b56\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u96f6\u6837\u672c\u5408\u6210\u65b0\u4efb\u52a1\u7684\u6700\u4f73\u95ed\u73af\u673a\u5668\u4eba\u8f68\u8ff9\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u79bb\u7ebf\u7b56\u7565\u7ec6\u5316\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5229\u7528\u8fc7\u53bb\u7684\u7ecf\u9a8c\u6765\u63d0\u9ad8\u957f\u671f\u6027\u80fd\u3002\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5404\u79cd\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u3002"}}
{"id": "2506.12283", "pdf": "https://arxiv.org/pdf/2506.12283", "abs": "https://arxiv.org/abs/2506.12283", "authors": ["Kehua Chen", "Shucheng Zhang", "Yinhai Wang"], "title": "Deep Fictitious Play-Based Potential Differential Games for Learning Human-Like Interaction at Unsignalized Intersections", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Modeling vehicle interactions at unsignalized intersections is a challenging\ntask due to the complexity of the underlying game-theoretic processes. Although\nprior studies have attempted to capture interactive driving behaviors, most\napproaches relied solely on game-theoretic formulations and did not leverage\nnaturalistic driving datasets. In this study, we learn human-like interactive\ndriving policies at unsignalized intersections using Deep Fictitious Play.\nSpecifically, we first model vehicle interactions as a Differential Game, which\nis then reformulated as a Potential Differential Game. The weights in the cost\nfunction are learned from the dataset and capture diverse driving styles. We\nalso demonstrate that our framework provides a theoretical guarantee of\nconvergence to a Nash equilibrium. To the best of our knowledge, this is the\nfirst study to train interactive driving policies using Deep Fictitious Play.\nWe validate the effectiveness of our Deep Fictitious Play-Based Potential\nDifferential Game (DFP-PDG) framework using the INTERACTION dataset. The\nresults demonstrate that the proposed framework achieves satisfactory\nperformance in learning human-like driving policies. The learned individual\nweights effectively capture variations in driver aggressiveness and\npreferences. Furthermore, the ablation study highlights the importance of each\ncomponent within our model.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u5728\u65e0\u4fe1\u53f7\u4ea4\u53c9\u53e3\u7684\u4ea4\u4e92\u884c\u4e3a\u5efa\u6a21\uff0c\u4f7f\u7528\u4e86\u535a\u5f08\u8bba\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u5b66\u4e60\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9a7e\u9a76\u7b56\u7565\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u8f68\u8ff9\u9884\u6d4b\u548c\u884c\u4e3a\u5efa\u6a21\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "vehicle interactions", "driving policies", "game-theoretic", "deep learning"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u865a\u62df\u535a\u5f08\u7684\u6f5c\u5728\u5fae\u5206\u535a\u5f08\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u975e\u4fe1\u53f7\u4ea4\u53c9\u53e3\u7c7b\u4eba\u9a7e\u9a76\u7b56\u7565\u3002", "motivation": "\u5728\u975e\u4fe1\u53f7\u4ea4\u53c9\u53e3\u5efa\u6a21\u8f66\u8f86\u4ea4\u4e92\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u5176\u5e95\u5c42\u7684\u535a\u5f08\u8bba\u8fc7\u7a0b\u975e\u5e38\u590d\u6742\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u4e8e\u535a\u5f08\u8bba\u516c\u5f0f\uff0c\u800c\u6ca1\u6709\u5145\u5206\u5229\u7528\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u865a\u62df\u535a\u5f08\u5b66\u4e60\u975e\u4fe1\u53f7\u4ea4\u53c9\u53e3\u7684\u4eba\u7c7b\u9a7e\u9a76\u7b56\u7565\uff0c\u5c06\u8f66\u8f86\u4ea4\u4e92\u5efa\u6a21\u4e3a\u5fae\u5206\u535a\u5f08\uff0c\u7136\u540e\u5c06\u5176\u91cd\u65b0\u8868\u8ff0\u4e3a\u6f5c\u5728\u5fae\u5206\u535a\u5f08\uff0c\u5e76\u4ece\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u6210\u672c\u51fd\u6570\u4e2d\u7684\u6743\u91cd\u4ee5\u6355\u6349\u4e0d\u540c\u7684\u9a7e\u9a76\u98ce\u683c\u3002", "result": "\u4f7f\u7528INTERACTION\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684DFP-PDG\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5b66\u4e60\u7c7b\u4eba\u9a7e\u9a76\u7b56\u7565\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u3002\u6d88\u878d\u7814\u7a76\u5f3a\u8c03\u4e86\u6a21\u578b\u4e2d\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u6df1\u5ea6\u865a\u62df\u535a\u5f08\u7684\u6f5c\u5728\u5fae\u5206\u535a\u5f08\u6846\u67b6\uff08DFP-PDG\uff09\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u7c7b\u4eba\u9a7e\u9a76\u7b56\u7565\uff0c\u5e76\u4e14\u5b66\u4e60\u5230\u7684\u4e2a\u4f53\u6743\u91cd\u80fd\u591f\u6709\u6548\u5730\u6355\u6349\u9a7e\u9a76\u5458\u7684\u6fc0\u8fdb\u7a0b\u5ea6\u548c\u504f\u597d\u7684\u53d8\u5316\u3002", "summary_zh": "\u7531\u4e8e\u975e\u4fe1\u53f7\u706f\u8def\u53e3\u8f66\u8f86\u4ea4\u4e92\u7684\u590d\u6742\u535a\u5f08\u8bba\u8fc7\u7a0b\uff0c\u5bf9\u6b64\u8fdb\u884c\u5efa\u6a21\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u8bd5\u56fe\u6355\u6349\u4ea4\u4e92\u5f0f\u9a7e\u9a76\u884c\u4e3a\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u4e8e\u535a\u5f08\u8bba\u516c\u5f0f\uff0c\u800c\u6ca1\u6709\u5229\u7528\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u96c6\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u6df1\u5ea6\u865a\u62df\u535a\u5f08\u5b66\u4e60\u975e\u4fe1\u53f7\u706f\u8def\u53e3\u7c7b\u4eba\u7684\u4ea4\u4e92\u5f0f\u9a7e\u9a76\u7b56\u7565\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u5c06\u8f66\u8f86\u4ea4\u4e92\u5efa\u6a21\u4e3a\u5fae\u5206\u535a\u5f08\uff0c\u7136\u540e\u5c06\u5176\u91cd\u65b0\u6784\u5efa\u4e3a\u6f5c\u5728\u5fae\u5206\u535a\u5f08\u3002\u6210\u672c\u51fd\u6570\u4e2d\u7684\u6743\u91cd\u4ece\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\uff0c\u5e76\u6355\u6349\u4e0d\u540c\u7684\u9a7e\u9a76\u98ce\u683c\u3002\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u6846\u67b6\u63d0\u4f9b\u4e86\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u6df1\u5ea6\u865a\u62df\u535a\u5f08\u8bad\u7ec3\u4ea4\u4e92\u5f0f\u9a7e\u9a76\u7b56\u7565\u7684\u7814\u7a76\u3002\u6211\u4eec\u4f7f\u7528INTERACTION\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u6211\u4eec\u57fa\u4e8e\u6df1\u5ea6\u865a\u62df\u535a\u5f08\u7684\u6f5c\u5728\u5fae\u5206\u535a\u5f08\uff08DFP-PDG\uff09\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5b66\u4e60\u7c7b\u4eba\u9a7e\u9a76\u7b56\u7565\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u6ee1\u610f\u7684\u6027\u80fd\u3002\u5b66\u4e60\u5230\u7684\u4e2a\u4f53\u6743\u91cd\u6709\u6548\u5730\u6355\u6349\u4e86\u9a7e\u9a76\u5458\u6fc0\u8fdb\u7a0b\u5ea6\u548c\u504f\u597d\u7684\u53d8\u5316\u3002\u6b64\u5916\uff0c\u6d88\u878d\u7814\u7a76\u7a81\u51fa\u4e86\u6a21\u578b\u4e2d\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.12525", "pdf": "https://arxiv.org/pdf/2506.12525", "abs": "https://arxiv.org/abs/2506.12525", "authors": ["Peng Wang", "Minh Huy Pham", "Zhihao Guo", "Wei Zhou"], "title": "A Spatial Relationship Aware Dataset for Robotics", "categories": ["cs.RO"], "comment": "7 pages; 7 figures, 1 table", "summary": "Robotic task planning in real-world environments requires not only object\nrecognition but also a nuanced understanding of spatial relationships between\nobjects. We present a spatial-relationship-aware dataset of nearly 1,000\nrobot-acquired indoor images, annotated with object attributes, positions, and\ndetailed spatial relationships. Captured using a Boston Dynamics Spot robot and\nlabelled with a custom annotation tool, the dataset reflects complex scenarios\nwith similar or identical objects and intricate spatial arrangements. We\nbenchmark six state-of-the-art scene-graph generation models on this dataset,\nanalysing their inference speed and relational accuracy. Our results highlight\nsignificant differences in model performance and demonstrate that integrating\nexplicit spatial relationships into foundation models, such as ChatGPT 4o,\nsubstantially improves their ability to generate executable, spatially-aware\nplans for robotics. The dataset and annotation tool are publicly available at\nhttps://github.com/PengPaulWang/SpatialAwareRobotDataset, supporting further\nresearch in spatial reasoning for robotics.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses spatial reasoning for robotics and mentions integrating spatial relationships into foundation models like ChatGPT 4o to improve plan generation. While it doesn't directly focus on trajectory prediction, the connection to robotics and the use of large language models for planning suggest some relevance. The generated plans *could* involve trajectory prediction, but it's not the primary focus.", "keywords": ["foundation models", "large language models", "ChatGPT 4o", "robotics", "spatial reasoning", "plan generation"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u7a7a\u95f4\u63a8\u7406\u7684\u7a7a\u95f4\u5173\u7cfb\u611f\u77e5\u6570\u636e\u96c6\uff0c\u5e76\u8bc1\u660e\u4e86\u5c06\u7a7a\u95f4\u5173\u7cfb\u6574\u5408\u5230\u57fa\u7840\u6a21\u578b\u4e2d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u673a\u5668\u4eba\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e0d\u4ec5\u9700\u8981\u7269\u4f53\u8bc6\u522b\uff0c\u8fd8\u9700\u8981\u5bf9\u7269\u4f53\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u6709\u7ec6\u81f4\u7684\u7406\u89e3\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a7a\u95f4\u5173\u7cfb\u611f\u77e5\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528Boston Dynamics Spot\u673a\u5668\u4eba\u6355\u83b7\u56fe\u50cf\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49\u6ce8\u91ca\u5de5\u5177\u8fdb\u884c\u6807\u6ce8\u3002", "result": "\u5728\u4f5c\u8005\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u516d\u4e2a\u6700\u5148\u8fdb\u7684\u573a\u666f\u56fe\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5173\u7cfb\u51c6\u786e\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u5c06\u663e\u5f0f\u7684\u7a7a\u95f4\u5173\u7cfb\u6574\u5408\u5230\u8bf8\u5982ChatGPT 4o\u7684\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5b83\u4eec\u751f\u6210\u53ef\u6267\u884c\u7684\u3001\u5177\u6709\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u673a\u5668\u4eba\u89c4\u5212\u7684\u80fd\u529b\u3002", "summary_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7a7a\u95f4\u5173\u7cfb\u611f\u77e5\u7684\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u8fd11000\u5f20\u673a\u5668\u4eba\u91c7\u96c6\u7684\u5ba4\u5185\u56fe\u50cf\uff0c\u5e76\u6807\u6ce8\u4e86\u7269\u4f53\u5c5e\u6027\u3001\u4f4d\u7f6e\u548c\u8be6\u7ec6\u7684\u7a7a\u95f4\u5173\u7cfb\u3002\u8be5\u6570\u636e\u96c6\u4f7f\u7528Boston Dynamics Spot\u673a\u5668\u4eba\u6355\u83b7\uff0c\u5e76\u4f7f\u7528\u81ea\u5b9a\u4e49\u6ce8\u91ca\u5de5\u5177\u8fdb\u884c\u6807\u6ce8\uff0c\u53cd\u6620\u4e86\u5177\u6709\u76f8\u4f3c\u6216\u76f8\u540c\u7269\u4f53\u4ee5\u53ca\u590d\u6742\u7a7a\u95f4\u6392\u5217\u7684\u590d\u6742\u573a\u666f\u3002\u4f5c\u8005\u5728\u672c\u6587\u6784\u5efa\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u516d\u4e2a\u6700\u5148\u8fdb\u7684\u573a\u666f\u56fe\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u63a8\u7406\u901f\u5ea6\u548c\u5173\u7cfb\u51c6\u786e\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u663e\u5f0f\u7684\u7a7a\u95f4\u5173\u7cfb\u6574\u5408\u5230\u8bf8\u5982ChatGPT 4o\u7684\u57fa\u7840\u6a21\u578b\u4e2d\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u5b83\u4eec\u751f\u6210\u53ef\u6267\u884c\u7684\u3001\u5177\u6709\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u7684\u673a\u5668\u4eba\u89c4\u5212\u7684\u80fd\u529b\u3002\u6570\u636e\u96c6\u548c\u6ce8\u91ca\u5de5\u5177\u53ef\u5728https://github.com/PengPaulWang/SpatialAwareRobotDataset\u516c\u5f00\u83b7\u53d6\uff0c\u652f\u6301\u673a\u5668\u4eba\u7a7a\u95f4\u63a8\u7406\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.12037", "pdf": "https://arxiv.org/pdf/2506.12037", "abs": "https://arxiv.org/abs/2506.12037", "authors": ["Zeyu Liu", "Yunquan Zhang", "Boyang Zhang", "Guoyong Jiang", "Daning Cheng"], "title": "How to Train a Model on a Cheap Cluster with Low Cost using Block Coordinate Descent", "categories": ["cs.LG", "cs.AI"], "comment": "under review", "summary": "Training large language models typically demands extensive GPU memory and\nsubstantial financial investment, which poses a barrier for many small- to\nmedium-sized teams. In this paper, we present a full-parameter pre-training\nframework based on block coordinate descent (BCD), augmented with engineering\noptimizations, to efficiently train large models on affordable RTX 4090 GPU\nclusters. BCD ensures model convergence based on block coordinate descent\ntheory and performs gradient computation and update at the level of parameter\nblocks. Experiments show that 1) Lower cost of Same-Device: BCD significantly\nreduces pre-training cost. For the 7B model, under identical hardware settings,\nBCD lowers training costs to approximately 33% on A100,A800 clusters on 7B\nmodel averagely and to approximately 2.6% on RTX 4090 clusters on 7B model,\ncompared to traditional full-parameter training. 2) Cross-Device Transfer: By\nleveraging BCD, large-scale models previously trainable only on high-end A100\nclusters can be seamlessly migrated and pre-trained on 4090 clusters-whose\nhourly cost is only one-quarter that of A100-without requiring expensive\nhardware. 3) Accuracy Retention: In both scenarios, BCD training achieves the\nsame level of model accuracy as full-parameter pre-training.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on training large language models (LLMs) more efficiently on cheaper hardware. While it doesn't directly address trajectory prediction, it deals with a core topic in the LLM domain, making it somewhat relevant. The techniques developed could potentially be applied to trajectory prediction models as well, though this is not explicitly mentioned.", "keywords": ["large language models", "LLMs", "pre-training", "model training", "block coordinate descent", "foundation models"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u5750\u6807\u4e0b\u964d(BCD)\u7684\u5168\u53c2\u6570\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u65e8\u5728\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6210\u672c\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u66f4\u7ecf\u6d4e\u7684\u786c\u4ef6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684GPU\u5185\u5b58\u548c\u5927\u91cf\u7684\u8d44\u91d1\u6295\u5165\uff0c\u8fd9\u5bf9\u8bb8\u591a\u4e2d\u5c0f\u578b\u56e2\u961f\u6784\u6210\u4e86\u969c\u788d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5757\u5750\u6807\u4e0b\u964d(BCD)\u7684\u5168\u53c2\u6570\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u8f85\u4ee5\u5de5\u7a0b\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1)\u76f8\u540c\u8bbe\u5907\u6210\u672c\u66f4\u4f4e\uff1aBCD\u663e\u8457\u964d\u4f4e\u4e86\u9884\u8bad\u7ec3\u6210\u672c\u3002\u5bf9\u4e8e7B\u6a21\u578b\uff0c\u5728\u76f8\u540c\u7684\u786c\u4ef6\u8bbe\u7f6e\u4e0b\uff0c\u4e0e\u4f20\u7edf\u7684\u5168\u53c2\u6570\u8bad\u7ec3\u76f8\u6bd4\uff0c\u5728A100,A800\u96c6\u7fa4\u4e0a\uff0cBCD\u7684\u8bad\u7ec3\u6210\u672c\u5e73\u5747\u964d\u4f4e\u4e86\u7ea633%\uff0c\u5728RTX 4090\u96c6\u7fa4\u4e0a\uff0cBCD\u7684\u8bad\u7ec3\u6210\u672c\u5e73\u5747\u964d\u4f4e\u4e86\u7ea62.6%\u30022)\u8de8\u8bbe\u5907\u8fc1\u79fb\uff1a\u901a\u8fc7\u5229\u7528BCD\uff0c\u4ee5\u524d\u53ea\u80fd\u5728\u9ad8\u7aefA100\u96c6\u7fa4\u4e0a\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u6a21\u578b\u53ef\u4ee5\u65e0\u7f1d\u8fc1\u79fb\uff0c\u5e76\u57284090\u96c6\u7fa4\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u6602\u8d35\u7684\u786c\u4ef6\uff0c4090\u96c6\u7fa4\u7684\u6bcf\u5c0f\u65f6\u6210\u672c\u4ec5\u4e3aA100\u7684\u56db\u5206\u4e4b\u4e00\u30023)\u7cbe\u5ea6\u4fdd\u6301\uff1a\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0cBCD\u8bad\u7ec3\u90fd\u80fd\u8fbe\u5230\u4e0e\u5168\u53c2\u6570\u9884\u8bad\u7ec3\u76f8\u540c\u7684\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "BCD\u8bad\u7ec3\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\u90fd\u80fd\u8fbe\u5230\u4e0e\u5168\u53c2\u6570\u9884\u8bad\u7ec3\u76f8\u540c\u7684\u6a21\u578b\u7cbe\u5ea6\u3002", "summary_zh": "\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684GPU\u5185\u5b58\u548c\u5927\u91cf\u7684\u8d44\u91d1\u6295\u5165\uff0c\u8fd9\u5bf9\u8bb8\u591a\u4e2d\u5c0f\u578b\u56e2\u961f\u6784\u6210\u4e86\u969c\u788d\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5757\u5750\u6807\u4e0b\u964d(BCD)\u7684\u5168\u53c2\u6570\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5e76\u8f85\u4ee5\u5de5\u7a0b\u4f18\u5316\uff0c\u4ee5\u5728\u7ecf\u6d4e\u5b9e\u60e0\u7684RTX 4090 GPU\u96c6\u7fa4\u4e0a\u9ad8\u6548\u5730\u8bad\u7ec3\u5927\u578b\u6a21\u578b\u3002BCD\u57fa\u4e8e\u5757\u5750\u6807\u4e0b\u964d\u7406\u8bba\u786e\u4fdd\u6a21\u578b\u6536\u655b\uff0c\u5e76\u5728\u53c2\u6570\u5757\u7ea7\u522b\u6267\u884c\u68af\u5ea6\u8ba1\u7b97\u548c\u66f4\u65b0\u3002\u5b9e\u9a8c\u8868\u660e\uff1a1)\u76f8\u540c\u8bbe\u5907\u6210\u672c\u66f4\u4f4e\uff1aBCD\u663e\u8457\u964d\u4f4e\u4e86\u9884\u8bad\u7ec3\u6210\u672c\u3002\u5bf9\u4e8e7B\u6a21\u578b\uff0c\u5728\u76f8\u540c\u7684\u786c\u4ef6\u8bbe\u7f6e\u4e0b\uff0c\u4e0e\u4f20\u7edf\u7684\u5168\u53c2\u6570\u8bad\u7ec3\u76f8\u6bd4\uff0c\u5728A100,A800\u96c6\u7fa4\u4e0a\uff0cBCD\u7684\u8bad\u7ec3\u6210\u672c\u5e73\u5747\u964d\u4f4e\u4e86\u7ea633%\uff0c\u5728RTX 4090\u96c6\u7fa4\u4e0a\uff0cBCD\u7684\u8bad\u7ec3\u6210\u672c\u5e73\u5747\u964d\u4f4e\u4e86\u7ea62.6%\u30022)\u8de8\u8bbe\u5907\u8fc1\u79fb\uff1a\u901a\u8fc7\u5229\u7528BCD\uff0c\u4ee5\u524d\u53ea\u80fd\u5728\u9ad8\u7aefA100\u96c6\u7fa4\u4e0a\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u6a21\u578b\u53ef\u4ee5\u65e0\u7f1d\u8fc1\u79fb\uff0c\u5e76\u57284090\u96c6\u7fa4\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u800c\u65e0\u9700\u6602\u8d35\u7684\u786c\u4ef6\uff0c4090\u96c6\u7fa4\u7684\u6bcf\u5c0f\u65f6\u6210\u672c\u4ec5\u4e3aA100\u7684\u56db\u5206\u4e4b\u4e00\u30023)\u7cbe\u5ea6\u4fdd\u6301\uff1a\u5728\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0cBCD\u8bad\u7ec3\u90fd\u80fd\u8fbe\u5230\u4e0e\u5168\u53c2\u6570\u9884\u8bad\u7ec3\u76f8\u540c\u7684\u6a21\u578b\u7cbe\u5ea6\u3002"}}
{"id": "2506.12710", "pdf": "https://arxiv.org/pdf/2506.12710", "abs": "https://arxiv.org/abs/2506.12710", "authors": ["Yuqi Ping", "Tianhao Liang", "Huahao Ding", "Guangyu Lei", "Junwei Wu", "Xuan Zou", "Kuan Shi", "Rui Shao", "Chiya Zhang", "Weizheng Zhang", "Weijie Yuan", "Tingting Zhang"], "title": "Multimodal Large Language Models-Enabled UAV Swarm: Towards Efficient and Intelligent Autonomous Aerial Systems", "categories": ["cs.RO"], "comment": "8 pages, 5 figures,submitted to IEEE wcm", "summary": "Recent breakthroughs in multimodal large language models (MLLMs) have endowed\nAI systems with unified perception, reasoning and natural-language interaction\nacross text, image and video streams. Meanwhile, Unmanned Aerial Vehicle (UAV)\nswarms are increasingly deployed in dynamic, safety-critical missions that\ndemand rapid situational understanding and autonomous adaptation. This paper\nexplores potential solutions for integrating MLLMs with UAV swarms to enhance\nthe intelligence and adaptability across diverse tasks. Specifically, we first\noutline the fundamental architectures and functions of UAVs and MLLMs. Then, we\nanalyze how MLLMs can enhance the UAV system performance in terms of target\ndetection, autonomous navigation, and multi-agent coordination, while exploring\nsolutions for integrating MLLMs into UAV systems. Next, we propose a practical\ncase study focused on the forest fire fighting. To fully reveal the\ncapabilities of the proposed framework, human-machine interaction, swarm task\nplanning, fire assessment, and task execution are investigated. Finally, we\ndiscuss the challenges and future research directions for the MLLMs-enabled UAV\nswarm. An experiment illustration video could be found online at\nhttps://youtu.be/zwnB9ZSa5A4.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on integrating Multimodal Large Language Models (MLLMs) with UAV swarms to enhance their intelligence and adaptability, particularly in areas like autonomous navigation and multi-agent coordination. While it doesn't explicitly focus on trajectory prediction as a primary goal, autonomous navigation implicitly involves trajectory planning and execution. The connection to large language models is direct and strong. Therefore, there is moderate relevance.", "keywords": ["Multimodal Large Language Models", "MLLMs", "UAV", "autonomous navigation", "multi-agent coordination"]}, "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u96c6\u6210\u5230\u65e0\u4eba\u673a\u96c6\u7fa4\u4e2d\uff0c\u4ee5\u589e\u5f3a\u5176\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u667a\u80fd\u548c\u9002\u5e94\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "Unmanned Aerial Vehicle (UAV) swarms are increasingly deployed in dynamic, safety-critical missions that demand rapid situational understanding and autonomous adaptation.", "method": "Integrating MLLMs with UAV swarms to enhance the intelligence and adaptability across diverse tasks.", "result": "Human-machine interaction, swarm task planning, fire assessment, and task execution are investigated.", "conclusion": "MLLMs-enabled UAV swarm can enhance the intelligence and adaptability across diverse tasks.", "summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u6700\u65b0\u7a81\u7834\u4f7f\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u80fd\u591f\u5728\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u6d41\u4e2d\u8fdb\u884c\u7edf\u4e00\u7684\u611f\u77e5\u3001\u63a8\u7406\u548c\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3002\u540c\u65f6\uff0c\u65e0\u4eba\u673a\uff08UAV\uff09\u96c6\u7fa4\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u52a8\u6001\u7684\u3001\u5bf9\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u7684\u4efb\u52a1\u4e2d\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u5feb\u901f\u7684\u6001\u52bf\u7406\u89e3\u548c\u81ea\u4e3b\u9002\u5e94\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5c06 MLLM \u4e0e\u65e0\u4eba\u673a\u96c6\u7fa4\u96c6\u6210\u4ee5\u589e\u5f3a\u5404\u79cd\u4efb\u52a1\u4e2d\u7684\u667a\u80fd\u548c\u9002\u5e94\u6027\u7684\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u9996\u5148\u6982\u8ff0\u4e86\u65e0\u4eba\u673a\u548c MLLM \u7684\u57fa\u672c\u67b6\u6784\u548c\u529f\u80fd\u3002\u7136\u540e\uff0c\u6211\u4eec\u5206\u6790\u4e86 MLLM \u5982\u4f55\u5728\u76ee\u6807\u68c0\u6d4b\u3001\u81ea\u4e3b\u5bfc\u822a\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u9762\u63d0\u9ad8\u65e0\u4eba\u673a\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63a2\u7d22\u5c06 MLLM \u96c6\u6210\u5230\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ee5\u68ee\u6797\u6d88\u9632\u4e3a\u91cd\u70b9\u7684\u5b9e\u9645\u6848\u4f8b\u7814\u7a76\u3002\u4e3a\u4e86\u5145\u5206\u5c55\u793a\u6240\u63d0\u51fa\u6846\u67b6\u7684\u80fd\u529b\uff0c\u6211\u4eec\u7814\u7a76\u4e86\u4eba\u673a\u4ea4\u4e92\u3001\u96c6\u7fa4\u4efb\u52a1\u89c4\u5212\u3001\u706b\u707e\u8bc4\u4f30\u548c\u4efb\u52a1\u6267\u884c\u3002\u6700\u540e\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86 MLLM \u652f\u6301\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u6240\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002\u5b9e\u9a8c\u6f14\u793a\u89c6\u9891\u53ef\u5728 https://youtu.be/zwnB9ZSa5A4 \u5728\u7ebf\u89c2\u770b\u3002"}}
{"id": "2506.12742", "pdf": "https://arxiv.org/pdf/2506.12742", "abs": "https://arxiv.org/abs/2506.12742", "authors": ["Yuchen Liu", "Alexiy Buynitsky", "Ruiqi Ni", "Ahmed H. Qureshi"], "title": "Physics-informed Neural Motion Planning via Domain Decomposition in Large Environments", "categories": ["cs.RO"], "comment": null, "summary": "Physics-informed Neural Motion Planners (PiNMPs) provide a data-efficient\nframework for solving the Eikonal Partial Differential Equation (PDE) and\nrepresenting the cost-to-go function for motion planning. However, their\nscalability remains limited by spectral bias and the complex loss landscape of\nPDE-driven training. Domain decomposition mitigates these issues by dividing\nthe environment into smaller subdomains, but existing methods enforce\ncontinuity only at individual spatial points. While effective for function\napproximation, these methods fail to capture the spatial connectivity required\nfor motion planning, where the cost-to-go function depends on both the start\nand goal coordinates rather than a single query point. We propose Finite Basis\nNeural Time Fields (FB-NTFields), a novel neural field representation for\nscalable cost-to-go estimation. Instead of enforcing continuity in output\nspace, FB-NTFields construct a latent space representation, computing the\ncost-to-go as a distance between the latent embeddings of start and goal\ncoordinates. This enables global spatial coherence while integrating domain\ndecomposition, ensuring efficient large-scale motion planning. We validate\nFB-NTFields in complex synthetic and real-world scenarios, demonstrating\nsubstantial improvements over existing PiNMPs. Finally, we deploy our method on\na Unitree B1 quadruped robot, successfully navigating indoor environments. The\nsupplementary videos can be found at https://youtu.be/OpRuCbLNOwM.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on motion planning using neural networks, specifically Physics-informed Neural Motion Planners (PiNMPs). While it doesn't directly involve Large Language Models, it addresses trajectory/motion planning in large environments, which falls under the broader scope of trajectory prediction. The connection to trajectory prediction is through motion planning, which can be seen as predicting a feasible trajectory. The use of neural fields also relates to representation learning for trajectories. However, the absence of LLMs reduces the relevance.", "keywords": ["motion planning", "neural motion planners", "trajectory", "domain decomposition", "cost-to-go function"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u573a\u8868\u793a\u65b9\u6cd5FB-NTFields\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u7684cost-to-go\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u5668(PiNMPs)\u5728\u53ef\u6269\u5c55\u6027\u65b9\u9762\u53d7\u5230\u8c31\u504f\u7f6e\u548cPDE\u9a71\u52a8\u8bad\u7ec3\u7684\u590d\u6742\u635f\u5931\u666f\u89c2\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u57df\u5206\u89e3\u65b9\u6cd5\u4ec5\u5728\u5355\u4e2a\u7a7a\u95f4\u70b9\u4e0a\u5f3a\u5236\u8fde\u7eed\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u8fd0\u52a8\u89c4\u5212\u6240\u9700\u7684\u7a7a\u95f4\u8fde\u901a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u573a\u8868\u793a\u65b9\u6cd5FB-NTFields\uff0c\u901a\u8fc7\u8ba1\u7b97\u8d77\u59cb\u5750\u6807\u548c\u76ee\u6807\u5750\u6807\u7684\u6f5c\u5728\u5d4c\u5165\u4e4b\u95f4\u7684\u8ddd\u79bb\u6765\u4f30\u8ba1cost-to-go\uff0c\u4ece\u800c\u5b9e\u73b0\u5168\u5c40\u7a7a\u95f4\u8fde\u8d2f\u6027\u3002", "result": "FB-NTFields\u5728\u590d\u6742\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6bd4\u73b0\u6709PiNMPs\u7684\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "FB-NTFields\u5728\u590d\u6742\u73af\u5883\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u5e76\u5728Unitree B1\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u5b9e\u73b0\u4e86\u5ba4\u5185\u73af\u5883\u5bfc\u822a\u3002", "summary_zh": "\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u795e\u7ecf\u8fd0\u52a8\u89c4\u5212\u5668(PiNMPs)\u4e3a\u6c42\u89e3Eikonal\u504f\u5fae\u5206\u65b9\u7a0b(PDE)\u548c\u8868\u793a\u8fd0\u52a8\u89c4\u5212\u7684cost-to-go\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6570\u636e\u9ad8\u6548\u7684\u6846\u67b6\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u53ef\u6269\u5c55\u6027\u4ecd\u7136\u53d7\u5230\u8c31\u504f\u7f6e\u548cPDE\u9a71\u52a8\u8bad\u7ec3\u7684\u590d\u6742\u635f\u5931\u666f\u89c2\u7684\u9650\u5236\u3002\u57df\u5206\u89e3\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u5212\u5206\u4e3a\u66f4\u5c0f\u7684\u5b50\u57df\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u5728\u5355\u4e2a\u7a7a\u95f4\u70b9\u4e0a\u5f3a\u5236\u8fde\u7eed\u6027\u3002\u867d\u7136\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u4e8e\u51fd\u6570\u903c\u8fd1\u6709\u6548\uff0c\u4f46\u5b83\u4eec\u65e0\u6cd5\u6355\u6349\u8fd0\u52a8\u89c4\u5212\u6240\u9700\u7684\u7a7a\u95f4\u8fde\u901a\u6027\uff0c\u5176\u4e2dcost-to-go\u51fd\u6570\u53d6\u51b3\u4e8e\u8d77\u59cb\u548c\u76ee\u6807\u5750\u6807\uff0c\u800c\u4e0d\u662f\u5355\u4e2a\u67e5\u8be2\u70b9\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u573a\u8868\u793a\u65b9\u6cd5\uff0c\u79f0\u4e3a\u6709\u9650\u57fa\u795e\u7ecf\u65f6\u95f4\u573a(FB-NTFields)\uff0c\u7528\u4e8e\u53ef\u6269\u5c55\u7684cost-to-go\u4f30\u8ba1\u3002FB-NTFields\u4e0d\u662f\u5728\u8f93\u51fa\u7a7a\u95f4\u4e2d\u5f3a\u5236\u8fde\u7eed\u6027\uff0c\u800c\u662f\u6784\u5efa\u4e00\u4e2a\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\uff0c\u5c06cost-to-go\u8ba1\u7b97\u4e3a\u8d77\u59cb\u5750\u6807\u548c\u76ee\u6807\u5750\u6807\u7684\u6f5c\u5728\u5d4c\u5165\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u8fd9\u5b9e\u73b0\u4e86\u5168\u5c40\u7a7a\u95f4\u8fde\u8d2f\u6027\uff0c\u540c\u65f6\u96c6\u6210\u4e86\u57df\u5206\u89e3\uff0c\u786e\u4fdd\u4e86\u9ad8\u6548\u7684\u5927\u89c4\u6a21\u8fd0\u52a8\u89c4\u5212\u3002\u6211\u4eec\u5728\u590d\u6742\u7684\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86FB-NTFields\uff0c\u8bc1\u660e\u4e86\u5176\u76f8\u5bf9\u4e8e\u73b0\u6709PiNMPs\u7684\u663e\u8457\u6539\u8fdb\u3002\u6700\u540e\uff0c\u6211\u4eec\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u90e8\u7f72\u5728Unitree B1\u56db\u8db3\u673a\u5668\u4eba\u4e0a\uff0c\u6210\u529f\u5730\u5bfc\u822a\u4e86\u5ba4\u5185\u73af\u5883\u3002\u8865\u5145\u89c6\u9891\u53ef\u5728https://youtu.be/OpRuCbLNOwM\u627e\u5230\u3002"}}
{"id": "2506.12421", "pdf": "https://arxiv.org/pdf/2506.12421", "abs": "https://arxiv.org/abs/2506.12421", "authors": ["Dongjie Yang", "Chengqiang Lu", "Qimeng Wang", "Xinbei Ma", "Yan Gao", "Yao Hu", "Hai Zhao"], "title": "Plan Your Travel and Travel with Your Plan: Wide-Horizon Planning and Evaluation via LLM", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Travel planning is a complex task requiring the integration of diverse\nreal-world information and user preferences. While LLMs show promise, existing\nmethods with long-horizon thinking struggle with handling multifaceted\nconstraints and preferences in the context, leading to suboptimal itineraries.\nWe formulate this as an $L^3$ planning problem, emphasizing long context, long\ninstruction, and long output. To tackle this, we introduce Multiple Aspects of\nPlanning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve\ncomplex planning problems. Instead of direct planning, MAoP leverages the\nstrategist to conduct pre-planning from various aspects and provide the\nplanning blueprint for planning models, enabling strong inference-time\nscalability for better performance. In addition, current benchmarks overlook\ntravel's dynamic nature, where past events impact subsequent journeys, failing\nto reflect real-world feasibility. To address this, we propose Travel-Sim, an\nagent-based benchmark assessing plans via real-world travel simulation. This\nwork advances LLM capabilities in complex planning and offers novel insights\nfor evaluating sophisticated scenarios through agent-based simulation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528LLM\u8fdb\u884c\u65c5\u884c\u89c4\u5212\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u89c4\u5212\u95ee\u9898\u4e0e\u8def\u5f84\u89c4\u5212\u548c\u79fb\u52a8\u7269\u4f53\u8def\u5f84\u89c4\u5212\u6709\u4e00\u5b9a\u5173\u8054\u3002\u8bba\u6587\u91cd\u70b9\u5728\u4e8eLLM\u5728\u957f\u7a0b\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u53ca\u4f7f\u7528agent-based simulation\u8bc4\u4f30\u8ba1\u5212\u7684\u6709\u6548\u6027\u3002\u56e0\u6b64\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90fd\u6709\u4e00\u5b9a\u76f8\u5173\u6027\uff0c\u4f46\u4e3b\u8981\u4fa7\u91cd\u4e8eLLM\u7684\u5e94\u7528\u3002", "keywords": ["LLM", "Large Language Models", "planning", "agent-based simulation", "long-horizon planning", "travel planning"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65b9\u9762\u89c4\u5212\uff08MAoP\uff09\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u65c5\u884c\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5Travel-Sim\uff0c\u4ee5\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u65c5\u884c\u89c4\u5212\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "Travel planning is a complex task requiring the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints and preferences in the context, leading to suboptimal itineraries.", "method": "We introduce Multiple Aspects of Planning (MAoP), enabling LLMs to conduct wide-horizon thinking to solve complex planning problems. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planning models", "result": "Travel-Sim, an agent-based benchmark assessing plans via real-world travel simulation.", "conclusion": "This work advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through agent-based simulation.", "summary_zh": "\u65c5\u884c\u89c4\u5212\u662f\u4e00\u9879\u590d\u6742\u7684\u4efb\u52a1\uff0c\u9700\u8981\u6574\u5408\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u7684\u4fe1\u606f\u548c\u7528\u6237\u504f\u597d\u3002\u867d\u7136\u5927\u578b\u8bed\u8a00\u6a21\u578b\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7684\u957f\u7a0b\u601d\u7ef4\u65b9\u6cd5\u5728\u5904\u7406\u4e0a\u4e0b\u6587\u4e2d\u591a\u65b9\u9762\u7684\u7ea6\u675f\u548c\u504f\u597d\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5bfc\u81f4\u884c\u7a0b\u5b89\u6392\u4e0d\u5c3d\u5982\u4eba\u610f\u3002\u6211\u4eec\u5c06\u6b64\u95ee\u9898\u5b9a\u4e49\u4e3a\u4e00\u4e2a L^3 \u89c4\u5212\u95ee\u9898\uff0c\u5f3a\u8c03\u957f\u4e0a\u4e0b\u6587\u3001\u957f\u6307\u4ee4\u548c\u957f\u8f93\u51fa\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u591a\u65b9\u9762\u89c4\u5212\uff08MAoP\uff09\uff0c\u4f7f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u5e7f\u6cdb\u7684\u6c34\u5e73\u601d\u8003\uff0c\u4ece\u800c\u89e3\u51b3\u590d\u6742\u7684\u89c4\u5212\u95ee\u9898\u3002MAoP \u5e76\u975e\u76f4\u63a5\u89c4\u5212\uff0c\u800c\u662f\u5229\u7528\u7b56\u7565\u5668\u4ece\u5404\u4e2a\u65b9\u9762\u8fdb\u884c\u9884\u89c4\u5212\uff0c\u5e76\u4e3a\u89c4\u5212\u6a21\u578b\u63d0\u4f9b\u89c4\u5212\u84dd\u56fe\uff0c\u4ece\u800c\u5b9e\u73b0\u5f3a\u5927\u7684\u63a8\u7406\u65f6\u53ef\u6269\u5c55\u6027\uff0c\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5f53\u524d\u7684\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u7565\u4e86\u65c5\u884c\u7684\u52a8\u6001\u6027\uff0c\u5373\u8fc7\u53bb\u7684\u4e8b\u4ef6\u4f1a\u5f71\u54cd\u540e\u7eed\u7684\u65c5\u7a0b\uff0c\u672a\u80fd\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u7684\u53ef\u884c\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 Travel-Sim\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684\u65c5\u884c\u6a21\u62df\u6765\u8bc4\u4f30\u8ba1\u5212\u3002\u8fd9\u9879\u5de5\u4f5c\u63d0\u9ad8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c4\u5212\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u4e3a\u901a\u8fc7\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u62df\u8bc4\u4f30\u590d\u6742\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2506.13106", "pdf": "https://arxiv.org/pdf/2506.13106", "abs": "https://arxiv.org/abs/2506.13106", "authors": ["Fen Liu", "Shenghai Yuan", "Thien-Minh Nguyen", "Rong Su"], "title": "Autonomous 3D Moving Target Encirclement and Interception with Range measurement", "categories": ["cs.RO", "eess.SP"], "comment": "Paper has been accepted into IROS 2025", "summary": "Commercial UAVs are an emerging security threat as they are capable of\ncarrying hazardous payloads or disrupting air traffic. To counter UAVs, we\nintroduce an autonomous 3D target encirclement and interception strategy.\nUnlike traditional ground-guided systems, this strategy employs autonomous\ndrones to track and engage non-cooperative hostile UAVs, which is effective in\nnon-line-of-sight conditions, GPS denial, and radar jamming, where conventional\ndetection and neutralization from ground guidance fail. Using two noisy\nreal-time distances measured by drones, guardian drones estimate the relative\nposition from their own to the target using observation and velocity\ncompensation methods, based on anti-synchronization (AS) and an X$-$Y circular\nmotion combined with vertical jitter. An encirclement control mechanism is\nproposed to enable UAVs to adaptively transition from encircling and protecting\na target to encircling and monitoring a hostile target. Upon breaching a\nwarning threshold, the UAVs may even employ a suicide attack to neutralize the\nhostile target. We validate this strategy through real-world UAV experiments\nand simulated analysis in MATLAB, demonstrating its effectiveness in detecting,\nencircling, and intercepting hostile drones. More details:\nhttps://youtu.be/5eHW56lPVto.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on autonomous drone encirclement and interception, which involves trajectory planning and prediction for UAVs. While it doesn't directly involve large language models, the autonomous navigation and interception strategies inherently rely on predicting the target's future trajectory based on noisy measurements. Therefore, there's a moderate level of relevance to trajectory prediction.", "keywords": ["trajectory prediction", "autonomous drones", "UAV", "interception", "target encirclement", "path planning"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u65e0\u4eba\u673a3D\u5305\u56f4\u62e6\u622a\u7b56\u7565\uff0c\u7528\u4e8e\u5bf9\u6297\u643a\u5e26\u5371\u9669\u7269\u54c1\u6216\u6270\u4e71\u7a7a\u4e2d\u4ea4\u901a\u7684\u5546\u7528\u65e0\u4eba\u673a\u5a01\u80c1\u3002", "motivation": "\u5546\u7528\u65e0\u4eba\u673a\u6b63\u6210\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u56e0\u4e3a\u5b83\u4eec\u80fd\u591f\u643a\u5e26\u5371\u9669\u7684\u6709\u6548\u8f7d\u8377\u6216\u6270\u4e71\u7a7a\u4e2d\u4ea4\u901a\u3002\u4e3a\u4e86\u5bf9\u6297\u65e0\u4eba\u673a\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u4e3b\u76843D\u76ee\u6807\u5305\u56f4\u548c\u62e6\u622a\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u540c\u6b65(AS)\u548cX-Y\u5706\u5f62\u8fd0\u52a8\u7ed3\u5408\u5782\u76f4\u6296\u52a8\u7684\u89c2\u6d4b\u548c\u901f\u5ea6\u8865\u507f\u65b9\u6cd5\uff0c\u5229\u7528\u65e0\u4eba\u673a\u6d4b\u91cf\u7684\u4e24\u4e2a\u566a\u58f0\u5b9e\u65f6\u8ddd\u79bb\u6765\u4f30\u8ba1\u76f8\u5bf9\u4f4d\u7f6e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u56f4\u63a7\u5236\u673a\u5236\uff0c\u4f7f\u65e0\u4eba\u673a\u80fd\u591f\u81ea\u9002\u5e94\u5730\u4ece\u5305\u56f4\u548c\u4fdd\u62a4\u76ee\u6807\u8fc7\u6e21\u5230\u5305\u56f4\u548c\u76d1\u89c6\u654c\u5bf9\u76ee\u6807\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684\u65e0\u4eba\u673a\u5b9e\u9a8c\u548cMATLAB\u7684\u6a21\u62df\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u5728\u63a2\u6d4b\u3001\u5305\u56f4\u548c\u62e6\u622a\u654c\u5bf9\u65e0\u4eba\u673a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u771f\u5b9e\u65e0\u4eba\u673a\u5b9e\u9a8c\u548cMATLAB\u6a21\u62df\u5206\u6790\u9a8c\u8bc1\u4e86\u8be5\u7b56\u7565\u5728\u68c0\u6d4b\u3001\u5305\u56f4\u548c\u62e6\u622a\u654c\u5bf9\u65e0\u4eba\u673a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "summary_zh": "\u5546\u7528\u65e0\u4eba\u673a\u6b63\u6210\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u56e0\u4e3a\u5b83\u4eec\u80fd\u591f\u643a\u5e26\u5371\u9669\u7684\u6709\u6548\u8f7d\u8377\u6216\u6270\u4e71\u7a7a\u4e2d\u4ea4\u901a\u3002\u4e3a\u4e86\u5bf9\u6297\u65e0\u4eba\u673a\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u81ea\u4e3b\u76843D\u76ee\u6807\u5305\u56f4\u548c\u62e6\u622a\u7b56\u7565\u3002\u4e0e\u4f20\u7edf\u7684\u5730\u9762\u5f15\u5bfc\u7cfb\u7edf\u4e0d\u540c\uff0c\u8be5\u7b56\u7565\u91c7\u7528\u81ea\u4e3b\u65e0\u4eba\u673a\u6765\u8ddf\u8e2a\u548c\u653b\u51fb\u975e\u5408\u4f5c\u7684\u654c\u5bf9\u65e0\u4eba\u673a\uff0c\u8be5\u7b56\u7565\u5728\u975e\u89c6\u8ddd\u6761\u4ef6\u4e0b\u3001GPS\u5e72\u6270\u548c\u96f7\u8fbe\u5e72\u6270\u4e0b\u6709\u6548\uff0c\u800c\u5728\u5730\u9762\u5f15\u5bfc\u4e0b\u7684\u4f20\u7edf\u63a2\u6d4b\u548c\u4e2d\u548c\u65b9\u6cd5\u5219\u4f1a\u5931\u6548\u3002\u5229\u7528\u65e0\u4eba\u673a\u6d4b\u91cf\u7684\u4e24\u4e2a\u566a\u58f0\u5b9e\u65f6\u8ddd\u79bb\uff0c\u5b88\u62a4\u65e0\u4eba\u673a\u4f7f\u7528\u57fa\u4e8e\u53cd\u540c\u6b65(AS)\u548cX-Y\u5706\u5f62\u8fd0\u52a8\u7ed3\u5408\u5782\u76f4\u6296\u52a8\u7684\u89c2\u6d4b\u548c\u901f\u5ea6\u8865\u507f\u65b9\u6cd5\uff0c\u4f30\u8ba1\u4ece\u81ea\u8eab\u5230\u76ee\u6807\u7684\u76f8\u5bf9\u4f4d\u7f6e\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u56f4\u63a7\u5236\u673a\u5236\uff0c\u4f7f\u65e0\u4eba\u673a\u80fd\u591f\u81ea\u9002\u5e94\u5730\u4ece\u5305\u56f4\u548c\u4fdd\u62a4\u76ee\u6807\u8fc7\u6e21\u5230\u5305\u56f4\u548c\u76d1\u89c6\u654c\u5bf9\u76ee\u6807\u3002\u4e00\u65e6\u7a81\u7834\u8b66\u544a\u9608\u503c\uff0c\u65e0\u4eba\u673a\u751a\u81f3\u53ef\u4ee5\u91c7\u53d6\u81ea\u6740\u5f0f\u653b\u51fb\u6765\u6467\u6bc1\u654c\u5bf9\u76ee\u6807\u3002\u6211\u4eec\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u7684\u65e0\u4eba\u673a\u5b9e\u9a8c\u548cMATLAB\u7684\u6a21\u62df\u5206\u6790\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63a2\u6d4b\u3001\u5305\u56f4\u548c\u62e6\u622a\u654c\u5bf9\u65e0\u4eba\u673a\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u66f4\u591a\u7ec6\u8282\u8bf7\u8bbf\u95ee\uff1ahttps://youtu.be/5eHW56lPVto\u3002"}}
{"id": "2506.12213", "pdf": "https://arxiv.org/pdf/2506.12213", "abs": "https://arxiv.org/abs/2506.12213", "authors": ["Zikai Zhang", "Ping Liu", "Jiahao Xu", "Rui Hu"], "title": "Fed-HeLLo: Efficient Federated Foundation Model Fine-Tuning with Heterogeneous LoRA Allocation", "categories": ["cs.LG", "cs.DC"], "comment": "Accepted to TNNLS 2025", "summary": "Federated Learning has recently been utilized to collaboratively fine-tune\nfoundation models across multiple clients. Notably, federated low-rank\nadaptation LoRA-based fine-tuning methods have recently gained attention, which\nallows clients to fine-tune FMs with a small portion of trainable parameters\nlocally. However, most existing methods do not account for the heterogeneous\nresources of clients or lack an effective local training strategy to maximize\nglobal fine-tuning performance under limited resources. In this work, we\npropose Fed-HeLLo, a novel federated LoRA-based fine-tuning framework that\nenables clients to collaboratively fine-tune an FM with different local\ntrainable LoRA layers. To ensure its effectiveness, we develop several\nheterogeneous LoRA allocation (HLA) strategies that adaptively allocate local\ntrainable LoRA layers based on clients' resource capabilities and the layer\nimportance. Specifically, based on the dynamic layer importance, we design a\nFisher Information Matrix score-based HLA that leverages dynamic gradient norm\ninformation. To better stabilize the training process, we consider the\nintrinsic importance of LoRA layers and design a Geometrically-Defined HLA\nstrategy. It shapes the collective distribution of trainable LoRA layers into\nspecific geometric patterns, such as Triangle, Inverted Triangle, Bottleneck,\nand Uniform. Moreover, we extend GD-HLA into a randomized version, named\nRandomized Geometrically-Defined HLA, for enhanced model accuracy with\nrandomness. By co-designing the proposed HLA strategies, we incorporate both\nthe dynamic and intrinsic layer importance into the design of our HLA strategy.\nWe evaluate our approach on five datasets under diverse federated LoRA\nfine-tuning settings, covering three levels of data distribution from IID to\nextreme Non-IID. Results show that Fed-HeLLo with HLA strategies is both\neffective and efficient.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Federated Learning for fine-tuning foundation models (specifically LoRA). While it doesn't directly address trajectory prediction, the use of foundation models places it within the scope of large language models. The federated learning aspect could be relevant if applied to trajectory prediction tasks in a distributed setting, but this is not explicitly mentioned.", "keywords": ["foundation models", "large language models", "federated learning", "LoRA"]}, "AI": {"tldr": "Fed-HeLLo\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6LoRA\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6784LoRA\u5206\u914d\u7b56\u7565\uff0c\u4f7f\u5ba2\u6237\u7aef\u80fd\u591f\u6839\u636e\u81ea\u8eab\u8d44\u6e90\u548c\u5c42\u91cd\u8981\u6027\u534f\u540c\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u6ca1\u6709\u8003\u8651\u5ba2\u6237\u7aef\u7684\u5f02\u6784\u8d44\u6e90\uff0c\u6216\u8005\u7f3a\u4e4f\u6709\u6548\u7684\u672c\u5730\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u5728\u6709\u9650\u7684\u8d44\u6e90\u4e0b\u6700\u5927\u5316\u5168\u5c40\u5fae\u8c03\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8054\u90a6LoRA\u7684\u5fae\u8c03\u6846\u67b6Fed-HeLLo\uff0c\u8be5\u6846\u67b6\u5141\u8bb8\u5ba2\u6237\u7aef\u4f7f\u7528\u4e0d\u540c\u7684\u672c\u5730\u53ef\u8bad\u7ec3LoRA\u5c42\u534f\u540c\u5fae\u8c03FM\u3002\u4e3a\u4e86\u786e\u4fdd\u5176\u6709\u6548\u6027\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u51e0\u79cd\u5f02\u6784LoRA\u5206\u914d\uff08HLA\uff09\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u57fa\u4e8e\u5ba2\u6237\u7aef\u7684\u8d44\u6e90\u80fd\u529b\u548c\u5c42\u91cd\u8981\u6027\u81ea\u9002\u5e94\u5730\u5206\u914d\u672c\u5730\u53ef\u8bad\u7ec3LoRA\u5c42\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u4eceIID\u5230\u6781\u7aefNon-IID\u7684\u4e09\u79cd\u6570\u636e\u5206\u5e03\u7ea7\u522b\u4e0b\uff0c\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5e26\u6709HLA\u7b56\u7565\u7684Fed-HeLLo\u65e2\u6709\u6548\u53c8\u9ad8\u6548\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFed-HeLLo\u4e0eHLA\u7b56\u7565\u5728\u4e0d\u540c\u8054\u90a6LoRA\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u5747\u6709\u6548\u4e14\u9ad8\u6548\u3002", "summary_zh": "\u8054\u90a6\u5b66\u4e60\u6700\u8fd1\u88ab\u7528\u4e8e\u8de8\u591a\u4e2a\u5ba2\u6237\u7aef\u534f\u540c\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u57fa\u4e8e\u8054\u90a6\u4f4e\u79e9\u9002\u914dLoRA\u7684\u5fae\u8c03\u65b9\u6cd5\u6700\u8fd1\u53d7\u5230\u4e86\u5173\u6ce8\uff0c\u5b83\u5141\u8bb8\u5ba2\u6237\u7aef\u5728\u672c\u5730\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206\u53ef\u8bad\u7ec3\u53c2\u6570\u6765\u5fae\u8c03FM\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u6ca1\u6709\u8003\u8651\u5ba2\u6237\u7aef\u7684\u5f02\u6784\u8d44\u6e90\uff0c\u6216\u8005\u7f3a\u4e4f\u6709\u6548\u7684\u672c\u5730\u8bad\u7ec3\u7b56\u7565\uff0c\u4ee5\u5728\u6709\u9650\u7684\u8d44\u6e90\u4e0b\u6700\u5927\u5316\u5168\u5c40\u5fae\u8c03\u6027\u80fd\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8054\u90a6LoRA\u7684\u5fae\u8c03\u6846\u67b6Fed-HeLLo\uff0c\u8be5\u6846\u67b6\u5141\u8bb8\u5ba2\u6237\u7aef\u4f7f\u7528\u4e0d\u540c\u7684\u672c\u5730\u53ef\u8bad\u7ec3LoRA\u5c42\u534f\u540c\u5fae\u8c03FM\u3002\u4e3a\u4e86\u786e\u4fdd\u5176\u6709\u6548\u6027\uff0c\u6211\u4eec\u5f00\u53d1\u4e86\u51e0\u79cd\u5f02\u6784LoRA\u5206\u914d\uff08HLA\uff09\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u57fa\u4e8e\u5ba2\u6237\u7aef\u7684\u8d44\u6e90\u80fd\u529b\u548c\u5c42\u91cd\u8981\u6027\u81ea\u9002\u5e94\u5730\u5206\u914d\u672c\u5730\u53ef\u8bad\u7ec3LoRA\u5c42\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u57fa\u4e8e\u52a8\u6001\u5c42\u91cd\u8981\u6027\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eFisher\u4fe1\u606f\u77e9\u9635\u8bc4\u5206\u7684HLA\uff0c\u5b83\u5229\u7528\u52a8\u6001\u68af\u5ea6\u8303\u6570\u4fe1\u606f\u3002\u4e3a\u4e86\u66f4\u597d\u5730\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u6211\u4eec\u8003\u8651\u4e86LoRA\u5c42\u7684\u5185\u5728\u91cd\u8981\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u51e0\u4f55\u5b9a\u4e49\u7684HLA\u7b56\u7565\u3002\u5b83\u5c06\u53ef\u8bad\u7ec3LoRA\u5c42\u7684\u96c6\u4f53\u5206\u5e03\u5851\u9020\u6210\u7279\u5b9a\u7684\u51e0\u4f55\u6a21\u5f0f\uff0c\u5982\u4e09\u89d2\u5f62\u3001\u5012\u4e09\u89d2\u5f62\u3001\u74f6\u9888\u548c\u5747\u5300\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c06GD-HLA\u6269\u5c55\u5230\u968f\u673a\u7248\u672c\uff0c\u540d\u4e3a\u968f\u673a\u51e0\u4f55\u5b9a\u4e49HLA\uff0c\u4ee5\u901a\u8fc7\u968f\u673a\u6027\u589e\u5f3a\u6a21\u578b\u7cbe\u5ea6\u3002\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u6240\u63d0\u51fa\u7684HLA\u7b56\u7565\uff0c\u6211\u4eec\u5c06\u52a8\u6001\u548c\u5185\u5728\u5c42\u91cd\u8981\u6027\u90fd\u7eb3\u5165\u4e86\u6211\u4eec\u7684HLA\u7b56\u7565\u7684\u8bbe\u8ba1\u4e2d\u3002\u6211\u4eec\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u4eceIID\u5230\u6781\u7aefNon-IID\u7684\u4e09\u79cd\u6570\u636e\u5206\u5e03\u7ea7\u522b\u4e0b\uff0c\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5e26\u6709HLA\u7b56\u7565\u7684Fed-HeLLo\u65e2\u6709\u6548\u53c8\u9ad8\u6548\u3002"}}
{"id": "2506.13202", "pdf": "https://arxiv.org/pdf/2506.13202", "abs": "https://arxiv.org/abs/2506.13202", "authors": ["Bin-Bin Hu", "Yanxin Zhou", "Henglai Wei", "Shuo Cheng", "Chen Lv"], "title": "C2TE: Coordinated Constrained Task Execution Design for Ordering-Flexible Multi-Vehicle Platoon Merging", "categories": ["cs.RO"], "comment": null, "summary": "In this paper, we propose a distributed coordinated constrained task\nexecution (C2TE) algorithm that enables a team of vehicles from different lanes\nto cooperatively merge into an {\\it ordering-flexible platoon} maneuvering on\nthe desired lane. Therein, the platoon is flexible in the sense that no\nspecific spatial ordering sequences of vehicles are predetermined. To attain\nsuch a flexible platoon, we first separate the multi-vehicle platoon (MVP)\nmerging mission into two stages, namely, pre-merging regulation and {\\it\nordering-flexible platoon} merging, and then formulate them into distributed\nconstraint-based optimization problems. Particularly, by encoding\nlongitudinal-distance regulation and same-lane collision avoidance subtasks\ninto the corresponding control barrier function (CBF) constraints, the proposed\nalgorithm in Stage 1 can safely enlarge sufficient longitudinal distances among\nadjacent vehicles. Then, by encoding lateral convergence, longitudinal-target\nattraction, and neighboring collision avoidance subtasks into CBF constraints,\nthe proposed algorithm in Stage~2 can efficiently achieve the {\\it\nordering-flexible platoon}. Note that the {\\it ordering-flexible platoon} is\nrealized through the interaction of the longitudinal-target attraction and\ntime-varying neighboring collision avoidance constraints simultaneously.\nFeasibility guarantee and rigorous convergence analysis are both provided under\nstrong nonlinear couplings induced by flexible orderings. Finally, experiments\nusing three autonomous mobile vehicles (AMVs) are conducted to verify the\neffectiveness and flexibility of the proposed algorithm, and extensive\nsimulations are performed to demonstrate its robustness, adaptability, and\nscalability when tackling vehicles' sudden breakdown, new appearing, different\nnumber of lanes, mixed autonomy, and large-scale scenarios, respectively.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u8f66\u8f86\u7f16\u961f\u5408\u5e76\u7684\u63a7\u5236\u7b97\u6cd5\uff0c\u6d89\u53ca\u8f66\u8f86\u8f68\u8ff9\u89c4\u5212\u548c\u907f\u649e\u7b49\u95ee\u9898\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u6a21\u578b\uff0c\u4f46\u5176\u4e2d\u6d89\u53ca\u7684\u4f18\u5316\u548c\u63a7\u5236\u7b56\u7565\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u4e0e\u5927\u6a21\u578b\u7ed3\u5408\uff0c\u4f8b\u5982\u4f7f\u7528\u5927\u6a21\u578b\u8fdb\u884c\u73af\u5883\u611f\u77e5\u548c\u9884\u6d4b\uff0c\u4ece\u800c\u6539\u8fdb\u7f16\u961f\u63a7\u5236\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b", "\u79fb\u52a8\u7269\u4f53\u8def\u5f84\u89c4\u5212", "\u591a\u8f66\u8f86\u7f16\u961f", "\u8f66\u8f86\u907f\u649e"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u534f\u8c03\u7ea6\u675f\u4efb\u52a1\u6267\u884c\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5f02\u8f66\u9053\u8f66\u8f86\u7075\u6d3b\u6392\u5e8f\u7684\u534f\u540c\u5408\u5e76\u3002", "motivation": "\u89e3\u51b3\u5f02\u8f66\u9053\u8f66\u8f86\u534f\u540c\u5408\u5e76\u4e3a\u671f\u671b\u8f66\u9053\u4e0a\u7684\u6392\u5e8f\u7075\u6d3b\u7f16\u961f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u534f\u8c03\u7ea6\u675f\u4efb\u52a1\u6267\u884c\uff08C2TE\uff09\u7b97\u6cd5\uff0c\u5c06\u591a\u8f66\u7f16\u961f\u5408\u5e76\u4efb\u52a1\u5206\u4e3a\u9884\u5408\u5e76\u8c03\u8282\u548c\u6392\u5e8f\u7075\u6d3b\u7684\u7f16\u961f\u5408\u5e76\u4e24\u4e2a\u9636\u6bb5\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u5206\u5e03\u5f0f\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "result": "\u6240\u63d0\u7b97\u6cd5\u80fd\u591f\u5b89\u5168\u5730\u6269\u5927\u76f8\u90bb\u8f66\u8f86\u4e4b\u95f4\u7684\u7eb5\u5411\u8ddd\u79bb\uff0c\u5e76\u6709\u6548\u5730\u5b9e\u73b0\u6392\u5e8f\u7075\u6d3b\u7684\u7f16\u961f\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u3001\u7075\u6d3b\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "summary_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u534f\u8c03\u7ea6\u675f\u4efb\u52a1\u6267\u884c\uff08C2TE\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u6765\u81ea\u4e0d\u540c\u8f66\u9053\u7684\u8f66\u8f86\u56e2\u961f\u80fd\u591f\u534f\u540c\u5408\u5e76\u5230\u671f\u671b\u8f66\u9053\u4e0a\u8fdb\u884c\u673a\u52a8\u7684\u201c\u6392\u5e8f\u7075\u6d3b\u7f16\u961f\u201d\u4e2d\u3002\u5176\u4e2d\uff0c\u7f16\u961f\u662f\u7075\u6d3b\u7684\uff0c\u56e0\u4e3a\u6ca1\u6709\u9884\u5148\u786e\u5b9a\u8f66\u8f86\u7684\u7279\u5b9a\u7a7a\u95f4\u6392\u5e8f\u5e8f\u5217\u3002\u4e3a\u4e86\u83b7\u5f97\u8fd9\u79cd\u7075\u6d3b\u7684\u7f16\u961f\uff0c\u6211\u4eec\u9996\u5148\u5c06\u591a\u8f66\u7f16\u961f\uff08MVP\uff09\u5408\u5e76\u4efb\u52a1\u5206\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff0c\u5373\u9884\u5408\u5e76\u8c03\u8282\u548c\u201c\u6392\u5e8f\u7075\u6d3b\u7f16\u961f\u201d\u5408\u5e76\uff0c\u7136\u540e\u5c06\u5b83\u4eec\u8f6c\u5316\u4e3a\u57fa\u4e8e\u5206\u5e03\u5f0f\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u3002\u7279\u522b\u662f\uff0c\u901a\u8fc7\u5c06\u7eb5\u5411\u8ddd\u79bb\u8c03\u8282\u548c\u540c\u8f66\u9053\u907f\u649e\u5b50\u4efb\u52a1\u7f16\u7801\u5230\u76f8\u5e94\u7684\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBF\uff09\u7ea6\u675f\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u7b2c\u4e00\u9636\u6bb5\u7b97\u6cd5\u53ef\u4ee5\u5b89\u5168\u5730\u6269\u5927\u76f8\u90bb\u8f66\u8f86\u4e4b\u95f4\u7684\u7eb5\u5411\u8ddd\u79bb\u3002\u7136\u540e\uff0c\u901a\u8fc7\u5c06\u6a2a\u5411\u6536\u655b\u3001\u7eb5\u5411\u76ee\u6807\u5438\u5f15\u548c\u76f8\u90bb\u907f\u649e\u5b50\u4efb\u52a1\u7f16\u7801\u5230CBF\u7ea6\u675f\u4e2d\uff0c\u6240\u63d0\u51fa\u7684\u7b2c\u4e8c\u9636\u6bb5\u7b97\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5b9e\u73b0\u201c\u6392\u5e8f\u7075\u6d3b\u7f16\u961f\u201d\u3002\u8bf7\u6ce8\u610f\uff0c\u201c\u6392\u5e8f\u7075\u6d3b\u7f16\u961f\u201d\u662f\u901a\u8fc7\u7eb5\u5411\u76ee\u6807\u5438\u5f15\u548c\u65f6\u53d8\u76f8\u90bb\u907f\u649e\u7ea6\u675f\u540c\u65f6\u76f8\u4e92\u4f5c\u7528\u6765\u5b9e\u73b0\u7684\u3002\u5728\u7075\u6d3b\u6392\u5e8f\u5f15\u8d77\u7684\u5f3a\u975e\u7ebf\u6027\u8026\u5408\u4e0b\uff0c\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u4fdd\u8bc1\u548c\u4e25\u683c\u7684\u6536\u655b\u6027\u5206\u6790\u3002\u6700\u540e\uff0c\u4f7f\u7528\u4e09\u4e2a\u81ea\u4e3b\u79fb\u52a8\u8f66\u8f86\uff08AMV\uff09\u8fdb\u884c\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b97\u6cd5\u7684\u6709\u6548\u6027\u548c\u7075\u6d3b\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u4eff\u771f\uff0c\u4ee5\u8bc1\u660e\u5176\u5728\u5904\u7406\u8f66\u8f86\u7a81\u7136\u6545\u969c\u3001\u65b0\u51fa\u73b0\u3001\u4e0d\u540c\u8f66\u9053\u6570\u91cf\u3001\u6df7\u5408\u81ea\u4e3b\u548c\u5927\u89c4\u6a21\u573a\u666f\u65f6\u7684\u9c81\u68d2\u6027\u3001\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.12664", "pdf": "https://arxiv.org/pdf/2506.12664", "abs": "https://arxiv.org/abs/2506.12664", "authors": ["Cong Chen", "Omer Karaduman", "Xu Kuang"], "title": "Behavioral Generative Agents for Energy Operations", "categories": ["cs.AI", "cs.SY", "eess.SY"], "comment": "33 pages, 14 figures", "summary": "Accurately modeling consumer behavior in energy operations remains\nchallenging due to inherent uncertainties, behavioral complexities, and limited\nempirical data. This paper introduces a novel approach leveraging generative\nagents--artificial agents powered by large language models--to realistically\nsimulate customer decision-making in dynamic energy operations. We demonstrate\nthat these agents behave more optimally and rationally in simpler market\nscenarios, while their performance becomes more variable and suboptimal as task\ncomplexity rises. Furthermore, the agents exhibit heterogeneous customer\npreferences, consistently maintaining distinct, persona-driven reasoning\npatterns. Our findings highlight the potential value of integrating generative\nagents into energy management simulations to improve the design and\neffectiveness of energy policies and incentive programs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses large language models to create generative agents that simulate customer behavior in energy operations. While it doesn't directly predict trajectories in a physical space, it does model decision-making processes over time, which can be seen as a form of behavioral trajectory prediction. The primary connection is through the use of large language models.", "keywords": ["large language models", "generative agents", "behavioral modeling"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u6a21\u62df\u5ba2\u6237\u51b3\u7b56\uff0c\u4ee5\u6539\u8fdb\u80fd\u6e90\u653f\u7b56\u548c\u6fc0\u52b1\u8ba1\u5212\u7684\u8bbe\u8ba1\u3002", "motivation": "\u7531\u4e8e\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u884c\u4e3a\u590d\u6742\u6027\u548c\u6709\u9650\u7684\u7ecf\u9a8c\u6570\u636e\uff0c\u51c6\u786e\u5730\u5efa\u6a21\u80fd\u6e90\u8fd0\u8425\u4e2d\u7684\u6d88\u8d39\u8005\u884c\u4e3a\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u5229\u7528\u751f\u6210\u5f0f\u667a\u80fd\u4f53\uff08\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4eba\u5de5\u667a\u80fd\u4f53\uff09\u6765\u771f\u5b9e\u5730\u6a21\u62df\u52a8\u6001\u80fd\u6e90\u8fd0\u8425\u4e2d\u7684\u5ba2\u6237\u51b3\u7b56\u3002", "result": "\u8fd9\u4e9b\u667a\u80fd\u4f53\u5728\u7b80\u5355\u7684\u5e02\u573a\u573a\u666f\u4e2d\u8868\u73b0\u5f97\u66f4\u4f18\u5316\u548c\u7406\u6027\uff0c\u800c\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u53d8\u5f97\u66f4\u52a0\u591a\u53d8\u548c\u6b21\u4f18\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u5ba2\u6237\u504f\u597d\uff0c\u59cb\u7ec8\u4fdd\u6301\u72ec\u7279\u7684\u3001\u7531\u89d2\u8272\u9a71\u52a8\u7684\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "\u5c06\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u96c6\u6210\u5230\u80fd\u6e90\u7ba1\u7406\u6a21\u62df\u4e2d\uff0c\u53ef\u4ee5\u63d0\u9ad8\u80fd\u6e90\u653f\u7b56\u548c\u6fc0\u52b1\u8ba1\u5212\u7684\u8bbe\u8ba1\u548c\u6709\u6548\u6027\u3002", "summary_zh": "\u7531\u4e8e\u56fa\u6709\u7684\u4e0d\u786e\u5b9a\u6027\u3001\u884c\u4e3a\u590d\u6742\u6027\u548c\u6709\u9650\u7684\u7ecf\u9a8c\u6570\u636e\uff0c\u51c6\u786e\u5730\u5efa\u6a21\u80fd\u6e90\u8fd0\u8425\u4e2d\u7684\u6d88\u8d39\u8005\u884c\u4e3a\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u5f0f\u667a\u80fd\u4f53\uff08\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4eba\u5de5\u667a\u80fd\u4f53\uff09\u6765\u771f\u5b9e\u5730\u6a21\u62df\u52a8\u6001\u80fd\u6e90\u8fd0\u8425\u4e2d\u7684\u5ba2\u6237\u51b3\u7b56\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u5728\u7b80\u5355\u7684\u5e02\u573a\u573a\u666f\u4e2d\u8868\u73b0\u5f97\u66f4\u4f18\u5316\u548c\u7406\u6027\uff0c\u800c\u968f\u7740\u4efb\u52a1\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u5b83\u4eec\u7684\u6027\u80fd\u53d8\u5f97\u66f4\u52a0\u591a\u53d8\u548c\u6b21\u4f18\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u667a\u80fd\u4f53\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u5ba2\u6237\u504f\u597d\uff0c\u59cb\u7ec8\u4fdd\u6301\u72ec\u7279\u7684\u3001\u7531\u89d2\u8272\u9a71\u52a8\u7684\u63a8\u7406\u6a21\u5f0f\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5c06\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u96c6\u6210\u5230\u80fd\u6e90\u7ba1\u7406\u6a21\u62df\u4e2d\uff0c\u53ef\u4ee5\u63d0\u9ad8\u80fd\u6e90\u653f\u7b56\u548c\u6fc0\u52b1\u8ba1\u5212\u7684\u8bbe\u8ba1\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2506.13428", "pdf": "https://arxiv.org/pdf/2506.13428", "abs": "https://arxiv.org/abs/2506.13428", "authors": ["Jiaming Chen", "Yiyu Jiang", "Aoshen Huang", "Yang Li", "Wei Pan"], "title": "VLM-SFD: VLM-Assisted Siamese Flow Diffusion Framework for Dual-Arm Cooperative Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Dual-arm cooperative manipulation holds great promise for tackling complex\nreal-world tasks that demand seamless coordination and adaptive dynamics.\nDespite substantial progress in learning-based motion planning, most approaches\nstruggle to generalize across diverse manipulation tasks and adapt to dynamic,\nunstructured environments, particularly in scenarios involving interactions\nbetween two objects such as assembly, tool use, and bimanual grasping. To\naddress these challenges, we introduce a novel VLM-Assisted Siamese Flow\nDiffusion (VLM-SFD) framework for efficient imitation learning in dual-arm\ncooperative manipulation. The proposed VLM-SFD framework exhibits outstanding\nadaptability, significantly enhancing the ability to rapidly adapt and\ngeneralize to diverse real-world tasks from only a minimal number of human\ndemonstrations. Specifically, we propose a Siamese Flow Diffusion Network\n(SFDNet) employs a dual-encoder-decoder Siamese architecture to embed two\ntarget objects into a shared latent space, while a diffusion-based conditioning\nprocess-conditioned by task instructions-generates two-stream object-centric\nmotion flows that guide dual-arm coordination. We further design a dynamic task\nassignment strategy that seamlessly maps the predicted 2D motion flows into 3D\nspace and incorporates a pre-trained vision-language model (VLM) to adaptively\nassign the optimal motion to each robotic arm over time. Experiments validate\nthe effectiveness of the proposed method, demonstrating its ability to\ngeneralize to diverse manipulation tasks while maintaining high efficiency and\nadaptability. The code and demo videos are publicly available on our project\nwebsite https://sites.google.com/view/vlm-sfd/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u53cc\u81c2\u534f\u540c\u64cd\u4f5c\uff0c\u5e76\u5229\u7528\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u6765\u8f85\u52a9\u8fd0\u52a8\u89c4\u5212\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u751f\u6210\u8fd0\u52a8\u8f68\u8ff9\u4ee5\u5b8c\u6210\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5927\u6a21\u578b\uff08VLM\uff09\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["VLM", "vision-language model", "motion planning", "diffusion"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cdVLM\u8f85\u52a9\u7684Siamese Flow Diffusion\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u53cc\u81c2\u534f\u540c\u64cd\u4f5c\u4e2d\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5728\u57fa\u4e8e\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u96be\u4ee5\u63a8\u5e7f\u5230\u4e0d\u540c\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u4e14\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u7684\u3001\u975e\u7ed3\u6784\u5316\u7684\u73af\u5883\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u4e24\u4e2a\u5bf9\u8c61\u4e4b\u95f4\u4ea4\u4e92\u7684\u573a\u666f\u4e2d\uff0c\u4f8b\u5982\u7ec4\u88c5\u3001\u5de5\u5177\u4f7f\u7528\u548c\u53cc\u624b\u6293\u53d6\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684VLM\u8f85\u52a9\u7684Siamese Flow Diffusion (VLM-SFD)\u6846\u67b6\uff0c\u7528\u4e8e\u53cc\u81c2\u534f\u540c\u64cd\u4f5c\u4e2d\u7684\u9ad8\u6548\u6a21\u4eff\u5b66\u4e60\u3002\u8be5\u6846\u67b6\u91c7\u7528Siamese Flow Diffusion Network (SFDNet)\uff0c\u5229\u7528\u53cc\u7f16\u7801\u5668-\u89e3\u7801\u5668Siamese\u67b6\u6784\u5c06\u4e24\u4e2a\u76ee\u6807\u5bf9\u8c61\u5d4c\u5165\u5230\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u540c\u65f6\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u6761\u4ef6\u8fc7\u7a0b\uff08\u4ee5\u4efb\u52a1\u6307\u4ee4\u4e3a\u6761\u4ef6\uff09\u751f\u6210\u53cc\u6d41\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u6d41\uff0c\u4ece\u800c\u6307\u5bfc\u53cc\u81c2\u534f\u8c03\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u53ef\u5c06\u9884\u6d4b\u76842D\u8fd0\u52a8\u6d41\u65e0\u7f1d\u6620\u5c04\u52303D\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u52a8\u6001\u5730\u4e3a\u6bcf\u4e2a\u673a\u68b0\u81c2\u5206\u914d\u6700\u4f73\u8fd0\u52a8\u3002", "result": "\u6240\u63d0\u51fa\u7684VLM-SFD\u6846\u67b6\u8868\u73b0\u51fa\u51fa\u8272\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u4ece\u6700\u5c11\u6570\u91cf\u7684\u4eba\u5de5\u6f14\u793a\u4e2d\u5feb\u901f\u9002\u5e94\u548c\u63a8\u5e7f\u5230\u5404\u79cd\u5b9e\u9645\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u548c\u9002\u5e94\u6027\u7684\u540c\u65f6\uff0c\u63a8\u5e7f\u5230\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u7684\u80fd\u529b\u3002", "summary_zh": "\u53cc\u81c2\u534f\u540c\u64cd\u4f5c\u5728\u89e3\u51b3\u9700\u8981\u65e0\u7f1d\u534f\u8c03\u548c\u81ea\u9002\u5e94\u52a8\u529b\u5b66\u7684\u590d\u6742\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u65b9\u9762\u5177\u6709\u5e7f\u9614\u7684\u524d\u666f\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684VLM\u8f85\u52a9\u7684Siamese Flow Diffusion (VLM-SFD)\u6846\u67b6\uff0c\u7528\u4e8e\u53cc\u81c2\u534f\u540c\u64cd\u4f5c\u4e2d\u7684\u9ad8\u6548\u6a21\u4eff\u5b66\u4e60\u3002\u6240\u63d0\u51fa\u7684VLM-SFD\u6846\u67b6\u8868\u73b0\u51fa\u51fa\u8272\u7684\u9002\u5e94\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u4ece\u6700\u5c11\u6570\u91cf\u7684\u4eba\u5de5\u6f14\u793a\u4e2d\u5feb\u901f\u9002\u5e94\u548c\u63a8\u5e7f\u5230\u5404\u79cd\u5b9e\u9645\u4efb\u52a1\u7684\u80fd\u529b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cdSiamese Flow Diffusion Network (SFDNet)\uff0c\u5b83\u91c7\u7528\u53cc\u7f16\u7801\u5668-\u89e3\u7801\u5668Siamese\u67b6\u6784\u5c06\u4e24\u4e2a\u76ee\u6807\u5bf9\u8c61\u5d4c\u5165\u5230\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u540c\u65f6\uff0c\u57fa\u4e8e\u6269\u6563\u7684\u6761\u4ef6\u8fc7\u7a0b\uff08\u4ee5\u4efb\u52a1\u6307\u4ee4\u4e3a\u6761\u4ef6\uff09\u751f\u6210\u53cc\u6d41\u4ee5\u5bf9\u8c61\u4e3a\u4e2d\u5fc3\u7684\u8fd0\u52a8\u6d41\uff0c\u4ece\u800c\u6307\u5bfc\u53cc\u81c2\u534f\u8c03\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u53ef\u5c06\u9884\u6d4b\u76842D\u8fd0\u52a8\u6d41\u65e0\u7f1d\u6620\u5c04\u52303D\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u6765\u52a8\u6001\u5730\u4e3a\u6bcf\u4e2a\u673a\u68b0\u81c2\u5206\u914d\u6700\u4f73\u8fd0\u52a8\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u548c\u9002\u5e94\u6027\u7684\u540c\u65f6\uff0c\u63a8\u5e7f\u5230\u5404\u79cd\u64cd\u4f5c\u4efb\u52a1\u7684\u80fd\u529b\u3002\u4ee3\u7801\u548c\u6f14\u793a\u89c6\u9891\u53ef\u5728\u6211\u4eec\u7684\u9879\u76ee\u7f51\u7ad9\u4e0a\u516c\u5f00\u83b7\u53d6\u3002"}}
{"id": "2506.13622", "pdf": "https://arxiv.org/pdf/2506.13622", "abs": "https://arxiv.org/abs/2506.13622", "authors": ["Martino Gulisano", "Matteo Masoni", "Marco Gabiccini", "Massimo Guiggiani"], "title": "Disturbance-aware minimum-time planning strategies for motorsport vehicles with probabilistic safety certificates", "categories": ["cs.RO"], "comment": "24 pages, 11 figures, paper under review", "summary": "This paper presents a disturbance-aware framework that embeds robustness into\nminimum-lap-time trajectory optimization for motorsport. Two formulations are\nintroduced. (i) Open-loop, horizon-based covariance propagation uses worst-case\nuncertainty growth over a finite window to tighten tire-friction and\ntrack-limit constraints. (ii) Closed-loop, covariance-aware planning\nincorporates a time-varying LQR feedback law in the optimizer, providing a\nfeedback-consistent estimate of disturbance attenuation and enabling sharper\nyet reliable constraint tightening. Both methods yield reference trajectories\nfor human or artificial drivers: in autonomous applications the modelled\ncontroller can replicate the on-board implementation, while for human driving\naccuracy increases with the extent to which the driver can be approximated by\nthe assumed time-varying LQR policy. Computational tests on a representative\nBarcelona-Catalunya sector show that both schemes meet the prescribed safety\nprobability, yet the closed-loop variant incurs smaller lap-time penalties than\nthe more conservative open-loop solution, while the nominal (non-robust)\ntrajectory remains infeasible under the same uncertainties. By accounting for\nuncertainty growth and feedback action during planning, the proposed framework\ndelivers trajectories that are both performance-optimal and probabilistically\nsafe, advancing minimum-time optimization toward real-world deployment in\nhigh-performance motorsport and autonomous racing.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8d5b\u8f66\u8fd0\u52a8\u4e2d\u8f66\u8f86\u7684\u8f68\u8ff9\u4f18\u5316\u548c\u89c4\u5212\uff0c\u7279\u522b\u662f\u8003\u8651\u4e86\u6270\u52a8\u548c\u5b89\u5168\u6027\u7684\u95ee\u9898\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u548c\u8def\u5f84\u89c4\u5212\u7684\u8303\u7574\u3002\u5173\u952e\u8bcd\u5982\u201ctrajectory optimization\u201d\u3001\u201cplanning\u201d\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002", "keywords": ["trajectory optimization", "planning", "disturbance-aware", "probabilistic safety", "autonomous"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u6270\u52a8\u7684\u9c81\u68d2\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6027\u80fd\u8d5b\u8f66\u8fd0\u52a8\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5f00\u73af\u548c\u95ed\u73af\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u5708\u901f\u8868\u73b0\u3002", "motivation": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u6270\u52a8\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u9c81\u68d2\u6027\u5d4c\u5165\u5230\u8d5b\u8f66\u8fd0\u52a8\u7684\u6700\u5c0f\u5708\u901f\u8f68\u8ff9\u4f18\u5316\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u516c\u5f0f\uff1a\uff08i\uff09\u5f00\u73af\u3001\u57fa\u4e8e\u8303\u56f4\u7684\u534f\u65b9\u5dee\u4f20\u64ad\uff0c\u4f7f\u7528\u6709\u9650\u7a97\u53e3\u5185\u7684\u6700\u574f\u60c5\u51b5\u4e0d\u786e\u5b9a\u6027\u589e\u957f\u6765\u6536\u7d27\u8f6e\u80ce\u6469\u64e6\u548c\u8d5b\u9053\u9650\u5236\u7ea6\u675f\u3002\uff08ii\uff09\u95ed\u73af\u3001\u534f\u65b9\u5dee\u611f\u77e5\u89c4\u5212\uff0c\u5728\u4f18\u5316\u5668\u4e2d\u7ed3\u5408\u4e86\u65f6\u53d8LQR\u53cd\u9988\u5f8b\uff0c\u63d0\u4f9b\u4e86\u5e72\u6270\u8870\u51cf\u7684\u53cd\u9988\u4e00\u81f4\u4f30\u8ba1\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u6e05\u6670\u4f46\u53ef\u9760\u7684\u7ea6\u675f\u6536\u7d27\u3002", "result": "\u5728\u5177\u6709\u4ee3\u8868\u6027\u7684\u5df4\u585e\u7f57\u90a3-\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8d5b\u9053\u7684\u8ba1\u7b97\u6d4b\u8bd5\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6848\u90fd\u6ee1\u8db3\u89c4\u5b9a\u7684\u5b89\u5168\u6982\u7387\uff0c\u4f46\u95ed\u73af\u65b9\u6848\u6bd4\u66f4\u4fdd\u5b88\u7684\u5f00\u73af\u65b9\u6848\u4ea7\u751f\u7684\u5708\u901f\u635f\u5931\u66f4\u5c0f\uff0c\u800c\u6807\u79f0\uff08\u975e\u9c81\u68d2\uff09\u8f68\u8ff9\u5728\u76f8\u540c\u7684\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ecd\u7136\u4e0d\u53ef\u884c\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u89c4\u5212\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u589e\u957f\u548c\u53cd\u9988\u4f5c\u7528\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u63d0\u4f9b\u7684\u8f68\u8ff9\u5728\u6027\u80fd\u4e0a\u662f\u6700\u4f18\u7684\uff0c\u5e76\u4e14\u5728\u6982\u7387\u4e0a\u662f\u5b89\u5168\u7684\uff0c\u4ece\u800c\u63a8\u52a8\u4e86\u6700\u77ed\u65f6\u95f4\u4f18\u5316\u5728\u9ad8\u6027\u80fd\u8d5b\u8f66\u548c\u81ea\u4e3b\u8d5b\u8f66\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "summary_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u6270\u52a8\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u9c81\u68d2\u6027\u5d4c\u5165\u5230\u8d5b\u8f66\u8fd0\u52a8\u7684\u6700\u5c0f\u5708\u901f\u8f68\u8ff9\u4f18\u5316\u4e2d\u3002\u4ecb\u7ecd\u4e86\u4e24\u79cd\u516c\u5f0f\uff1a\uff08i\uff09\u5f00\u73af\u3001\u57fa\u4e8e\u8303\u56f4\u7684\u534f\u65b9\u5dee\u4f20\u64ad\uff0c\u4f7f\u7528\u6709\u9650\u7a97\u53e3\u5185\u7684\u6700\u574f\u60c5\u51b5\u4e0d\u786e\u5b9a\u6027\u589e\u957f\u6765\u6536\u7d27\u8f6e\u80ce\u6469\u64e6\u548c\u8d5b\u9053\u9650\u5236\u7ea6\u675f\u3002\uff08ii\uff09\u95ed\u73af\u3001\u534f\u65b9\u5dee\u611f\u77e5\u89c4\u5212\uff0c\u5728\u4f18\u5316\u5668\u4e2d\u7ed3\u5408\u4e86\u65f6\u53d8LQR\u53cd\u9988\u5f8b\uff0c\u63d0\u4f9b\u4e86\u5e72\u6270\u8870\u51cf\u7684\u53cd\u9988\u4e00\u81f4\u4f30\u8ba1\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u6e05\u6670\u4f46\u53ef\u9760\u7684\u7ea6\u675f\u6536\u7d27\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u4e3a\u4eba\u7c7b\u6216\u4eba\u5de5\u667a\u80fd\u9a7e\u9a76\u5458\u63d0\u4f9b\u4e86\u53c2\u8003\u8f68\u8ff9\uff1a\u5728\u81ea\u4e3b\u5e94\u7528\u4e2d\uff0c\u5efa\u6a21\u7684\u63a7\u5236\u5668\u53ef\u4ee5\u590d\u5236\u8f66\u8f7d\u5b9e\u73b0\uff0c\u800c\u5bf9\u4e8e\u4eba\u7c7b\u9a7e\u9a76\uff0c\u7cbe\u5ea6\u4f1a\u968f\u7740\u9a7e\u9a76\u5458\u53ef\u4ee5\u88ab\u5047\u8bbe\u7684\u65f6\u53d8LQR\u7b56\u7565\u8fd1\u4f3c\u7684\u7a0b\u5ea6\u800c\u63d0\u9ad8\u3002\u5728\u5177\u6709\u4ee3\u8868\u6027\u7684\u5df4\u585e\u7f57\u90a3-\u52a0\u6cf0\u7f57\u5c3c\u4e9a\u8d5b\u9053\u7684\u8ba1\u7b97\u6d4b\u8bd5\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6848\u90fd\u6ee1\u8db3\u89c4\u5b9a\u7684\u5b89\u5168\u6982\u7387\uff0c\u4f46\u95ed\u73af\u65b9\u6848\u6bd4\u66f4\u4fdd\u5b88\u7684\u5f00\u73af\u65b9\u6848\u4ea7\u751f\u7684\u5708\u901f\u635f\u5931\u66f4\u5c0f\uff0c\u800c\u6807\u79f0\uff08\u975e\u9c81\u68d2\uff09\u8f68\u8ff9\u5728\u76f8\u540c\u7684\u4e0d\u786e\u5b9a\u6027\u4e0b\u4ecd\u7136\u4e0d\u53ef\u884c\u3002\u901a\u8fc7\u8003\u8651\u89c4\u5212\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u589e\u957f\u548c\u53cd\u9988\u4f5c\u7528\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u63d0\u4f9b\u7684\u8f68\u8ff9\u5728\u6027\u80fd\u4e0a\u662f\u6700\u4f18\u7684\uff0c\u5e76\u4e14\u5728\u6982\u7387\u4e0a\u662f\u5b89\u5168\u7684\uff0c\u4ece\u800c\u63a8\u52a8\u4e86\u6700\u77ed\u65f6\u95f4\u4f18\u5316\u5728\u9ad8\u6027\u80fd\u8d5b\u8f66\u548c\u81ea\u4e3b\u8d5b\u8f66\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.12600", "pdf": "https://arxiv.org/pdf/2506.12600", "abs": "https://arxiv.org/abs/2506.12600", "authors": ["Jie Pan", "Tianyi Wang", "Christian Claudel", "Jing Shi"], "title": "Trust-MARL: Trust-Based Multi-Agent Reinforcement Learning Framework for Cooperative On-Ramp Merging Control in Heterogeneous Traffic Flow", "categories": ["cs.MA", "cs.AI", "cs.ET", "cs.GT", "cs.RO"], "comment": "34 pages, 7 figures, 4 tables", "summary": "Intelligent transportation systems require connected and automated vehicles\n(CAVs) to conduct safe and efficient cooperation with human-driven vehicles\n(HVs) in complex real-world traffic environments. However, the inherent\nunpredictability of human behaviour, especially at bottlenecks such as highway\non-ramp merging areas, often disrupts traffic flow and compromises system\nperformance. To address the challenge of cooperative on-ramp merging in\nheterogeneous traffic environments, this study proposes a trust-based\nmulti-agent reinforcement learning (Trust-MARL) framework. At the macro level,\nTrust-MARL enhances global traffic efficiency by leveraging inter-agent trust\nto improve bottleneck throughput and mitigate traffic shockwave through\nemergent group-level coordination. At the micro level, a dynamic trust\nmechanism is designed to enable CAVs to adjust their cooperative strategies in\nresponse to real-time behaviors and historical interactions with both HVs and\nother CAVs. Furthermore, a trust-triggered game-theoretic decision-making\nmodule is integrated to guide each CAV in adapting its cooperation factor and\nexecuting context-aware lane-changing decisions under safety, comfort, and\nefficiency constraints. An extensive set of ablation studies and comparative\nexperiments validates the effectiveness of the proposed Trust-MARL approach,\ndemonstrating significant improvements in safety, efficiency, comfort, and\nadaptability across varying CAV penetration rates and traffic densities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on cooperative on-ramp merging control in heterogeneous traffic flow using multi-agent reinforcement learning. While it doesn't directly involve Large Language Models, it does deal with predicting the behavior of human-driven vehicles, which is related to trajectory prediction. The core focus is on traffic flow optimization using reinforcement learning, with a component of predicting/reacting to human behavior.", "keywords": ["trajectory prediction", "human behavior", "multi-agent reinforcement learning", "traffic flow"]}, "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u4efb\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6(Trust-MARL)\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u6784\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u534f\u540c\u531d\u9053\u5408\u5e76\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u4ea4\u901a\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f02\u6784\u4ea4\u901a\u73af\u5883\u4e2d\u534f\u540c\u531d\u9053\u5408\u5e76\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u4efb\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(Trust-MARL)\u6846\u67b6\u3002", "result": "\u5927\u91cf\u7684\u6d88\u878d\u7814\u7a76\u548c\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684Trust-MARL\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684Trust-MARL\u65b9\u6cd5\u5728\u4e0d\u540cCAV\u6e17\u900f\u7387\u548c\u4ea4\u901a\u5bc6\u5ea6\u4e0b\uff0c\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002", "summary_zh": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u8981\u6c42\u8054\u7f51\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86(CAV)\u5728\u590d\u6742\u7684\u73b0\u5b9e\u4ea4\u901a\u73af\u5883\u4e2d\u4e0e\u4eba\u5de5\u9a7e\u9a76\u8f66\u8f86(HV)\u8fdb\u884c\u5b89\u5168\u9ad8\u6548\u7684\u5408\u4f5c\u3002\u7136\u800c\uff0c\u4eba\u7c7b\u884c\u4e3a\u56fa\u6709\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u901f\u516c\u8def\u531d\u9053\u5408\u5e76\u533a\u57df\u7b49\u74f6\u9888\u5904\uff0c\u7ecf\u5e38\u6270\u4e71\u4ea4\u901a\u6d41\u91cf\u5e76\u635f\u5bb3\u7cfb\u7edf\u6027\u80fd\u3002\u4e3a\u4e86\u89e3\u51b3\u5f02\u6784\u4ea4\u901a\u73af\u5883\u4e2d\u534f\u540c\u531d\u9053\u5408\u5e76\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u4efb\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60(Trust-MARL)\u6846\u67b6\u3002\u5728\u5b8f\u89c2\u5c42\u9762\uff0cTrust-MARL\u901a\u8fc7\u5229\u7528\u667a\u80fd\u4f53\u95f4\u7684\u4fe1\u4efb\u6765\u63d0\u9ad8\u74f6\u9888\u541e\u5410\u91cf\uff0c\u5e76\u901a\u8fc7\u6d8c\u73b0\u7684\u7fa4\u4f53\u5c42\u9762\u534f\u8c03\u6765\u7f13\u89e3\u4ea4\u901a\u51b2\u51fb\u6ce2\uff0c\u4ece\u800c\u63d0\u9ad8\u5168\u5c40\u4ea4\u901a\u6548\u7387\u3002\u5728\u5fae\u89c2\u5c42\u9762\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a8\u6001\u4fe1\u4efb\u673a\u5236\uff0c\u4f7fCAV\u80fd\u591f\u6839\u636e\u4e0eHV\u548c\u5176\u4ed6CAV\u7684\u5b9e\u65f6\u884c\u4e3a\u548c\u5386\u53f2\u4e92\u52a8\u6765\u8c03\u6574\u5176\u5408\u4f5c\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2a\u4fe1\u4efb\u89e6\u53d1\u7684\u535a\u5f08\u8bba\u51b3\u7b56\u6a21\u5757\uff0c\u4ee5\u6307\u5bfc\u6bcf\u4e2aCAV\u8c03\u6574\u5176\u5408\u4f5c\u56e0\u5b50\uff0c\u5e76\u5728\u5b89\u5168\u6027\u3001\u8212\u9002\u6027\u548c\u6548\u7387\u7ea6\u675f\u4e0b\u6267\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u53d8\u9053\u51b3\u7b56\u3002\u5927\u91cf\u7684\u6d88\u878d\u7814\u7a76\u548c\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684Trust-MARL\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u540c\u7684CAV\u6e17\u900f\u7387\u548c\u4ea4\u901a\u5bc6\u5ea6\u4e0b\uff0c\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2506.12723", "pdf": "https://arxiv.org/pdf/2506.12723", "abs": "https://arxiv.org/abs/2506.12723", "authors": ["Ye Li", "Yuan Meng", "Zewen Sun", "Kangye Ji", "Chen Tang", "Jiajun Fan", "Xinzhu Ma", "Shutao Xia", "Zhi Wang", "Wenwu Zhu"], "title": "SP-VLA: A Joint Model Scheduling and Token Pruning Approach for VLA Model Acceleration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language-Action (VLA) models have attracted increasing attention for\ntheir strong control capabilities. However, their high computational cost and\nlow execution frequency hinder their suitability for real-time tasks such as\nrobotic manipulation and autonomous navigation. Existing VLA acceleration\nmethods primarily focus on structural optimization, overlooking the fact that\nthese models operate in sequential decision-making environments. As a result,\ntemporal redundancy in sequential action generation and spatial redundancy in\nvisual input remain unaddressed. To this end, we propose SP-VLA, a unified\nframework that accelerates VLA models by jointly scheduling models and pruning\ntokens. Specifically, we design an action-aware model scheduling mechanism that\nreduces temporal redundancy by dynamically switching between VLA model and a\nlightweight generator. Inspired by the human motion pattern of focusing on key\ndecision points while relying on intuition for other actions, we categorize VLA\nactions into deliberative and intuitive, assigning the former to the VLA model\nand the latter to the lightweight generator, enabling frequency-adaptive\nexecution through collaborative model scheduling. To address spatial\nredundancy, we further develop a spatio-semantic dual-aware token pruning\nmethod. Tokens are classified into spatial and semantic types and pruned based\non their dual-aware importance to accelerate VLA inference. These two\nmechanisms work jointly to guide the VLA in focusing on critical actions and\nsalient visual information, achieving effective acceleration while maintaining\nhigh accuracy. Experimental results demonstrate that our method achieves up to\n1.5$\\times$ acceleration with less than 3% drop in accuracy, outperforming\nexisting approaches in multiple tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on accelerating Vision-Language-Action (VLA) models, which are relevant to autonomous navigation (a trajectory prediction related task) and utilizes large models. While it doesn't directly address trajectory prediction algorithms or the inner workings of LLMs, it optimizes the efficiency of VLA models used in related applications. The connection to trajectory prediction is through the application in autonomous navigation, and the connection to large models is that VLA models are generally large.", "keywords": ["VLA models", "Vision-Language-Action", "autonomous navigation", "model acceleration"]}, "AI": {"tldr": "SP-VLA\u901a\u8fc7\u8054\u5408\u8c03\u5ea6\u6a21\u578b\u548ctoken\u4fee\u526a\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u52a0\u901f\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002", "motivation": "VLA\u6a21\u578b\u5177\u6709\u5f3a\u5927\u7684\u63a7\u5236\u80fd\u529b\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4f4e\u6267\u884c\u9891\u7387\u4f7f\u5176\u4e0d\u9002\u5408\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u81ea\u52a8\u5bfc\u822a\u7b49\u5b9e\u65f6\u4efb\u52a1\u3002\u73b0\u6709\u7684VLA\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u7ed3\u6784\u4f18\u5316\u4e0a\uff0c\u5ffd\u7565\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u987a\u5e8f\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u8fd0\u884c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6SP-VLA\uff0c\u901a\u8fc7\u8054\u5408\u8c03\u5ea6\u6a21\u578b\u548c\u4fee\u526atokens\u6765\u52a0\u901fVLA\u6a21\u578b\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u52a8\u4f5c\u611f\u77e5\u7684\u6a21\u578b\u8c03\u5ea6\u673a\u5236\uff0c\u901a\u8fc7\u5728VLA\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u751f\u6210\u5668\u4e4b\u95f4\u52a8\u6001\u5207\u6362\u6765\u51cf\u5c11\u65f6\u95f4\u5197\u4f59\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u7a7a\u95f4\u8bed\u4e49\u53cc\u91cd\u611f\u77e5\u7684token\u4fee\u526a\u65b9\u6cd5\uff0c\u57fa\u4e8etokens\u5bf9\u53cc\u91cd\u611f\u77e5\u91cd\u8981\u6027\u8fdb\u884c\u5206\u7c7b\u548c\u4fee\u526a\uff0c\u4ee5\u52a0\u901fVLA\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.5\u500d\u7684\u52a0\u901f\uff0c\u800c\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u52303%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u8f83\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.5\u500d\u7684\u52a0\u901f\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "summary_zh": "\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u56e0\u5176\u5f3a\u5927\u7684\u63a7\u5236\u80fd\u529b\u800c\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5b83\u4eec\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u4f4e\u6267\u884c\u9891\u7387\u4f7f\u5176\u4e0d\u9002\u5408\u673a\u5668\u4eba\u64cd\u4f5c\u548c\u81ea\u52a8\u5bfc\u822a\u7b49\u5b9e\u65f6\u4efb\u52a1\u3002\u73b0\u6709\u7684VLA\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u7ed3\u6784\u4f18\u5316\u4e0a\uff0c\u5ffd\u7565\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u987a\u5e8f\u51b3\u7b56\u73af\u5883\u4e2d\u7684\u8fd0\u884c\u3002\u56e0\u6b64\uff0c\u987a\u5e8f\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u65f6\u95f4\u5197\u4f59\u548c\u89c6\u89c9\u8f93\u5165\u4e2d\u7684\u7a7a\u95f4\u5197\u4f59\u4ecd\u7136\u6ca1\u6709\u89e3\u51b3\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86SP-VLA\uff0c\u8fd9\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8c03\u5ea6\u6a21\u578b\u548c\u4fee\u526atokens\u6765\u52a0\u901fVLA\u6a21\u578b\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u52a8\u4f5c\u611f\u77e5\u7684\u6a21\u578b\u8c03\u5ea6\u673a\u5236\uff0c\u901a\u8fc7\u5728VLA\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u751f\u6210\u5668\u4e4b\u95f4\u52a8\u6001\u5207\u6362\u6765\u51cf\u5c11\u65f6\u95f4\u5197\u4f59\u3002\u53d7\u5230\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\u7684\u542f\u53d1\uff0c\u5373\u4e13\u6ce8\u4e8e\u5173\u952e\u51b3\u7b56\u70b9\uff0c\u540c\u65f6\u4f9d\u9760\u76f4\u89c9\u8fdb\u884c\u5176\u4ed6\u52a8\u4f5c\uff0c\u6211\u4eec\u5c06VLA\u52a8\u4f5c\u5206\u4e3a\u5ba1\u614e\u578b\u548c\u76f4\u89c9\u578b\uff0c\u5c06\u524d\u8005\u5206\u914d\u7ed9VLA\u6a21\u578b\uff0c\u540e\u8005\u5206\u914d\u7ed9\u8f7b\u91cf\u7ea7\u751f\u6210\u5668\uff0c\u901a\u8fc7\u534f\u4f5c\u6a21\u578b\u8c03\u5ea6\u5b9e\u73b0\u9891\u7387\u81ea\u9002\u5e94\u6267\u884c\u3002\u4e3a\u4e86\u89e3\u51b3\u7a7a\u95f4\u5197\u4f59\u95ee\u9898\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u4e00\u79cd\u7a7a\u95f4\u8bed\u4e49\u53cc\u91cd\u611f\u77e5\u7684token\u4fee\u526a\u65b9\u6cd5\u3002Tokens\u88ab\u5206\u4e3a\u7a7a\u95f4\u548c\u8bed\u4e49\u7c7b\u578b\uff0c\u5e76\u6839\u636e\u5b83\u4eec\u5bf9\u53cc\u91cd\u611f\u77e5\u91cd\u8981\u6027\u8fdb\u884c\u4fee\u526a\uff0c\u4ee5\u52a0\u901fVLA\u63a8\u7406\u3002\u8fd9\u4e24\u79cd\u673a\u5236\u5171\u540c\u4f5c\u7528\uff0c\u5f15\u5bfcVLA\u4e13\u6ce8\u4e8e\u5173\u952e\u52a8\u4f5c\u548c\u663e\u7740\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u6709\u6548\u7684\u52a0\u901f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe1.5\u500d\u7684\u52a0\u901f\uff0c\u800c\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u52303%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.13189", "pdf": "https://arxiv.org/pdf/2506.13189", "abs": "https://arxiv.org/abs/2506.13189", "authors": ["Yuchong Zhang", "Bastian Orthmann", "Shichen Ji", "Michael Welle", "Jonne Van Haastregt", "Danica Kragic"], "title": "Multimodal \"Puppeteer\": An Exploration of Robot Teleoperation Via Virtual Counterpart with LLM-Driven Voice and Gesture Interaction in Augmented Reality", "categories": ["cs.HC", "cs.RO"], "comment": "This work has been submitted to the IEEE TVCG for possible\n  publication", "summary": "The integration of robotics and augmented reality (AR) holds transformative\npotential for advancing human-robot interaction (HRI), offering enhancements in\nusability, intuitiveness, accessibility, and collaborative task performance.\nThis paper introduces and evaluates a novel multimodal AR-based robot puppeteer\nframework that enables intuitive teleoperation via virtual counterpart through\nlarge language model (LLM)-driven voice commands and hand gesture interactions.\nUtilizing the Meta Quest 3, users interact with a virtual counterpart robot in\nreal-time, effectively \"puppeteering\" its physical counterpart within an AR\nenvironment. We conducted a within-subject user study with 42 participants\nperforming robotic cube pick-and-place with pattern matching tasks under two\nconditions: gesture-only interaction and combined voice-and-gesture\ninteraction. Both objective performance metrics and subjective user experience\n(UX) measures were assessed, including an extended comparative analysis between\nroboticists and non-roboticists. The results provide key insights into how\nmultimodal input influences contextual task efficiency, usability, and user\nsatisfaction in AR-based HRI. Our findings offer practical design implications\nfor designing effective AR-enhanced HRI systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on robot teleoperation using AR and LLM-driven voice and gesture interaction. While it involves robot control, it doesn't directly address trajectory prediction. The mention of LLM and its application in controlling the robot contributes to the relevance, but the core focus is not trajectory prediction.", "keywords": ["Large Language Models (LLMs)", "robotics", "human-robot interaction"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAR\u7684\u591a\u6a21\u6001\u673a\u5668\u4eba\u64cd\u63a7\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u97f3\u548c\u624b\u52bf\u5b9e\u73b0\u76f4\u89c2\u7684\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u3002", "motivation": "\u673a\u5668\u4eba\u4e0e\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7684\u96c6\u6210\u5177\u6709\u6539\u53d8\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u7684\u6f5c\u529b\uff0c\u53ef\u5728\u53ef\u7528\u6027\u3001\u76f4\u89c2\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u534f\u4f5c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u63d0\u4f9b\u589e\u5f3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001AR\u7684\u673a\u5668\u4eba\u5080\u5121\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u8bed\u97f3\u547d\u4ee4\u548c\u624b\u52bf\u4ea4\u4e92\uff0c\u5b9e\u73b0\u901a\u8fc7\u865a\u62df\u5bf9\u5e94\u7269\u8fdb\u884c\u76f4\u89c2\u7684\u9065\u64cd\u4f5c\u3002", "result": "\u5bf942\u540d\u53c2\u4e0e\u8005\u8fdb\u884c\u4e86\u4e00\u9879\u53d7\u8bd5\u8005\u5185\u7528\u6237\u7814\u7a76\uff0c\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u6267\u884c\u673a\u5668\u4eba\u7acb\u65b9\u4f53\u62fe\u53d6\u548c\u653e\u7f6e\u4e0e\u6a21\u5f0f\u5339\u914d\u4efb\u52a1\uff1a\u4ec5\u624b\u52bf\u4ea4\u4e92\u548c\u8bed\u97f3\u4e0e\u624b\u52bf\u7ec4\u5408\u4ea4\u4e92\u3002\u8bc4\u4f30\u4e86\u5ba2\u89c2\u6027\u80fd\u6307\u6807\u548c\u4e3b\u89c2\u7528\u6237\u4f53\u9a8c\uff08UX\uff09\u6307\u6807\uff0c\u5305\u62ec\u673a\u5668\u4eba\u4e13\u5bb6\u548c\u975e\u673a\u5668\u4eba\u4e13\u5bb6\u4e4b\u95f4\u7684\u6269\u5c55\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u591a\u6a21\u6001\u8f93\u5165\u5f71\u54cd\u57fa\u4e8eAR\u7684\u4eba\u673a\u4ea4\u4e92\u7684\u4e0a\u4e0b\u6587\u4efb\u52a1\u6548\u7387\u3001\u53ef\u7528\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\uff0c\u4e3a\u8bbe\u8ba1\u6709\u6548\u7684AR\u589e\u5f3a\u578b\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8df5\u610f\u4e49\u3002", "summary_zh": "\u673a\u5668\u4eba\u4e0e\u589e\u5f3a\u73b0\u5b9e\uff08AR\uff09\u7684\u7ed3\u5408\uff0c\u5728\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u7684\u53ef\u7528\u6027\u3001\u76f4\u89c2\u6027\u3001\u53ef\u8bbf\u95ee\u6027\u548c\u534f\u4f5c\u4efb\u52a1\u6027\u80fd\u65b9\u9762\uff0c\u5177\u6709\u53d8\u9769\u6027\u7684\u6f5c\u529b\u3002\u672c\u6587\u4ecb\u7ecd\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u591a\u6a21\u6001AR\u7684\u673a\u5668\u4eba\u64cd\u63a7\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u8bed\u97f3\u6307\u4ee4\u548c\u624b\u52bf\u4e92\u52a8\uff0c\u5b9e\u73b0\u4e86\u901a\u8fc7\u865a\u62df\u66ff\u8eab\u8fdb\u884c\u76f4\u89c2\u7684\u8fdc\u7a0b\u64cd\u4f5c\u3002\u7528\u6237\u4f7f\u7528Meta Quest 3\uff0c\u4e0eAR\u73af\u5883\u4e2d\u7684\u865a\u62df\u66ff\u8eab\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u65f6\u4e92\u52a8\uff0c\u4ece\u800c\u6709\u6548\u5730\u201c\u64cd\u7eb5\u201d\u5176\u5b9e\u4f53\u673a\u5668\u4eba\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u4e00\u9879\u9488\u5bf942\u540d\u53c2\u4e0e\u8005\u7684\u53d7\u8bd5\u8005\u5185\u7528\u6237\u7814\u7a76\uff0c\u8ba9\u4ed6\u4eec\u5728\u4e24\u79cd\u6761\u4ef6\u4e0b\u6267\u884c\u673a\u5668\u4eba\u7acb\u65b9\u4f53\u62fe\u53d6\u548c\u653e\u7f6e\u4e0e\u6a21\u5f0f\u5339\u914d\u4efb\u52a1\uff1a\u4ec5\u624b\u52bf\u4ea4\u4e92\u548c\u8bed\u97f3\u4e0e\u624b\u52bf\u7ec4\u5408\u4ea4\u4e92\u3002\u8bc4\u4f30\u4e86\u5ba2\u89c2\u6027\u80fd\u6307\u6807\u548c\u4e3b\u89c2\u7528\u6237\u4f53\u9a8c\uff08UX\uff09\u6307\u6807\uff0c\u5305\u62ec\u673a\u5668\u4eba\u4e13\u5bb6\u548c\u975e\u673a\u5668\u4eba\u4e13\u5bb6\u4e4b\u95f4\u7684\u6269\u5c55\u6bd4\u8f83\u5206\u6790\u3002\u7ed3\u679c\u6df1\u5165\u4e86\u89e3\u4e86\u591a\u6a21\u6001\u8f93\u5165\u5982\u4f55\u5f71\u54cd\u57fa\u4e8eAR\u7684HRI\u4e2d\u7684\u60c5\u5883\u4efb\u52a1\u6548\u7387\u3001\u53ef\u7528\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u8bbe\u8ba1\u6709\u6548\u7684AR\u589e\u5f3a\u578bHRI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bbe\u8ba1\u610f\u4e49\u3002"}}
{"id": "2506.12815", "pdf": "https://arxiv.org/pdf/2506.12815", "abs": "https://arxiv.org/abs/2506.12815", "authors": ["Yang Dai", "Oubo Ma", "Longfei Zhang", "Xingxing Liang", "Xiaochun Cao", "Shouling Ji", "Jiaheng Zhang", "Jincai Huang", "Li Shen"], "title": "TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models", "categories": ["cs.LG"], "comment": "23 pages, 6 figures", "summary": "Recent advances in Trajectory Optimization (TO) models have achieved\nremarkable success in offline reinforcement learning. However, their\nvulnerabilities against backdoor attacks are poorly understood. We find that\nexisting backdoor attacks in reinforcement learning are based on reward\nmanipulation, which are largely ineffective against the TO model due to its\ninherent sequence modeling nature. Moreover, the complexities introduced by\nhigh-dimensional action spaces further compound the challenge of action\nmanipulation. To address these gaps, we propose TrojanTO, the first\naction-level backdoor attack against TO models. TrojanTO employs alternating\ntraining to enhance the connection between triggers and target actions for\nattack effectiveness. To improve attack stealth, it utilizes precise poisoning\nvia trajectory filtering for normal performance and batch poisoning for trigger\nconsistency. Extensive evaluations demonstrate that TrojanTO effectively\nimplants backdoor attacks across diverse tasks and attack objectives with a low\nattack budget (0.3\\% of trajectories). Furthermore, TrojanTO exhibits broad\napplicability to DT, GDT, and DC, underscoring its scalability across diverse\nTO model architectures.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u4f18\u5316\u6a21\u578b\uff08Trajectory Optimization, TO\uff09\u7684\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9TO\u6a21\u578b\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u4f18\u5316\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u76f8\u5173\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u8f68\u8ff9\u9884\u6d4b\u7684\u5e95\u5c42\u6280\u672f\u4e0a\u3002\u5173\u952e\u8bcd\u5305\u542b'Trajectory Optimization'\u3002", "keywords": ["Trajectory Optimization", "trajectory"]}, "AI": {"tldr": "TrojanTO\u662f\u4e00\u79cd\u9488\u5bf9\u8f68\u8ff9\u4f18\u5316\u6a21\u578b\u7684\u52a8\u4f5c\u7ea7\u540e\u95e8\u653b\u51fb\uff0c\u901a\u8fc7\u4ea4\u66ff\u8bad\u7ec3\u548c\u7cbe\u786e\u4e2d\u6bd2\u6765\u6709\u6548\u690d\u5165\u540e\u95e8\uff0c\u4e14\u9690\u853d\u6027\u9ad8\uff0c\u9002\u7528\u6027\u5e7f\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u95e8\u653b\u51fb\u4e3b\u8981\u57fa\u4e8e\u5956\u52b1\u64cd\u7eb5\uff0c\u5bf9TO\u6a21\u578b\u65e0\u6548\uff0c\u4e14\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u589e\u52a0\u4e86\u52a8\u4f5c\u64cd\u7eb5\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51faTrojanTO\uff0c\u4e00\u79cd\u9488\u5bf9TO\u6a21\u578b\u7684\u52a8\u4f5c\u7ea7\u540e\u95e8\u653b\u51fb\uff0c\u91c7\u7528\u4ea4\u66ff\u8bad\u7ec3\u589e\u5f3a\u89e6\u53d1\u5668\u548c\u76ee\u6807\u52a8\u4f5c\u7684\u8fde\u63a5\uff0c\u5229\u7528\u8f68\u8ff9\u8fc7\u6ee4\u8fdb\u884c\u7cbe\u786e\u4e2d\u6bd2\u4ee5\u4fdd\u8bc1\u6b63\u5e38\u6027\u80fd\uff0c\u5e76\u4f7f\u7528\u6279\u91cf\u4e2d\u6bd2\u6765\u4fdd\u8bc1\u89e6\u53d1\u5668\u4e00\u81f4\u6027\u3002", "result": "TrojanTO\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u653b\u51fb\u76ee\u6807\u4e0b\u5747\u80fd\u6709\u6548\u690d\u5165\u540e\u95e8\u653b\u51fb\uff0c\u4e14\u653b\u51fb\u9884\u7b97\u4f4e\uff080.3%\u7684\u8f68\u8ff9\uff09\uff0c\u5e76\u5e7f\u6cdb\u9002\u7528\u4e8eDT\u3001GDT\u548cDC\u3002", "conclusion": "TrojanTO\u653b\u51fb\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u76ee\u6807\u4e0b\u5747\u6709\u6548\uff0c\u4e14\u653b\u51fb\u9884\u7b97\u4f4e\uff0c\u9002\u7528\u6027\u5e7f\u3002", "summary_zh": "\u8f68\u8ff9\u4f18\u5316\uff08TO\uff09\u6a21\u578b\u7684\u6700\u65b0\u8fdb\u5c55\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5bf9\u540e\u95e8\u653b\u51fb\u7684\u8106\u5f31\u6027\u5374\u9c9c\u4e3a\u4eba\u77e5\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u95e8\u653b\u51fb\u4e3b\u8981\u57fa\u4e8e\u5956\u52b1\u64cd\u7eb5\uff0c\u7531\u4e8eTO\u6a21\u578b\u56fa\u6709\u7684\u5e8f\u5217\u5efa\u6a21\u7279\u6027\uff0c\u8fd9\u4e9b\u653b\u51fb\u5bf9TO\u6a21\u578b\u57fa\u672c\u65e0\u6548\u3002\u6b64\u5916\uff0c\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u5e26\u6765\u7684\u590d\u6742\u6027\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u52a8\u4f5c\u64cd\u7eb5\u7684\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86TrojanTO\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u9488\u5bf9TO\u6a21\u578b\u7684\u52a8\u4f5c\u7ea7\u540e\u95e8\u653b\u51fb\u3002TrojanTO\u91c7\u7528\u4ea4\u66ff\u8bad\u7ec3\u6765\u589e\u5f3a\u89e6\u53d1\u5668\u548c\u76ee\u6807\u52a8\u4f5c\u4e4b\u95f4\u7684\u8fde\u63a5\uff0c\u4ece\u800c\u63d0\u9ad8\u653b\u51fb\u7684\u6709\u6548\u6027\u3002\u4e3a\u4e86\u63d0\u9ad8\u653b\u51fb\u7684\u9690\u853d\u6027\uff0c\u5b83\u5229\u7528\u8f68\u8ff9\u8fc7\u6ee4\u8fdb\u884c\u7cbe\u786e\u4e2d\u6bd2\uff0c\u4ee5\u4fdd\u8bc1\u6b63\u5e38\u6027\u80fd\uff0c\u5e76\u4f7f\u7528\u6279\u91cf\u4e2d\u6bd2\u6765\u4fdd\u8bc1\u89e6\u53d1\u5668\u7684\u4e00\u81f4\u6027\u3002\u5e7f\u6cdb\u7684\u8bc4\u4f30\u8868\u660e\uff0cTrojanTO\u5728\u4e0d\u540c\u7684\u4efb\u52a1\u548c\u653b\u51fb\u76ee\u6807\u4e0b\u90fd\u80fd\u6709\u6548\u5730\u690d\u5165\u540e\u95e8\u653b\u51fb\uff0c\u4e14\u653b\u51fb\u9884\u7b97\u8f83\u4f4e\uff080.3%\u7684\u8f68\u8ff9\uff09\u3002\u6b64\u5916\uff0cTrojanTO\u8fd8\u8868\u73b0\u51fa\u5bf9DT\u3001GDT\u548cDC\u7684\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u7a81\u663e\u4e86\u5176\u5728\u4e0d\u540cTO\u6a21\u578b\u67b6\u6784\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2506.13138", "pdf": "https://arxiv.org/pdf/2506.13138", "abs": "https://arxiv.org/abs/2506.13138", "authors": ["Jiamin Wang", "Yichen Yao", "Xiang Feng", "Hang Wu", "Yaming Wang", "Qingqiu Huang", "Yuexin Ma", "Xinge Zhu"], "title": "STAGE: A Stream-Centric Generative World Model for Long-Horizon Driving-Scene Simulation", "categories": ["cs.CV"], "comment": null, "summary": "The generation of temporally consistent, high-fidelity driving videos over\nextended horizons presents a fundamental challenge in autonomous driving world\nmodeling. Existing approaches often suffer from error accumulation and feature\nmisalignment due to inadequate decoupling of spatio-temporal dynamics and\nlimited cross-frame feature propagation mechanisms. To address these\nlimitations, we present STAGE (Streaming Temporal Attention Generative Engine),\na novel auto-regressive framework that pioneers hierarchical feature\ncoordination and multi-phase optimization for sustainable video synthesis. To\nachieve high-quality long-horizon driving video generation, we introduce\nHierarchical Temporal Feature Transfer (HTFT) and a novel multi-stage training\nstrategy. HTFT enhances temporal consistency between video frames throughout\nthe video generation process by modeling the temporal and denoising process\nseparately and transferring denoising features between frames. The multi-stage\ntraining strategy is to divide the training into three stages, through model\ndecoupling and auto-regressive inference process simulation, thereby\naccelerating model convergence and reducing error accumulation. Experiments on\nthe Nuscenes dataset show that STAGE has significantly surpassed existing\nmethods in the long-horizon driving video generation task. In addition, we also\nexplored STAGE's ability to generate unlimited-length driving videos. We\ngenerated 600 frames of high-quality driving videos on the Nuscenes dataset,\nwhich far exceeds the maximum length achievable by existing methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4e8e\u957f\u65f6\u57df\u9a7e\u9a76\u573a\u666f\u6a21\u62df\uff0c\u7279\u522b\u662f\u751f\u6210\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u9ad8\u4fdd\u771f\u9a7e\u9a76\u89c6\u9891\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u751f\u6210\u7684\u9a7e\u9a76\u573a\u666f\u89c6\u9891\u53ef\u4ee5\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u3002 \u6458\u8981\u4e2d\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u6a21\u578b\uff0c\u4f46\u751f\u6210\u9ad8\u8d28\u91cf\u957f\u65f6\u57df\u89c6\u9891\u901a\u5e38\u9700\u8981\u8f83\u5927\u7684\u6a21\u578b\u5bb9\u91cf\u3002", "keywords": ["driving-scene simulation", "long-horizon", "video generation", "autonomous driving"]}, "AI": {"tldr": "STAGE\u662f\u4e00\u79cd\u7528\u4e8e\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u3001\u9ad8\u4fdd\u771f\u3001\u957f\u65f6\u7a0b\u9a7e\u9a76\u89c6\u9891\u7684\u65b0\u578b\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5206\u5c42\u7279\u5f81\u534f\u8c03\u548c\u591a\u9636\u6bb5\u4f18\u5316\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u5efa\u6a21\u4e2d\uff0c\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u3001\u9ad8\u4fdd\u771f\u3001\u957f\u65f6\u7a0b\u7684\u9a7e\u9a76\u89c6\u9891\u662f\u4e00\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u7531\u4e8e\u65f6\u7a7a\u52a8\u6001\u89e3\u8026\u4e0d\u8db3\u548c\u8de8\u5e27\u7279\u5f81\u4f20\u64ad\u673a\u5236\u6709\u9650\u800c\u906d\u53d7\u8bef\u5dee\u7d2f\u79ef\u548c\u7279\u5f81\u9519\u4f4d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u56de\u5f52\u6846\u67b6STAGE\uff08\u6d41\u5f0f\u65f6\u5e8f\u6ce8\u610f\u529b\u751f\u6210\u5f15\u64ce\uff09\uff0c\u8be5\u6846\u67b6\u5f00\u521b\u4e86\u5206\u5c42\u7279\u5f81\u534f\u8c03\u548c\u591a\u9636\u6bb5\u4f18\u5316\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u89c6\u9891\u5408\u6210\u3002\u5f15\u5165\u4e86\u5206\u5c42\u65f6\u95f4\u7279\u5f81\u8f6c\u79fb\uff08HTFT\uff09\u548c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728Nuscenes\u6570\u636e\u96c6\u4e0a\u751f\u6210\u4e86600\u5e27\u9ad8\u8d28\u91cf\u9a7e\u9a76\u89c6\u9891\uff0c\u8fdc\u8fdc\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u6240\u80fd\u5b9e\u73b0\u7684\u6700\u5927\u957f\u5ea6\u3002", "conclusion": "\u5728Nuscenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTAGE\u5728\u957f\u65f6\u7a0b\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u751f\u6210\u65e0\u9650\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u9a7e\u9a76\u89c6\u9891\u3002", "summary_zh": "\u5728\u81ea\u52a8\u9a7e\u9a76\u4e16\u754c\u5efa\u6a21\u4e2d\uff0c\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u3001\u9ad8\u4fdd\u771f\u3001\u957f\u65f6\u7a0b\u7684\u9a7e\u9a76\u89c6\u9891\u662f\u4e00\u4e2a\u6839\u672c\u6027\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u7531\u4e8e\u65f6\u7a7a\u52a8\u6001\u89e3\u8026\u4e0d\u8db3\u548c\u8de8\u5e27\u7279\u5f81\u4f20\u64ad\u673a\u5236\u6709\u9650\u800c\u906d\u53d7\u8bef\u5dee\u7d2f\u79ef\u548c\u7279\u5f81\u9519\u4f4d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u56de\u5f52\u6846\u67b6STAGE\uff08\u6d41\u5f0f\u65f6\u5e8f\u6ce8\u610f\u529b\u751f\u6210\u5f15\u64ce\uff09\uff0c\u8be5\u6846\u67b6\u5f00\u521b\u4e86\u5206\u5c42\u7279\u5f81\u534f\u8c03\u548c\u591a\u9636\u6bb5\u4f18\u5316\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u7684\u89c6\u9891\u5408\u6210\u3002\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u957f\u65f6\u7a0b\u9a7e\u9a76\u89c6\u9891\u751f\u6210\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u5206\u5c42\u65f6\u95f4\u7279\u5f81\u8f6c\u79fb\uff08HTFT\uff09\u548c\u4e00\u79cd\u65b0\u9896\u7684\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u3002HTFT\u901a\u8fc7\u5bf9\u65f6\u95f4\u548c\u53bb\u566a\u8fc7\u7a0b\u8fdb\u884c\u5355\u72ec\u5efa\u6a21\uff0c\u5e76\u5728\u5e27\u4e4b\u95f4\u8f6c\u79fb\u53bb\u566a\u7279\u5f81\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u6574\u4e2a\u89c6\u9891\u751f\u6210\u8fc7\u7a0b\u4e2d\u89c6\u9891\u5e27\u4e4b\u95f4\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u662f\u5c06\u8bad\u7ec3\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff0c\u901a\u8fc7\u6a21\u578b\u89e3\u8026\u548c\u81ea\u56de\u5f52\u63a8\u7406\u8fc7\u7a0b\u4eff\u771f\uff0c\u4ece\u800c\u52a0\u901f\u6a21\u578b\u6536\u655b\u5e76\u51cf\u5c11\u8bef\u5dee\u7d2f\u79ef\u3002\u5728Nuscenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTAGE\u5728\u957f\u65f6\u7a0b\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63a2\u7d22\u4e86STAGE\u751f\u6210\u65e0\u9650\u957f\u5ea6\u9a7e\u9a76\u89c6\u9891\u7684\u80fd\u529b\u3002\u6211\u4eec\u5728Nuscenes\u6570\u636e\u96c6\u4e0a\u751f\u6210\u4e86600\u5e27\u9ad8\u8d28\u91cf\u9a7e\u9a76\u89c6\u9891\uff0c\u8fdc\u8fdc\u8d85\u8fc7\u4e86\u73b0\u6709\u65b9\u6cd5\u6240\u80fd\u5b9e\u73b0\u7684\u6700\u5927\u957f\u5ea6\u3002"}}
{"id": "2506.12078", "pdf": "https://arxiv.org/pdf/2506.12078", "abs": "https://arxiv.org/abs/2506.12078", "authors": ["Haoxiang Guan", "Jiyan He", "Liyang Fan", "Zhenzhen Ren", "Shaobin He", "Xin Yu", "Yuan Chen", "Shuxin Zheng", "Tie-Yan Liu", "Zhen Liu"], "title": "Modeling Earth-Scale Human-Like Societies with One Billion Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY", "cs.SI"], "comment": "Work in progress", "summary": "Understanding how complex societal behaviors emerge from individual cognition\nand interactions requires both high-fidelity modeling of human behavior and\nlarge-scale simulations. Traditional agent-based models (ABMs) have been\nemployed to study these dynamics for decades, but are constrained by simplified\nagent behaviors that fail to capture human complexity. Recent advances in large\nlanguage models (LLMs) offer new opportunities by enabling agents to exhibit\nsophisticated social behaviors that go beyond rule-based logic, yet face\nsignificant scaling challenges. Here we present Light Society, an agent-based\nsimulation framework that advances both fronts, efficiently modeling human-like\nsocieties at planetary scale powered by LLMs. Light Society formalizes social\nprocesses as structured transitions of agent and environment states, governed\nby a set of LLM-powered simulation operations, and executed through an event\nqueue. This modular design supports both independent and joint component\noptimization, supporting efficient simulation of societies with over one\nbillion agents. Large-scale simulations of trust games and opinion\npropagation--spanning up to one billion agents--demonstrate Light Society's\nhigh fidelity and efficiency in modeling social trust and information\ndiffusion, while revealing scaling laws whereby larger simulations yield more\nstable and realistic emergent behaviors.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on agent-based modeling of human-like societies using large language models (LLMs). While it doesn't directly address trajectory prediction, the simulation of agent behavior and interactions could potentially be relevant to understanding and predicting movement patterns in social contexts. The use of LLMs is a strong indicator of relevance to the specified domains.", "keywords": ["Large Language Models", "LLMs", "agent-based model", "simulation"]}, "AI": {"tldr": "Light Society\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u884c\u661f\u5c3a\u5ea6\u4e0a\u9ad8\u6548\u5efa\u6a21\u7c7b\u4eba\u793e\u4f1a\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6a21\u578b\uff08ABM\uff09\u5df2\u88ab\u7528\u4e8e\u7814\u7a76\u8fd9\u4e9b\u52a8\u6001\u6570\u5341\u5e74\uff0c\u4f46\u53d7\u5230\u7b80\u5316\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u7684\u590d\u6742\u6027\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u901a\u8fc7\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u8868\u73b0\u51fa\u8d85\u8d8a\u57fa\u4e8e\u89c4\u5219\u7684\u903b\u8f91\u7684\u590d\u6742\u793e\u4f1a\u884c\u4e3a\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u9762\u4e34\u7740\u5de8\u5927\u7684\u6269\u5c55\u6311\u6218\u3002", "method": "\u5f62\u5f0f\u5316\u793e\u4f1a\u8fc7\u7a0b\u4e3a\u667a\u80fd\u4f53\u548c\u73af\u5883\u72b6\u6001\u7684\u7ed3\u6784\u5316\u8f6c\u6362\uff0c\u7531\u4e00\u7ec4LLM\u9a71\u52a8\u7684\u6a21\u62df\u64cd\u4f5c\u7ba1\u7406\uff0c\u5e76\u901a\u8fc7\u4e8b\u4ef6\u961f\u5217\u6267\u884c\u3002", "result": "\u5bf9\u4fe1\u4efb\u535a\u5f08\u548c\u89c2\u70b9\u4f20\u64ad\u7684\u5927\u89c4\u6a21\u6a21\u62df\uff08\u8de8\u8d8a\u9ad8\u8fbe10\u4ebf\u4e2a\u667a\u80fd\u4f53\uff09\u8bc1\u660e\u4e86Light Society\u5728\u5efa\u6a21\u793e\u4f1a\u4fe1\u4efb\u548c\u4fe1\u606f\u4f20\u64ad\u65b9\u9762\u7684\u9ad8\u4fdd\u771f\u5ea6\u548c\u6548\u7387\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u66f4\u5927\u7684\u6a21\u62df\u4ea7\u751f\u66f4\u7a33\u5b9a\u548c\u771f\u5b9e\u7684\u6d8c\u73b0\u884c\u4e3a\u7684\u7f29\u653e\u89c4\u5f8b\u3002", "conclusion": "\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u62df\u80fd\u591f\u4ea7\u751f\u66f4\u7a33\u5b9a\u548c\u771f\u5b9e\u7684\u6d8c\u73b0\u884c\u4e3a\u3002", "summary_zh": "\u7406\u89e3\u590d\u6742\u7684\u793e\u4f1a\u884c\u4e3a\u5982\u4f55\u4ece\u4e2a\u4f53\u8ba4\u77e5\u548c\u4e92\u52a8\u4e2d\u4ea7\u751f\uff0c\u9700\u8981\u5bf9\u4eba\u7c7b\u884c\u4e3a\u8fdb\u884c\u9ad8\u4fdd\u771f\u5efa\u6a21\u548c\u5927\u89c4\u6a21\u6a21\u62df\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6a21\u578b\uff08ABM\uff09\u5df2\u88ab\u7528\u4e8e\u7814\u7a76\u8fd9\u4e9b\u52a8\u6001\u6570\u5341\u5e74\uff0c\u4f46\u53d7\u5230\u7b80\u5316\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u9650\u5236\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u7684\u590d\u6742\u6027\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6700\u65b0\u8fdb\u5c55\u901a\u8fc7\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u8868\u73b0\u51fa\u8d85\u8d8a\u57fa\u4e8e\u89c4\u5219\u7684\u903b\u8f91\u7684\u590d\u6742\u793e\u4f1a\u884c\u4e3a\uff0c\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u9762\u4e34\u7740\u5de8\u5927\u7684\u6269\u5c55\u6311\u6218\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Light Society\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u6a21\u62df\u6846\u67b6\uff0c\u5b83\u5728\u4e24\u4e2a\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u6709\u6548\u5730\u6a21\u62df\u4e86\u7531LLM\u9a71\u52a8\u7684\u884c\u661f\u89c4\u6a21\u7c7b\u4eba\u793e\u4f1a\u3002Light Society\u5c06\u793e\u4f1a\u8fc7\u7a0b\u5f62\u5f0f\u5316\u4e3a\u667a\u80fd\u4f53\u548c\u73af\u5883\u72b6\u6001\u7684\u7ed3\u6784\u5316\u8f6c\u6362\uff0c\u7531\u4e00\u7ec4LLM\u9a71\u52a8\u7684\u6a21\u62df\u64cd\u4f5c\u7ba1\u7406\uff0c\u5e76\u901a\u8fc7\u4e8b\u4ef6\u961f\u5217\u6267\u884c\u3002\u8fd9\u79cd\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u72ec\u7acb\u548c\u8054\u5408\u7ec4\u4ef6\u4f18\u5316\uff0c\u652f\u6301\u5bf9\u8d85\u8fc710\u4ebf\u4e2a\u667a\u80fd\u4f53\u7684\u793e\u4f1a\u8fdb\u884c\u6709\u6548\u6a21\u62df\u3002\u5bf9\u4fe1\u4efb\u535a\u5f08\u548c\u89c2\u70b9\u4f20\u64ad\u7684\u5927\u89c4\u6a21\u6a21\u62df\uff08\u8de8\u8d8a\u9ad8\u8fbe10\u4ebf\u4e2a\u667a\u80fd\u4f53\uff09\u8bc1\u660e\u4e86Light Society\u5728\u5efa\u6a21\u793e\u4f1a\u4fe1\u4efb\u548c\u4fe1\u606f\u4f20\u64ad\u65b9\u9762\u7684\u9ad8\u4fdd\u771f\u5ea6\u548c\u6548\u7387\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u66f4\u5927\u7684\u6a21\u62df\u4ea7\u751f\u66f4\u7a33\u5b9a\u548c\u771f\u5b9e\u7684\u6d8c\u73b0\u884c\u4e3a\u7684\u7f29\u653e\u89c4\u5f8b\u3002"}}
{"id": "2506.13629", "pdf": "https://arxiv.org/pdf/2506.13629", "abs": "https://arxiv.org/abs/2506.13629", "authors": ["Chenlu Zhan", "Gaoang Wang", "Hongwei Wang"], "title": "FreeQ-Graph: Free-form Querying with Semantic Consistent Scene Graph for 3D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Semantic querying in complex 3D scenes through free-form language presents a\nsignificant challenge. Existing 3D scene understanding methods use large-scale\ntraining data and CLIP to align text queries with 3D semantic features.\nHowever, their reliance on predefined vocabulary priors from training data\nhinders free-form semantic querying. Besides, recent advanced methods rely on\nLLMs for scene understanding but lack comprehensive 3D scene-level information\nand often overlook the potential inconsistencies in LLM-generated outputs. In\nour paper, we propose FreeQ-Graph, which enables Free-form Querying with a\nsemantic consistent scene Graph for 3D scene understanding. The core idea is to\nencode free-form queries from a complete and accurate 3D scene graph without\npredefined vocabularies, and to align them with 3D consistent semantic labels,\nwhich accomplished through three key steps. We initiate by constructing a\ncomplete and accurate 3D scene graph that maps free-form objects and their\nrelations through LLM and LVLM guidance, entirely free from training data or\npredefined priors. Most importantly, we align graph nodes with accurate\nsemantic labels by leveraging 3D semantic aligned features from merged\nsuperpoints, enhancing 3D semantic consistency. To enable free-form semantic\nquerying, we then design an LLM-based reasoning algorithm that combines\nscene-level and object-level information to intricate reasoning. We conducted\nextensive experiments on 3D semantic grounding, segmentation, and complex\nquerying tasks, while also validating the accuracy of graph generation.\nExperiments on 6 datasets show that our model excels in both complex free-form\nsemantic queries and intricate relational reasoning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce83D\u573a\u666f\u7406\u89e3\uff0c\u5e76\u5229\u7528LLM\u8fdb\u884c\u63a8\u7406\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u5bf9\u9f50\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4f7f\u7528\u4e86LLM\uff0c\u4e14\u573a\u666f\u7406\u89e3\u53ef\u4ee5\u4f5c\u4e3a\u8f68\u8ff9\u9884\u6d4b\u7684\u5148\u51b3\u6761\u4ef6\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLM", "3D scene understanding", "semantic querying"]}, "AI": {"tldr": "FreeQ-Graph\u901a\u8fc7\u6784\u5efa\u8bed\u4e49\u4e00\u81f4\u76843D\u573a\u666f\u56fe\uff0c\u5b9e\u73b0\u4e86\u5bf9\u590d\u67423D\u573a\u666f\u7684\u81ea\u7531\u5f62\u5f0f\u8bed\u4e49\u67e5\u8be2\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6765\u81ea\u8bad\u7ec3\u6570\u636e\u7684\u9884\u5b9a\u4e49\u8bcd\u6c47\u5148\u9a8c\uff0c\u963b\u788d\u4e86\u81ea\u7531\u5f62\u5f0f\u7684\u8bed\u4e49\u67e5\u8be2\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u7684\u5148\u8fdb\u65b9\u6cd5\u4f9d\u8d56\u4e8eLLM\u8fdb\u884c\u573a\u666f\u7406\u89e3\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u76843D\u573a\u666f\u7ea7\u522b\u4fe1\u606f\uff0c\u5e76\u4e14\u7ecf\u5e38\u5ffd\u7565LLM\u751f\u6210\u8f93\u51fa\u4e2d\u6f5c\u5728\u7684\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFreeQ-Graph\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u5b8c\u6574\u7684\u3001\u51c6\u786e\u76843D\u573a\u666f\u56fe\u6765\u7f16\u7801\u81ea\u7531\u5f62\u5f0f\u7684\u67e5\u8be2\uff0c\u5e76\u5c06\u5176\u4e0e3D\u4e00\u81f4\u7684\u8bed\u4e49\u6807\u7b7e\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\uff1a\u6784\u5efa3D\u573a\u666f\u56fe\uff0c\u5c06\u56fe\u8282\u70b9\u4e0e\u8bed\u4e49\u6807\u7b7e\u5bf9\u9f50\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u57fa\u4e8eLLM\u7684\u63a8\u7406\u7b97\u6cd5\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u590d\u6742\u81ea\u7531\u5f62\u5f0f\u8bed\u4e49\u67e5\u8be2\u548c\u590d\u6742\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4e5f\u9a8c\u8bc1\u4e86\u56fe\u751f\u6210\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u590d\u6742\u81ea\u7531\u5f62\u5f0f\u8bed\u4e49\u67e5\u8be2\u548c\u590d\u6742\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "summary_zh": "\u901a\u8fc7\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u5728\u590d\u67423D\u573a\u666f\u4e2d\u8fdb\u884c\u8bed\u4e49\u67e5\u8be2\u63d0\u51fa\u4e86\u4e00\u4e2a\u5de8\u5927\u7684\u6311\u6218\u3002\u73b0\u6709\u76843D\u573a\u666f\u7406\u89e3\u65b9\u6cd5\u4f7f\u7528\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u548cCLIP\u6765\u5bf9\u9f50\u6587\u672c\u67e5\u8be2\u548c3D\u8bed\u4e49\u7279\u5f81\u3002\u7136\u800c\uff0c\u5b83\u4eec\u5bf9\u6765\u81ea\u8bad\u7ec3\u6570\u636e\u7684\u9884\u5b9a\u4e49\u8bcd\u6c47\u5148\u9a8c\u7684\u4f9d\u8d56\u963b\u788d\u4e86\u81ea\u7531\u5f62\u5f0f\u7684\u8bed\u4e49\u67e5\u8be2\u3002\u6b64\u5916\uff0c\u6700\u8fd1\u7684\u5148\u8fdb\u65b9\u6cd5\u4f9d\u8d56\u4e8eLLM\u8fdb\u884c\u573a\u666f\u7406\u89e3\uff0c\u4f46\u7f3a\u4e4f\u5168\u9762\u76843D\u573a\u666f\u7ea7\u522b\u4fe1\u606f\uff0c\u5e76\u4e14\u7ecf\u5e38\u5ffd\u7565LLM\u751f\u6210\u8f93\u51fa\u4e2d\u6f5c\u5728\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u5728\u6211\u4eec\u7684\u8bba\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86FreeQ-Graph\uff0c\u5b83\u652f\u6301\u4f7f\u7528\u8bed\u4e49\u4e00\u81f4\u7684\u573a\u666f\u56fe\u8fdb\u884c\u81ea\u7531\u5f62\u5f0f\u67e5\u8be2\uff0c\u4ee5\u8fdb\u884c3D\u573a\u666f\u7406\u89e3\u3002\u5176\u6838\u5fc3\u601d\u60f3\u662f\u4ece\u4e00\u4e2a\u5b8c\u6574\u7684\u3001\u51c6\u786e\u76843D\u573a\u666f\u56fe\u7f16\u7801\u81ea\u7531\u5f62\u5f0f\u7684\u67e5\u8be2\uff0c\u800c\u65e0\u9700\u9884\u5b9a\u4e49\u7684\u8bcd\u6c47\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e3D\u4e00\u81f4\u7684\u8bed\u4e49\u6807\u7b7e\u5bf9\u9f50\uff0c\u8fd9\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u6b65\u9aa4\u5b8c\u6210\u3002\u6211\u4eec\u9996\u5148\u6784\u5efa\u4e00\u4e2a\u5b8c\u6574\u7684\u3001\u51c6\u786e\u76843D\u573a\u666f\u56fe\uff0c\u8be5\u56fe\u901a\u8fc7LLM\u548cLVLM\u6307\u5bfc\u6620\u5c04\u81ea\u7531\u5f62\u5f0f\u7684\u5bf9\u8c61\u53ca\u5176\u5173\u7cfb\uff0c\u5b8c\u5168\u4e0d\u53d7\u8bad\u7ec3\u6570\u636e\u6216\u9884\u5b9a\u4e49\u5148\u9a8c\u7684\u5f71\u54cd\u3002\u6700\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u901a\u8fc7\u5229\u7528\u6765\u81ea\u5408\u5e76\u7684\u8d85\u70b9\u76843D\u8bed\u4e49\u5bf9\u9f50\u7279\u5f81\uff0c\u5c06\u56fe\u8282\u70b9\u4e0e\u51c6\u786e\u7684\u8bed\u4e49\u6807\u7b7e\u5bf9\u9f50\uff0c\u4ece\u800c\u589e\u5f3a3D\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u4e3a\u4e86\u652f\u6301\u81ea\u7531\u5f62\u5f0f\u7684\u8bed\u4e49\u67e5\u8be2\uff0c\u6211\u4eec\u7136\u540e\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u63a8\u7406\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u7ed3\u5408\u4e86\u573a\u666f\u7ea7\u522b\u548c\u5bf9\u8c61\u7ea7\u522b\u7684\u4fe1\u606f\u4ee5\u8fdb\u884c\u590d\u6742\u7684\u63a8\u7406\u3002\u6211\u4eec\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5305\u62ec3D\u8bed\u4e49\u5b9a\u4f4d\u3001\u5206\u5272\u548c\u590d\u6742\u67e5\u8be2\u4efb\u52a1\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u56fe\u751f\u6210\u7684\u51c6\u786e\u6027\u3002\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u590d\u6742\u81ea\u7531\u5f62\u5f0f\u8bed\u4e49\u67e5\u8be2\u548c\u590d\u6742\u5173\u7cfb\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.13697", "pdf": "https://arxiv.org/pdf/2506.13697", "abs": "https://arxiv.org/abs/2506.13697", "authors": ["Junyoung Seo", "Jisang Han", "Jaewoo Jung", "Siyoon Jin", "Joungbin Lee", "Takuya Narihira", "Kazumi Fukuda", "Takashi Shibuya", "Donghoon Ahn", "Shoukang Hu", "Seungryong Kim", "Yuki Mitsufuji"], "title": "Vid-CamEdit: Video Camera Trajectory Editing with Generative Rendering from Estimated Geometry", "categories": ["cs.CV"], "comment": "Our project page can be found at\n  https://cvlab-kaist.github.io/Vid-CamEdit/", "summary": "We introduce Vid-CamEdit, a novel framework for video camera trajectory\nediting, enabling the re-synthesis of monocular videos along user-defined\ncamera paths. This task is challenging due to its ill-posed nature and the\nlimited multi-view video data for training. Traditional reconstruction methods\nstruggle with extreme trajectory changes, and existing generative models for\ndynamic novel view synthesis cannot handle in-the-wild videos. Our approach\nconsists of two steps: estimating temporally consistent geometry, and\ngenerative rendering guided by this geometry. By integrating geometric priors,\nthe generative model focuses on synthesizing realistic details where the\nestimated geometry is uncertain. We eliminate the need for extensive 4D\ntraining data through a factorized fine-tuning framework that separately trains\nspatial and temporal components using multi-view image and video data. Our\nmethod outperforms baselines in producing plausible videos from novel camera\ntrajectories, especially in extreme extrapolation scenarios on real-world\nfootage.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on editing camera trajectories in videos, which is related to trajectory prediction in the context of camera movements. While it doesn't explicitly use large language models, the generative rendering aspect might involve large generative models trained on image and video data. The connection to trajectory prediction is present, but the absence of LLMs lowers the relevance score.", "keywords": ["trajectory editing", "camera trajectory", "generative rendering"]}, "AI": {"tldr": "Vid-CamEdit \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u76f8\u673a\u8f68\u8ff9\u7f16\u8f91\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u5206\u89e3\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e86\u4ece\u7528\u6237\u5b9a\u4e49\u76f8\u673a\u8def\u5f84\u751f\u6210\u903c\u771f\u89c6\u9891\u3002", "motivation": "\u7531\u4e8e\u5176\u4e0d\u9002\u5b9a\u6027\u548c\u6709\u9650\u7684\u591a\u89c6\u56fe\u89c6\u9891\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\uff0c\u89c6\u9891\u76f8\u673a\u8f68\u8ff9\u7f16\u8f91\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u7684\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6781\u7aef\u7684\u8f68\u8ff9\u53d8\u5316\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u91ce\u5916\u89c6\u9891\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1a\u4f30\u8ba1\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u51e0\u4f55\u4f53\uff0c\u4ee5\u53ca\u7531\u8be5\u51e0\u4f55\u4f53\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u6e32\u67d3\u3002\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u5148\u9a8c\uff0c\u751f\u6210\u6a21\u578b\u4fa7\u91cd\u4e8e\u5408\u6210\u4f30\u8ba1\u51e0\u4f55\u4f53\u4e0d\u786e\u5b9a\u7684\u903c\u771f\u7ec6\u8282\u3002\u901a\u8fc7\u5206\u89e3\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u5927\u91cf\u7684 4D \u8bad\u7ec3\u6570\u636e\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u591a\u89c6\u56fe\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u5206\u522b\u8bad\u7ec3\u7a7a\u95f4\u548c\u65f6\u95f4\u7ec4\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4ece\u65b0\u9896\u76f8\u673a\u8f68\u8ff9\u751f\u6210\u903c\u771f\u89c6\u9891\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6781\u7aef\u5916\u63a8\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4ece\u65b0\u9896\u76f8\u673a\u8f68\u8ff9\u751f\u6210\u903c\u771f\u89c6\u9891\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6781\u7aef\u5916\u63a8\u60c5\u51b5\u4e0b\u3002", "summary_zh": "\u6211\u4eec\u4ecb\u7ecd\u4e86 Vid-CamEdit\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u9891\u76f8\u673a\u8f68\u8ff9\u7f16\u8f91\u6846\u67b6\uff0c\u80fd\u591f\u6cbf\u7740\u7528\u6237\u5b9a\u4e49\u7684\u76f8\u673a\u8def\u5f84\u91cd\u65b0\u5408\u6210\u5355\u76ee\u89c6\u9891\u3002\u7531\u4e8e\u5176\u4e0d\u9002\u5b9a\u6027\u548c\u6709\u9650\u7684\u591a\u89c6\u56fe\u89c6\u9891\u6570\u636e\u7528\u4e8e\u8bad\u7ec3\uff0c\u8fd9\u9879\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002\u4f20\u7edf\u7684\u91cd\u5efa\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u6781\u7aef\u7684\u8f68\u8ff9\u53d8\u5316\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u52a8\u6001\u65b0\u89c6\u89d2\u5408\u6210\u751f\u6210\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u91ce\u5916\u89c6\u9891\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u6b65\u9aa4\uff1a\u4f30\u8ba1\u65f6\u95f4\u4e0a\u4e00\u81f4\u7684\u51e0\u4f55\u4f53\uff0c\u4ee5\u53ca\u7531\u8be5\u51e0\u4f55\u4f53\u5f15\u5bfc\u7684\u751f\u6210\u5f0f\u6e32\u67d3\u3002\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u5148\u9a8c\uff0c\u751f\u6210\u6a21\u578b\u4fa7\u91cd\u4e8e\u5408\u6210\u4f30\u8ba1\u51e0\u4f55\u4f53\u4e0d\u786e\u5b9a\u7684\u903c\u771f\u7ec6\u8282\u3002\u901a\u8fc7\u5206\u89e3\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u6211\u4eec\u65e0\u9700\u5927\u91cf\u7684 4D \u8bad\u7ec3\u6570\u636e\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u591a\u89c6\u56fe\u56fe\u50cf\u548c\u89c6\u9891\u6570\u636e\u5206\u522b\u8bad\u7ec3\u7a7a\u95f4\u548c\u65f6\u95f4\u7ec4\u4ef6\u3002\u8be5\u65b9\u6cd5\u5728\u4ece\u65b0\u9896\u76f8\u673a\u8f68\u8ff9\u751f\u6210\u903c\u771f\u89c6\u9891\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6781\u7aef\u5916\u63a8\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2506.12349", "pdf": "https://arxiv.org/pdf/2506.12349", "abs": "https://arxiv.org/abs/2506.12349", "authors": ["Peiran Qiu", "Siyi Zhou", "Emilio Ferrara"], "title": "Information Suppression in Large Language Models: Auditing, Quantifying, and Characterizing Censorship in DeepSeek", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "This study examines information suppression mechanisms in DeepSeek, an\nopen-source large language model (LLM) developed in China. We propose an\nauditing framework and use it to analyze the model's responses to 646\npolitically sensitive prompts by comparing its final output with intermediate\nchain-of-thought (CoT) reasoning. Our audit unveils evidence of semantic-level\ninformation suppression in DeepSeek: sensitive content often appears within the\nmodel's internal reasoning but is omitted or rephrased in the final output.\nSpecifically, DeepSeek suppresses references to transparency, government\naccountability, and civic mobilization, while occasionally amplifying language\naligned with state propaganda. This study underscores the need for systematic\nauditing of alignment, content moderation, information suppression, and\ncensorship practices implemented into widely-adopted AI models, to ensure\ntransparency, accountability, and equitable access to unbiased information\nobtained by means of these systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on information suppression in Large Language Models (LLMs), specifically DeepSeek. While it doesn't directly address trajectory prediction, it is highly relevant to the domain of Large Language Models. The abstract mentions auditing, content moderation, and censorship practices within LLMs, which are important aspects of understanding and controlling the behavior of these models. Therefore, it has a moderate relevance.", "keywords": ["Large Language Models", "LLM", "DeepSeek", "information suppression", "censorship", "content moderation", "auditing"]}, "AI": {"tldr": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86DeepSeek\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u8bed\u4e49\u7ea7\u522b\u7684\u4fe1\u606f\u538b\u5236\u73b0\u8c61\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u653f\u6cbb\u654f\u611f\u5185\u5bb9\u65f6\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u7814\u7a76\u4e2d\u56fd\u5f00\u53d1\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578bDeepSeek\u4e2d\u7684\u4fe1\u606f\u538b\u5236\u673a\u5236\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5ba1\u8ba1\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5c06\u6a21\u578b\u7684\u6700\u7ec8\u8f93\u51fa\u4e0e\u5176\u5185\u90e8\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u8fdb\u884c\u6bd4\u8f83\uff0c\u6765\u5206\u6790\u6a21\u578b\u5bf9646\u4e2a\u653f\u6cbb\u654f\u611f\u63d0\u793a\u7684\u54cd\u5e94\u3002", "result": "\u6211\u4eec\u7684\u5ba1\u8ba1\u63ed\u793a\u4e86DeepSeek\u4e2d\u8bed\u4e49\u7ea7\u522b\u7684\u4fe1\u606f\u538b\u5236\u8bc1\u636e\uff1a\u654f\u611f\u5185\u5bb9\u7ecf\u5e38\u51fa\u73b0\u5728\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u4e2d\uff0c\u4f46\u5728\u6700\u7ec8\u8f93\u51fa\u4e2d\u88ab\u7701\u7565\u6216\u6539\u5199\u3002\u5177\u4f53\u6765\u8bf4\uff0cDeepSeek\u538b\u5236\u4e86\u5bf9\u900f\u660e\u5ea6\u3001\u653f\u5e9c\u95ee\u8d23\u5236\u548c\u516c\u6c11\u52a8\u5458\u7684\u5f15\u7528\uff0c\u540c\u65f6\u5076\u5c14\u653e\u5927\u4e86\u4e0e\u56fd\u5bb6\u5ba3\u4f20\u76f8\u4e00\u81f4\u7684\u8bed\u8a00\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u5bf9\u5e7f\u6cdb\u91c7\u7528\u7684AI\u6a21\u578b\u4e2d\u5b9e\u65bd\u7684\u5bf9\u9f50\u3001\u5185\u5bb9\u5ba1\u6838\u3001\u4fe1\u606f\u538b\u5236\u548c\u5ba1\u67e5\u5b9e\u8df5\u8fdb\u884c\u7cfb\u7edf\u5ba1\u8ba1\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u4ee5\u53ca\u901a\u8fc7\u8fd9\u4e9b\u7cfb\u7edf\u83b7\u5f97\u516c\u6b63\u4fe1\u606f\u7684\u516c\u5e73\u8bbf\u95ee\u3002", "summary_zh": "\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u4e2d\u56fd\u5f00\u53d1\u7684\u5f00\u6e90\u5927\u578b\u8bed\u8a00\u6a21\u578bDeepSeek\u4e2d\u7684\u4fe1\u606f\u538b\u5236\u673a\u5236\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u5ba1\u8ba1\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6bd4\u8f83\u6a21\u578b\u5bf9646\u4e2a\u653f\u6cbb\u654f\u611f\u63d0\u793a\u7684\u6700\u7ec8\u8f93\u51fa\u4e0e\u4e2d\u95f4\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\uff0c\u6765\u5206\u6790\u5176\u54cd\u5e94\u3002\u6211\u4eec\u7684\u5ba1\u8ba1\u63ed\u793a\u4e86DeepSeek\u4e2d\u8bed\u4e49\u7ea7\u522b\u7684\u4fe1\u606f\u538b\u5236\u8bc1\u636e\uff1a\u654f\u611f\u5185\u5bb9\u7ecf\u5e38\u51fa\u73b0\u5728\u6a21\u578b\u7684\u5185\u90e8\u63a8\u7406\u4e2d\uff0c\u4f46\u5728\u6700\u7ec8\u8f93\u51fa\u4e2d\u88ab\u7701\u7565\u6216\u6539\u5199\u3002\u5177\u4f53\u6765\u8bf4\uff0cDeepSeek\u538b\u5236\u4e86\u5bf9\u900f\u660e\u5ea6\u3001\u653f\u5e9c\u95ee\u8d23\u5236\u548c\u516c\u6c11\u52a8\u5458\u7684\u5f15\u7528\uff0c\u540c\u65f6\u5076\u5c14\u653e\u5927\u4e86\u4e0e\u56fd\u5bb6\u5ba3\u4f20\u76f8\u4e00\u81f4\u7684\u8bed\u8a00\u3002\u8fd9\u9879\u7814\u7a76\u5f3a\u8c03\u4e86\u5bf9\u5e7f\u6cdb\u91c7\u7528\u7684AI\u6a21\u578b\u4e2d\u5b9e\u65bd\u7684\u5bf9\u9f50\u3001\u5185\u5bb9\u5ba1\u6838\u3001\u4fe1\u606f\u538b\u5236\u548c\u5ba1\u67e5\u5b9e\u8df5\u8fdb\u884c\u7cfb\u7edf\u5ba1\u8ba1\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u3001\u95ee\u8d23\u5236\u4ee5\u53ca\u901a\u8fc7\u8fd9\u4e9b\u7cfb\u7edf\u83b7\u5f97\u516c\u6b63\u4fe1\u606f\u7684\u516c\u5e73\u8bbf\u95ee\u3002"}}
{"id": "2506.13705", "pdf": "https://arxiv.org/pdf/2506.13705", "abs": "https://arxiv.org/abs/2506.13705", "authors": ["Junru Zhang", "Lang Feng", "Xu Guo", "Yuhan Wu", "Yabo Dong", "Duanqing Xu"], "title": "TimeMaster: Training Time-Series Multimodal LLMs to Reason via Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Time-series reasoning remains a significant challenge in multimodal large\nlanguage models (MLLMs) due to the dynamic temporal patterns, ambiguous\nsemantics, and lack of temporal priors. In this work, we introduce TimeMaster,\na reinforcement learning (RL)-based method that enables time-series MLLMs to\nperform structured, interpretable reasoning directly over visualized\ntime-series inputs and task prompts. TimeMaster adopts a three-part structured\noutput format, reasoning, classification, and domain-specific extension, and is\noptimized via a composite reward function that aligns format adherence,\nprediction accuracy, and open-ended insight quality. The model is trained using\na two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish\na good initialization, followed by Group Relative Policy Optimization (GRPO) at\nthe token level to enable stable and targeted reward-driven improvement in\ntime-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across\nsix real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster\nachieves state-of-the-art performance, outperforming both classical time-series\nmodels and few-shot GPT-4o by over 14.6% and 7.3% performance gain,\nrespectively. Notably, TimeMaster goes beyond time-series classification: it\nalso exhibits expert-like reasoning behavior, generates context-aware\nexplanations, and delivers domain-aligned insights. Our results highlight that\nreward-driven RL can be a scalable and promising path toward integrating\ntemporal understanding into time-series MLLMs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on time-series data and multimodal large language models (MLLMs), which are related to trajectory prediction as trajectories are a specific type of time-series data. The paper explores reasoning with time-series data using reinforcement learning and large language models. Although the specific application to trajectory prediction isn't explicitly mentioned, the core concepts and techniques are applicable and relevant.", "keywords": ["large language models", "MLLMs", "time-series", "reinforcement learning", "reasoning"]}, "AI": {"tldr": "TimeMaster\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u51fa\u4e13\u5bb6\u7ea7\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u52a8\u6001\u7684\u65f6\u95f4\u6a21\u5f0f\u3001\u6a21\u7cca\u7684\u8bed\u4e49\u548c\u7f3a\u4e4f\u65f6\u95f4\u5148\u9a8c\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684TimeMaster\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u4e09\u90e8\u5206\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f\uff08\u63a8\u7406\u3001\u5206\u7c7b\u548c\u9886\u57df\u7279\u5b9a\u6269\u5c55\uff09\uff0c\u5e76\u901a\u8fc7\u590d\u5408\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u540c\u65f6\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\uff1a\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u3002", "result": "TimeMaster\u5728TimerBed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u548c\u5c11\u6837\u672cGPT-4o\uff0c\u5206\u522b\u53d6\u5f97\u4e86\u8d85\u8fc714.6%\u548c7.3%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578bTimeMaster\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u4e13\u5bb6\u7ea7\u7684\u63a8\u7406\u80fd\u529b\u548c\u9886\u57df\u77e5\u8bc6\u3002", "summary_zh": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aTimeMaster\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u65f6\u95f4\u5e8f\u5217\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u76f4\u63a5\u5bf9\u53ef\u89c6\u5316\u7684\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u548c\u4efb\u52a1\u63d0\u793a\u6267\u884c\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002TimeMaster\u91c7\u7528\u4e09\u90e8\u5206\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f\uff1a\u63a8\u7406\u3001\u5206\u7c7b\u548c\u9886\u57df\u7279\u5b9a\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u590d\u5408\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u4f18\u5316\uff0c\u8be5\u51fd\u6570\u7b26\u5408\u683c\u5f0f\u8981\u6c42\u3001\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5f00\u653e\u5f0f\u6d1e\u5bdf\u8d28\u91cf\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\u8fdb\u884c\u8bad\u7ec3\uff1a\u9996\u5148\u5e94\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u5efa\u7acb\u826f\u597d\u7684\u521d\u59cb\u5316\uff0c\u7136\u540e\u5728token\u7ea7\u522b\u5e94\u7528\u7fa4\u4f53\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\uff0c\u4ee5\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4e2d\u7a33\u5b9a\u548c\u6709\u9488\u5bf9\u6027\u7684\u5956\u52b1\u9a71\u52a8\u6539\u8fdb\u3002\u6211\u4eec\u5728TimerBed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eQwen2.5-VL-3B-Instruct\uff0c\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u5206\u7c7b\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86TimeMaster\u3002TimeMaster\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\u548c\u5c11\u6837\u672cGPT-4o\uff0c\u5206\u522b\u53d6\u5f97\u4e86\u8d85\u8fc714.6%\u548c7.3%\u7684\u6027\u80fd\u63d0\u5347\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cTimeMaster\u8d85\u8d8a\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\uff1a\u5b83\u8fd8\u8868\u73b0\u51fa\u4e13\u5bb6\u822c\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u91ca\uff0c\u5e76\u63d0\u4f9b\u4e0e\u9886\u57df\u5bf9\u9f50\u7684\u89c1\u89e3\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u80fd\u662f\u5c06\u65f6\u95f4\u7406\u89e3\u96c6\u6210\u5230\u65f6\u95f4\u5e8f\u5217\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u5e0c\u671b\u7684\u9014\u5f84\u3002"}}
{"id": "2506.13034", "pdf": "https://arxiv.org/pdf/2506.13034", "abs": "https://arxiv.org/abs/2506.13034", "authors": ["Zhixin Guo", "Qi Shi", "Xiaofan Xu", "Sixiang Shan", "Limin Qin", "Linqiang Ge", "Rui Zhang", "Ya Dai", "Hua Zhu", "Guowei Jiang"], "title": "SpaceTrack-TimeSeries: Time Series Dataset towards Satellite Orbit Analysis", "categories": ["astro-ph.EP", "astro-ph.IM", "cs.AI"], "comment": null, "summary": "With the rapid advancement of aerospace technology and the large-scale\ndeployment of low Earth orbit (LEO) satellite constellations, the challenges\nfacing astronomical observations and deep space exploration have become\nincreasingly pronounced. As a result, the demand for high-precision orbital\ndata on space objects-along with comprehensive analyses of satellite\npositioning, constellation configurations, and deep space satellite\ndynamics-has grown more urgent. However, there remains a notable lack of\npublicly accessible, real-world datasets to support research in areas such as\nspace object maneuver behavior prediction and collision risk assessment. This\nstudy seeks to address this gap by collecting and curating a representative\ndataset of maneuvering behavior from Starlink satellites. The dataset\nintegrates Two-Line Element (TLE) catalog data with corresponding\nhigh-precision ephemeris data, thereby enabling a more realistic and\nmultidimensional modeling of space object behavior. It provides valuable\ninsights into practical deployment of maneuver detection methods and the\nevaluation of collision risks in increasingly congested orbital environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on satellite orbit analysis and prediction, which falls under the umbrella of trajectory prediction. While it doesn't explicitly mention large language models, the prediction of satellite maneuvers can potentially benefit from or be integrated with large models for enhanced accuracy or efficiency. The core focus is on trajectory prediction in a specific domain (satellite orbits).", "keywords": ["trajectory prediction", "satellite orbit analysis", "maneuver behavior prediction", "collision risk assessment"]}, "AI": {"tldr": "\u672c\u7814\u7a76\u6536\u96c6\u5e76\u7ba1\u7406\u4e86 Starlink \u536b\u661f\u7684\u673a\u52a8\u884c\u4e3a\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u7a7a\u95f4\u7269\u4f53\u884c\u4e3a\u9884\u6d4b\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u7b49\u9886\u57df\u7684\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u822a\u7a7a\u822a\u5929\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u548c\u4f4e\u5730\u7403\u8f68\u9053 (LEO) \u536b\u661f\u661f\u5ea7\u7684\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u5929\u6587\u89c2\u6d4b\u548c\u6df1\u7a7a\u63a2\u6d4b\u9762\u4e34\u7684\u6311\u6218\u65e5\u76ca\u4e25\u5cfb\u3002\u56e0\u6b64\uff0c\u5bf9\u7a7a\u95f4\u7269\u4f53\u7684\u9ad8\u7cbe\u5ea6\u8f68\u9053\u6570\u636e\u4ee5\u53ca\u5bf9\u536b\u661f\u5b9a\u4f4d\u3001\u661f\u5ea7\u914d\u7f6e\u548c\u6df1\u7a7a\u536b\u661f\u52a8\u529b\u5b66\u7684\u7efc\u5408\u5206\u6790\u7684\u9700\u6c42\u53d8\u5f97\u8d8a\u6765\u8d8a\u8feb\u5207\u3002\u7136\u800c\uff0c\u4ecd\u7136\u660e\u663e\u7f3a\u4e4f\u516c\u5f00\u53ef\u7528\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u6765\u652f\u6301\u7a7a\u95f4\u7269\u4f53\u673a\u52a8\u884c\u4e3a\u9884\u6d4b\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u7b49\u9886\u57df\u7684\u7814\u7a76\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u6574\u5408\u53cc\u7ebf\u5143\u7d20 (TLE) \u76ee\u5f55\u6570\u636e\u4e0e\u76f8\u5e94\u7684\u9ad8\u7cbe\u5ea6\u661f\u5386\u6570\u636e\uff0c\u6536\u96c6\u548c\u7ba1\u7406\u4e86 Starlink \u536b\u661f\u7684\u673a\u52a8\u884c\u4e3a\u4ee3\u8868\u6027\u6570\u636e\u96c6\u3002", "result": "\u8be5\u6570\u636e\u96c6\u80fd\u591f\u5bf9\u7a7a\u95f4\u7269\u4f53\u884c\u4e3a\u8fdb\u884c\u66f4\u771f\u5b9e\u548c\u591a\u7ef4\u5ea6\u7684\u5efa\u6a21\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u7a7a\u95f4\u7269\u4f53\u884c\u4e3a\u7684\u5b9e\u9645\u90e8\u7f72\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002", "summary_zh": "\u968f\u7740\u822a\u7a7a\u822a\u5929\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\u548c\u4f4e\u5730\u7403\u8f68\u9053 (LEO) \u536b\u661f\u661f\u5ea7\u7684\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u5929\u6587\u89c2\u6d4b\u548c\u6df1\u7a7a\u63a2\u6d4b\u9762\u4e34\u7684\u6311\u6218\u65e5\u76ca\u4e25\u5cfb\u3002\u56e0\u6b64\uff0c\u5bf9\u7a7a\u95f4\u7269\u4f53\u7684\u9ad8\u7cbe\u5ea6\u8f68\u9053\u6570\u636e\u4ee5\u53ca\u5bf9\u536b\u661f\u5b9a\u4f4d\u3001\u661f\u5ea7\u914d\u7f6e\u548c\u6df1\u7a7a\u536b\u661f\u52a8\u529b\u5b66\u7684\u7efc\u5408\u5206\u6790\u7684\u9700\u6c42\u53d8\u5f97\u8d8a\u6765\u8d8a\u8feb\u5207\u3002\u7136\u800c\uff0c\u4ecd\u7136\u660e\u663e\u7f3a\u4e4f\u516c\u5f00\u53ef\u7528\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u6765\u652f\u6301\u7a7a\u95f4\u7269\u4f53\u673a\u52a8\u884c\u4e3a\u9884\u6d4b\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u7b49\u9886\u57df\u7684\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6536\u96c6\u548c\u7ba1\u7406 Starlink \u536b\u661f\u7684\u673a\u52a8\u884c\u4e3a\u4ee3\u8868\u6027\u6570\u636e\u96c6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002\u8be5\u6570\u636e\u96c6\u6574\u5408\u4e86\u53cc\u7ebf\u5143\u7d20 (TLE) \u76ee\u5f55\u6570\u636e\u4e0e\u76f8\u5e94\u7684\u9ad8\u7cbe\u5ea6\u661f\u5386\u6570\u636e\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u7a7a\u95f4\u7269\u4f53\u884c\u4e3a\u8fdb\u884c\u66f4\u771f\u5b9e\u548c\u591a\u7ef4\u5ea6\u7684\u5efa\u6a21\u3002\u5b83\u4e3a\u673a\u52a8\u68c0\u6d4b\u65b9\u6cd5\u7684\u5b9e\u9645\u90e8\u7f72\u4ee5\u53ca\u5728\u65e5\u76ca\u62e5\u6324\u7684\u8f68\u9053\u73af\u5883\u4e2d\u8bc4\u4f30\u78b0\u649e\u98ce\u9669\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
