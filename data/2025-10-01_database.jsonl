{"id": "2509.23338", "pdf": "https://arxiv.org/pdf/2509.23338", "abs": "https://arxiv.org/abs/2509.23338", "authors": ["Wei Zhou", "Guoliang Li", "Haoyu Wang", "Yuxing Han", "Xufei Wu", "Fan Wu", "Xuanhe Zhou"], "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "To appear in NeurIPS 2025. Welcome your submission to challenge our\n  leaderboard at: https://code4db.github.io/parrot-bench/. Also visit our code\n  repository at: https://github.com/weAIDB/PARROT", "summary": "Large language models (LLMS) have shown increasing effectiveness in\nText-to-SQL tasks. However, another closely related problem, Cross-System SQL\nTranslation (a.k.a., SQL-to-SQL), which adapts a query written for one database\nsystem (e.g., MySQL) into its equivalent one for another system (e.g.,\nClickHouse), is of great practical importance but remains underexplored.\nExisting SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which\n(1) focus on a limited set of database systems (often just SQLite) and (2)\ncannot capture many system-specific SQL dialects (e.g., customized functions,\ndata types, and syntax rules). Thus, in this paper, we introduce PARROT, a\nPractical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT\ncomprises 598 translation pairs from 38 open-source benchmarks and real-world\nbusiness services, specifically prepared to challenge system-specific SQL\nunderstanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We\nalso provide multiple benchmark variants, including PARROT-Diverse with 28,003\ntranslations (for extensive syntax testing) and PARROT-Simple with 5,306\nrepresentative samples (for focused stress testing), covering 22\nproduction-grade database systems. To promote future research, we release a\npublic leaderboard and source code at: https://code4db.github.io/parrot-bench/."}
{"id": "2509.23577", "pdf": "https://arxiv.org/pdf/2509.23577", "abs": "https://arxiv.org/abs/2509.23577", "authors": ["Mengying Wang", "Moming Duan", "Yicong Huang", "Chen Li", "Bingsheng He", "Yinghui Wu"], "title": "ML-Asset Management: Curation, Discovery, and Utilization", "categories": ["cs.DB", "cs.AI", "cs.IR"], "comment": "Tutorial, VLDB 2025. Project page:\n  https://ml-assets-management.github.io/", "summary": "Machine learning (ML) assets, such as models, datasets, and metadata, are\ncentral to modern ML workflows. Despite their explosive growth in practice,\nthese assets are often underutilized due to fragmented documentation, siloed\nstorage, inconsistent licensing, and lack of unified discovery mechanisms,\nmaking ML-asset management an urgent challenge. This tutorial offers a\ncomprehensive overview of ML-asset management activities across its lifecycle,\nincluding curation, discovery, and utilization. We provide a categorization of\nML assets, and major management issues, survey state-of-the-art techniques, and\nidentify emerging opportunities at each stage. We further highlight\nsystem-level challenges related to scalability, lineage, and unified indexing.\nThrough live demonstrations of systems, this tutorial equips both researchers\nand practitioners with actionable insights and practical tools for advancing\nML-asset management in real-world and domain-specific settings."}
{"id": "2509.23775", "pdf": "https://arxiv.org/pdf/2509.23775", "abs": "https://arxiv.org/abs/2509.23775", "authors": ["Linglin Yang", "Lei Zou", "Chunshan Zhao"], "title": "NeuSO: Neural Optimizer for Subgraph Queries", "categories": ["cs.DB"], "comment": "Full version of \"NeuSO: Neural Optimizer for Subgraph Queries\",\n  accepted to SIGMOD 2026", "summary": "Subgraph query is a critical task in graph analysis with a wide range of\napplications across various domains. Most existing methods rely on heuristic\nvertex matching orderings, which may significantly degrade enumeration\nperformance for certain queries. While learning-based optimizers have recently\ngained attention in the context of relational databases, they cannot be\ndirectly applied to subgraph queries due to the heterogeneous and\nschema-flexible nature of graph data, as well as the large number of joins\ninvolved in subgraph queries. These complexities often leads to inefficient\nonline performance, making such approaches impractical for real-world graph\ndatabase systems. To address this challenge, we propose NeuSO, a novel\nlearning-based optimizer for subgraph queries that achieves both high accuracy\nand efficiency. NeuSO features an efficient query graph encoder and an\nestimator which are trained using a multi-task framework to estimate both\nsubquery cardinality and execution cost. Based on these estimates, NeuSO\nemploys a top-down plan enumerator to generate high-quality execution plans for\nsubgraph queries. Extensive experiments on multiple datasets demonstrate that\nNeuSO outperforms existing subgraph query ordering approaches in both\nperformance and efficiency."}
{"id": "2509.23645", "pdf": "https://arxiv.org/pdf/2509.23645", "abs": "https://arxiv.org/abs/2509.23645", "authors": ["A S M Shahadat Hossain", "Colin Brown", "David Koop", "Tanu Malik"], "title": "Similarity-Based Assessment of Computational Reproducibility in Jupyter Notebooks", "categories": ["cs.SE", "cs.DB"], "comment": "10 pages", "summary": "Computational reproducibility refers to obtaining consistent results when\nrerunning an experiment. Jupyter Notebook, a web-based computational notebook\napplication, facilitates running, publishing, and sharing computational\nexperiments along with their results. However, rerunning a Jupyter Notebook may\nnot always generate identical results due to various factors, such as\nrandomness, changes in library versions, or variations in the computational\nenvironment. This paper introduces the Similarity-based Reproducibility Index\n(SRI) -- a metric for assessing the reproducibility of results in Jupyter\nNotebooks. SRI employs novel methods developed based on similarity metrics\nspecific to different types of Python objects to compare rerun outputs against\noriginal outputs. For every cell generating an output in a rerun notebook, SRI\nreports a quantitative score in the range [0, 1] as well as some qualitative\ninsights to assess reproducibility. The paper also includes a case study in\nwhich the proposed metric is applied to a set of Jupyter Notebooks,\ndemonstrating how various similarity metrics can be leveraged to quantify\ncomputational reproducibility."}
{"id": "2509.23834", "pdf": "https://arxiv.org/pdf/2509.23834", "abs": "https://arxiv.org/abs/2509.23834", "authors": ["Haochen Sun", "Xi He"], "title": "GPM: The Gaussian Pancake Mechanism for Planting Undetectable Backdoors in Differential Privacy", "categories": ["cs.CR", "cs.DB"], "comment": "16 pages, 7 figures. Not published yet. Code and raw experimental\n  logs will be available after publication, or upon email request", "summary": "Differential privacy (DP) has become the gold standard for preserving\nindividual privacy in data analysis. However, an implicit yet fundamental\nassumption underlying these rigorous privacy guarantees is the correct\nimplementation and execution of DP mechanisms. Several incidents of unintended\nprivacy loss have occurred due to numerical issues and inappropriate\nconfigurations of DP software, which have been successfully exploited in\nprivacy attacks. To better understand the seriousness of defective DP software,\nwe ask the following question: is it possible to elevate these passive defects\ninto active privacy attacks while maintaining covertness?\n  To address this question, we present the Gaussian pancake mechanism (GPM), a\nnovel mechanism that is computationally indistinguishable from the widely used\nGaussian mechanism (GM), yet exhibits arbitrarily weaker statistical DP\nguarantees. This unprecedented separation enables a new class of backdoor\nattacks: by indistinguishably passing off as the authentic GM, GPM can covertly\ndegrade statistical privacy. Unlike the unintentional privacy loss caused by\nGM's numerical issues, GPM is an adversarial yet undetectable backdoor attack\nagainst data privacy. We formally prove GPM's covertness, characterize its\nstatistical leakage, and demonstrate a concrete distinguishing attack that can\nachieve near-perfect success rates under suitable parameter choices, both\ntheoretically and empirically.\n  Our results underscore the importance of using transparent, open-source DP\nlibraries and highlight the need for rigorous scrutiny and formal verification\nof DP implementations to prevent subtle, undetectable privacy compromises in\nreal-world systems."}
{"id": "2509.23942", "pdf": "https://arxiv.org/pdf/2509.23942", "abs": "https://arxiv.org/abs/2509.23942", "authors": ["John N. Daras"], "title": "Efficient Identification of High Similarity Clusters in Polygon Datasets", "categories": ["cs.LG", "cs.DB", "q-bio.QM"], "comment": "11 pages, 3 figures", "summary": "Advancements in tools like Shapely 2.0 and Triton can significantly improve\nthe efficiency of spatial similarity computations by enabling faster and more\nscalable geometric operations. However, for extremely large datasets, these\noptimizations may face challenges due to the sheer volume of computations\nrequired. To address this, we propose a framework that reduces the number of\nclusters requiring verification, thereby decreasing the computational load on\nthese systems. The framework integrates dynamic similarity index thresholding,\nsupervised scheduling, and recall-constrained optimization to efficiently\nidentify clusters with the highest spatial similarity while meeting\nuser-defined precision and recall requirements. By leveraging Kernel Density\nEstimation (KDE) to dynamically determine similarity thresholds and machine\nlearning models to prioritize clusters, our approach achieves substantial\nreductions in computational cost without sacrificing accuracy. Experimental\nresults demonstrate the scalability and effectiveness of the method, offering a\npractical solution for large-scale geospatial analysis."}
{"id": "2509.23988", "pdf": "https://arxiv.org/pdf/2509.23988", "abs": "https://arxiv.org/abs/2509.23988", "authors": ["Zirui Tang", "Weizheng Wang", "Zihang Zhou", "Yang Jiao", "Bangrui Xu", "Boyu Niu", "Xuanhe Zhou", "Guoliang Li", "Yeye He", "Wei Zhou", "Yitong Song", "Cheng Tan", "Bin Wang", "Conghui He", "Xiaoyang Wang", "Fan Wu"], "title": "LLM/Agent-as-Data-Analyst: A Survey", "categories": ["cs.AI", "cs.DB"], "comment": "35 page, 11 figures", "summary": "Large language model (LLM) and agent techniques for data analysis (a.k.a\nLLM/Agent-as-Data-Analyst) have demonstrated substantial impact in both\nacademica and industry. In comparison with traditional rule or small-model\nbased approaches, (agentic) LLMs enable complex data understanding, natural\nlanguage interfaces, semantic analysis functions, and autonomous pipeline\norchestration. The technical evolution further distills five key design goals\nfor intelligent data analysis agents, namely semantic-aware design,\nmodality-hybrid integration, autonomous pipelines, tool-augmented workflows,\nand support for open-world tasks. From a modality perspective, we review\nLLM-based techniques for (i) structured data (e.g., table question answering\nfor relational data and NL2GQL for graph data), (ii) semi-structured data\n(e.g., markup languages understanding and semi-structured table modeling),\n(iii) unstructured data (e.g., chart understanding, document understanding,\nprogramming languages vulnerable detection), and (iv) heterogeneous data (e.g.,\ndata retrieval and modality alignment for data lakes). Finally, we outline the\nremaining challenges and propose several insights and practical directions for\nadvancing LLM/Agent-powered data analysis."}
{"id": "2509.24127", "pdf": "https://arxiv.org/pdf/2509.24127", "abs": "https://arxiv.org/abs/2509.24127", "authors": ["Nooshin Bahador"], "title": "Transparent, Evaluable, and Accessible Data Agents: A Proof-of-Concept Framework", "categories": ["cs.AI", "cs.DB"], "comment": "20 pages, 11 figures", "summary": "This article presents a modular, component-based architecture for developing\nand evaluating AI agents that bridge the gap between natural language\ninterfaces and complex enterprise data warehouses. The system directly\naddresses core challenges in data accessibility by enabling non-technical users\nto interact with complex data warehouses through a conversational interface,\ntranslating ambiguous user intent into precise, executable database queries to\novercome semantic gaps. A cornerstone of the design is its commitment to\ntransparent decision-making, achieved through a multi-layered reasoning\nframework that explains the \"why\" behind every decision, allowing for full\ninterpretability by tracing conclusions through specific, activated business\nrules and data points. The architecture integrates a robust quality assurance\nmechanism via an automated evaluation framework that serves multiple functions:\nit enables performance benchmarking by objectively measuring agent performance\nagainst golden standards, and it ensures system reliability by automating the\ndetection of performance regressions during updates. The agent's analytical\ndepth is enhanced by a statistical context module, which quantifies deviations\nfrom normative behavior, ensuring all conclusions are supported by quantitative\nevidence including concrete data, percentages, and statistical comparisons. We\ndemonstrate the efficacy of this integrated agent-development-with-evaluation\nframework through a case study on an insurance claims processing system. The\nagent, built on a modular architecture, leverages the BigQuery ecosystem to\nperform secure data retrieval, apply domain-specific business rules, and\ngenerate human-auditable justifications. The results confirm that this approach\ncreates a robust, evaluable, and trustworthy system for deploying LLM-powered\nagents in data-sensitive, high-stakes domains."}
{"id": "2509.24403", "pdf": "https://arxiv.org/pdf/2509.24403", "abs": "https://arxiv.org/abs/2509.24403", "authors": ["Pengfei Wang", "Baolin Sun", "Xuemei Dong", "Yaxun Dai", "Hongwei Yuan", "Mengdie Chu", "Yingqi Gao", "Xiang Qi", "Peng Zhang", "Ying Yan"], "title": "Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind\nhuman experts on challenging benchmarks like BIRD. Current approaches that\nexplore test-time scaling lack an orchestrated strategy and neglect the model's\ninternal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,\na novel framework leveraging scalable computation to improve performance.\nAgentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that\nsynergistically combines three distinct perspectives: i) Internal Scaling via\nRL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative\nRefinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament\nSelection. Agentar-Scale-SQL is a general-purpose framework designed for easy\nadaptation to new databases and more powerful language models. Extensive\nexperiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD\nbenchmark, reaching 81.67\\% execution accuracy on the test set and ranking\nfirst on the official leaderboard, demonstrating an effective path toward\nhuman-level performance."}
{"id": "2509.24405", "pdf": "https://arxiv.org/pdf/2509.24405", "abs": "https://arxiv.org/abs/2509.24405", "authors": ["Khanh Trinh Pham", "Thu Huong Nguyen", "Jun Jo", "Quoc Viet Hung Nguyen", "Thanh Tam Nguyen"], "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.ET", "cs.IR"], "comment": null, "summary": "Text-to-SQL enables natural access to databases, yet most benchmarks are\nEnglish-only, limiting multilingual progress. We introduce MultiSpider 2.0,\nextending Spider 2.0 to eight languages (English, German, French, Spanish,\nPortuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's\nstructural difficulty while adding linguistic and dialectal variability,\ndemanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art\nLLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when\nrelying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we\nprovide a collaboration-driven language agents baseline that iteratively\nrefines queries, improving accuracy to 15\\%. These results reveal a substantial\nmultilingual gap and motivate methods that are robust across languages and\nready for real-world enterprise deployment. Our benchmark is available at\nhttps://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL."}
