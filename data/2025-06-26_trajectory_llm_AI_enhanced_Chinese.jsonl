{"id": "2506.18939", "pdf": "https://arxiv.org/pdf/2506.18939", "abs": "https://arxiv.org/abs/2506.18939", "authors": ["Rui An", "Yifeng Zhang", "Ziran Liang", "Wenqi Fan", "Yuxuan Liang", "Xuequn Shang", "Qing Li"], "title": "Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Training urban spatio-temporal foundation models that generalize well across\ndiverse regions and cities is critical for deploying urban services in unseen\nor data-scarce regions. Recent studies have typically focused on fusing\ncross-domain spatio-temporal data to train unified Transformer-based models.\nHowever, these models suffer from quadratic computational complexity and high\nmemory overhead, limiting their scalability and practical deployment. Inspired\nby the efficiency of Mamba, a state space model with linear time complexity, we\nexplore its potential for efficient urban spatio-temporal prediction. However,\ndirectly applying Mamba as a spatio-temporal backbone leads to negative\ntransfer and severe performance degradation. This is primarily due to\nspatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden\nstate updates, which limit cross-domain generalization. To overcome these\nchallenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for\nefficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear\ncomplexity advantage while significantly enhancing its adaptability to\nheterogeneous domains. Specifically, we introduce two core innovations: (1) a\ndomain-adaptive state space model that partitions the latent representation\nspace into a shared subspace for learning cross-domain commonalities and\nindependent, domain-specific subspaces for capturing intra-domain\ndiscriminative features; (2) three distinct Domain Adapters, which serve as\ndomain-aware proxies to bridge disparate domain distributions and facilitate\nthe alignment of cross-domain commonalities. Extensive experiments demonstrate\nthe generalization and efficiency of Damba-ST. It achieves state-of-the-art\nperformance on prediction tasks and demonstrates strong zero-shot\ngeneralization, enabling seamless deployment in new urban environments without\nextensive retraining or fine-tuning.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on spatio-temporal prediction, which is related to trajectory prediction, and explores the use of a state space model (Mamba) for efficient prediction. While not directly using large language models, it addresses the scalability limitations of Transformer-based models, which are commonly used in LLMs, and aims to create a foundation model for urban spatio-temporal data. The domain adaptation aspect also increases its relevance.", "keywords": ["spatio-temporal prediction", "domain adaptation", "foundation models", "Mamba", "urban"]}, "AI": {"tldr": "Damba-ST\u662f\u4e00\u79cd\u9886\u57df\u81ea\u9002\u5e94\u7684Mamba\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u57ce\u5e02\u65f6\u7a7a\u9884\u6d4b\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u9ad8\u5185\u5b58\u5f00\u9500\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u3002\u76f4\u63a5\u5e94\u7528Mamba\u4f5c\u4e3a\u65f6\u7a7a\u9aa8\u5e72\u4f1a\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u548c\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u65f6\u7a7a\u5f02\u8d28\u6027\u548cMamba\u9690\u85cf\u72b6\u6001\u66f4\u65b0\u7684\u9012\u5f52\u673a\u5236\u9650\u5236\u4e86\u8de8\u57df\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eMamba\u7684\u9886\u57df\u81ea\u9002\u5e94\u6a21\u578bDamba-ST\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u57ce\u5e02\u65f6\u7a7a\u9884\u6d4b\u3002\u5b83\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a(1) \u4e00\u79cd\u9886\u57df\u81ea\u9002\u5e94\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5c06\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u5212\u5206\u4e3a\u4e00\u4e2a\u5171\u4eab\u5b50\u7a7a\u95f4\uff08\u7528\u4e8e\u5b66\u4e60\u8de8\u57df\u5171\u6027\uff09\u548c\u72ec\u7acb\u7684\u9886\u57df\u7279\u5b9a\u5b50\u7a7a\u95f4\uff08\u7528\u4e8e\u6355\u83b7\u57df\u5185\u5224\u522b\u7279\u5f81\uff09\uff1b(2) \u4e09\u4e2a\u4e0d\u540c\u7684\u9886\u57df\u9002\u914d\u5668\uff0c\u5b83\u4eec\u5145\u5f53\u9886\u57df\u611f\u77e5\u4ee3\u7406\uff0c\u4ee5\u6865\u63a5\u4e0d\u540c\u7684\u9886\u57df\u5206\u5e03\u5e76\u4fc3\u8fdb\u8de8\u9886\u57df\u5171\u6027\u7684\u5bf9\u9f50\u3002", "result": "Damba-ST\u5728\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Damba-ST\u5728\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u5728\u65b0\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u7f1d\u90e8\u7f72\u3002", "summary_zh": "\u8bad\u7ec3\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u4e0d\u540c\u533a\u57df\u548c\u57ce\u5e02\u7684\u57ce\u5e02\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff0c\u5bf9\u4e8e\u5728\u672a\u89c1\u8fc7\u7684\u6216\u6570\u636e\u7a00\u7f3a\u7684\u533a\u57df\u90e8\u7f72\u57ce\u5e02\u670d\u52a1\u81f3\u5173\u91cd\u8981\u3002\u6700\u8fd1\u7684\u7814\u7a76\u901a\u5e38\u4fa7\u91cd\u4e8e\u878d\u5408\u8de8\u9886\u57df\u65f6\u7a7a\u6570\u636e\u6765\u8bad\u7ec3\u7edf\u4e00\u7684\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u9ad8\u5185\u5b58\u5f00\u9500\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u3002\u53d7\u5230\u5177\u6709\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578bMamba\u7684\u6548\u7387\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u5176\u5728\u9ad8\u6548\u57ce\u5e02\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u6f5c\u529b\u3002\u7136\u800c\uff0c\u76f4\u63a5\u5e94\u7528Mamba\u4f5c\u4e3a\u65f6\u7a7a\u9aa8\u5e72\u4f1a\u5bfc\u81f4\u8d1f\u8fc1\u79fb\u548c\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u65f6\u7a7a\u5f02\u8d28\u6027\u548cMamba\u9690\u85cf\u72b6\u6001\u66f4\u65b0\u7684\u9012\u5f52\u673a\u5236\u9650\u5236\u4e86\u8de8\u57df\u6cdb\u5316\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eMamba\u7684\u9886\u57df\u81ea\u9002\u5e94\u6a21\u578bDamba-ST\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u57ce\u5e02\u65f6\u7a7a\u9884\u6d4b\u3002Damba-ST\u4fdd\u7559\u4e86Mamba\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u4f18\u52bf\uff0c\u540c\u65f6\u663e\u7740\u589e\u5f3a\u4e86\u5176\u5bf9\u5f02\u6784\u9886\u57df\u7684\u9002\u5e94\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a(1) \u4e00\u79cd\u9886\u57df\u81ea\u9002\u5e94\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5b83\u5c06\u6f5c\u5728\u8868\u793a\u7a7a\u95f4\u5212\u5206\u4e3a\u4e00\u4e2a\u5171\u4eab\u5b50\u7a7a\u95f4\uff08\u7528\u4e8e\u5b66\u4e60\u8de8\u57df\u5171\u6027\uff09\u548c\u72ec\u7acb\u7684\u9886\u57df\u7279\u5b9a\u5b50\u7a7a\u95f4\uff08\u7528\u4e8e\u6355\u83b7\u57df\u5185\u5224\u522b\u7279\u5f81\uff09\uff1b(2) \u4e09\u4e2a\u4e0d\u540c\u7684\u9886\u57df\u9002\u914d\u5668\uff0c\u5b83\u4eec\u5145\u5f53\u9886\u57df\u611f\u77e5\u4ee3\u7406\uff0c\u4ee5\u6865\u63a5\u4e0d\u540c\u7684\u9886\u57df\u5206\u5e03\u5e76\u4fc3\u8fdb\u8de8\u9886\u57df\u5171\u6027\u7684\u5bf9\u9f50\u3002 \u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86Damba-ST\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002\u5b83\u5728\u9884\u6d4b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u5373\u53ef\u5728\u65b0\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u65e0\u7f1d\u90e8\u7f72\u3002"}}
{"id": "2506.19341", "pdf": "https://arxiv.org/pdf/2506.19341", "abs": "https://arxiv.org/abs/2506.19341", "authors": ["Zhongping Dong", "Liming Chen", "Mohand Tahar Kechadi"], "title": "Trajectory Prediction in Dynamic Object Tracking: A Critical Study", "categories": ["cs.CV"], "comment": null, "summary": "This study provides a detailed analysis of current advancements in dynamic\nobject tracking (DOT) and trajectory prediction (TP) methodologies, including\ntheir applications and challenges. It covers various approaches, such as\nfeature-based, segmentation-based, estimation-based, and learning-based\nmethods, evaluating their effectiveness, deployment, and limitations in\nreal-world scenarios. The study highlights the significant impact of these\ntechnologies in automotive and autonomous vehicles, surveillance and security,\nhealthcare, and industrial automation, contributing to safety and efficiency.\nDespite the progress, challenges such as improved generalization, computational\nefficiency, reduced data dependency, and ethical considerations still exist.\nThe study suggests future research directions to address these challenges,\nemphasizing the importance of multimodal data integration, semantic information\nfusion, and developing context-aware systems, along with ethical and\nprivacy-preserving frameworks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u8f68\u8ff9\u9884\u6d4b\uff08Trajectory Prediction\uff09\uff0c\u5e76\u5206\u6790\u4e86\u52a8\u6001\u7269\u4f53\u8ddf\u8e2a\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u662f\u76f8\u5173\u9886\u57df\uff0c\u4e14\u6458\u8981\u63d0\u5230\u4e86\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u53ef\u80fd\u6d89\u53ca\u4f7f\u7528\u5927\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u6216\u9884\u6d4b\u3002", "keywords": ["trajectory prediction", "dynamic object tracking", "learning-based methods"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.19212", "pdf": "https://arxiv.org/pdf/2506.19212", "abs": "https://arxiv.org/abs/2506.19212", "authors": ["Vincent de Bakker", "Joey Hejna", "Tyler Ga Wei Lum", "Onur Celik", "Aleksandar Taranovic", "Denis Blessing", "Gerhard Neumann", "Jeannette Bohg", "Dorsa Sadigh"], "title": "Scaffolding Dexterous Manipulation with Vision-Language Models", "categories": ["cs.RO"], "comment": null, "summary": "Dexterous robotic hands are essential for performing complex manipulation\ntasks, yet remain difficult to train due to the challenges of demonstration\ncollection and high-dimensional control. While reinforcement learning (RL) can\nalleviate the data bottleneck by generating experience in simulation, it\ntypically relies on carefully designed, task-specific reward functions, which\nhinder scalability and generalization. Thus, contemporary works in dexterous\nmanipulation have often bootstrapped from reference trajectories. These\ntrajectories specify target hand poses that guide the exploration of RL\npolicies and object poses that enable dense, task-agnostic rewards. However,\nsourcing suitable trajectories - particularly for dexterous hands - remains a\nsignificant challenge. Yet, the precise details in explicit reference\ntrajectories are often unnecessary, as RL ultimately refines the motion. Our\nkey insight is that modern vision-language models (VLMs) already encode the\ncommonsense spatial and semantic knowledge needed to specify tasks and guide\nexploration effectively. Given a task description (e.g., \"open the cabinet\")\nand a visual scene, our method uses an off-the-shelf VLM to first identify\ntask-relevant keypoints (e.g., handles, buttons) and then synthesize 3D\ntrajectories for hand motion and object motion. Subsequently, we train a\nlow-level residual RL policy in simulation to track these coarse trajectories\nor \"scaffolds\" with high fidelity. Across a number of simulated tasks involving\narticulated objects and semantic understanding, we demonstrate that our method\nis able to learn robust dexterous manipulation policies. Moreover, we showcase\nthat our method transfers to real-world robotic hands without any human\ndemonstrations or handcrafted rewards.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper utilizes Vision-Language Models (VLMs) to generate trajectories for robotic hand manipulation. While the focus is on manipulation rather than typical trajectory prediction scenarios like pedestrian or vehicle movement, the use of VLMs to generate trajectories and guide RL aligns with the core concept. The trajectory generation aspect and the use of a large model contribute to the relevance, though the specific application is more niche.", "keywords": ["Vision-Language Models", "VLMs", "trajectories", "manipulation", "reinforcement learning"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8f68\u8ff9\uff0c\u5e76\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u4eba\u5de5\u6f14\u793a\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u3002", "motivation": "\u7075\u5de7\u7684\u673a\u5668\u4eba\u624b\u5bf9\u4e8e\u6267\u884c\u590d\u6742\u7684\u64cd\u7eb5\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u6f14\u793a\u6536\u96c6\u548c\u9ad8\u7ef4\u63a7\u5236\u7684\u6311\u6218\uff0c\u8bad\u7ec3\u4ecd\u7136\u5f88\u56f0\u96be\u3002\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u53ef\u4ee5\u901a\u8fc7\u5728\u6a21\u62df\u4e2d\u751f\u6210\u7ecf\u9a8c\u6765\u7f13\u89e3\u6570\u636e\u74f6\u9888\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e8e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u3001\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8fd9\u963b\u788d\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7684\u5173\u952e\u70b9\uff0c\u5e76\u5408\u6210\u624b\u90e8\u8fd0\u52a8\u548c\u7269\u4f53\u8fd0\u52a8\u76843D\u8f68\u8ff9\uff0c\u7136\u540e\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u4f4e\u7ea7\u522b\u7684\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u9ad8\u7cbe\u5ea6\u5730\u8ddf\u8e2a\u8fd9\u4e9b\u7c97\u7565\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u591a\u4e2a\u6d89\u53ca\u94f0\u63a5\u7269\u4f53\u548c\u8bed\u4e49\u7406\u89e3\u7684\u6a21\u62df\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5230\u9c81\u68d2\u7684\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u8f6c\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u624b\u4e0a\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6f14\u793a\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u5956\u52b1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5b66\u4e60\u5230\u9c81\u68d2\u7684\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\uff0c\u5e76\u4e14\u65e0\u9700\u4eba\u5de5\u6f14\u793a\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u5373\u53ef\u8f6c\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u624b\u4e0a\u3002", "summary_zh": "\u7075\u5de7\u7684\u673a\u5668\u4eba\u624b\u5bf9\u4e8e\u6267\u884c\u590d\u6742\u7684\u64cd\u7eb5\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bad\u7ec3\u5b83\u4eec\u4ecd\u7136\u5f88\u56f0\u96be\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u901a\u8fc7\u5728\u6a21\u62df\u4e2d\u751f\u6210\u7ecf\u9a8c\u6765\u7f13\u89e3\u6570\u636e\u74f6\u9888\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e8e\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u3001\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u8fd9\u963b\u788d\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u3002\u56e0\u6b64\uff0c\u76ee\u524d\u7075\u5de7\u64cd\u4f5c\u7684\u5de5\u4f5c\u901a\u5e38\u4ece\u53c2\u8003\u8f68\u8ff9\u5f15\u5bfc\u3002\u8fd9\u4e9b\u8f68\u8ff9\u6307\u5b9a\u4e86\u76ee\u6807\u624b\u90e8\u59ff\u52bf\uff0c\u4ee5\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u63a2\u7d22\uff0c\u4ee5\u53ca\u4f7f\u5bc6\u96c6\u3001\u4efb\u52a1\u65e0\u5173\u7684\u5956\u52b1\u6210\u4e3a\u53ef\u80fd\u7684\u7269\u4f53\u59ff\u52bf\u3002\u7136\u800c\uff0c\u5bfb\u627e\u5408\u9002\u7684\u8f68\u8ff9\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u7075\u5de7\u7684\u624b\u3002\u7136\u800c\uff0c\u663e\u5f0f\u53c2\u8003\u8f68\u8ff9\u4e2d\u7684\u7cbe\u786e\u7ec6\u8282\u901a\u5e38\u662f\u4e0d\u5fc5\u8981\u7684\uff0c\u56e0\u4e3a\u5f3a\u5316\u5b66\u4e60\u6700\u7ec8\u4f1a\u6539\u8fdb\u8fd0\u52a8\u3002\u6211\u4eec\u7684\u5173\u952e\u89c1\u89e3\u662f\uff0c\u73b0\u4ee3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5df2\u7ecf\u7f16\u7801\u4e86\u6307\u5b9a\u4efb\u52a1\u548c\u6709\u6548\u6307\u5bfc\u63a2\u7d22\u6240\u9700\u7684\u5e38\u8bc6\u7a7a\u95f4\u548c\u8bed\u4e49\u77e5\u8bc6\u3002\u7ed9\u5b9a\u4e00\u4e2a\u4efb\u52a1\u63cf\u8ff0\uff08\u4f8b\u5982\uff0c\u201c\u6253\u5f00\u67dc\u5b50\u201d\uff09\u548c\u4e00\u4e2a\u89c6\u89c9\u573a\u666f\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f7f\u7528\u73b0\u6210\u7684VLM\u9996\u5148\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7684\u5173\u952e\u70b9\uff08\u4f8b\u5982\uff0c\u628a\u624b\u3001\u6309\u94ae\uff09\uff0c\u7136\u540e\u5408\u6210\u624b\u90e8\u8fd0\u52a8\u548c\u7269\u4f53\u8fd0\u52a8\u76843D\u8f68\u8ff9\u3002\u968f\u540e\uff0c\u6211\u4eec\u5728\u6a21\u62df\u4e2d\u8bad\u7ec3\u4e00\u4e2a\u4f4e\u7ea7\u522b\u7684\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u9ad8\u7cbe\u5ea6\u5730\u8ddf\u8e2a\u8fd9\u4e9b\u7c97\u7565\u7684\u8f68\u8ff9\u6216\u201c\u652f\u67b6\u201d\u3002\u5728\u591a\u4e2a\u6d89\u53ca\u94f0\u63a5\u7269\u4f53\u548c\u8bed\u4e49\u7406\u89e3\u7684\u6a21\u62df\u4efb\u52a1\u4e2d\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5230\u9c81\u68d2\u7684\u7075\u5de7\u64cd\u4f5c\u7b56\u7565\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\u53ef\u4ee5\u8f6c\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u624b\u4e0a\uff0c\u800c\u65e0\u9700\u4efb\u4f55\u4eba\u5de5\u6f14\u793a\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u5956\u52b1\u3002"}}
{"id": "2506.19816", "pdf": "https://arxiv.org/pdf/2506.19816", "abs": "https://arxiv.org/abs/2506.19816", "authors": ["Hao Li", "Shuai Yang", "Yilun Chen", "Yang Tian", "Xiaoda Yang", "Xinyi Chen", "Hanqing Wang", "Tai Wang", "Feng Zhao", "Dahua Lin", "Jiangmiao Pang"], "title": "CronusVLA: Transferring Latent Motion Across Time for Multi-Frame Prediction in Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "36 pages, 21 figures", "summary": "Recent vision-language-action (VLA) models built on pretrained\nvision-language models (VLMs) have demonstrated strong generalization across\nmanipulation tasks. However, they remain constrained by a single-frame\nobservation paradigm and cannot fully benefit from the motion information\noffered by aggregated multi-frame historical observations, as the large\nvision-language backbone introduces substantial computational cost and\ninference latency. We propose CronusVLA, a unified framework that extends\nsingle-frame VLA models to the multi-frame paradigm through an efficient\npost-training stage. CronusVLA comprises three key components: (1) single-frame\npretraining on large-scale embodied datasets with autoregressive action tokens\nprediction, which establishes an embodied vision-language foundation; (2)\nmulti-frame encoding, adapting the prediction of vision-language backbones from\ndiscrete action tokens to motion features during post-training, and aggregating\nmotion features from historical frames into a feature chunking; (3) cross-frame\ndecoding, which maps the feature chunking to accurate actions via a shared\ndecoder with cross-attention. By reducing redundant token computation and\ncaching past motion features, CronusVLA achieves efficient inference. As an\napplication of motion features, we further propose an action adaptation\nmechanism based on feature-action retrieval to improve model performance during\nfinetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with\n70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world\nFranka experiments also show the strong performance and robustness.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u52a8\u4f5c\u9884\u6d4b\uff08action prediction\uff09\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e2d\u63d0\u5230\u4e86Vision-Language-Action (VLA) models \u548c Vision-Language Models (VLMs)\uff0c\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u3002\u8bba\u6587\u91cd\u70b9\u662f\u591a\u5e27\u9884\u6d4b\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\uff0c\u4f46\u66f4\u4fa7\u91cd\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u3002", "keywords": ["action prediction", "vision-language-action models", "VLA", "VLMs", "multi-frame prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.19288", "pdf": "https://arxiv.org/pdf/2506.19288", "abs": "https://arxiv.org/abs/2506.19288", "authors": ["Runwei Guan", "Ningwei Ouyang", "Tianhao Xu", "Shaofeng Liang", "Wei Dai", "Yafeng Sun", "Shang Gao", "Songning Lai", "Shanliang Yao", "Xuming Hu", "Ryan Wen Liu", "Yutao Yue", "Hui Xiong"], "title": "Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding", "categories": ["cs.CV", "cs.RO"], "comment": "14 pages, 13 figures", "summary": "Automated waterway environment perception is crucial for enabling unmanned\nsurface vessels (USVs) to understand their surroundings and make informed\ndecisions. Most existing waterway perception models primarily focus on\ninstance-level object perception paradigms (e.g., detection, segmentation).\nHowever, due to the complexity of waterway environments, current perception\ndatasets and models fail to achieve global semantic understanding of waterways,\nlimiting large-scale monitoring and structured log generation. With the\nadvancement of vision-language models (VLMs), we leverage image captioning to\nintroduce WaterCaption, the first captioning dataset specifically designed for\nwaterway environments. WaterCaption focuses on fine-grained, multi-region\nlong-text descriptions, providing a new research direction for visual\ngeo-understanding and spatial scene cognition. Exactly, it includes 20.2k\nimage-text pair data with 1.8 million vocabulary size. Additionally, we propose\nDa Yu, an edge-deployable multi-modal large language model for USVs, where we\npropose a novel vision-to-language projector called Nano Transformer Adaptor\n(NTA). NTA effectively balances computational efficiency with the capacity for\nboth global and fine-grained local modeling of visual features, thereby\nsignificantly enhancing the model's ability to generate long-form textual\noutputs. Da Yu achieves an optimal balance between performance and efficiency,\nsurpassing state-of-the-art models on WaterCaption and several other captioning\nbenchmarks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper is somewhat relevant because it utilizes a vision-language model (a type of large language model) for waterway scene understanding, which can potentially contribute to trajectory prediction for unmanned surface vessels (USVs). While the paper focuses on image captioning, the underlying goal of environmental understanding directly relates to the perception necessary for accurate trajectory prediction. The use of a multi-modal large language model increases its relevance. However, it doesn't directly address trajectory prediction as its primary focus.", "keywords": ["large language models", "vision-language models", "USV", "scene understanding", "multi-modal"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.19375", "pdf": "https://arxiv.org/pdf/2506.19375", "abs": "https://arxiv.org/abs/2506.19375", "authors": ["Kohei Miyaguchi"], "title": "Path Learning with Trajectory Advantage Regression", "categories": ["cs.LG"], "comment": null, "summary": "In this paper, we propose trajectory advantage regression, a method of\noffline path learning and path attribution based on reinforcement learning. The\nproposed method can be used to solve path optimization problems while\nalgorithmically only solving a regression problem.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on path learning and path optimization using reinforcement learning, which is related to trajectory prediction. While it doesn't explicitly mention large language models, the concept of learning optimal paths is relevant to trajectory prediction tasks often addressed in the field. The use of 'trajectory advantage regression' hints at a connection to trajectory analysis.", "keywords": ["path learning", "trajectory", "path optimization", "reinforcement learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.19843", "pdf": "https://arxiv.org/pdf/2506.19843", "abs": "https://arxiv.org/abs/2506.19843", "authors": ["Guo Li", "Zixiang Xu", "Wei Zhang", "Yikuan Hu", "Xinyu Yang", "Nikolay Aristov", "Mingjie Tang", "Elenna R Dugundji"], "title": "Temporal-IRL: Modeling Port Congestion and Berth Scheduling with Inverse Reinforcement Learning", "categories": ["cs.AI"], "comment": "TRB2025", "summary": "Predicting port congestion is crucial for maintaining reliable global supply\nchains. Accurate forecasts enableimprovedshipment planning, reducedelaysand\ncosts, and optimizeinventoryanddistributionstrategies, thereby ensuring timely\ndeliveries and enhancing supply chain resilience. To achieve accurate\npredictions, analyzing vessel behavior and their stay times at specific port\nterminals is essential, focusing particularly on berth scheduling under various\nconditions. Crucially, the model must capture and learn the underlying\npriorities and patterns of berth scheduling. Berth scheduling and planning are\ninfluenced by a range of factors, including incoming vessel size, waiting\ntimes, and the status of vessels within the port terminal. By observing\nhistorical Automatic Identification System (AIS) positions of vessels, we\nreconstruct berth schedules, which are subsequently utilized to determine the\nreward function via Inverse Reinforcement Learning (IRL). For this purpose, we\nmodeled a specific terminal at the Port of New York/New Jersey and developed\nTemporal-IRL. This Temporal-IRL model learns berth scheduling to predict vessel\nsequencing at the terminal and estimate vessel port stay, encompassing both\nwaiting and berthing times, to forecast port congestion. Utilizing data from\nMaher Terminal spanning January 2015 to September 2023, we trained and tested\nthe model, achieving demonstrably excellent results.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on predicting vessel behavior and stay times in ports using historical AIS data and Inverse Reinforcement Learning (IRL). While it doesn't directly involve Large Language Models, it does address trajectory prediction (vessel movement) and path planning (berth scheduling). The connection to trajectory prediction is relatively strong, although it's in a specific domain (maritime) and doesn't explicitly combine it with LLMs.", "keywords": ["trajectory prediction", "vessel behavior", "berth scheduling", "Inverse Reinforcement Learning", "AIS data", "port congestion"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.19850", "pdf": "https://arxiv.org/pdf/2506.19850", "abs": "https://arxiv.org/abs/2506.19850", "authors": ["Yuqi Wang", "Xinghang Li", "Wenxuan Wang", "Junbo Zhang", "Yingyan Li", "Yuntao Chen", "Xinlong Wang", "Zhaoxiang Zhang"], "title": "Unified Vision-Language-Action Model", "categories": ["cs.CV", "cs.RO"], "comment": "technical report", "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on a unified vision-language-action model (UniVLA) that uses vision-language models (VLMs) for robotic manipulation and autonomous driving. While it doesn't explicitly mention trajectory prediction, the 'action' component and its application in autonomous driving suggest a potential connection. The mention of 'large-scale video data' and 'world modeling' hints at the use of large models, even if not explicitly stated as Large Language Models. The inclusion of 'autonomous driving' increases the relevance as trajectory prediction is a key aspect.", "keywords": ["vision-language-action models", "VLMs", "autonomous driving", "world modeling", "large-scale video data", "policy learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.19433", "pdf": "https://arxiv.org/pdf/2506.19433", "abs": "https://arxiv.org/abs/2506.19433", "authors": ["Lixuan He", "Haoyu Dong", "Zhenxing Chen", "Yangcheng Yu", "Jie Feng", "Yong Li"], "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8Vision-and-Language Navigation (VLN)\uff0c\u6d89\u53ca\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u6839\u636e\u8bed\u8a00\u6307\u4ee4\u8fdb\u884c\u5bfc\u822a\u3002\u867d\u7136\u6838\u5fc3\u662f\u5bfc\u822a\u4efb\u52a1\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86prompt-based LLM\u548cstrided-attention MLLM\uff0c\u8868\u660e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e5f\u6d89\u53ca\u8def\u5f84\u89c4\u5212\u548c\u73af\u5883\u7406\u89e3\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002", "keywords": ["Vision-and-Language Navigation", "VLN", "LLM", "MLLM", "navigation", "path planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
