{"id": "2511.20726", "pdf": "https://arxiv.org/pdf/2511.20726", "abs": "https://arxiv.org/abs/2511.20726", "authors": ["Yuhang Wang", "Heye Huang", "Zhenhua Xu", "Kailai Sun", "Baoshen Guo", "Jinhua Zhao"], "title": "Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages, 6 figures", "summary": "Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "This paper is highly relevant as it combines trajectory prediction (scenario generation for autonomous driving) with large language models (LLMs) for guiding the generation of safety-critical scenarios. It leverages LLMs to reason about risk and optimize scenario generation, directly linking both trajectory prediction and large language model research areas.", "keywords": ["trajectory prediction", "large language model", "LLM", "autonomous driving", "scenario generation", "risk", "safety validation"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u4fdd\u771f\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5728\u7f55\u89c1\u957f\u5c3e\u4e8b\u4ef6\u548c\u590d\u6742\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u7684\u5b89\u5168\u9a8c\u8bc1\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5728\u7f55\u89c1\u957f\u5c3e\u4e8b\u4ef6\u548c\u590d\u6742\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u9762\u4e34\u5173\u952e\u6311\u6218\uff0c\u800c\u8fd9\u4e9b\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u7a00\u7f3a\uff0c\u4f46\u5bf9\u4e8e\u7a33\u5065\u7684\u5b89\u5168\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u8be5\u6846\u67b6\u96c6\u6210\u4e86CVAE\u548cLLM\u3002CVAE\u7528\u4e8e\u5b66\u4e60\u6f5c\u5728\u7684\u4ea4\u901a\u7ed3\u6784\uff0c\u751f\u6210\u7269\u7406\u4e0a\u4e00\u81f4\u7684\u57fa\u7840\u573a\u666f\uff1bLLM\u4f5c\u4e3a\u5bf9\u6297\u63a8\u7406\u5f15\u64ce\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684\u573a\u666f\u63cf\u8ff0\u89e3\u6790\u4e3a\u7279\u5b9a\u9886\u57df\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u52a8\u6001\u5730\u6307\u5bfc\u8de8\u4e0d\u540c\u98ce\u9669\u7ea7\u522b\u7684\u573a\u666f\u751f\u6210\u3002", "result": "\u5728CARLA\u548cSMARTS\u4e2d\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u589e\u52a0\u4e86\u9ad8\u98ce\u9669\u548c\u957f\u5c3e\u4e8b\u4ef6\u7684\u8986\u76d6\u8303\u56f4\uff0c\u63d0\u9ad8\u4e86\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u5206\u5e03\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u4f7f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u66b4\u9732\u4e8e\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u6311\u6218\u6027\u7684\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684\u5b89\u5168\u9a8c\u8bc1\u9014\u5f84\uff0c\u80fd\u591f\u5728\u7f55\u89c1\u4f46\u91cd\u8981\u7684\u4e8b\u4ef6\u4e0b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8fdb\u884c\u6709\u539f\u5219\u7684\u538b\u529b\u6d4b\u8bd5\u3002", "summary_zh": "\u81ea\u52a8\u9a7e\u9a76\u5728\u7f55\u89c1\u7684\u957f\u5c3e\u4e8b\u4ef6\u548c\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u4e2d\u9762\u4e34\u7740\u4e25\u5cfb\u7684\u6311\u6218\u3002\u8fd9\u4e9b\u60c5\u51b5\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u4e2d\u975e\u5e38\u5c11\u89c1\uff0c\u4f46\u5bf9\u4e8e\u786e\u4fdd\u5b89\u5168\u9a8c\u8bc1\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u4fdd\u771f\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CVAE\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002CVAE \u7528\u4e8e\u7f16\u7801\u6765\u81ea\u5927\u89c4\u6a21\u81ea\u7136\u6570\u636e\u96c6\u7684\u5386\u53f2\u8f68\u8ff9\u548c\u5730\u56fe\u4fe1\u606f\uff0c\u4ece\u800c\u5b66\u4e60\u6f5c\u5728\u7684\u4ea4\u901a\u7ed3\u6784\uff0c\u5e76\u751f\u6210\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u57fa\u7840\u573a\u666f\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cLLM \u5145\u5f53\u5bf9\u6297\u6027\u63a8\u7406\u5f15\u64ce\uff0c\u5c06\u975e\u7ed3\u6784\u5316\u7684\u573a\u666f\u63cf\u8ff0\u89e3\u6790\u4e3a\u7279\u5b9a\u9886\u57df\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u52a8\u6001\u5730\u6307\u5bfc\u8de8\u4e0d\u540c\u98ce\u9669\u7ea7\u522b\u7684\u573a\u666f\u751f\u6210\u3002\u8fd9\u79cd\u77e5\u8bc6\u9a71\u52a8\u7684\u4f18\u5316\u5e73\u8861\u4e86\u771f\u5b9e\u6027\u4e0e\u53ef\u63a7\u6027\uff0c\u786e\u4fdd\u751f\u6210\u7684\u573a\u666f\u65e2\u5408\u7406\u53c8\u5bf9\u98ce\u9669\u654f\u611f\u3002\u5728 CARLA \u548c SMARTS \u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u6846\u67b6\u663e\u8457\u589e\u52a0\u4e86\u9ad8\u98ce\u9669\u548c\u957f\u5c3e\u4e8b\u4ef6\u7684\u8986\u76d6\u8303\u56f4\uff0c\u63d0\u9ad8\u4e86\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4ea4\u901a\u5206\u5e03\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u4f7f\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9762\u4e34\u7684\u4ea4\u4e92\u6311\u6218\u8fdc\u5927\u4e8e\u73b0\u6709\u89c4\u5219\u6216\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u6240\u4ea7\u751f\u7684\u6311\u6218\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5b89\u5168\u9a8c\u8bc1\u5efa\u7acb\u4e86\u4e00\u6761\u65b0\u9014\u5f84\uff0c\u4ece\u800c\u80fd\u591f\u5728\u7f55\u89c1\u4f46\u91cd\u8981\u7684\u4e8b\u4ef6\u4e0b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8fdb\u884c\u6709\u539f\u5219\u7684\u538b\u529b\u6d4b\u8bd5\u3002"}}
{"id": "2511.20729", "pdf": "https://arxiv.org/pdf/2511.20729", "abs": "https://arxiv.org/abs/2511.20729", "authors": ["Sean Bin Yang", "Ying Sun", "Yunyao Cheng", "Yan Lin", "Kristian Torp", "Jilin Hu"], "title": "Spatio-Temporal Trajectory Foundation Model - Recent Advances and Future Directions", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted by CIKM 2025 STIntelligence Workshop", "summary": "Foundation models (FMs) have emerged as a powerful paradigm, enabling a diverse range of data analytics and knowledge discovery tasks across scientific fields. Inspired by the success of FMs, particularly large language models, researchers have recently begun to explore spatio-temporal foundation models (STFMs) to improve adaptability and generalization across a wide spectrum of spatio-temporal (ST) tasks. Despite rapid progress, a systematic investigation of trajectory foundation models (TFMs), a crucial subclass of STFMs, is largely lacking. This tutorial addresses this gap by offering a comprehensive overview of recent advances in TFMs, including a taxonomy of existing methodologies and a critical analysis of their strengths and limitations. In addition, the tutorial highlights open challenges and outlines promising research directions to advance spatio-temporal general intelligence through the development of robust, responsible, and transferable TFMs.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper explicitly focuses on trajectory foundation models (TFMs), a subclass of spatio-temporal foundation models (STFMs), drawing inspiration from the success of large language models. It discusses recent advances and future directions in this area, making it highly relevant to both trajectory prediction and large models.", "keywords": ["trajectory prediction", "foundation models", "large language models", "spatio-temporal foundation models", "trajectory foundation models", "TFMs", "STFMs"]}, "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\uff08TFMs\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4e3a\u65f6\u7a7a\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u63d0\u4f9b\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\uff08TFMs\uff09\u7684\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u800cTFMs\u662f\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff08STFMs\uff09\u7684\u4e00\u4e2a\u91cd\u8981\u5b50\u7c7b\u3002", "method": "\u672c\u6587\u5bf9\u73b0\u6709TFMs\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u5bf9\u5176\u4f18\u7f3a\u70b9\u8fdb\u884c\u4e86 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u5206\u6790\u3002", "result": "\u672c\u6587\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5f00\u653e\u6027\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u7a33\u5065\u3001\u8d1f\u8d23\u548c\u53ef\u8f6c\u79fb\u7684TFMs\uff0c\u53ef\u4ee5\u63a8\u8fdb\u65f6\u7a7a\u901a\u7528\u667a\u80fd\u7684\u53d1\u5c55\u3002", "summary_zh": "\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5df2\u6210\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u8303\u4f8b\uff0c\u80fd\u591f\u5728\u5404\u4e2a\u79d1\u5b66\u9886\u57df\u5b9e\u73b0\u5404\u79cd\u6570\u636e\u5206\u6790\u548c\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u3002\u53d7\u5230FMs\uff0c\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6210\u529f\u542f\u53d1\uff0c\u7814\u7a76\u4eba\u5458\u6700\u8fd1\u5f00\u59cb\u63a2\u7d22\u65f6\u7a7a\u57fa\u7840\u6a21\u578b\uff08STFMs\uff09\uff0c\u4ee5\u63d0\u9ad8\u5404\u79cd\u65f6\u7a7a\uff08ST\uff09\u4efb\u52a1\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5c3d\u7ba1\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u5bf9\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\uff08TFMs\uff09\uff08STFMs \u7684\u4e00\u4e2a\u5173\u952e\u5b50\u7c7b\uff09\u7684\u7cfb\u7edf\u7814\u7a76\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4ecd\u7136\u7f3a\u4e4f\u3002\u672c\u6559\u7a0b\u901a\u8fc7\u5168\u9762\u6982\u8ff0 TFM \u7684\u6700\u65b0\u8fdb\u5c55\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5305\u62ec\u73b0\u6709\u65b9\u6cd5\u7684\u5206\u7c7b\u4ee5\u53ca\u5bf9\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684 \u043a\u0440\u0438\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u5206\u6790\u3002\u6b64\u5916\uff0c\u672c\u6559\u7a0b\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5f00\u653e\u6027\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u901a\u8fc7\u5f00\u53d1\u7a33\u5065\u3001\u8d1f\u8d23\u548c\u53ef\u8f6c\u79fb\u7684 TFM \u6765\u63a8\u8fdb\u65f6\u7a7a\u901a\u7528\u667a\u80fd\u3002"}}
{"id": "2511.20720", "pdf": "https://arxiv.org/pdf/2511.20720", "abs": "https://arxiv.org/abs/2511.20720", "authors": ["Haibo HU", "Lianming Huang", "Nan Guan", "Chun Jason Xue"], "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff08VLA\uff09\u8fdb\u884c\u81ea\u52a8\u9a7e\u9a76\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u5e76\u5173\u6ce8\u5982\u4f55\u63d0\u9ad8VLA\u6a21\u578b\u7684\u6548\u7387\u3002VLA\u6a21\u578b\u901a\u5e38\u5305\u542b\u5927\u578bTransformer\u67b6\u6784\uff0c\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5927\u6a21\u578b\u3002\u540c\u65f6\uff0c\u8bba\u6587\u7684\u6838\u5fc3\u662f\u8f68\u8ff9\u751f\u6210\u548c\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5747\u76f8\u5173\u3002", "keywords": ["trajectory generation", "autonomous driving", "Vision-Language Action models", "VLA", "transformer", "planning"]}, "AI": {"tldr": "DeeAD\u901a\u8fc7\u8bc4\u4f30\u4e2d\u95f4\u8f68\u8ff9\u7684\u7269\u7406\u53ef\u884c\u6027\u6765\u52a0\u901fVLA\u89c4\u5212\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u96c6\u6210\u5230\u73b0\u6709VLA\u6a21\u578b\u4e2d\u3002", "motivation": "VLA\u6a21\u578b\u5728\u81ea\u4e3b\u9a7e\u9a76\u4e2d\u7edf\u4e00\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u4f46\u7531\u4e8e\u6df1\u5ea6transformer\u5806\u6808\u800c\u5b58\u5728\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u514d\u8bad\u7ec3\u7684\u3001\u52a8\u4f5c\u5f15\u5bfc\u7684\u65e9\u9000\u6846\u67b6DeeAD\uff0c\u901a\u8fc7\u8bc4\u4f30\u4e2d\u95f4\u8f68\u8ff9\u7684\u7269\u7406\u53ef\u884c\u6027\u6765\u52a0\u901fVLA\u89c4\u5212\u3002\u5f15\u5165\u591a\u8df3\u63a7\u5236\u5668\uff0c\u81ea\u9002\u5e94\u5730\u8df3\u8fc7\u5197\u4f59\u5c42\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeeAD\u5c55\u793a\u4e86\u9ad8\u8fbe28%\u7684transformer\u5c42\u7a00\u758f\u6027\u548c29%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c4\u5212\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002", "conclusion": "DeeAD\u901a\u8fc7\u5728VLA\u6a21\u578b\u4e2d\u96c6\u6210\u52a8\u4f5c\u5f15\u5bfc\u7684\u65e9\u9000\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c4\u5212\u8d28\u91cf\u548c\u5b89\u5168\u6027\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "summary_zh": "\u89c6\u89c9-\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7edf\u4e00\u4e86\u611f\u77e5\u3001\u63a8\u7406\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\uff0c\u4f46\u7531\u4e8e\u6df1\u5ea6transformer\u5806\u6808\u800c\u5b58\u5728\u663e\u8457\u7684\u63a8\u7406\u5ef6\u8fdf\u3002\u6211\u4eec\u63d0\u51fa\u4e86DeeAD\uff0c\u4e00\u4e2a\u514d\u8bad\u7ec3\u7684\u3001\u52a8\u4f5c\u5f15\u5bfc\u7684\u65e9\u9000\u6846\u67b6\uff0c\u901a\u8fc7\u8bc4\u4f30\u4e2d\u95f4\u8f68\u8ff9\u7684\u7269\u7406\u53ef\u884c\u6027\u6765\u52a0\u901fVLA\u89c4\u5212\u3002DeeAD\u4e0d\u662f\u4f9d\u8d56\u4e8e\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u800c\u662f\u5728\u9884\u6d4b\u7684\u8f68\u8ff9\u4e0e\u8f7b\u91cf\u7ea7\u89c4\u5212\u5148\u9a8c\uff08\u4f8b\u5982\uff0c\u5bfc\u822a\u6216\u4f4e\u7cbe\u5ea6\u89c4\u5212\uff09\u5728\u53ef\u5bb9\u5fcd\u7684\u504f\u5dee\uff08<2m\uff09\u5185\u5bf9\u9f50\u65f6\uff0c\u7ec8\u6b62\u63a8\u7406\u3002\u4e3a\u4e86\u63d0\u9ad8\u6548\u7387\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u8df3\u63a7\u5236\u5668\uff0c\u8be5\u63a7\u5236\u5668\u57fa\u4e8e\u5206\u6570\u7684\u53d8\u5316\u7387\u81ea\u9002\u5e94\u5730\u8df3\u8fc7\u5197\u4f59\u5c42\u3002DeeAD\u96c6\u6210\u5230\u73b0\u6709\u7684VLA\u6a21\u578b\u4e2d\uff0c\u4f8b\u5982ORION\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u9ad8\u8fbe28%\u7684transformer\u5c42\u7a00\u758f\u6027\u548c29%\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u89c4\u5212\u8d28\u91cf\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2511.21135", "pdf": "https://arxiv.org/pdf/2511.21135", "abs": "https://arxiv.org/abs/2511.21135", "authors": ["Ziyi Chen", "Yingnan Guo", "Zedong Chu", "Minghua Luo", "Yanfen Shen", "Mingchao Sun", "Junjun Hu", "Shichao Xie", "Kuan Yang", "Pei Shi", "Zhining Gu", "Lu Liu", "Honglin Han", "Xiaolong Wu", "Mu Xu", "Yu Zhang"], "title": "SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Embodied navigation that adheres to social norms remains an open research challenge. Our \\textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical \"brain-action\" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on socially-aware embodied navigation, which involves trajectory generation and planning. While it doesn't explicitly mention \"Large Language Models\", it uses a foundational model architecture and chain-of-thought explanations, suggesting a connection to large-scale AI models and reasoning capabilities. The generation of socially compliant trajectories is a key aspect of trajectory prediction.", "keywords": ["embodied navigation", "trajectory generation", "socially-aware navigation", "foundation model", "chain-of-thought", "reinforcement learning", "social compliance"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.20713", "pdf": "https://arxiv.org/pdf/2511.20713", "abs": "https://arxiv.org/abs/2511.20713", "authors": ["Minhui Zhang", "Prahar Ijner", "Yoav Wald", "Elliot Creager"], "title": "Active Slice Discovery in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted for presentation at NeurIPS 2025 - Reliable ML Workshop", "summary": "Large Language Models (LLMs) often exhibit systematic errors on specific subsets of data, known as error slices. For instance, a slice can correspond to a certain demographic, where a model does poorly in identifying toxic comments regarding that demographic. Identifying error slices is crucial to understanding and improving models, but it is also challenging. An appealing approach to reduce the amount of manual annotation required is to actively group errors that are likely to belong to the same slice, while using limited access to an annotator to verify whether the chosen samples share the same pattern of model mistake. In this paper, we formalize this approach as Active Slice Discovery and explore it empirically on a problem of discovering human-defined slices in toxicity classification. We examine the efficacy of active slice discovery under different choices of feature representations and active learning algorithms. On several slices, we find that uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information, while significantly outperforming baselines.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u9519\u8bef\u5207\u7247\u53d1\u73b0\uff0c\u5c5e\u4e8e\u5927\u6a21\u578b\u9886\u57df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u7814\u7a76\u5982\u4f55\u6539\u8fdbLLM\u7684\u6027\u80fd\u5bf9\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f7f\u7528\u7684LLM\u4e5f\u6709\u6f5c\u5728\u7684\u95f4\u63a5\u5f71\u54cd\u3002", "keywords": ["Large Language Models", "LLMs", "error slices", "active learning", "toxicity classification"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21690", "pdf": "https://arxiv.org/pdf/2511.21690", "abs": "https://arxiv.org/abs/2511.21690", "authors": ["Seungjae Lee", "Yoonkyo Jung", "Inkook Chun", "Yao-Chih Lee", "Zikui Cai", "Hongjia Huang", "Aayush Talreja", "Tan Dat Dao", "Yongyuan Liang", "Jia-Bin Huang", "Furong Huang"], "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u5c24\u5176\u662f3D\u8f68\u8ff9\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u9884\u6d4b\u3002\u5b83\u4f7f\u7528\u4e86\u4e00\u4e2a\u4e16\u754c\u6a21\u578b\u6765\u9884\u6d4b\u8f68\u8ff9\uff0c\u5e76\u5229\u7528\u4ece\u5927\u91cf\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u867d\u7136\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cobservation-trace-language triplets\u201d\uff0c\u6697\u793a\u4e86\u53ef\u80fd\u4e0e\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5173\u8054\uff0c\u4f46\u91cd\u70b9\u8fd8\u662f\u5728\u8f68\u8ff9\u9884\u6d4b\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4e0a\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4f46\u5e76\u975e\u5b8c\u5168\u96c6\u4e2d\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["trajectory prediction", "world model", "3D trace space", "motion prediction", "cross-embodiment", "trace-space"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.20965", "pdf": "https://arxiv.org/pdf/2511.20965", "abs": "https://arxiv.org/abs/2511.20965", "authors": ["Md Adnan Arefeen", "Biplob Debnath", "Srimat Chakradhar"], "title": "TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs", "categories": ["cs.CV", "cs.CL"], "comment": "2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)", "summary": "Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\\times$ while maintaining information accuracy.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6765\u5206\u6790\u4ea4\u901a\u89c6\u9891\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4ea4\u901a\u89c6\u9891\u5206\u6790\u901a\u5e38\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e0a\u6e38\u4efb\u52a1\u6216\u76f8\u5173\u5e94\u7528\u573a\u666f\u3002\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86LLMs\uff0c\u5e76\u63d0\u5230\u4e86\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u5e94\u7528\u9886\u57df\u76f8\u5173\u3002", "keywords": ["Large Language Models", "LLMs", "Vision-Language Model", "VLM", "traffic video analysis", "intelligent transportation systems"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.20848", "pdf": "https://arxiv.org/pdf/2511.20848", "abs": "https://arxiv.org/abs/2511.20848", "authors": ["Tasha Kim", "Yingke Wang", "Hanvit Cho", "Alex Hodges"], "title": "NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "eess.SY"], "comment": "Conference on Robot Learning (CoRL 2024), CoRoboLearn", "summary": "Neural Signal Operated Intelligent Robots (NOIR) system is a versatile brain-robot interface that allows humans to control robots for daily tasks using their brain signals. This interface utilizes electroencephalography (EEG) to translate human intentions regarding specific objects and desired actions directly into commands that robots can execute. We present NOIR 2.0, an enhanced version of NOIR. NOIR 2.0 includes faster and more accurate brain decoding algorithms, which reduce task completion time by 46%. NOIR 2.0 uses few-shot robot learning algorithms to adapt to individual users and predict their intentions. The new learning algorithms leverage foundation models for more sample-efficient learning and adaptation (15 demos vs. a single demo), significantly reducing overall human time by 65%.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper describes a brain-robot interface (NOIR 2.0) that uses EEG signals to control robots. While it doesn't directly focus on trajectory prediction, it mentions using foundation models (a type of large model) for few-shot robot learning to predict user intentions and adapt to individual users. The prediction of user intention can be loosely related to action prediction, which is a subfield of trajectory prediction.", "keywords": ["foundation models", "brain-robot interface", "robot learning", "action prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21053", "pdf": "https://arxiv.org/pdf/2511.21053", "abs": "https://arxiv.org/abs/2511.21053", "authors": ["Chenglizhao Chen", "Shaofeng Liang", "Runwei Guan", "Xiaolou Sun", "Haocheng Zhao", "Haiyun Jiang", "Tao Huang", "Henghui Ding", "Qing-Long Han"], "title": "AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios", "categories": ["cs.RO", "cs.CV"], "comment": "AAAI 2026", "summary": "Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on referring multi-object tracking (RMOT) in UAV scenarios, which involves tracking and path planning based on natural language instructions. While it doesn't directly address trajectory prediction as the primary focus, the tracking aspect has connections to predicting future object locations. The use of natural language instructions also hints at a potential connection to large language models, although this is not explicitly stated. The mention of 'Embodied Intelligence' and demand for systems capable of 'natural language interaction' suggests a potential future integration with LLMs.", "keywords": ["multi-object tracking", "path planning", "natural language interaction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21280", "pdf": "https://arxiv.org/pdf/2511.21280", "abs": "https://arxiv.org/abs/2511.21280", "authors": ["Jamal Raiyn"], "title": "Improvement of Collision Avoidance in Cut-In Maneuvers Using Time-to-Collision Metrics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper proposes a new strategy for collision avoidance system leveraging Time-to-Collision (TTC) metrics for handling cut-in scenarios, which are particularly challenging for autonomous vehicles (AVs). By integrating a deep learning with TTC calculations, the system predicts potential collisions and determines appropriate evasive actions compared to traditional TTC -based approaches.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on collision avoidance for autonomous vehicles using Time-to-Collision (TTC) metrics and deep learning. While it involves trajectory prediction (implicitly through collision prediction and evasive actions) and uses deep learning, it doesn't explicitly mention or leverage large language models. The connection to trajectory prediction is through the prediction of potential collisions which directly relates to future trajectories.", "keywords": ["collision avoidance", "autonomous vehicles", "deep learning", "Time-to-Collision", "trajectory prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21542", "pdf": "https://arxiv.org/pdf/2511.21542", "abs": "https://arxiv.org/abs/2511.21542", "authors": ["Zhihao Zhan", "Jiaying Zhou", "Likui Zhang", "Qinhan Lv", "Hao Liu", "Jusheng Zhang", "Weizheng Li", "Ziliang Chen", "Tianshui Chen", "Keze Wang", "Liang Lin", "Guangrun Wang"], "title": "$\\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Vision-Language-Action (VLA) models for robotic manipulation. While it doesn't directly mention trajectory prediction, the action generation aspect can be considered related. The use of VLMs (Vision-Language Models) connects it to the broader theme of large models, although it's not a pure language model. The action generation could involve predicting a sequence of actions, implicitly predicting a trajectory.", "keywords": ["Vision-Language-Action models", "VLA models", "robotic manipulation", "action generation", "VLMs", "diffusion models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21584", "pdf": "https://arxiv.org/pdf/2511.21584", "abs": "https://arxiv.org/abs/2511.21584", "authors": ["Haohong Lin", "Yunzhi Zhang", "Wenhao Ding", "Jiajun Wu", "Ding Zhao"], "title": "Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "Published at NeurIPS 2025: https://openreview.net/forum?id=4OLbpaTKJe", "summary": "End-to-end (E2E) autonomous driving models have demonstrated strong performance in open-loop evaluations but often suffer from cascading errors and poor generalization in closed-loop settings. To address this gap, we propose Model-based Policy Adaptation (MPA), a general framework that enhances the robustness and safety of pretrained E2E driving agents during deployment. MPA first generates diverse counterfactual trajectories using a geometry-consistent simulation engine, exposing the agent to scenarios beyond the original dataset. Based on this generated data, MPA trains a diffusion-based policy adapter to refine the base policy's predictions and a multi-step Q value model to evaluate long-term outcomes. At inference time, the adapter proposes multiple trajectory candidates, and the Q value model selects the one with the highest expected utility. Experiments on the nuScenes benchmark using a photorealistic closed-loop simulator demonstrate that MPA significantly improves performance across in-domain, out-of-domain, and safety-critical scenarios. We further investigate how the scale of counterfactual data and inference-time guidance strategies affect overall effectiveness.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving the robustness of end-to-end autonomous driving models using model-based policy adaptation. While it doesn't directly use large language models, it does deal with trajectory prediction and policy refinement for autonomous driving, making it moderately relevant. The use of a diffusion model for policy adaptation hints at a connection to generative models, which are related to large language models in terms of architecture and training techniques, but the connection is not explicit.", "keywords": ["trajectory prediction", "autonomous driving", "policy adaptation", "diffusion model", "closed-loop"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21460", "pdf": "https://arxiv.org/pdf/2511.21460", "abs": "https://arxiv.org/abs/2511.21460", "authors": ["Junjian Wang", "Lidan Zhao", "Xi Sheryl Zhang"], "title": "MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning", "categories": ["cs.AI"], "comment": null, "summary": "Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on risk-aware embodied planning using LLMs for safety in embodied agents. While it doesn't directly address trajectory prediction, the embodied planning aspect and the use of LLMs make it moderately relevant. The task planning involves a sequence of actions, which implicitly includes trajectory considerations, and the core mechanism relies on LLMs.", "keywords": ["Large Language Models", "LLMs", "embodied planning", "risk assessment", "multi-agent debate", "task planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21471", "pdf": "https://arxiv.org/pdf/2511.21471", "abs": "https://arxiv.org/abs/2511.21471", "authors": ["Peiran Xu", "Sudong Wang", "Yao Zhu", "Jianing Li", "Yunjian Zhang"], "title": "SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition", "categories": ["cs.AI"], "comment": null, "summary": "Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on benchmarking multimodal large language models (MLLMs) for spatial cognition, which includes high-level planning. While not directly addressing trajectory prediction, the planning aspect and the use of MLLMs make it somewhat relevant. The emphasis is more on spatial reasoning and understanding than specifically predicting trajectories.", "keywords": ["multimodal large language models", "MLLMs", "spatial cognition", "planning", "foundation models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.20993", "pdf": "https://arxiv.org/pdf/2511.20993", "abs": "https://arxiv.org/abs/2511.20993", "authors": ["Shanwei Fan"], "title": "Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game \"Crafter\" demonstrate the effectiveness of our proposed method.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Large Language Models (LLMs) for high-level planning in reinforcement learning. While it doesn't directly address trajectory prediction, the planning aspect could be relevant as it involves generating subgoals and actions within an environment, which can be seen as a form of high-level trajectory generation. The use of LLMs is a significant factor in its relevance.", "keywords": ["Large Language Models", "LLMs", "reinforcement learning", "planning", "subgoals"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21054", "pdf": "https://arxiv.org/pdf/2511.21054", "abs": "https://arxiv.org/abs/2511.21054", "authors": ["Jiaming Guo", "Rui Zhang", "Zerun Li", "Yunkai Gao", "Shaohui Peng", "Siming Lan", "Xing Hu", "Zidong Du", "Xishan Zhang", "Ling Li"], "title": "Efficient Diffusion Planning with Temporal Diffusion", "categories": ["cs.LG"], "comment": "Accepted by the AAAI26 Conference Main Track", "summary": "Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u6269\u6563\u6a21\u578b\u5728\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u6269\u6563\u89c4\u5212\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u89c4\u5212\u901a\u5e38\u4e0e\u8f68\u8ff9\u751f\u6210\u548c\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u7684\u201c\u8ba1\u5212\u201d\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u8f68\u8ff9\u7684\u8868\u793a\u3002\u8bba\u6587\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6269\u6563\u6a21\u578b\u672c\u8eab\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5927\u578b\u6a21\u578b\u7684\u4e00\u79cd\u3002", "keywords": ["diffusion planning", "planning", "temporal diffusion", "offline data", "policy learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.21105", "pdf": "https://arxiv.org/pdf/2511.21105", "abs": "https://arxiv.org/abs/2511.21105", "authors": ["Pushkal Mishra", "Kshitiz Bansal", "Dinesh Bharadia"], "title": "Scaling Foundation Models for Radar Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on scaling foundation models for radar scene understanding. While it doesn't directly deal with trajectory prediction, the use of radar data and foundation models in the context of driving scenarios suggests a potential connection. Radar data is commonly used in trajectory prediction, and the foundation model aspect aligns with large language models. The structured caption framework and scene understanding could be used to improve trajectory prediction models.", "keywords": ["foundation models", "radar", "scene understanding", "driving scenarios"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
