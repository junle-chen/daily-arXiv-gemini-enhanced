{"id": "2510.09646", "pdf": "https://arxiv.org/pdf/2510.09646", "abs": "https://arxiv.org/abs/2510.09646", "authors": ["Ritesh Chandra", "Sonali Agarwal", "Navjot Singh"], "title": "Real-Time Health Analytics Using Ontology-Driven Complex Event Processing and LLM Reasoning: A Tuberculosis Case Study", "categories": ["cs.DB", "cs.AI"], "comment": "14 table. 20 figure", "summary": "Timely detection of critical health conditions remains a major challenge in\npublic health analytics, especially in Big Data environments characterized by\nhigh volume, rapid velocity, and diverse variety of clinical data. This study\npresents an ontology-enabled real-time analytics framework that integrates\nComplex Event Processing (CEP) and Large Language Models (LLMs) to enable\nintelligent health event detection and semantic reasoning over heterogeneous,\nhigh-velocity health data streams. The architecture leverages the Basic Formal\nOntology (BFO) and Semantic Web Rule Language (SWRL) to model diagnostic rules\nand domain knowledge. Patient data is ingested and processed using Apache Kafka\nand Spark Streaming, where CEP engines detect clinically significant event\npatterns. LLMs support adaptive reasoning, event interpretation, and ontology\nrefinement. Clinical information is semantically structured as Resource\nDescription Framework (RDF) triples in Graph DB, enabling SPARQL-based querying\nand knowledge-driven decision support. The framework is evaluated using a\ndataset of 1,000 Tuberculosis (TB) patients as a use case, demonstrating\nlow-latency event detection, scalable reasoning, and high model performance (in\nterms of precision, recall, and F1-score). These results validate the system's\npotential for generalizable, real-time health analytics in complex Big Data\nscenarios."}
{"id": "2510.10115", "pdf": "https://arxiv.org/pdf/2510.10115", "abs": "https://arxiv.org/abs/2510.10115", "authors": ["Kai Cao", "Yucong Duan", "Wensheng Gan"], "title": "Targeted Sequential Pattern Mining with High Average Utility", "categories": ["cs.DB"], "comment": "preprint, 9 figures, 3 tables", "summary": "Incorporating utility into targeted pattern mining can address the practical\nlimitations of traditional frequency-based approaches. However, utility-based\nmethods often suffer from generating a large number of long and complicated\nsequences. To improve pattern relevance and interpretability, average utility\nprovides a more balanced metric by considering both utility and sequence\nlength. Moreover, incorporating user-defined query targets into the mining\nprocess enhances usability and interactivity by retaining only patterns\ncontaining user-specified goals. To address challenges related to mining\nefficiency in large-scale, long-sequence datasets, this study introduces\naverage utility into targeted sequential pattern mining. A novel algorithm,\nTAUSQ-PG, is designed to find targeted high average utility sequential\npatterns. It incorporates efficient filtering and pruning strategies, tighter\nupper bound models, as well as novel specialized evaluation metrics and query\nflags tailored to this task. Extensive comparative experiments on different\ndatasets demonstrate that TAUSQ-PG effectively controls the candidate set size,\nthereby reducing redundant sequence generation and significantly improving\nruntime and memory efficiency."}
{"id": "2510.10123", "pdf": "https://arxiv.org/pdf/2510.10123", "abs": "https://arxiv.org/abs/2510.10123", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet", "Yong Zhang"], "title": "The Hybrid Multimodal Graph Index (HMGI): A Comprehensive Framework for Integrated Relational and Vector Search", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "The proliferation of complex, multimodal datasets has exposed a critical gap\nbetween the capabilities of specialized vector databases and traditional graph\ndatabases. While vector databases excel at semantic similarity search, they\nlack the capacity for deep relational querying. Conversely, graph databases\nmaster complex traversals but are not natively optimized for high-dimensional\nvector search. This paper introduces the Hybrid Multimodal Graph Index (HMGI),\na novel framework designed to bridge this gap by creating a unified system for\nefficient, hybrid queries on multimodal data. HMGI leverages the native graph\ndatabase architecture and integrated vector search capabilities, exemplified by\nplatforms like Neo4j, to combine Approximate Nearest Neighbor Search (ANNS)\nwith expressive graph traversal queries. Key innovations of the HMGI framework\ninclude modality-aware partitioning of embeddings to optimize index structure\nand query performance, and a system for adaptive, low-overhead index updates to\nsupport dynamic data ingestion, drawing inspiration from the architectural\nprinciples of systems like TigerVector. By integrating semantic similarity\nsearch directly with relational context, HMGI aims to outperform pure vector\ndatabases like Milvus in complex, relationship-heavy query scenarios and\nachieve sub-linear query times for hybrid tasks."}
{"id": "2510.10243", "pdf": "https://arxiv.org/pdf/2510.10243", "abs": "https://arxiv.org/abs/2510.10243", "authors": ["Jian Zhu", "Zhidong Lin", "Wensheng Gan", "Ruichu Cai", "Zhifeng Hao", "Philip S. Yu"], "title": "Efficient Mining of Low-Utility Sequential Patterns", "categories": ["cs.DB"], "comment": "Preprint, 4 tables, 9 figures", "summary": "Discovering valuable insights from rich data is a crucial task for\nexploratory data analysis. Sequential pattern mining (SPM) has found widespread\napplications across various domains. In recent years, low-utility sequential\npattern mining (LUSPM) has shown strong potential in applications such as\nintrusion detection and genomic sequence analysis. However, existing research\nin utility-based SPM focuses on high-utility sequential patterns, and the\ndefinitions and strategies used in high-utility SPM cannot be directly applied\nto LUSPM. Moreover, no algorithms have yet been developed specifically for\nmining low-utility sequential patterns. To address these problems, we formalize\nthe LUSPM problem, redefine sequence utility, and introduce a compact data\nstructure called the sequence-utility chain to efficiently record utility\ninformation. Furthermore, we propose three novel algorithm--LUSPM_b, LUSPM_s,\nand LUSPM_e--to discover the complete set of low-utility sequential patterns.\nLUSPM_b serves as an exhaustive baseline, while LUSPM_s and LUSPM_e build upon\nit, generating subsequences through shrinkage and extension operations,\nrespectively. In addition, we introduce the maximal non-mutually contained\nsequence set and incorporate multiple pruning strategies, which significantly\nreduce redundant operations in both LUSPM_s and LUSPM_e. Finally, extensive\nexperimental results demonstrate that both LUSPM_s and LUSPM_e substantially\noutperform LUSPM_b and exhibit excellent scalability. Notably, LUSPM_e achieves\nsuperior efficiency, requiring less runtime and memory consumption than\nLUSPM_s. Our code is available at https://github.com/Zhidong-Lin/LUSPM."}
{"id": "2510.10348", "pdf": "https://arxiv.org/pdf/2510.10348", "abs": "https://arxiv.org/abs/2510.10348", "authors": ["Ling Zhang", "Shaleen Deep", "Jignesh M. Patel", "Karthikeyan Sankaralingam"], "title": "Regular Expression Indexing for Log Analysis. Extended Version", "categories": ["cs.DB"], "comment": null, "summary": "In this paper, we present the design and architecture of REI, a novel system\nfor indexing log data for regular expression queries. Our main contribution is\nan $n$-gram-based indexing strategy and an efficient storage mechanism that\nresults in a speedup of up to 14x compared to state-of-the-art regex processing\nengines that do not use indexing, using only 2.1% of extra space. We perform a\ndetailed study that analyzes the space usage of the index and the improvement\nin workload execution time, uncovering interesting insights. Specifically, we\nshow that even an optimized implementation of strategies such as inverted\nindexing, which are widely used in text processing libraries, may lead to\nsuboptimal performance for regex indexing on log analysis tasks. Overall, the\nREI approach presented in this paper provides a significant boost when\nevaluating regular expression queries on log data. REI is also modular and can\nwork with existing regular expression packages, making it easy to deploy in a\nvariety of settings. The code of REI is available at\nhttps://github.com/mush-zhang/REI-Regular-Expression-Indexing."}
{"id": "2510.10580", "pdf": "https://arxiv.org/pdf/2510.10580", "abs": "https://arxiv.org/abs/2510.10580", "authors": ["Jiahao He", "Yutao Cui", "Cuiping Li", "Jikang Jiang", "Yuheng Hou", "Hong Chen"], "title": "AQORA: A Learned Adaptive Query Optimizer for Spark SQL", "categories": ["cs.DB"], "comment": "14 pages, 11 figures", "summary": "Recent studies have identified two main approaches to improve query\noptimization: learned query optimization (LQO), which generates or selects\nbetter query plans before execution based on models trained in advance, and\nadaptive query processing (AQP), which adapts the query plan during execution\nbased on statistical feedback collected at runtime. Although both approaches\nhave shown promise, they also face critical limitations. LQO must commit to a\nfixed plan without access to actual cardinalities and typically rely on a\nsingle end-to-end feedback signal, making learning inefficient. On the other\nhand, AQP depends heavily on rule-based heuristics and lacks the ability to\nlearn from experience. In this paper, we present AQORA, an adaptive query\noptimizer with a reinforcement learning architecture that combines the\nstrengths of both LQO and AQP. AQORA addresses the above challenges through\nfour core strategies: (1) realistic feature encoding, (2) query stage-level\nfeedback and intervention, (3) automatic strategy adaptation, and (4) low-cost\nintegration. Experiments show that AQORA reduces end-to-end execution time by\nup to 90% compared to other learned methods and by up to 70% compared to Spark\nSQL's default configuration with adaptive query execution."}
{"id": "2510.10858", "pdf": "https://arxiv.org/pdf/2510.10858", "abs": "https://arxiv.org/abs/2510.10858", "authors": ["Guanli Liu", "Renata Borovica-Gajic"], "title": "DriftBench: Defining and Generating Data and Query Workload Drift for Benchmarking", "categories": ["cs.DB"], "comment": null, "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."}
{"id": "2510.11011", "pdf": "https://arxiv.org/pdf/2510.11011", "abs": "https://arxiv.org/abs/2510.11011", "authors": ["Farzaneh Zirak", "Farhana Choudhury", "Renata Borovica-Gajic"], "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable Transactional and Analytical Workloads", "categories": ["cs.DB", "cs.LG"], "comment": "This is a preprint version", "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."}
{"id": "2510.11166", "pdf": "https://arxiv.org/pdf/2510.11166", "abs": "https://arxiv.org/abs/2510.11166", "authors": ["Brad Bebee", "\u00dcmit V. \u00c7ataly\u00fcrek", "Olaf Hartig", "Ankesh Khandelwal", "Simone Rondelli", "Michael Schmidt", "Lefteris Sidirourgos", "Bryan Thompson"], "title": "Poseidon: A OneGraph Engine", "categories": ["cs.DB"], "comment": null, "summary": "We present the Poseidon engine behind the Neptune Analytics graph database\nservice. Customers interact with Poseidon using the declarative openCypher\nquery language, which enables requests that seamlessly combine traditional\nquerying paradigms (such as graph pattern matching, variable length paths,\naggregation) with algorithm invocations and has been syntactically extended to\nfacilitate OneGraph interoperability, such as the disambiguation between\nglobally unique IRIs (as exposed via RDF) vs. local identifiers (as encountered\nin LPG data). Poseidon supports a broad range of graph workloads, from simple\ntransactions, to top-k beam search algorithms on dynamic graphs, to whole graph\nanalytics requiring multiple full passes over the data. For example, real-time\nfraud detection, like many other use cases, needs to reflect current committed\nstate of the dynamic graph. If a users cell phone is compromised, then all\nnewer actions by that user become immediately suspect. To address such dynamic\ngraph use cases, Poseidon combines state-of-the-art transaction processing with\nnovel graph data indexing, including lock-free maintenance of adjacency lists,\nsecondary succinct indices, partitioned heaps for data tuple storage with\nuniform placement, and innovative statistics for cost-based query optimization.\nThe Poseidon engine uses a logical log for durability, enabling rapid evolution\nof in-memory data structures. Bulk data loads achieve more than 10 million\nproperty values per second on many data sets while simple transactions can\nexecute in under 20ms against the storage engine."}
{"id": "2510.10810", "pdf": "https://arxiv.org/pdf/2510.10810", "abs": "https://arxiv.org/abs/2510.10810", "authors": ["Omar Islam Laskar", "Fatemeh Ramezani Khozestani", "Ishika Nankani", "Sohrab Namazi Nia", "Senjuti Basu Roy", "Kaustubh Beedkar"], "title": "Aegis: A Correlation-Based Data Masking Advisor for Data Sharing Ecosystems", "categories": ["cs.LG", "cs.DB"], "comment": "Accepted at SIGMOD 2026", "summary": "Data-sharing ecosystems enable entities -- such as providers, consumers, and\nintermediaries -- to access, exchange, and utilize data for various downstream\ntasks and applications. Due to privacy concerns, data providers typically\nanonymize datasets before sharing them; however, the existence of multiple\nmasking configurations results in masked datasets with varying utility.\nConsequently, a key challenge lies in efficiently determining the optimal\nmasking configuration that maximizes a dataset's utility. This paper presents\nAEGIS, a middleware framework for identifying the optimal masking configuration\nfor machine learning datasets that consist of features and a class label. We\nintroduce a utility optimizer that minimizes predictive utility deviation -- a\nmetric based on the changes in feature-label correlations before and after\nmasking. Our framework leverages limited data summaries (such as 1D histograms)\nor none to estimate the feature-label joint distribution, making it suitable\nfor scenarios where raw data is inaccessible due to privacy restrictions. To\nachieve this, we propose a joint distribution estimator based on iterative\nproportional fitting, which allows supporting various feature-label correlation\nquantification methods such as g3, mutual information, or chi-square. Our\nexperimental evaluation on real-world datasets shows that AEGIS identifies\noptimal masking configurations over an order of magnitude faster, while the\nresulting masked datasets achieve predictive performance on downstream ML tasks\nthat is on par with baseline approaches."}
{"id": "2510.10885", "pdf": "https://arxiv.org/pdf/2510.10885", "abs": "https://arxiv.org/abs/2510.10885", "authors": ["Jiajing Guo", "Kenil Patel", "Jorge Piazentin Ono", "Wenbin He", "Liu Ren"], "title": "Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks", "categories": ["cs.CL", "cs.DB"], "comment": "Accepted at COLM 2025 SCALR Workshop", "summary": "Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)\nsystems, enabling non-expert users to query industrial databases using natural\nlanguage. While test-time scaling strategies have shown promise in LLM-based\nsolutions, their effectiveness in real-world applications, especially with the\nlatest reasoning models, remains uncertain. In this work, we benchmark six\nlightweight, industry-oriented test-time scaling strategies and four LLMs,\nincluding two reasoning models, evaluating their performance on the BIRD\nMini-Dev benchmark. Beyond standard accuracy metrics, we also report inference\nlatency and token consumption, providing insights relevant for practical system\ndeployment. Our findings reveal that Divide-and-Conquer prompting and few-shot\ndemonstrations consistently enhance performance for both general-purpose and\nreasoning-focused LLMs. However, introducing additional workflow steps yields\nmixed results, and base model selection plays a critical role. This work sheds\nlight on the practical trade-offs between accuracy, efficiency, and complexity\nwhen deploying Text2SQL systems."}
{"id": "2510.10942", "pdf": "https://arxiv.org/pdf/2510.10942", "abs": "https://arxiv.org/abs/2510.10942", "authors": ["Nilima Rao", "Jagriti Srivastava", "Pradeep Kumar Sharma", "Hritvik Shrivastava"], "title": "Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval", "categories": ["cs.AI", "cs.DB"], "comment": null, "summary": "Modern enterprises manage vast knowledge distributed across heterogeneous\nsystems such as Jira, Git repositories, Confluence, and wikis. Conventional\nretrieval methods based on keyword search or static embeddings often fail to\nanswer complex queries that require contextual reasoning and multi-hop\ninference across artifacts. We present a modular hybrid retrieval framework for\nadaptive enterprise information access that integrates Knowledge Base\nLanguage-Augmented Models (KBLam), DeepGraph representations, and\nembedding-driven semantic search. The framework builds a unified knowledge\ngraph from parsed repositories including code, pull requests, and commit\nhistories, enabling semantic similarity search, structural inference, and\nmulti-hop reasoning. Query analysis dynamically determines the optimal\nretrieval strategy, supporting both structured and unstructured data sources\nthrough independent or fused processing. An interactive interface provides\ngraph visualizations, subgraph exploration, and context-aware query routing to\ngenerate concise and explainable answers. Experiments on large-scale Git\nrepositories show that the unified reasoning layer improves answer relevance by\nup to 80 percent compared with standalone GPT-based retrieval pipelines. By\ncombining graph construction, hybrid reasoning, and interactive visualization,\nthe proposed framework offers a scalable, explainable, and user-centric\nfoundation for intelligent knowledge assistants in enterprise environments."}
{"id": "2510.11299", "pdf": "https://arxiv.org/pdf/2510.11299", "abs": "https://arxiv.org/abs/2510.11299", "authors": ["Josep Domingo-Ferrer", "David S\u00e1nchez"], "title": "How to Get Actual Privacy and Utility from Privacy Models: the k-Anonymity and Differential Privacy Families", "categories": ["cs.CR", "cs.DB", "68", "K.4.1"], "comment": "13 pages", "summary": "Privacy models were introduced in privacy-preserving data publishing and\nstatistical disclosure control with the promise to end the need for costly\nempirical assessment of disclosure risk. We examine how well this promise is\nkept by the main privacy models. We find they may fail to provide adequate\nprotection guarantees because of problems in their definition or incur\nunacceptable trade-offs between privacy protection and utility preservation.\nSpecifically, k-anonymity may not entirely exclude disclosure if enforced with\ndeterministic mechanisms or without constraints on the confidential values. On\nthe other hand, differential privacy (DP) incurs unacceptable utility loss for\nsmall budgets and its privacy guarantee becomes meaningless for large budgets.\nIn the latter case, an ex post empirical assessment of disclosure risk becomes\nnecessary, undermining the main appeal of privacy models. Whereas the utility\npreservation of DP can only be improved by relaxing its privacy guarantees, we\nargue that a semantic reformulation of k-anonymity can offer more robust\nprivacy without losing utility with respect to traditional syntactic\nk-anonymity."}
