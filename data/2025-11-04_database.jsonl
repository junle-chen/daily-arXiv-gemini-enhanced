{"id": "2510.26835", "pdf": "https://arxiv.org/pdf/2510.26835", "abs": "https://arxiv.org/abs/2510.26835", "authors": ["Chen Wang", "Xunzhuo Liu", "Yue Zhu", "Alaa Youssef", "Priya Nagpurkar", "Huamin Chen"], "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads", "categories": ["cs.DB", "cs.AI", "cs.LG"], "comment": "13 pages including reference, position paper", "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections."}
{"id": "2510.26840", "pdf": "https://arxiv.org/pdf/2510.26840", "abs": "https://arxiv.org/abs/2510.26840", "authors": ["Rocky Klopfenstein", "Yang He", "Andrew Tremante", "Yuepeng Wang", "Nina Narodytska", "Haoze Wu"], "title": "SpotIt: Evaluating Text-to-SQL Evaluation with Formal Verification", "categories": ["cs.DB", "cs.AI", "cs.FL", "cs.LO"], "comment": null, "summary": "Community-driven Text-to-SQL evaluation platforms play a pivotal role in\ntracking the state of the art of Text-to-SQL performance. The reliability of\nthe evaluation process is critical for driving progress in the field. Current\nevaluation methods are largely test-based, which involves comparing the\nexecution results of a generated SQL query and a human-labeled ground-truth on\na static test database. Such an evaluation is optimistic, as two queries can\ncoincidentally produce the same output on the test database while actually\nbeing different. In this work, we propose a new alternative evaluation\npipeline, called SpotIt, where a formal bounded equivalence verification engine\nactively searches for a database that differentiates the generated and\nground-truth SQL queries. We develop techniques to extend existing verifiers to\nsupport a richer SQL subset relevant to Text-to-SQL. A performance evaluation\nof ten Text-to-SQL methods on the high-profile BIRD dataset suggests that\ntest-based methods can often overlook differences between the generated query\nand the ground-truth. Further analysis of the verification results reveals a\nmore complex picture of the current Text-to-SQL evaluation."}
{"id": "2510.26868", "pdf": "https://arxiv.org/pdf/2510.26868", "abs": "https://arxiv.org/abs/2510.26868", "authors": ["Reham Faqehi", "Haya Alhuraib", "Hamad Saiari", "Zyad Bamigdad"], "title": "The Impact of Data Compression in Real-Time and Historical Data Acquisition Systems on the Accuracy of Analytical Solutions", "categories": ["cs.DB"], "comment": "9 pages", "summary": "In industrial and IoT environments, massive amounts of real-time and\nhistorical process data are continuously generated and archived. With sensors\nand devices capturing every operational detail, the volume of time-series data\nhas become a critical challenge for storage and processing systems. Efficient\ndata management is essential to ensure scalability, cost-effectiveness, and\ntimely analytics. To minimize storage expenses and optimize performance, data\ncompression algorithms are frequently utilized in data historians and\nacquisition systems. However, compression comes with trade-offs that may\ncompromise the accuracy and reliability of engineering analytics that depend on\nthis compressed data. Understanding these trade-offs is essential for\ndeveloping data strategies that support both operational efficiency and\naccurate, reliable analytics. This paper assesses the relation of common\ncompression mechanisms used in real-time and historical data systems and the\naccuracy of analytical solutions, including statistical analysis, anomaly\ndetection, and machine learning models. Through theoretical analysis, simulated\nsignal compression, and empirical assessment, we illustrate that excessive\ncompression can lose critical patterns, skew statistical measures, and diminish\npredictive accuracy. The study suggests optimum methods and best practices for\nstriking a compromise between analytical integrity and compression efficiency."}
{"id": "2510.27119", "pdf": "https://arxiv.org/pdf/2510.27119", "abs": "https://arxiv.org/abs/2510.27119", "authors": ["Qiyan Deng", "Jianhui Li", "Chengliang Chai", "Jinqi Liu", "Junzhi She", "Kaisen Jin", "Zhaoze Sun", "Yuhao Deng", "Jia Yuan", "Ye Yuan", "Guoren Wang", "Lei Cao"], "title": "Unstructured Data Analysis using LLMs: A Comprehensive Benchmark", "categories": ["cs.DB"], "comment": null, "summary": "Nowadays, the explosion of unstructured data presents immense analytical\nvalue. Leveraging the remarkable capability of large language models (LLMs) in\nextracting attributes of structured tables from unstructured data, researchers\nare developing LLM-powered data systems for users to analyze unstructured\ndocuments as working with a database. These unstructured data analysis (UDA)\nsystems differ significantly in all aspects, including query interfaces, query\noptimization strategies, and operator implementations, making it unclear which\nperforms best in which scenario. Unfortunately, there does not exist a\ncomprehensive benchmark that offers high-quality, large-volume, and diverse\ndatasets as well as rich query workload to thoroughly evaluate such systems. To\nfill this gap, we present UDA-Bench, the first benchmark for unstructured data\nanalysis that meets all the above requirements. Specifically, we organize a\nteam with 30 graduate students that spends over in total 10,000 hours on\ncurating 5 datasets from various domains and constructing a relational database\nview from these datasets by manual annotation. These relational databases can\nbe used as ground truth to evaluate any of these UDA systems despite their\ndifferences in programming interfaces. Moreover, we design diverse queries to\nanalyze the attributes defined in the database schema, covering different types\nof analytical operators with varying selectivities and complexities. We conduct\nin-depth analysis of the key building blocks of existing UDA systems: query\ninterface, query optimization, operator design, and data processing. We run\nexhaustive experiments over the benchmark to fully evaluate these systems and\ndifferent techniques w.r.t. the above building blocks."}
{"id": "2510.27141", "pdf": "https://arxiv.org/pdf/2510.27141", "abs": "https://arxiv.org/abs/2510.27141", "authors": ["Chunxiao Ye", "Xiao Yan", "Eric Lo"], "title": "Compass: General Filtered Search across Vector and Structured Data", "categories": ["cs.DB", "cs.IR"], "comment": null, "summary": "The increasing prevalence of hybrid vector and relational data necessitates\nefficient, general support for queries that combine high-dimensional vector\nsearch with complex relational filtering. However, existing filtered search\nsolutions are fundamentally limited by specialized indices, which restrict\narbitrary filtering and hinder integration with general-purpose DBMSs. This\nwork introduces \\textsc{Compass}, a unified framework that enables general\nfiltered search across vector and structured data without relying on new index\ndesigns. Compass leverages established index structures -- such as HNSW and IVF\nfor vector attributes, and B+-trees for relational attributes -- implementing a\nprincipled cooperative query execution strategy that coordinates candidate\ngeneration and predicate evaluation across modalities. Uniquely, Compass\nmaintains generality by allowing arbitrary conjunctions, disjunctions, and\nrange predicates, while ensuring robustness even with highly-selective or\nmulti-attribute filters. Comprehensive empirical evaluations demonstrate that\nCompass consistently outperforms NaviX, the only existing performant general\nframework, across diverse hybrid query workloads. It also matches the query\nthroughput of specialized single-attribute indices in their favorite settings\nwith only a single attribute involved, all while maintaining full generality\nand DBMS compatibility. Overall, Compass offers a practical and robust solution\nfor achieving truly general filtered search in vector database systems."}
{"id": "2510.27168", "pdf": "https://arxiv.org/pdf/2510.27168", "abs": "https://arxiv.org/abs/2510.27168", "authors": ["Jing Chang", "Chang Liu", "Jinbin Huang", "Shuyuan Zheng", "Rui Mao", "Jianbin Qin"], "title": "ShapleyPipe: Hierarchical Shapley Search for Data Preparation Pipeline Construction", "categories": ["cs.DB"], "comment": null, "summary": "Automated data preparation pipeline construction is critical for machine\nlearning success, yet existing methods suffer from two fundamental limitations:\nthey treat pipeline construction as black-box optimization without quantifying\nindividual operator contributions, and they struggle with the combinatorial\nexplosion of the search space ($N^M$ configurations for N operators and\npipeline length M). We introduce ShapleyPipe, a principled framework that\nleverages game-theoretic Shapley values to systematically quantify each\noperator's marginal contribution while maintaining full interpretability. Our\nkey innovation is a hierarchical decomposition that separates category-level\nstructure search from operator-level refinement, reducing the search complexity\nfrom exponential to polynomial. To make Shapley computation tractable, we\ndevelop: (1) a Multi-Armed Bandit mechanism for intelligent category evaluation\nwith provable convergence guarantees, and (2) Permutation Shapley values to\ncorrectly capture position-dependent operator interactions. Extensive\nevaluation on 18 diverse datasets demonstrates that ShapleyPipe achieves 98.1\\%\nof high-budget baseline performance while using 24\\% fewer evaluations, and\noutperforms the state-of-the-art reinforcement learning method by 3.6\\%. Beyond\nperformance gains, ShapleyPipe provides interpretable operator valuations\n($\\rho$=0.933 correlation with empirical performance) that enable data-driven\npipeline analysis and systematic operator library refinement."}
{"id": "2510.27238", "pdf": "https://arxiv.org/pdf/2510.27238", "abs": "https://arxiv.org/abs/2510.27238", "authors": ["Chuxuan Hu", "Maxwell Yang", "James Weiland", "Yeji Lim", "Suhas Palawala", "Daniel Kang"], "title": "DRAMA: Unifying Data Retrieval and Analysis for Open-Domain Analytic Queries", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "comment": "Accepted to SIGMOD 2026", "summary": "Manually conducting real-world data analyses is labor-intensive and\ninefficient. Despite numerous attempts to automate data science workflows, none\nof the existing paradigms or systems fully demonstrate all three key\ncapabilities required to support them effectively: (1) open-domain data\ncollection, (2) structured data transformation, and (3) analytic reasoning.\n  To overcome these limitations, we propose DRAMA, an end-to-end paradigm that\nanswers users' analytic queries in natural language on large-scale open-domain\ndata. DRAMA unifies data collection, transformation, and analysis as a single\npipeline. To quantitatively evaluate system performance on tasks representative\nof DRAMA, we construct a benchmark, DRAMA-Bench, consisting of two categories\nof tasks: claim verification and question answering, each comprising 100\ninstances. These tasks are derived from real-world applications that have\ngained significant public attention and require the retrieval and analysis of\nopen-domain data. We develop DRAMA-Bot, a multi-agent system designed following\nDRAMA. It comprises a data retriever that collects and transforms data by\ncoordinating the execution of sub-agents, and a data analyzer that performs\nstructured reasoning over the retrieved data. We evaluate DRAMA-Bot on\nDRAMA-Bench together with five state-of-the-art baseline agents. DRAMA-Bot\nachieves 86.5% task accuracy at a cost of $0.05, outperforming all baselines\nwith up to 6.9 times the accuracy and less than 1/6 of the cost. DRAMA is\npublicly available at https://github.com/uiuc-kang-lab/drama."}
{"id": "2510.27243", "pdf": "https://arxiv.org/pdf/2510.27243", "abs": "https://arxiv.org/abs/2510.27243", "authors": ["Jiachen Zhao", "Xiao Yan", "Eric Lo"], "title": "Approximate Diverse $k$-nearest Neighbor Search in Vector Database", "categories": ["cs.DB"], "comment": null, "summary": "Approximate $k$-nearest neighbor search (A$k$-NNS) is a core operation in\nvector databases, underpinning applications such as retrieval-augmented\ngeneration (RAG) and image retrieval. In these scenarios, users often prefer\ndiverse result sets to minimize redundancy and enhance information value.\nHowever, existing greedy-based diverse methods frequently yield sub-optimal\nresults, failing to adequately approximate the optimal similarity score under\ncertain diversification level. Furthermore, there is a need for flexible\nalgorithms that can adapt to varying user-defined result sizes and diversity\nrequirements.\n  To address these challenges, we propose a novel approach that seamlessly\nintegrates result diversification into state-of-the-art (SOTA) A$k$-NNS\nmethods. Our approach introduces a progressive search framework, consisting of\niterative searching, diversification, and verification phases. Carefully\ndesigned diversification and verification steps enable our approach to\nefficiently approximate the optimal diverse result set according to\nuser-specified diversification levels without additional indexing overhead.\n  We evaluate our method on three million-scale benchmark datasets, LAION-art,\nDeep1M, and Txt2img, using latency, similarity, and recall as performance\nmetrics across a range of $k$ values and diversification thresholds.\nExperimental results demonstrate that our approach consistently retrieves\nnear-optimal diverse results with minimal latency overhead, particularly under\nmedium and high diversity settings."}
{"id": "2510.27145", "pdf": "https://arxiv.org/pdf/2510.27145", "abs": "https://arxiv.org/abs/2510.27145", "authors": ["Sein Kwon", "Seulgi Baek", "Hyunseo Yang", "Youngwan Jo", "Sanghyun Park"], "title": "Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores", "categories": ["cs.LG", "cs.DB"], "comment": "13 pages", "summary": "Database Management Systems (DBMSs) are fundamental for managing large-scale\nand heterogeneous data, and their performance is critically influenced by\nconfiguration parameters. Effective tuning of these parameters is essential for\nadapting to diverse workloads and maximizing throughput while minimizing\nlatency. Recent research has focused on automated configuration optimization\nusing machine learning; however, existing approaches still exhibit several key\nlimitations. Most tuning frameworks disregard the dependencies among\nparameters, assuming that each operates independently. This simplification\nprevents optimizers from leveraging relational effects across parameters,\nlimiting their capacity to capture performancesensitive interactions. Moreover,\nto reduce the complexity of the high-dimensional search space, prior work often\nselects only the top few parameters for optimization, overlooking others that\ncontribute meaningfully to performance. Bayesian Optimization (BO), the most\ncommon method for automatic tuning, is also constrained by its reliance on\nsurrogate models, which can lead to unstable predictions and inefficient\nexploration. To overcome these limitations, we propose RelTune, a novel\nframework that represents parameter dependencies as a Relational Graph and\nlearns GNN-based latent embeddings that encode performancerelevant semantics.\nRelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),\nwhich combines surrogate predictions with an Affinity Score measuring proximity\nto previously high-performing configurations. Experimental results on multiple\nDBMSs and workloads demonstrate that RelTune achieves faster convergence and\nhigher optimization efficiency than conventional BO-based methods, achieving\nstate-of-the-art performance across all evaluated scenarios."}
{"id": "2510.27588", "pdf": "https://arxiv.org/pdf/2510.27588", "abs": "https://arxiv.org/abs/2510.27588", "authors": ["Stefan Hermann", "Hans-Peter Lehmann", "Giorgio Vinciguerra", "Stefan Walzer"], "title": "Learned Static Function Data Structures", "categories": ["cs.DS", "cs.DB", "cs.LG"], "comment": null, "summary": "We consider the task of constructing a data structure for associating a\nstatic set of keys with values, while allowing arbitrary output values for\nqueries involving keys outside the set. Compared to hash tables, these\nso-called static function data structures do not need to store the key set and\nthus use significantly less memory. Several techniques are known, with\ncompressed static functions approaching the zero-order empirical entropy of the\nvalue sequence. In this paper, we introduce learned static functions, which use\nmachine learning to capture correlations between keys and values. For each key,\na model predicts a probability distribution over the values, from which we\nderive a key-specific prefix code to compactly encode the true value. The\nresulting codeword is stored in a classic static function data structure. This\ndesign allows learned static functions to break the zero-order entropy barrier\nwhile still supporting point queries. Our experiments show substantial space\nsavings: up to one order of magnitude on real data, and up to three orders of\nmagnitude on synthetic data."}
{"id": "2510.27614", "pdf": "https://arxiv.org/pdf/2510.27614", "abs": "https://arxiv.org/abs/2510.27614", "authors": ["Pedro Silva Gomes", "Carlos Baquero"], "title": "Rateless Bloom Filters: Set Reconciliation for Divergent Replicas with Variable-Sized Elements", "categories": ["cs.DS", "cs.DB", "H.0; E.4; D.4"], "comment": "Under submission", "summary": "Set reconciliation protocols typically make two critical assumptions: they\nare designed for fixed-sized elements and they are optimized for when the\ndifference cardinality, d, is very small. When adapting to variable-sized\nelements, the current practice is to synchronize fixed-size element digests.\nHowever, when the number of differences is considerable, such as after a\nnetwork partition, this approach can be inefficient. Our solution is a\ntwo-stage hybrid protocol that introduces a preliminary Bloom filter step,\nspecifically designed for this regime. The novelty of this approach, however,\nis in solving a core technical challenge: determining the optimal Bloom filter\nsize without knowing d. Our solution is the Rateless Bloom Filter (RBF), a\ndynamic filter that naturally adapts to arbitrary symmetric differences,\nclosely matching the communication complexity of an optimally configured static\nfilter without requiring any prior parametrization. Our evaluation in sets of\nvariable-sized elements shows that for Jaccard indices below 85%, our RBF-IBLT\nhybrid protocol reduces the total communication cost by up to over 20% compared\nto the state-of-the-art."}
