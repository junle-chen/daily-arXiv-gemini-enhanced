{"id": "2510.20275", "pdf": "https://arxiv.org/pdf/2510.20275", "abs": "https://arxiv.org/abs/2510.20275", "authors": ["Yunzhi Liu", "Haokai Tan", "Rushi Kanjaria", "Lihuan Li", "Flora D. Salim"], "title": "Classical Feature Embeddings Help in BERT-Based Human Mobility Prediction", "categories": ["cs.AI"], "comment": "This paper has been accepted by ACM SIGSPATIAL 2025 as a short paper", "summary": "Human mobility forecasting is crucial for disaster relief, city planning, and\npublic health. However, existing models either only model location sequences or\ninclude time information merely as auxiliary input, thereby failing to leverage\nthe rich semantic context provided by points of interest (POIs). To address\nthis, we enrich a BERT-based mobility model with derived temporal descriptors\nand POI embeddings to better capture the semantics underlying human movement.\nWe propose STaBERT (Semantic-Temporal aware BERT), which integrates both POI\nand temporal information at each location to construct a unified, semantically\nenriched representation of mobility. Experimental results show that STaBERT\nsignificantly improves prediction accuracy: for single-city prediction, the\nGEO-BLEU score improved from 0.34 to 0.75; for multi-city prediction, from 0.34\nto 0.56.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528BERT\uff08\u4e00\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u4eba\u7c7b\u79fb\u52a8\u9884\u6d4b\uff08\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\uff09\u3002\u8bba\u6587\u5c06\u65f6\u95f4\u4fe1\u606f\u548c\u5174\u8da3\u70b9\uff08POIs\uff09\u5d4c\u5165\u5230BERT\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90fd\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "human mobility prediction", "BERT", "Large Language Models", "mobility forecasting", "points of interest (POIs)", "temporal information"]}}
{"id": "2510.20182", "pdf": "https://arxiv.org/pdf/2510.20182", "abs": "https://arxiv.org/abs/2510.20182", "authors": ["Aaron Appelle", "Jerome P. Lynch"], "title": "Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories", "categories": ["cs.CV"], "comment": "Preprint, under review", "summary": "Large-scale video generation models have demonstrated high visual realism in\ndiverse contexts, spurring interest in their potential as general-purpose world\nsimulators. Existing benchmarks focus on individual subjects rather than scenes\nwith multiple interacting people. However, the plausibility of multi-agent\ndynamics in generated videos remains unverified. We propose a rigorous\nevaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V)\nmodels as implicit simulators of pedestrian dynamics. For I2V, we leverage\nstart frames from established datasets to enable comparison with a ground truth\nvideo dataset. For T2V, we develop a prompt suite to explore diverse pedestrian\ndensities and interactions. A key component is a method to reconstruct 2D\nbird's-eye view trajectories from pixel-space without known camera parameters.\nOur analysis reveals that leading models have learned surprisingly effective\npriors for plausible multi-agent behavior. However, failure modes like merging\nand disappearing people highlight areas for future improvement.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper evaluates video models (which can be considered large models) as simulators of pedestrian trajectories. It focuses on evaluating the plausibility of multi-agent dynamics in generated videos, specifically pedestrian trajectories. The paper explicitly mentions trajectory reconstruction and evaluation of pedestrian behavior, linking it to both trajectory prediction and large models.", "keywords": ["trajectory prediction", "pedestrian trajectories", "large-scale video generation models", "text-to-video (T2V)", "image-to-video (I2V)", "multi-agent behavior", "pedestrian dynamics"]}}
{"id": "2510.19889", "pdf": "https://arxiv.org/pdf/2510.19889", "abs": "https://arxiv.org/abs/2510.19889", "authors": ["Mostafa Ameli", "Van Anh Le", "Sulthana Shams", "Alexander Skabardonis"], "title": "From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem", "categories": ["cs.LG", "cs.AI", "math.OC"], "comment": null, "summary": "The traffic assignment problem is essential for traffic flow analysis,\ntraditionally solved using mathematical programs under the Equilibrium\nprinciple. These methods become computationally prohibitive for large-scale\nnetworks due to non-linear growth in complexity with the number of OD pairs.\nThis study introduces a novel data-driven approach using deep neural networks,\nspecifically leveraging the Transformer architecture, to predict equilibrium\npath flows directly. By focusing on path-level traffic distribution, the\nproposed model captures intricate correlations between OD pairs, offering a\nmore detailed and flexible analysis compared to traditional link-level\napproaches. The Transformer-based model drastically reduces computation time,\nwhile adapting to changes in demand and network structure without the need for\nrecalculation. Numerical experiments are conducted on the Manhattan-like\nsynthetic network, the Sioux Falls network, and the Eastern-Massachusetts\nnetwork. The results demonstrate that the proposed model is orders of magnitude\nfaster than conventional optimization. It efficiently estimates path-level\ntraffic flows in multi-class networks, reducing computational costs and\nimproving prediction accuracy by capturing detailed trip and flow information.\nThe model also adapts flexibly to varying demand and network conditions,\nsupporting traffic management and enabling rapid `what-if' analyses for\nenhanced transportation planning and policy-making.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper uses a Transformer-based model to predict traffic flow paths, which is related to trajectory prediction (specifically, vehicle trajectory). While it uses a Transformer architecture, which is a component often used in large language models, the paper doesn't explicitly focus on or utilize large language models themselves. The focus is on optimization and prediction of traffic flow, rather than leveraging the capabilities of LLMs for trajectory prediction or related tasks.", "keywords": ["trajectory prediction", "path flow estimation", "Transformer", "traffic assignment problem", "traffic flow"]}}
{"id": "2510.20328", "pdf": "https://arxiv.org/pdf/2510.20328", "abs": "https://arxiv.org/abs/2510.20328", "authors": ["Ajay Sridhar", "Jennifer Pan", "Satvik Sharma", "Chelsea Finn"], "title": "MemER: Scaling Up Memory for Robot Control via Experience Retrieval", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Project page: https://jen-pan.github.io/memer/", "summary": "Humans routinely rely on memory to perform tasks, yet most robot policies\nlack this capability; our goal is to endow robot policies with the same\nability. Naively conditioning on long observation histories is computationally\nexpensive and brittle under covariate shift, while indiscriminate subsampling\nof history leads to irrelevant or redundant information. We propose a\nhierarchical policy framework, where the high-level policy is trained to select\nand track previous relevant keyframes from its experience. The high-level\npolicy uses selected keyframes and the most recent frames when generating text\ninstructions for a low-level policy to execute. This design is compatible with\nexisting vision-language-action (VLA) models and enables the system to\nefficiently reason over long-horizon dependencies. In our experiments, we\nfinetune Qwen2.5-VL-7B-Instruct and $\\pi_{0.5}$ as the high-level and low-level\npolicies respectively, using demonstrations supplemented with minimal language\nannotations. Our approach, MemER, outperforms prior methods on three real-world\nlong-horizon robotic manipulation tasks that require minutes of memory. Videos\nand code can be found at https://jen-pan.github.io/memer/.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-VL-7B-Instruct\uff09\u8fdb\u884c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u68c0\u7d22\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\u3002\u867d\u7136\u4e3b\u8981\u4fa7\u91cd\u4e8e\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4f46\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u6027\u8f83\u9ad8\uff0c\u4e14\u673a\u5668\u4eba\u63a7\u5236\u901a\u5e38\u6d89\u53ca\u8f68\u8ff9\u89c4\u5212\u548c\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "Qwen2.5-VL-7B-Instruct", "robot control", "vision-language-action models", "VLA models"]}}
{"id": "2510.19981", "pdf": "https://arxiv.org/pdf/2510.19981", "abs": "https://arxiv.org/abs/2510.19981", "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"], "title": "FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework\nthat builds on existing 3D detectors by introducing a transformer-based\nsmoother and a fusion-driven tracker. Inspired by query-based tracking\nframeworks, FutrTrack employs a multimodal two-stage transformer refinement and\ntracking pipeline. Our fusion tracker integrates bounding boxes with multimodal\nbird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without\nthe need for an explicit motion model. The tracker assigns and propagates\nidentities across frames, leveraging both geometric and semantic cues for\nrobust re-identification under occlusion and viewpoint changes. Prior to\ntracking, we refine sequences of bounding boxes with a temporal smoother over a\nmoving window to refine trajectories, reduce jitter, and improve spatial\nconsistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that\nquery-based transformer tracking methods benefit significantly from multimodal\nsensor features compared with previous single-sensor approaches. With an aMOTA\nof 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D\nMOT benchmarks, reducing identity switches while maintaining competitive\naccuracy. Our approach provides an efficient framework for improving\ntransformer-based trackers to compete with other neural-network-based methods\neven with limited data and without pretraining.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u76ee\u6807\u8ddf\u8e2a(MOT)\uff0c\u7279\u522b\u662f3D MOT\uff0c\u5e76\u4f7f\u7528Transformer\u7ed3\u6784\u8fdb\u884c\u8f68\u8ff9\u5e73\u6ed1\u548c\u8ddf\u8e2a\u3002 \u867d\u7136\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4f46\u5b83\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002 Transformer\u7684\u4f7f\u7528\u4f7f\u5176\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\uff0c\u56e0\u4e3aTransformer\u4e5f\u662fLLM\u7684\u57fa\u7840\u3002", "keywords": ["multi-object tracking", "trajectory", "transformer", "fusion", "LiDAR", "camera", "tracking"]}}
{"id": "2510.20008", "pdf": "https://arxiv.org/pdf/2510.20008", "abs": "https://arxiv.org/abs/2510.20008", "authors": ["Swati Dantu", "Robert P\u011bni\u010dka", "Martin Saska"], "title": "Simultaneous learning of state-to-state minimum-time planning and control", "categories": ["cs.RO"], "comment": null, "summary": "This paper tackles the challenge of learning a generalizable minimum-time\nflight policy for UAVs, capable of navigating between arbitrary start and goal\nstates while balancing agile flight and stable hovering. Traditional\napproaches, particularly in autonomous drone racing, achieve impressive speeds\nand agility but are constrained to predefined track layouts, limiting\nreal-world applicability. To address this, we propose a reinforcement\nlearning-based framework that simultaneously learns state-to-state minimum-time\nplanning and control and generalizes to arbitrary state-to-state flights. Our\napproach leverages Point Mass Model (PMM) trajectories as proxy rewards to\napproximate the true optimal flight objective and employs curriculum learning\nto scale the training process efficiently and to achieve generalization. We\nvalidate our method through simulation experiments, comparing it against\nNonlinear Model Predictive Control (NMPC) tracking PMM-generated trajectories\nand conducting ablation studies to assess the impact of curriculum learning.\nFinally, real-world experiments confirm the robustness of our learned policy in\noutdoor environments, demonstrating its ability to generalize and operate on a\nsmall ARM-based single-board computer.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u8f68\u8ff9\u89c4\u5212\u548c\u63a7\u5236\uff0c\u91cd\u70b9\u5728\u4e8e\u5b66\u4e60\u4e00\u79cd\u901a\u7528\u7684\u6700\u77ed\u65f6\u95f4\u98de\u884c\u7b56\u7565\u3002\u867d\u7136\u4f7f\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\uff0c\u4f46\u6ca1\u6709\u660e\u786e\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u8bba\u6587\u4e2d\u6d89\u53ca\u5230\u4e86\u8f68\u8ff9\u89c4\u5212\u3001\u8fd0\u52a8\u63a7\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u7b49\u6982\u5ff5\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e0e\u5927\u6a21\u578b\u7684\u5173\u7cfb\u4e0d\u5927\u3002", "keywords": ["trajectory planning", "reinforcement learning", "UAV", "flight policy", "control"]}}
{"id": "2510.20161", "pdf": "https://arxiv.org/pdf/2510.20161", "abs": "https://arxiv.org/abs/2510.20161", "authors": ["Ahmed Alanazi", "Duy Ho", "Yugyung Lee"], "title": "PathFormer: A Transformer with 3D Grid Constraints for Digital Twin Robot-Arm Trajectory Generation", "categories": ["cs.RO", "68T07, 68T40", "I.2.9; I.2.10; I.2.11"], "comment": "8 pages, 7 figures, 7 tables", "summary": "Robotic arms require precise, task-aware trajectory planning, yet sequence\nmodels that ignore motion structure often yield invalid or inefficient\nexecutions. We present a Path-based Transformer that encodes robot motion with\na 3-grid (where/what/when) representation and constraint-masked decoding,\nenforcing lattice-adjacent moves and workspace bounds while reasoning over task\ngraphs and action order. Trained on 53,755 trajectories (80% train / 20%\nvalidation), the model aligns closely with ground truth -- 89.44% stepwise\naccuracy, 93.32% precision, 89.44% recall, and 90.40% F1 -- with 99.99% of\npaths legal by construction. Compiled to motor primitives on an xArm Lite 6\nwith a depth-camera digital twin, it attains up to 97.5% reach and 92.5% pick\nsuccess in controlled tests, and 86.7% end-to-end success across 60\nlanguage-specified tasks in cluttered scenes, absorbing slips and occlusions\nvia local re-grounding without global re-planning. These results show that\npath-structured representations enable Transformers to generate accurate,\nreliable, and interpretable robot trajectories, bridging graph-based planning\nand sequence-based learning and providing a practical foundation for\ngeneral-purpose manipulation and sim-to-real transfer.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u624b\u81c2\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u5e76\u4f7f\u7528\u4e86Transformer\u6a21\u578b\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u751f\u6210\uff0c\u4f46\u4e0e\u884c\u4eba\u6216\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u4e0d\u540c\uff0c\u4e14\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory generation", "robot-arm", "Transformer"]}}
{"id": "2510.20335", "pdf": "https://arxiv.org/pdf/2510.20335", "abs": "https://arxiv.org/abs/2510.20335", "authors": ["Zixuan Wu", "Hengyuan Zhang", "Ting-Hsuan Chen", "Yuliang Guo", "David Paz", "Xinyu Huang", "Liu Ren"], "title": "Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking", "categories": ["cs.RO", "cs.CV"], "comment": "Code is at\n  https://github.com/ChampagneAndfragrance/Dino_Diffusion_Parking_Official", "summary": "Parking is a critical pillar of driving safety. While recent end-to-end (E2E)\napproaches have achieved promising in-domain results, robustness under domain\nshifts (e.g., weather and lighting changes) remains a key challenge. Rather\nthan relying on additional data, in this paper, we propose Dino-Diffusion\nParking (DDP), a domain-agnostic autonomous parking pipeline that integrates\nvisual foundation models with diffusion-based planning to enable generalized\nperception and robust motion planning under distribution shifts. We train our\npipeline in CARLA at regular setting and transfer it to more adversarial\nsettings in a zero-shot fashion. Our model consistently achieves a parking\nsuccess rate above 90% across all tested out-of-distribution (OOD) scenarios,\nwith ablation studies confirming that both the network architecture and\nalgorithmic design significantly enhance cross-domain performance over existing\nbaselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environment\nreconstructed from a real-world parking lot demonstrates promising sim-to-real\ntransfer.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u8fd0\u52a8\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff08autonomous parking, motion planning\uff09\u3002\u867d\u7136\u63d0\u5230\u4e86foundation models\uff0c\u4f46\u4e3b\u8981\u805a\u7126\u5728\u89c6\u89c9\u9886\u57df\uff0c\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5173\u7cfb\u8f83\u5f31\u3002DDP\u6a21\u578b\u672c\u8eab\u662f\u57fa\u4e8eDiffusion\u7684\uff0c\u4e5f\u4e0e\u8f68\u8ff9\u89c4\u5212\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002", "keywords": ["autonomous parking", "motion planning", "foundation models", "diffusion"]}}
{"id": "2510.20126", "pdf": "https://arxiv.org/pdf/2510.20126", "abs": "https://arxiv.org/abs/2510.20126", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony S. Maida", "Alan B. Barhorst", "Vijaya Gopu"], "title": "Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects", "categories": ["cs.CV"], "comment": "13 pages, 6 figures", "summary": "While computer vision has advanced considerably for general object detection\nand tracking, the specific problem of fast-moving tiny objects remains\nunderexplored. This paper addresses the significant challenge of detecting and\ntracking rapidly moving small objects using an RGB-D camera. Our novel system\ncombines deep learning-based detection with physics-based tracking to overcome\nthe limitations of existing approaches. Our contributions include: (1) a\ncomprehensive system design for object detection and tracking of fast-moving\nsmall objects in 3D space, (2) an innovative physics-based tracking algorithm\nthat integrates kinematics motion equations to handle outliers and missed\ndetections, and (3) an outlier detection and correction module that\nsignificantly improves tracking performance in challenging scenarios such as\nocclusions and rapid direction changes. We evaluated our proposed system on a\ncustom racquetball dataset. Our evaluation shows our system surpassing kalman\nfilter based trackers with up to 70\\% less Average Displacement Error. Our\nsystem has significant applications for improving robot perception on\nautonomous platforms and demonstrates the effectiveness of combining\nphysics-based models with deep learning approaches for real-time 3D detection\nand tracking of challenging small objects.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on tracking fast-moving small objects using a physics-guided fusion approach. While it involves 3D tracking and motion prediction (using kinematics equations), it does not mention or utilize large language models. The connection to trajectory prediction is present but not central, and there's no mention of LLMs.", "keywords": ["3D tracking", "object tracking", "motion prediction", "kinematics", "deep learning"]}}
{"id": "2510.20473", "pdf": "https://arxiv.org/pdf/2510.20473", "abs": "https://arxiv.org/abs/2510.20473", "authors": ["Bernhard Rameder", "Hubert Gattringer", "Andreas Mueller", "Ronald Naderer"], "title": "Robot Path and Trajectory Planning Considering a Spatially Fixed TCP", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a method for planning a trajectory in workspace\ncoordinates using a spatially fixed tool center point (TCP), while taking into\naccount the processing path on a part. This approach is beneficial if it is\neasier to move the part rather than moving the tool. Whether a mathematical\ndescription that defines the shape to be processed or single points from a\ndesign program are used, the robot path is finally represented using B-splines.\nThe use of splines enables the path to be continuous with a desired degree,\nwhich finally leads to a smooth robot trajectory. While calculating the robot\ntrajectory through prescribed orientation, additionally a given velocity at the\nTCP has to be considered. The procedure was validated on a real system using an\nindustrial robot moving an arbitrary defined part.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u673a\u5668\u4eba\u9886\u57df\u53ef\u4ee5\u5229\u7528\u5927\u6a21\u578b\u8fdb\u884c\u4f18\u5316\u6216\u63a7\u5236\u3002", "keywords": ["\u8f68\u8ff9\u89c4\u5212", "trajectory planning", "\u673a\u5668\u4eba", "robot"]}}
{"id": "2510.20022", "pdf": "https://arxiv.org/pdf/2510.20022", "abs": "https://arxiv.org/abs/2510.20022", "authors": ["Jiazheng Li", "Yawei Wang", "David Yan", "Yijun Tian", "Zhichao Xu", "Huan Song", "Panpan Xu", "Lin Lee Cheong"], "title": "SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph", "categories": ["cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities,\nenabling language agents to excel at single-turn tasks. However, their\napplication to complex, multi-step, and long-horizon tasks remains challenging.\nWhile reinforcement learning (RL) offers a promising avenue for addressing\nthese challenges, mainstream approaches typically rely solely on sparse,\noutcome-based rewards, a limitation that becomes especially problematic for\ngroup-based RL algorithms lacking critic models, such as Group Relative Policy\nOptimization (GRPO). In such methods, uniformly rewarding or penalizing all\nactions within a trajectory can lead to training instability and suboptimal\npolicies, because beneficial and detrimental actions are often entangled across\nmulti-step interactions. To address this challenge, we propose SALT, a novel\nand lightweight framework that provides a finer-grained advantage assignment,\nderived solely from outcome rewards. We achieve this by constructing a graph\nfrom trajectories of the same prompt, which allows us to quantify the quality\nof each step and assign advantages accordingly. Crucially, SALT is designed as\na plug-and-play module that seamlessly integrates with existing group-based RL\nalgorithms, requiring no modifications to the rollout procedure and introducing\nnegligible computational overhead. Extensive experiments on the WebShop,\nALFWorld, and AppWorld benchmarks with various model sizes demonstrate that\nSALT consistently improves performance. We also conduct a thorough analysis to\nvalidate the design choices behind SALT and offer actionable insights.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u5230\u4e86Large Language Models (LLMs)\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8emulti-step\u548clong-horizon tasks\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46long-horizon tasks\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u8bba\u6587\u4f7f\u7528\u4e86\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u4e5f\u5e38\u88ab\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e2d\u3002", "keywords": ["Large Language Models", "LLMs", "reinforcement learning", "long-horizon tasks"]}}
{"id": "2510.20818", "pdf": "https://arxiv.org/pdf/2510.20818", "abs": "https://arxiv.org/abs/2510.20818", "authors": ["Mateo Guaman Castro", "Sidharth Rajagopal", "Daniel Gorbatov", "Matt Schmittle", "Rohan Baijal", "Octi Zhang", "Rosario Scalise", "Sidharth Talia", "Emma Romig", "Celso de Melo", "Byron Boots", "Abhishek Gupta"], "title": "VAMOS: A Hierarchical Vision-Language-Action Model for Capability-Modulated and Steerable Navigation", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "A fundamental challenge in robot navigation lies in learning policies that\ngeneralize across diverse environments while conforming to the unique physical\nconstraints and capabilities of a specific embodiment (e.g., quadrupeds can\nwalk up stairs, but rovers cannot). We propose VAMOS, a hierarchical VLA that\ndecouples semantic planning from embodiment grounding: a generalist planner\nlearns from diverse, open-world data, while a specialist affordance model\nlearns the robot's physical constraints and capabilities in safe, low-cost\nsimulation. We enabled this separation by carefully designing an interface that\nlets a high-level planner propose candidate paths directly in image space that\nthe affordance model then evaluates and re-ranks. Our real-world experiments\nshow that VAMOS achieves higher success rates in both indoor and complex\noutdoor navigation than state-of-the-art model-based and end-to-end learning\nmethods. We also show that our hierarchical design enables cross-embodied\nnavigation across legged and wheeled robots and is easily steerable using\nnatural language. Real-world ablations confirm that the specialist model is key\nto embodiment grounding, enabling a single high-level planner to be deployed\nacross physically distinct wheeled and legged robots. Finally, this model\nsignificantly enhances single-robot reliability, achieving 3X higher success\nrates by rejecting physically infeasible plans. Website:\nhttps://vamos-vla.github.io/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses robot navigation, which is related to trajectory prediction. It also mentions a \"Vision-Language-Action Model\", suggesting the use of a large model, although it doesn't explicitly state that it's a large language model. The 'steerable navigation using natural language' part further hints at the involvement of language models.", "keywords": ["navigation", "Vision-Language-Action Model", "robot navigation", "path planning"]}}
{"id": "2510.20437", "pdf": "https://arxiv.org/pdf/2510.20437", "abs": "https://arxiv.org/abs/2510.20437", "authors": ["Alvaro Carrizosa-Rendon", "Jian Zhou", "Erik Frisk", "Vicenc Puig", "Fatiha Nejjari"], "title": "Behavior-Aware Online Prediction of Obstacle Occupancy using Zonotopes", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": "64th IEEE Conference on Decision and Control", "summary": "Predicting the motion of surrounding vehicles is key to safe autonomous\ndriving, especially in unstructured environments without prior information.\nThis paper proposes a novel online method to accurately predict the occupancy\nsets of surrounding vehicles based solely on motion observations. The approach\nis divided into two stages: first, an Extended Kalman Filter and a Linear\nProgramming (LP) problem are used to estimate a compact zonotopic set of\ncontrol actions; then, a reachability analysis propagates this set to predict\nfuture occupancy. The effectiveness of the method has been validated through\nsimulations in an urban environment, showing accurate and compact predictions\nwithout relying on prior assumptions or prior training data.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on predicting the motion of surrounding vehicles for autonomous driving. While it doesn't directly involve large language models, it falls under the umbrella of trajectory prediction, specifically vehicle trajectory prediction. The method uses Kalman Filter and Linear Programming for occupancy prediction, which are relevant techniques in the field.", "keywords": ["trajectory prediction", "vehicle trajectory prediction", "autonomous driving", "motion prediction", "occupancy prediction"]}}
{"id": "2510.20578", "pdf": "https://arxiv.org/pdf/2510.20578", "abs": "https://arxiv.org/abs/2510.20578", "authors": ["Ding Zou", "Feifan Wang", "Mengyu Ge", "Siyuan Fan", "Zongbing Zhang", "Wei Chen", "Lingfeng Wang", "Zhongyou Hu", "Wenrui Yan", "Zhengwei Gao", "Hao Wang", "Weizhao Jin", "Yu Zhang", "Hainan Zhao", "Mingliang Zhang", "Xianxian Xi", "Yaru Zhang", "Wenyuan Li", "Zhengguang Gao", "Yurui Zhu"], "title": "EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "The realization of Artificial General Intelligence (AGI) necessitates\nEmbodied AI agents capable of robust spatial perception, effective task\nplanning, and adaptive execution in physical environments. However, current\nlarge language models (LLMs) and multimodal LLMs (MLLMs) for embodied tasks\nsuffer from key limitations, including a significant gap between model design\nand agent requirements, an unavoidable trade-off between real-time latency and\nperformance, and the use of unauthentic, offline evaluation metrics. To address\nthese challenges, we propose EmbodiedBrain, a novel vision-language foundation\nmodel available in both 7B and 32B parameter sizes. Our framework features an\nagent-aligned data structure and employs a powerful training methodology that\nintegrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented Group\nRelative Policy Optimization (Step-GRPO), which boosts long-horizon task\nsuccess by integrating preceding steps as Guided Precursors. Furthermore, we\nincorporate a comprehensive reward system, including a Generative Reward Model\n(GRM) accelerated at the infrastructure level, to improve training efficiency.\nFor enable thorough validation, we establish a three-part evaluation system\nencompassing General, Planning, and End-to-End Simulation Benchmarks,\nhighlighted by the proposal and open-sourcing of a novel, challenging\nsimulation environment. Experimental results demonstrate that EmbodiedBrain\nachieves superior performance across all metrics, establishing a new\nstate-of-the-art for embodied foundation models. Towards paving the way for the\nnext generation of generalist embodied agents, we open-source all of our data,\nmodel weight, and evaluating methods, which are available at\nhttps://zterobot.github.io/EmbodiedBrain.github.io.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on embodied AI agents and uses large language models (LLMs) as a foundation. While it doesn't directly address trajectory prediction, the task planning aspect for embodied agents implicitly involves planning and predicting future states/actions, which relates to trajectory prediction. The core focus is on LLMs for embodied tasks.", "keywords": ["large language models", "LLMs", "foundation models", "embodied AI", "task planning"]}}
{"id": "2510.20539", "pdf": "https://arxiv.org/pdf/2510.20539", "abs": "https://arxiv.org/abs/2510.20539", "authors": ["Guillermo Carbajal", "Andr\u00e9s Almansa", "Pablo Mus\u00e9"], "title": "Blur2seq: Blind Deblurring and Camera Trajectory Estimation from a Single Camera Motion-blurred Image", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Motion blur caused by camera shake, particularly under large or rotational\nmovements, remains a major challenge in image restoration. We propose a deep\nlearning framework that jointly estimates the latent sharp image and the\nunderlying camera motion trajectory from a single blurry image. Our method\nleverages the Projective Motion Blur Model (PMBM), implemented efficiently\nusing a differentiable blur creation module compatible with modern networks. A\nneural network predicts a full 3D rotation trajectory, which guides a\nmodel-based restoration network trained end-to-end. This modular architecture\nprovides interpretability by revealing the camera motion that produced the\nblur. Moreover, this trajectory enables the reconstruction of the sequence of\nsharp images that generated the observed blurry image. To further refine\nresults, we optimize the trajectory post-inference via a reblur loss, improving\nconsistency between the blurry input and the restored output. Extensive\nexperiments show that our method achieves state-of-the-art performance on both\nsynthetic and real datasets, particularly in cases with severe or spatially\nvariant blur, where end-to-end deblurring networks struggle.\n  Code and trained models are available at\nhttps://github.com/GuillermoCarbajal/Blur2Seq/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on estimating camera trajectory from a blurry image, which is related to trajectory prediction. However, it doesn't involve large language models. The trajectory estimation is crucial for deblurring the image and recovering the original sequence of sharp images.", "keywords": ["trajectory estimation", "motion trajectory"]}}
