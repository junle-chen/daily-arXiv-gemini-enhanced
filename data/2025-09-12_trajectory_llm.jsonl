{"id": "2509.08222", "pdf": "https://arxiv.org/pdf/2509.08222", "abs": "https://arxiv.org/abs/2509.08222", "authors": ["Minjong Yoo", "Jinwoo Jang", "Wei-jin Park", "Honguk Woo"], "title": "Exploratory Retrieval-Augmented Planning For Continual Embodied Instruction Following", "categories": ["cs.AI"], "comment": "21 pages. NeurIPS 2024", "summary": "This study presents an Exploratory Retrieval-Augmented Planning (ExRAP)\nframework, designed to tackle continual instruction following tasks of embodied\nagents in dynamic, non-stationary environments. The framework enhances Large\nLanguage Models' (LLMs) embodied reasoning capabilities by efficiently\nexploring the physical environment and establishing the environmental context\nmemory, thereby effectively grounding the task planning process in time-varying\nenvironment contexts. In ExRAP, given multiple continual instruction following\ntasks, each instruction is decomposed into queries on the environmental context\nmemory and task executions conditioned on the query results. To efficiently\nhandle these multiple tasks that are performed continuously and simultaneously,\nwe implement an exploration-integrated task planning scheme by incorporating\nthe {information-based exploration} into the LLM-based planning process.\nCombined with memory-augmented query evaluation, this integrated scheme not\nonly allows for a better balance between the validity of the environmental\ncontext memory and the load of environment exploration, but also improves\noverall task performance. Furthermore, we devise a {temporal consistency\nrefinement} scheme for query evaluation to address the inherent decay of\nknowledge in the memory. Through experiments with VirtualHome, ALFRED, and\nCARLA, our approach demonstrates robustness against a variety of embodied\ninstruction following scenarios involving different instruction scales and\ntypes, and non-stationarity degrees, and it consistently outperforms other\nstate-of-the-art LLM-based task planning approaches in terms of both goal\nsuccess rate and execution efficiency.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on embodied agents and instruction following in dynamic environments, utilizing Large Language Models (LLMs) for task planning. While it doesn't directly address trajectory prediction in the traditional sense, the embodied reasoning and planning aspects, particularly in environments like CARLA (a simulator often used for autonomous driving research), suggest a connection to predicting agent behavior and future states, thus relating to trajectory prediction. The use of LLMs is a major component.", "keywords": ["Large Language Models", "LLMs", "embodied agents", "task planning", "CARLA", "instruction following"]}}
{"id": "2509.08699", "pdf": "https://arxiv.org/pdf/2509.08699", "abs": "https://arxiv.org/abs/2509.08699", "authors": ["Stefan Podgorski", "Sourav Garg", "Mehdi Hosseinzadeh", "Lachlan Mares", "Feras Dayoub", "Ian Reid"], "title": "TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": "9 pages, 5 figures, ICRA 2025", "summary": "Visual navigation in robotics traditionally relies on globally-consistent 3D\nmaps or learned controllers, which can be computationally expensive and\ndifficult to generalize across diverse environments. In this work, we present a\nnovel RGB-only, object-level topometric navigation pipeline that enables\nzero-shot, long-horizon robot navigation without requiring 3D maps or\npre-trained controllers. Our approach integrates global topological path\nplanning with local metric trajectory control, allowing the robot to navigate\ntowards object-level sub-goals while avoiding obstacles. We address key\nlimitations of previous methods by continuously predicting local trajectory\nusing monocular depth and traversability estimation, and incorporating an\nauto-switching mechanism that falls back to a baseline controller when\nnecessary. The system operates using foundational models, ensuring open-set\napplicability without the need for domain-specific fine-tuning. We demonstrate\nthe effectiveness of our method in both simulated environments and real-world\ntests, highlighting its robustness and deployability. Our approach outperforms\nexisting state-of-the-art methods, offering a more adaptable and effective\nsolution for visual navigation in open-set environments. The source code is\nmade publicly available: https://github.com/podgorki/TANGO.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on robot navigation, which involves trajectory control. It also mentions using foundational models (a type of large model) for open-set applicability. Therefore, it has moderate relevance to both trajectory prediction and large language models.", "keywords": ["trajectory control", "navigation", "foundational models"]}}
{"id": "2509.08147", "pdf": "https://arxiv.org/pdf/2509.08147", "abs": "https://arxiv.org/abs/2509.08147", "authors": ["Zhen Tian", "Fujiang Yuan", "Chunhong Yuan", "Yanhong Peng"], "title": "Mean Field Game-Based Interactive Trajectory Planning Using Physics-Inspired Unified Potential Fields", "categories": ["cs.RO"], "comment": null, "summary": "Interactive trajectory planning in autonomous driving must balance safety,\nefficiency, and scalability under heterogeneous driving behaviors. Existing\nmethods often face high computational cost or rely on external safety critics.\nTo address this, we propose an Interaction-Enriched Unified Potential Field\n(IUPF) framework that fuses style-dependent benefit and risk fields through a\nphysics-inspired variational model, grounded in mean field game theory. The\napproach captures conservative, aggressive, and cooperative behaviors without\nadditional safety modules, and employs stochastic differential equations to\nguarantee Nash equilibrium with exponential convergence. Simulations on lane\nchanging and overtaking scenarios show that IUPF ensures safe distances,\ngenerates smooth and efficient trajectories, and outperforms traditional\noptimization and game-theoretic baselines in both adaptability and\ncomputational efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory planning for autonomous driving using a game-theoretic approach. While it deals with trajectory prediction/planning, it does not involve large language models. The connection to trajectory prediction is strong, but the absence of LLMs lowers the overall relevance.", "keywords": ["trajectory planning", "autonomous driving", "mean field game", "interactive trajectory planning"]}}
{"id": "2509.08160", "pdf": "https://arxiv.org/pdf/2509.08160", "abs": "https://arxiv.org/abs/2509.08160", "authors": ["Viraj Parimi", "Brian C. Williams"], "title": "Diffusion-Guided Multi-Arm Motion Planning", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-arm motion planning is fundamental for enabling arms to complete\ncomplex long-horizon tasks in shared spaces efficiently but current methods\nstruggle with scalability due to exponential state-space growth and reliance on\nlarge training datasets for learned models. Inspired by Multi-Agent Path\nFinding (MAPF), which decomposes planning into single-agent problems coupled\nwith collision resolution, we propose a novel diffusion-guided multi-arm\nplanner (DG-MAP) that enhances scalability of learning-based models while\nreducing their reliance on massive multi-arm datasets. Recognizing that\ncollisions are primarily pairwise, we train two conditional diffusion models,\none to generate feasible single-arm trajectories, and a second, to model the\ndual-arm dynamics required for effective pairwise collision resolution. By\nintegrating these specialized generative models within a MAPF-inspired\nstructured decomposition, our planner efficiently scales to larger number of\narms. Evaluations against alternative learning-based methods across various\nteam sizes demonstrate our method's effectiveness and practical applicability.\nProject website can be found at https://diff-mapf-mers.csail.mit.edu", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-arm motion planning, which is related to trajectory prediction and path planning. It uses diffusion models, which can be considered a type of generative model, although not directly a Large Language Model. The connection to LLMs is weak, but the motion planning aspect and the use of diffusion models justify a moderate relevance score.", "keywords": ["motion planning", "trajectory", "diffusion models"]}}
{"id": "2509.08302", "pdf": "https://arxiv.org/pdf/2509.08302", "abs": "https://arxiv.org/abs/2509.08302", "authors": ["Rajendramayavan Sathyam", "Yueqi Li"], "title": "Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities", "categories": ["cs.RO", "cs.CV"], "comment": "32 pages, 14 figures, accepted at IEEE Open Journal of Vehicular\n  Technology (OJVT)", "summary": "Foundation models are revolutionizing autonomous driving perception,\ntransitioning the field from narrow, task-specific deep learning models to\nversatile, general-purpose architectures trained on vast, diverse datasets.\nThis survey examines how these models address critical challenges in autonomous\nperception, including limitations in generalization, scalability, and\nrobustness to distributional shifts. The survey introduces a novel taxonomy\nstructured around four essential capabilities for robust performance in dynamic\ndriving environments: generalized knowledge, spatial understanding,\nmulti-sensor robustness, and temporal reasoning. For each capability, the\nsurvey elucidates its significance and comprehensively reviews cutting-edge\napproaches. Diverging from traditional method-centric surveys, our unique\nframework prioritizes conceptual design principles, providing a\ncapability-driven guide for model development and clearer insights into\nfoundational aspects. We conclude by discussing key challenges, particularly\nthose associated with the integration of these capabilities into real-time,\nscalable systems, and broader deployment challenges related to computational\ndemands and ensuring model reliability against issues like hallucinations and\nout-of-distribution failures. The survey also outlines crucial future research\ndirections to enable the safe and effective deployment of foundation models in\nautonomous driving systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper primarily focuses on foundation models in autonomous driving perception. While it doesn't explicitly mention trajectory prediction, the temporal reasoning capability and the overall context of autonomous driving suggest a potential, albeit indirect, connection. The mention of foundation models indicates relevance to large language models.", "keywords": ["foundation models", "autonomous driving", "temporal reasoning", "large language models"]}}
{"id": "2509.08435", "pdf": "https://arxiv.org/pdf/2509.08435", "abs": "https://arxiv.org/abs/2509.08435", "authors": ["Lei Ye", "Haibo Gao", "Peng Xu", "Zhelin Zhang", "Junqi Shan", "Ao Zhang", "Wei Zhang", "Ruyi Zhou", "Zongquan Deng", "Liang Ding"], "title": "PegasusFlow: Parallel Rolling-Denoising Score Sampling for Robot Diffusion Planner Flow Matching", "categories": ["cs.RO"], "comment": "8 pages, 7 figures, conference paper", "summary": "Diffusion models offer powerful generative capabilities for robot trajectory\nplanning, yet their practical deployment on robots is hindered by a critical\nbottleneck: a reliance on imitation learning from expert demonstrations. This\nparadigm is often impractical for specialized robots where data is scarce and\ncreates an inefficient, theoretically suboptimal training pipeline. To overcome\nthis, we introduce PegasusFlow, a hierarchical rolling-denoising framework that\nenables direct and parallel sampling of trajectory score gradients from\nenvironmental interaction, completely bypassing the need for expert data. Our\ncore innovation is a novel sampling algorithm, Weighted Basis Function\nOptimization (WBFO), which leverages spline basis representations to achieve\nsuperior sample efficiency and faster convergence compared to traditional\nmethods like MPPI. The framework is embedded within a scalable, asynchronous\nparallel simulation architecture that supports massively parallel rollouts for\nefficient data collection. Extensive experiments on trajectory optimization and\nrobotic navigation tasks demonstrate that our approach, particularly\nAction-Value WBFO (AVWBFO) combined with a reinforcement learning warm-start,\nsignificantly outperforms baselines. In a challenging barrier-crossing task,\nour method achieved a 100% success rate and was 18% faster than the next-best\nmethod, validating its effectiveness for complex terrain locomotion planning.\nhttps://masteryip.github.io/pegasusflow.github.io/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on robot trajectory planning using diffusion models, which falls under the umbrella of trajectory prediction. While it doesn't directly involve Large Language Models, the use of diffusion models for trajectory generation connects to the broader theme of generative models in AI. The mention of reinforcement learning further strengthens the connection to advanced AI techniques used in trajectory planning.", "keywords": ["trajectory planning", "diffusion models", "robotics", "trajectory optimization", "reinforcement learning"]}}
{"id": "2509.08743", "pdf": "https://arxiv.org/pdf/2509.08743", "abs": "https://arxiv.org/abs/2509.08743", "authors": ["Anoop Bhat", "Geordan Gutow", "Bhaskar Vundurthy", "Zhongqiang Ren", "Sivakumar Rathinam", "Howie Choset"], "title": "Parallel, Asymptotically Optimal Algorithms for Moving Target Traveling Salesman Problems", "categories": ["cs.RO"], "comment": null, "summary": "The Moving Target Traveling Salesman Problem (MT-TSP) seeks an agent\ntrajectory that intercepts several moving targets, within a particular time\nwindow for each target. In the presence of generic nonlinear target\ntrajectories or kinematic constraints on the agent, no prior algorithm\nguarantees convergence to an optimal MT-TSP solution. Therefore, we introduce\nthe Iterated Random Generalized (IRG) TSP framework. The key idea behind IRG is\nto alternate between randomly sampling a set of agent configuration-time\npoints, corresponding to interceptions of targets, and finding a sequence of\ninterception points by solving a generalized TSP (GTSP). This alternation\nenables asymptotic convergence to the optimum. We introduce two parallel\nalgorithms within the IRG framework. The first algorithm, IRG-PGLNS, solves\nGTSPs using PGLNS, our parallelized extension of the state-of-the-art solver\nGLNS. The second algorithm, Parallel Communicating GTSPs (PCG), solves GTSPs\ncorresponding to several sets of points simultaneously. We present numerical\nresults for three variants of the MT-TSP: one where intercepting a target only\nrequires coming within a particular distance, another where the agent is a\nvariable-speed Dubins car, and a third where the agent is a redundant robot\narm. We show that IRG-PGLNS and PCG both converge faster than a baseline based\non prior work.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u79fb\u52a8\u76ee\u6807\u65c5\u884c\u5546\u95ee\u9898\uff08MT-TSP\uff09\uff0c\u6d89\u53ca\u8f68\u8ff9\u89c4\u5212\u548c\u4f18\u5316\uff0c\u7279\u522b\u662f\u9488\u5bf9\u79fb\u52a8\u76ee\u6807\u7684\u8def\u5f84\u89c4\u5212\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u5c24\u5176\u662f\u79fb\u52a8\u7269\u4f53\u8f68\u8ff9\u9884\u6d4b\u548c\u8def\u5f84\u89c4\u5212\u65b9\u9762\u3002\u7b97\u6cd5\u6d89\u53ca\u5230\u8f68\u8ff9\u7684\u4f18\u5316\u548c\u6c42\u89e3\uff0c\u56e0\u6b64\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory planning", "moving target", "path planning", "optimization"]}}
{"id": "2509.08757", "pdf": "https://arxiv.org/pdf/2509.08757", "abs": "https://arxiv.org/abs/2509.08757", "authors": ["Michael J. Munje", "Chen Tang", "Shuijing Liu", "Zichao Hu", "Yifeng Zhu", "Jiaxun Cui", "Garrett Warnell", "Joydeep Biswas", "Peter Stone"], "title": "SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation", "categories": ["cs.RO", "cs.CV"], "comment": "Conference on Robot Learning (CoRL) 2025 Project site:\n  https://larg.github.io/socialnav-sub", "summary": "Robot navigation in dynamic, human-centered environments requires\nsocially-compliant decisions grounded in robust scene understanding. Recent\nVision-Language Models (VLMs) exhibit promising capabilities such as object\nrecognition, common-sense reasoning, and contextual understanding-capabilities\nthat align with the nuanced requirements of social robot navigation. However,\nit remains unclear whether VLMs can accurately understand complex social\nnavigation scenes (e.g., inferring the spatial-temporal relations among agents\nand human intentions), which is essential for safe and socially compliant robot\nnavigation. While some recent works have explored the use of VLMs in social\nrobot navigation, no existing work systematically evaluates their ability to\nmeet these necessary conditions. In this paper, we introduce the Social\nNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual Question\nAnswering (VQA) dataset and benchmark designed to evaluate VLMs for scene\nunderstanding in real-world social robot navigation scenarios. SocialNav-SUB\nprovides a unified framework for evaluating VLMs against human and rule-based\nbaselines across VQA tasks requiring spatial, spatiotemporal, and social\nreasoning in social robot navigation. Through experiments with state-of-the-art\nVLMs, we find that while the best-performing VLM achieves an encouraging\nprobability of agreeing with human answers, it still underperforms simpler\nrule-based approach and human consensus baselines, indicating critical gaps in\nsocial scene understanding of current VLMs. Our benchmark sets the stage for\nfurther research on foundation models for social robot navigation, offering a\nframework to explore how VLMs can be tailored to meet real-world social robot\nnavigation needs. An overview of this paper along with the code and data can be\nfound at https://larg.github.io/socialnav-sub .", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Vision-Language Models (VLMs) for scene understanding in social robot navigation. While it doesn't directly address trajectory prediction, navigation inherently involves predicting future states and paths. The use of VLMs connects it to the large language model theme. The 'spatiotemporal reasoning' aspect relates to trajectory prediction, but the primary focus is on scene understanding and VLM evaluation.", "keywords": ["Vision-Language Models", "VLMs", "robot navigation", "scene understanding", "spatiotemporal reasoning", "foundation models"]}}
{"id": "2509.08661", "pdf": "https://arxiv.org/pdf/2509.08661", "abs": "https://arxiv.org/abs/2509.08661", "authors": ["Liangjin Liu", "Haoyang Zheng", "Pei Zhou"], "title": "Skeleton-based sign language recognition using a dual-stream spatio-temporal dynamic graph convolutional network", "categories": ["cs.CV", "cs.AI", "I.2.m; I.2.0"], "comment": "5 pages, 3 figures, ICASSP", "summary": "Isolated Sign Language Recognition (ISLR) is challenged by gestures that are\nmorphologically similar yet semantically distinct, a problem rooted in the\ncomplex interplay between hand shape and motion trajectory. Existing methods,\noften relying on a single reference frame, struggle to resolve this geometric\nambiguity. This paper introduces Dual-SignLanguageNet (DSLNet), a\ndual-reference, dual-stream architecture that decouples and models gesture\nmorphology and trajectory in separate, complementary coordinate systems. Our\napproach utilizes a wrist-centric frame for view-invariant shape analysis and a\nfacial-centric frame for context-aware trajectory modeling. These streams are\nprocessed by specialized networks-a topology-aware graph convolution for shape\nand a Finsler geometry-based encoder for trajectory-and are integrated via a\ngeometry-driven optimal transport fusion mechanism. DSLNet sets a new\nstate-of-the-art, achieving 93.70%, 89.97% and 99.79% accuracy on the\nchallenging WLASL-100, WLASL-300 and LSA64 datasets, respectively, with\nsignificantly fewer parameters than competing models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on sign language recognition using skeleton data and graph convolutional networks. While it involves modeling motion trajectory, which is related to trajectory prediction, it doesn't explicitly connect to trajectory prediction in the context of autonomous driving, robotics, or other common trajectory prediction applications. Also, it doesn't involve large language models. The use of \"trajectory modeling\" contributes to the relevance score, but the lack of connection to standard trajectory prediction tasks or LLMs keeps the score from being higher.", "keywords": ["trajectory modeling", "motion trajectory"]}}
