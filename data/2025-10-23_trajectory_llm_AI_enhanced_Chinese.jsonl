{"id": "2510.18546", "pdf": "https://arxiv.org/pdf/2510.18546", "abs": "https://arxiv.org/abs/2510.18546", "authors": ["Zebin Yang", "Sunjian Zheng", "Tong Xie", "Tianshi Xu", "Bo Yu", "Fan Wang", "Jie Tang", "Shaoshan Liu", "Meng Li"], "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval", "categories": ["cs.RO", "cs.AI"], "comment": "NeurIPS 2025", "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\uff0c\u800c\u76ee\u6807\u5bfc\u822a\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\u7684\u5f62\u5f0f\uff08\u9884\u6d4bagent\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u7684\u8f68\u8ff9\uff09\u3002\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u548c\u4f18\u5316\u3002", "keywords": ["object-goal navigation", "navigation maps", "Large Language Models", "LLMs", "GPT-4", "LLaMA3.2-11b", "zero-shot"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.18054", "pdf": "https://arxiv.org/pdf/2510.18054", "abs": "https://arxiv.org/abs/2510.18054", "authors": ["Ata \u00c7elen", "Marc Pollefeys", "Daniel Barath", "Iro Armeni"], "title": "HouseTour: A Virtual Real Estate A(I)gent", "categories": ["cs.CV", "cs.CL"], "comment": "Published on ICCV 2025", "summary": "We introduce HouseTour, a method for spatially-aware 3D camera trajectory and\nnatural language summary generation from a collection of images depicting an\nexisting 3D space. Unlike existing vision-language models (VLMs), which\nstruggle with geometric reasoning, our approach generates smooth video\ntrajectories via a diffusion process constrained by known camera poses and\nintegrates this information into the VLM for 3D-grounded descriptions. We\nsynthesize the final video using 3D Gaussian splatting to render novel views\nalong the trajectory. To support this task, we present the HouseTour dataset,\nwhich includes over 1,200 house-tour videos with camera poses, 3D\nreconstructions, and real estate descriptions. Experiments demonstrate that\nincorporating 3D camera trajectories into the text generation process improves\nperformance over methods handling each task independently. We evaluate both\nindividual and end-to-end performance, introducing a new joint metric. Our work\nenables automated, professional-quality video creation for real estate and\ntouristic applications without requiring specialized expertise or equipment.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on generating 3D camera trajectories and natural language summaries. It leverages diffusion processes for trajectory generation and integrates this information with VLMs for 3D-grounded descriptions. While the primary focus isn't on traditional trajectory prediction as in pedestrian or vehicle movement, it does involve trajectory generation and utilizes a VLM, thus showing a significant connection to both areas.", "keywords": ["trajectory", "diffusion process", "vision-language models", "VLMs", "3D camera trajectory"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.18060", "pdf": "https://arxiv.org/pdf/2510.18060", "abs": "https://arxiv.org/abs/2510.18060", "authors": ["Wei-Jer Chang", "Akshay Rangesh", "Kevin Joseph", "Matthew Strong", "Masayoshi Tomizuka", "Yihan Hu", "Wei Zhan"], "title": "SPACeR: Self-Play Anchoring with Centralized Reference Models", "categories": ["cs.LG", "cs.AI", "cs.RO", "I.2.9; I.2.6"], "comment": "Project page: https://spacer-ai.github.io/", "summary": "Developing autonomous vehicles (AVs) requires not only safety and efficiency,\nbut also realistic, human-like behaviors that are socially aware and\npredictable. Achieving this requires sim agent policies that are human-like,\nfast, and scalable in multi-agent settings. Recent progress in imitation\nlearning with large diffusion-based or tokenized models has shown that\nbehaviors can be captured directly from human driving data, producing realistic\npolicies. However, these models are computationally expensive, slow during\ninference, and struggle to adapt in reactive, closed-loop scenarios. In\ncontrast, self-play reinforcement learning (RL) scales efficiently and\nnaturally captures multi-agent interactions, but it often relies on heuristics\nand reward shaping, and the resulting policies can diverge from human norms. We\npropose SPACeR, a framework that leverages a pretrained tokenized\nautoregressive motion model as a centralized reference policy to guide\ndecentralized self-play. The reference model provides likelihood rewards and KL\ndivergence, anchoring policies to the human driving distribution while\npreserving RL scalability. Evaluated on the Waymo Sim Agents Challenge, our\nmethod achieves competitive performance with imitation-learned policies while\nbeing up to 10x faster at inference and 50x smaller in parameter size than\nlarge generative models. In addition, we demonstrate in closed-loop ego\nplanning evaluation tasks that our sim agents can effectively measure planner\nquality with fast and scalable traffic simulation, establishing a new paradigm\nfor testing autonomous driving policies.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6a21\u62df\u667a\u80fd\u4f53\u7b56\u7565\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684tokenized\u81ea\u56de\u5f52\u8fd0\u52a8\u6a21\u578b\u4f5c\u4e3a\u96c6\u4e2d\u53c2\u8003\u7b56\u7565\u6765\u6307\u5bfc\u53bb\u4e2d\u5fc3\u5316\u7684\u81ea\u535a\u5f08\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u662f\u63d0\u5230\u4e86\u5927\u578b\u6269\u6563\u6a21\u578b\u548ctokenized\u6a21\u578b\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4e14\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\uff08\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff09\u3002\u8bba\u6587\u8ba8\u8bba\u4e86\u7b56\u7565\u7684\u771f\u5b9e\u6027\u3001\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e0e\u6a21\u4eff\u5b66\u4e60\u7684\u7b56\u7565\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u90fd\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002", "keywords": ["trajectory prediction", "autonomous vehicles", "self-play reinforcement learning", "tokenized autoregressive motion model", "imitation learning", "large diffusion models", "sim agents", "motion model"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.18123", "pdf": "https://arxiv.org/pdf/2510.18123", "abs": "https://arxiv.org/abs/2510.18123", "authors": ["Xiangbo Gao", "Tzu-Hsiang Lin", "Ruojing Song", "Yuheng Wu", "Kuan-Ru Huang", "Zicheng Jin", "Fangzhou Lin", "Shinan Liu", "Zhengzhong Tu"], "title": "SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Collaborative driving systems leverage vehicle-to-everything (V2X)\ncommunication across multiple agents to enhance driving safety and efficiency.\nTraditional V2X systems take raw sensor data, neural features, or perception\nresults as communication media, which face persistent challenges, including\nhigh bandwidth demands, semantic loss, and interoperability issues. Recent\nadvances investigate natural language as a promising medium, which can provide\nsemantic richness, decision-level reasoning, and human-machine interoperability\nat significantly lower bandwidth. Despite great promise, this paradigm shift\nalso introduces new vulnerabilities within language communication, including\nmessage loss, hallucinations, semantic manipulation, and adversarial attacks.\nIn this work, we present the first systematic study of full-stack safety and\nsecurity issues in natural-language-based collaborative driving. Specifically,\nwe develop a comprehensive taxonomy of attack strategies, including connection\ndisruption, relay/replay interference, content spoofing, and multi-connection\nforgery. To mitigate these risks, we introduce an agentic defense pipeline,\nwhich we call SafeCoop, that integrates a semantic firewall,\nlanguage-perception consistency checks, and multi-source consensus, enabled by\nan agentic transformation function for cross-frame spatial alignment. We\nsystematically evaluate SafeCoop in closed-loop CARLA simulation across 32\ncritical scenarios, achieving 69.15% driving score improvement under malicious\nattacks and up to 67.32% F1 score for malicious detection. This study provides\nguidance for advancing research on safe, secure, and trustworthy\nlanguage-driven collaboration in transportation systems. Our project page is\nhttps://xiangbogaobarry.github.io/SafeCoop.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on collaborative driving systems using natural language as a communication medium. While it doesn't explicitly mention trajectory prediction algorithms, the context of collaborative driving inherently involves predicting the future trajectories of vehicles. Furthermore, the use of natural language suggests a connection to large language models, even if it's not directly stated. The mention of semantic understanding and reasoning also points towards LLM capabilities. Therefore, there is a moderate relevance to both trajectory prediction and large language models.", "keywords": ["collaborative driving", "V2X", "natural language", "semantic", "agentic", "driving safety", "transportation systems"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.17948", "pdf": "https://arxiv.org/pdf/2510.17948", "abs": "https://arxiv.org/abs/2510.17948", "authors": ["Christopher A McClurg", "Alan R Wagner"], "title": "Studying the Effects of Robot Intervention on School Shooters in Virtual Reality", "categories": ["cs.RO", "cs.AI", "cs.CY", "cs.HC"], "comment": "Preprint under review for conference publication. 10 pages, 9\n  figures, 3 tables (including 1-page appendix)", "summary": "We advance the understanding of robotic intervention in high-risk scenarios\nby examining their potential to distract and impede a school shooter. To\nevaluate this concept, we conducted a virtual reality study with 150 university\nparticipants role-playing as a school shooter. Within the simulation, an\nautonomous robot predicted the shooter's movements and positioned itself\nstrategically to interfere and distract. The strategy the robot used to\napproach the shooter was manipulated -- either moving directly in front of the\nshooter (aggressive) or maintaining distance (passive) -- and the distraction\nmethod, ranging from no additional cues (low), to siren and lights (medium), to\nsiren, lights, and smoke to impair visibility (high). An aggressive,\nhigh-distraction robot reduced the number of victims by 46.6% relative to a\nno-robot control. This outcome underscores both the potential of robotic\nintervention to enhance safety and the pressing ethical questions surrounding\ntheir use in school environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper describes a virtual reality study where a robot predicts the shooter's movements to intervene. While it involves trajectory prediction (the robot predicting the shooter's movements), it doesn't mention or utilize large language models. The focus is on robotic intervention and the impact of different strategies.", "keywords": ["trajectory prediction", "autonomous robot"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.17899", "pdf": "https://arxiv.org/pdf/2510.17899", "abs": "https://arxiv.org/abs/2510.17899", "authors": ["Floris-Jan Willemsen", "Niki van Stein", "Ben van Werkhoven"], "title": "Automated Algorithm Design for Auto-Tuning Optimizers", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Automatic performance tuning (auto-tuning) is essential for optimizing\nhigh-performance applications, where vast and irregular parameter spaces make\nmanual exploration infeasible. Traditionally, auto-tuning relies on\nwell-established optimization algorithms such as evolutionary algorithms,\nannealing methods, or surrogate model-based optimizers to efficiently find\nnear-optimal configurations. However, designing effective optimizers remains\nchallenging, as no single method performs best across all tuning tasks.\n  In this work, we explore a new paradigm: using large language models (LLMs)\nto automatically generate optimization algorithms tailored to auto-tuning\nproblems. We introduce a framework that prompts LLMs with problem descriptions\nand search-space characteristics results to produce specialized optimization\nstrategies, which are iteratively examined and improved.\n  These generated algorithms are evaluated on four real-world auto-tuning\napplications across six hardware platforms and compared against the\nstate-of-the-art in optimization algorithms of two contemporary auto-tuning\nframeworks. The evaluation demonstrates that providing additional application-\nand search space-specific information in the generation stage results in an\naverage performance improvement of 30.7\\% and 14.6\\%, respectively. In\naddition, our results show that LLM-generated optimizers can rival, and in\nvarious cases outperform, existing human-designed algorithms, with our\nbest-performing generated optimization algorithms achieving, on average, 72.4\\%\nimprovement over state-of-the-art optimizers for auto-tuning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using large language models (LLMs) to automatically design optimization algorithms for auto-tuning. While it doesn't directly deal with trajectory prediction, the use of LLMs is a key aspect, and optimization algorithms are relevant to trajectory prediction tasks, albeit indirectly. The connection is not strong, but the use of LLMs justifies a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "optimization algorithms", "auto-tuning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.18034", "pdf": "https://arxiv.org/pdf/2510.18034", "abs": "https://arxiv.org/abs/2510.18034", "authors": ["Roberto Brusnicki", "David Pop", "Yuan Gao", "Mattia Piccinini", "Johannes Betz"], "title": "SAVANT: Semantic Analysis with Vision-Augmented Anomaly deTection", "categories": ["cs.CV", "cs.AI", "cs.RO", "I.2.9; I.4.8"], "comment": "8 pages, 5 figures", "summary": "Autonomous driving systems remain critically vulnerable to the long-tail of\nrare, out-of-distribution scenarios with semantic anomalies. While Vision\nLanguage Models (VLMs) offer promising reasoning capabilities, naive prompting\napproaches yield unreliable performance and depend on expensive proprietary\nmodels, limiting practical deployment. We introduce SAVANT (Semantic Analysis\nwith Vision-Augmented Anomaly deTection), a structured reasoning framework that\nachieves high accuracy and recall in detecting anomalous driving scenarios from\ninput images through layered scene analysis and a two-phase pipeline:\nstructured scene description extraction followed by multi-modal evaluation. Our\napproach transforms VLM reasoning from ad-hoc prompting to systematic analysis\nacross four semantic layers: Street, Infrastructure, Movable Objects, and\nEnvironment. SAVANT achieves 89.6% recall and 88.0% accuracy on real-world\ndriving scenarios, significantly outperforming unstructured baselines. More\nimportantly, we demonstrate that our structured framework enables a fine-tuned\n7B parameter open-source model (Qwen2.5VL) to achieve 90.8% recall and 93.8%\naccuracy - surpassing all models evaluated while enabling local deployment at\nnear-zero cost. By automatically labeling over 9,640 real-world images with\nhigh accuracy, SAVANT addresses the critical data scarcity problem in anomaly\ndetection and provides a practical path toward reliable, accessible semantic\nmonitoring for autonomous systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on anomaly detection in autonomous driving scenarios using Vision Language Models (VLMs). While not directly dealing with trajectory prediction, the context of autonomous driving makes it potentially relevant. It leverages large language models (Qwen2.5VL) for scene analysis and reasoning, increasing its relevance. The connection to trajectory prediction is indirect, as anomaly detection can inform trajectory planning, but it is not the primary focus.", "keywords": ["Vision Language Models", "VLMs", "autonomous driving", "anomaly detection", "large language models", "Qwen2.5VL"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.18766", "pdf": "https://arxiv.org/pdf/2510.18766", "abs": "https://arxiv.org/abs/2510.18766", "authors": ["Alexander Krawciw", "Sven Lilge", "Luka Antonyshyn", "Timothy D. Barfoot"], "title": "Sharing the Load: Distributed Model-Predictive Control for Precise Multi-Rover Cargo Transport", "categories": ["cs.RO"], "comment": "8 pages, 4 figures", "summary": "For autonomous cargo transportation, teams of mobile robots can provide more\noperational flexibility than a single large robot. In these scenarios,\nprecision in both inter-vehicle distance and path tracking is key. With this\nmotivation, we develop a distributed model-predictive controller (MPC) for\nmulti-vehicle cargo operations that builds on the precise path-tracking of\nlidar teach and repeat. To carry cargo, a following vehicle must maintain a\nEuclidean distance offset from a lead vehicle regardless of the path curvature.\nOur approach uses a shared map to localize the robots relative to each other\nwithout GNSS or direct observations. We compare our approach to a centralized\nMPC and a baseline approach that directly measures the inter-vehicle distance.\nThe distributed MPC shows equivalent nominal performance to the more complex\ncentralized MPC. Using a direct measurement of the relative distance between\nthe leader and follower shows improved tracking performance in close-range\nscenarios but struggles with long-range offsets. The operational flexibility\nprovided by distributing the computation makes it well suited for real\ndeployments. We evaluate four types of convoyed path trackers with over 10 km\nof driving in a coupled convoy. With convoys of two and three rovers, the\nproposed distributed MPC method works in real-time to allow map-based convoying\nto maintain maximum spacing within 20 cm of the target in various conditions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on distributed model-predictive control (MPC) for multi-vehicle cargo transport. While it doesn't directly involve large language models, it is relevant to trajectory prediction and path tracking within the context of autonomous robots and path planning. The core problem addresses precise path tracking and maintaining inter-vehicle distances, which falls under trajectory prediction. However, the absence of any mention or application of large language models lowers the relevance score.", "keywords": ["trajectory prediction", "path tracking", "model-predictive control", "autonomous robots", "path planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.18187", "pdf": "https://arxiv.org/pdf/2510.18187", "abs": "https://arxiv.org/abs/2510.18187", "authors": ["Fatima AlGhamdi", "Omar Alharbi", "Abdullah Aldwyish", "Raied Aljadaany", "Muhammad Kamran J Khan", "Huda Alamri"], "title": "VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 3 figures", "summary": "Detecting anomalies in crowded scenes is challenging due to severe\ninter-person occlusions and highly dynamic, context-dependent motion patterns.\nExisting approaches often struggle to adapt to varying crowd densities and lack\ninterpretable anomaly indicators. To address these limitations, we introduce\nVelocityNet, a dual-pipeline framework that combines head detection and dense\noptical flow to extract person-specific velocities. Hierarchical clustering\ncategorizes these velocities into semantic motion classes (halt, slow, normal,\nand fast), and a percentile-based anomaly scoring system measures deviations\nfrom learned normal patterns. Experiments demonstrate the effectiveness of our\nframework in real-time detection of diverse anomalous motion patterns within\ndensely crowded environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4eba\u7fa4\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u5206\u6790\u4eba\u7fa4\u7684\u901f\u5ea6\u6765\u8bc6\u522b\u5f02\u5e38\u884c\u4e3a\u3002\u867d\u7136\u6d89\u53ca\u5230\u901f\u5ea6\u5206\u6790\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\uff0c\u4f46\u5e76\u672a\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u5efa\u6a21\u6216\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u3002\u5173\u952e\u8bcd\u8868\u660e\uff0c\u5b83\u5173\u6ce8\u7684\u662f\u4eba\u7fa4\u8fd0\u52a8\u6a21\u5f0f\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u901f\u5ea6\u5206\u6790\u90e8\u5206\u76f8\u5173\u3002", "keywords": ["velocity", "motion patterns", "anomaly detection", "crowd"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.18442", "pdf": "https://arxiv.org/pdf/2510.18442", "abs": "https://arxiv.org/abs/2510.18442", "authors": ["Ziwei Deng", "Mian Deng", "Chenjing Liang", "Zeming Gao", "Chennan Ma", "Chenxing Lin", "Haipeng Zhang", "Songzhu Mei", "Cheng Wang", "Siqi Shen"], "title": "PlanU: Large Language Model Decision Making through Planning under Uncertainty", "categories": ["cs.AI"], "comment": "38 pages, 19 figures, NeurIPS 2025 Accepted", "summary": "Large Language Models (LLMs) are increasingly being explored across a range\nof decision-making tasks. However, LLMs sometimes struggle with decision-making\ntasks under uncertainty that are relatively easy for humans, such as planning\nactions in stochastic environments. The adoption of LLMs for decision-making is\nimpeded by uncertainty challenges, such as LLM uncertainty and environmental\nuncertainty. LLM uncertainty arises from the stochastic sampling process\ninherent to LLMs. Most LLM-based Decision-Making (LDM) approaches address LLM\nuncertainty through multiple reasoning chains or search trees. However, these\napproaches overlook environmental uncertainty, which leads to poor performance\nin environments with stochastic state transitions. Some recent LDM approaches\ndeal with uncertainty by forecasting the probability of unknown variables.\nHowever, they are not designed for multi-step decision-making tasks that\nrequire interaction with the environment. To address uncertainty in LLM\ndecision-making, we introduce PlanU, an LLM-based planning method that captures\nuncertainty within Monte Carlo Tree Search (MCTS). PlanU models the return of\neach node in the MCTS as a quantile distribution, which uses a set of quantiles\nto represent the return distribution. To balance exploration and exploitation\nduring tree search, PlanU introduces an Upper Confidence Bounds with Curiosity\n(UCC) score which estimates the uncertainty of MCTS nodes. Through extensive\nexperiments, we demonstrate the effectiveness of PlanU in LLM-based\ndecision-making tasks under uncertainty.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on decision-making under uncertainty using Large Language Models (LLMs). While it doesn't directly address trajectory prediction, the concept of planning actions in stochastic environments is related to path planning and decision-making for moving objects, which are relevant to trajectory prediction. The core focus is on LLMs and decision-making, so the relevance is moderate.", "keywords": ["Large Language Models", "LLMs", "decision-making", "uncertainty", "planning", "stochastic environments", "Monte Carlo Tree Search", "MCTS"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
