{"id": "2506.17590", "pdf": "https://arxiv.org/pdf/2506.17590", "abs": "https://arxiv.org/abs/2506.17590", "authors": ["Mihir Godbole", "Xiangbo Gao", "Zhengzhong Tu"], "title": "DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "19 pages, 5 figures, Preprint under review. Code available at:\n  https://github.com/taco-group/DRAMA-X", "summary": "Understanding the short-term motion of vulnerable road users (VRUs) like\npedestrians and cyclists is critical for safe autonomous driving, especially in\nurban scenarios with ambiguous or high-risk behaviors. While vision-language\nmodels (VLMs) have enabled open-vocabulary perception, their utility for\nfine-grained intent reasoning remains underexplored. Notably, no existing\nbenchmark evaluates multi-class intent prediction in safety-critical\nsituations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark\nconstructed from the DRAMA dataset via an automated annotation pipeline.\nDRAMA-X contains 5,686 accident-prone frames labeled with object bounding\nboxes, a nine-class directional intent taxonomy, binary risk scores,\nexpert-generated action suggestions for the ego vehicle, and descriptive motion\nsummaries. These annotations enable a structured evaluation of four\ninterrelated tasks central to autonomous decision-making: object detection,\nintent prediction, risk assessment, and action suggestion. As a reference\nbaseline, we propose SGG-Intent, a lightweight, training-free framework that\nmirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene\ngraph from visual input using VLM-backed detectors, infers intent, assesses\nrisk, and recommends an action using a compositional reasoning stage powered by\na large language model. We evaluate a range of recent VLMs, comparing\nperformance across all four DRAMA-X tasks. Our experiments demonstrate that\nscene-graph-based reasoning enhances intent prediction and risk assessment,\nespecially when contextual cues are explicitly modeled.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant because it focuses on intent prediction of vulnerable road users, which is directly related to trajectory prediction (as intent informs future trajectory). It also explicitly utilizes a large language model for reasoning and action suggestion within the driving context. The paper bridges trajectory prediction and large language models.", "keywords": ["intent prediction", "trajectory prediction", "large language model", "driving", "autonomous driving", "risk assessment", "action suggestion", "vision-language models", "VLMs"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17333", "pdf": "https://arxiv.org/pdf/2506.17333", "abs": "https://arxiv.org/abs/2506.17333", "authors": ["Jaime A. Berkovich", "Noah S. David", "Markus J. Buehler"], "title": "AutomataGPT: Forecasting and Ruleset Inference for Two-Dimensional Cellular Automata", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "q-bio.QM"], "comment": null, "summary": "Cellular automata (CA) provide a minimal formalism for investigating how\nsimple local interactions generate rich spatiotemporal behavior in domains as\ndiverse as traffic flow, ecology, tissue morphogenesis and crystal growth.\nHowever, automatically discovering the local update rules for a given\nphenomenon and using them for quantitative prediction remains challenging. Here\nwe present AutomataGPT, a decoder-only transformer pretrained on around 1\nmillion simulated trajectories that span 100 distinct two-dimensional binary\ndeterministic CA rules on toroidal grids. When evaluated on previously unseen\nrules drawn from the same CA family, AutomataGPT attains 98.5% perfect one-step\nforecasts and reconstructs the governing update rule with up to 96% functional\n(application) accuracy and 82% exact rule-matrix match. These results\ndemonstrate that large-scale pretraining over wider regions of rule space\nyields substantial generalization in both the forward (state forecasting) and\ninverse (rule inference) problems, without hand-crafted priors. By showing that\ntransformer models can faithfully infer and execute CA dynamics from data\nalone, our work lays the groundwork for abstracting real-world dynamical\nphenomena into data-efficient CA surrogates, opening avenues in biology, tissue\nengineering, physics and AI-driven scientific discovery.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on forecasting the evolution of cellular automata using a transformer-based model pretrained on a large dataset of CA trajectories. While not directly related to typical trajectory prediction tasks like pedestrian or vehicle movement, it shares the core concept of predicting future states based on past observations. The use of a large, pretrained transformer model connects it to the theme of large language models, although in a more general 'large model' sense. The paper predicts the next state of a system, which can be seen as a form of trajectory prediction in a discrete state space.", "keywords": ["forecasting", "transformer", "pretrained", "trajectories", "large-scale pretraining"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17705", "pdf": "https://arxiv.org/pdf/2506.17705", "abs": "https://arxiv.org/abs/2506.17705", "authors": ["Bo Pan", "Yang Chen", "Yingwei Pan", "Ting Yao", "Wei Chen", "Tao Mei"], "title": "DreamJourney: Perpetual View Generation with Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Perpetual view generation aims to synthesize a long-term video corresponding\nto an arbitrary camera trajectory solely from a single input image. Recent\nmethods commonly utilize a pre-trained text-to-image diffusion model to\nsynthesize new content of previously unseen regions along camera movement.\nHowever, the underlying 2D diffusion model lacks 3D awareness and results in\ndistorted artifacts. Moreover, they are limited to generating views of static\n3D scenes, neglecting to capture object movements within the dynamic 4D world.\nTo alleviate these issues, we present DreamJourney, a two-stage framework that\nleverages the world simulation capacity of video diffusion models to trigger a\nnew perpetual scene view generation task with both camera movements and object\ndynamics. Specifically, in stage I, DreamJourney first lifts the input image to\n3D point cloud and renders a sequence of partial images from a specific camera\ntrajectory. A video diffusion model is then utilized as generative prior to\ncomplete the missing regions and enhance visual coherence across the sequence,\nproducing a cross-view consistent video adheres to the 3D scene and camera\ntrajectory. Meanwhile, we introduce two simple yet effective strategies (early\nstopping and view padding) to further stabilize the generation process and\nimprove visual quality. Next, in stage II, DreamJourney leverages a multimodal\nlarge language model to produce a text prompt describing object movements in\ncurrent view, and uses video diffusion model to animate current view with\nobject movements. Stage I and II are repeated recurrently, enabling perpetual\ndynamic scene view generation. Extensive experiments demonstrate the\nsuperiority of our DreamJourney over state-of-the-art methods both\nquantitatively and qualitatively. Our project page:\nhttps://dream-journey.vercel.app.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u4e0e\u76f8\u673a\u8f68\u8ff9\u76f8\u5bf9\u5e94\u7684\u957f\u65f6\u89c6\u9891\uff0c\u5e76\u4e14\u5229\u7528\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u63cf\u8ff0\u7269\u4f53\u8fd0\u52a8\u3002 \u867d\u7136\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u89c6\u56fe\u751f\u6210\uff0c\u4f46\u5176\u4e2d\u76f8\u673a\u8f68\u8ff9\u548c\u7269\u4f53\u8fd0\u52a8\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4e14\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory", "video diffusion models", "large language model", "camera trajectory", "object movements"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17631", "pdf": "https://arxiv.org/pdf/2506.17631", "abs": "https://arxiv.org/abs/2506.17631", "authors": ["Zesen Wang", "Yonggang Li", "Lijuan Lan"], "title": "LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series forecasting aims to model temporal dependencies among variables\nfor future state inference, holding significant importance and widespread\napplications in real-world scenarios. Although deep learning-based methods have\nachieved remarkable progress, they still exhibit suboptimal performance in\nlong-term forecasting and data-scarce scenarios. Recent research demonstrates\nthat large language models (LLMs) achieve promising performance in time series\nforecasting. However, we find existing LLM-based methods still have\nshortcomings: (1) the absence of a unified paradigm for textual prompt\nformulation and (2) the neglect of modality discrepancies between textual\nprompts and time series. To address this, we propose LLM-Prompt, an LLM-based\ntime series forecasting framework integrating multi-prompt information and\ncross-modal semantic alignment. Specifically, we first construct a unified\ntextual prompt paradigm containing learnable soft prompts and textualized hard\nprompts. Second, to enhance LLMs' comprehensive understanding of the\nforecasting task, we design a semantic space embedding and cross-modal\nalignment module to achieve cross-modal fusion of temporal and textual\ninformation. Finally, the transformed time series from the LLMs are projected\nto obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3\ncarbon emission datasets demonstrate that LLM-Prompt is a powerful framework\nfor time series forecasting.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on time series forecasting using Large Language Models (LLMs). While not directly related to trajectory prediction, time series forecasting is a related field. The paper explicitly uses LLMs, which makes it relevant to the prompt. The connection to trajectory prediction is weaker, but the general forecasting aspect warrants a higher relevance score.", "keywords": ["Large Language Models", "LLMs", "time series forecasting", "forecasting"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17328", "pdf": "https://arxiv.org/pdf/2506.17328", "abs": "https://arxiv.org/abs/2506.17328", "authors": ["Yufan Liu", "Yi Wu", "Gweneth Ge", "Haoliang Cheng", "Rui Liu"], "title": "Reflective VLM Planning for Dual-Arm Desktop Cleaning: Bridging Open-Vocabulary Perception and Precise Manipulation", "categories": ["cs.RO"], "comment": null, "summary": "Desktop cleaning demands open-vocabulary recognition and precise manipulation\nfor heterogeneous debris. We propose a hierarchical framework integrating\nreflective Vision-Language Model (VLM) planning with dual-arm execution via\nstructured scene representation. Grounded-SAM2 facilitates open-vocabulary\ndetection, while a memory-augmented VLM generates, critiques, and revises\nmanipulation sequences. These sequences are converted into parametric\ntrajectories for five primitives executed by coordinated Franka arms. Evaluated\nin simulated scenarios, our system achieving 87.2% task completion, a 28.8%\nimprovement over static VLM and 36.2% over single-arm baselines. Structured\nmemory integration proves crucial for robust, generalizable manipulation while\nmaintaining real-time control performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses a Vision-Language Model (VLM) for planning manipulation sequences for a dual-arm robot. While the core application is desktop cleaning, the generation of manipulation sequences and their conversion into parametric trajectories makes it relevant to trajectory prediction. The use of a VLM also brings in the large language model aspect, though indirectly. The connection to trajectory prediction is somewhat weak, as the focus is more on planning than precise trajectory prediction algorithms.", "keywords": ["Vision-Language Model", "VLM", "manipulation sequences", "trajectories", "Grounded-SAM2"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17462", "pdf": "https://arxiv.org/pdf/2506.17462", "abs": "https://arxiv.org/abs/2506.17462", "authors": ["Bernard Lange", "Anil Yildiz", "Mansur Arief", "Shehryar Khattak", "Mykel Kochenderfer", "Georgios Georgakis"], "title": "General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Developing general-purpose navigation policies for unknown environments\nremains a core challenge in robotics. Most existing systems rely on\ntask-specific neural networks and fixed data flows, limiting generalizability.\nLarge Vision-Language Models (LVLMs) offer a promising alternative by embedding\nhuman-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot\nintegrations typically depend on pre-mapped spaces, hard-coded representations,\nand myopic exploration. We introduce the Agentic Robotic Navigation\nArchitecture (ARNA), a general-purpose navigation framework that equips an\nLVLM-based agent with a library of perception, reasoning, and navigation tools\navailable within modern robotic stacks. At runtime, the agent autonomously\ndefines and executes task-specific workflows that iteratively query the robotic\nmodules, reason over multimodal inputs, and select appropriate navigation\nactions. This approach enables robust navigation and reasoning in previously\nunmapped environments, providing a new perspective on robotic stack design.\nEvaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves\nstate-of-the-art performance, demonstrating effective exploration, navigation,\nand embodied question answering without relying on handcrafted plans, fixed\ninput representations, or pre-existing maps.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u8fdb\u884c\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5bfc\u822a\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8def\u5f84\u89c4\u5212\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\u3002\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86LVLMs\u8fdb\u884c\u611f\u77e5\u3001\u63a8\u7406\u548c\u884c\u52a8\uff0c\u8fd9\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Vision-Language Models", "LVLMs", "robotic navigation", "navigation", "reasoning", "perception"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17486", "pdf": "https://arxiv.org/pdf/2506.17486", "abs": "https://arxiv.org/abs/2506.17486", "authors": ["Zachary Ravichandran", "Ignacio Hounie", "Fernando Cladera", "Alejandro Ribeiro", "George J. Pappas", "Vijay Kumar"], "title": "Distilling On-device Language Models for Robot Planning with Minimal Human Intervention", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) provide robots with powerful contextual\nreasoning abilities and a natural human interface. Yet, current LLM-enabled\nrobots typically depend on cloud-hosted models, limiting their usability in\nenvironments with unreliable communication infrastructure, such as outdoor or\nindustrial settings. We present PRISM, a framework for distilling small\nlanguage model (SLM)-enabled robot planners that run on-device with minimal\nhuman supervision. Starting from an existing LLM-enabled planner, PRISM\nautomatically synthesizes diverse tasks and environments, elicits plans from\nthe LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in\nreplacement of the source model. We apply PRISM to three LLM-enabled planners\nfor mapping and exploration, manipulation, and household assistance, and we\ndemonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of\nGPT-4o's performance to over 93% - using only synthetic data. We further\ndemonstrate that the distilled planners generalize across heterogeneous robotic\nplatforms (ground and aerial) and diverse environments (indoor and outdoor). We\nrelease all software, trained models, and datasets at\nhttps://zacravichandran.github.io/PRISM.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u673a\u5668\u4eba\u89c4\u5212\uff0c\u5e76\u5c06\u5927\u578b\u6a21\u578b\u63d0\u70bc\u4e3a\u5c0f\u578b\u6a21\u578b\u4ee5\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u673a\u5668\u4eba\u89c4\u5212\u901a\u5e38\u4f1a\u6d89\u53ca\u5230\u8def\u5f84\u89c4\u5212\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u8fd9\u4e9b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u8bba\u6587\u4e2d\u660e\u786e\u63d0\u5230\u4e86LLMs\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002", "keywords": ["Large language models", "LLMs", "robot planning", "distilling language model", "foundation models", "Llama-3.2-3B", "GPT-4o"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17508", "pdf": "https://arxiv.org/pdf/2506.17508", "abs": "https://arxiv.org/abs/2506.17508", "authors": ["Sajratul Y. Rubaiat", "Syed N. Sakib", "Hasan M. Jamil"], "title": "Mapping the Evolution of Research Contributions using KnoVo", "categories": ["cs.DL", "cs.AI", "cs.DB", "cs.ET", "cs.IR"], "comment": null, "summary": "This paper presents KnoVo (Knowledge Evolution), an intelligent framework\ndesigned for quantifying and analyzing the evolution of research novelty in the\nscientific literature. Moving beyond traditional citation analysis, which\nprimarily measures impact, KnoVo determines a paper's novelty relative to both\nprior and subsequent work within its multilayered citation network. Given a\ntarget paper's abstract, KnoVo utilizes Large Language Models (LLMs) to\ndynamically extract dimensions of comparison (e.g., methodology, application,\ndataset). The target paper is then compared to related publications along these\nsame extracted dimensions. This comparative analysis, inspired by tournament\nselection, yields quantitative novelty scores reflecting the relative\nimprovement, equivalence, or inferiority of the target paper in specific\naspects. By aggregating these scores and visualizing their progression, for\ninstance, through dynamic evolution graphs and comparative radar charts, KnoVo\nfacilitates researchers not only to assess originality and identify similar\nwork, but also to track knowledge evolution along specific research dimensions,\nuncover research gaps, and explore cross-disciplinary connections. We\ndemonstrate these capabilities through a detailed analysis of 20 diverse papers\nfrom multiple scientific fields and report on the performance of various\nopen-source LLMs within the KnoVo framework.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses Large Language Models (LLMs) to analyze research novelty by comparing papers along different dimensions extracted from their abstracts. While it doesn't directly address trajectory prediction, it leverages LLMs for analyzing scientific literature which could potentially include papers on trajectory prediction. Thus, it is moderately related to LLMs.", "keywords": ["Large Language Models", "LLMs"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17697", "pdf": "https://arxiv.org/pdf/2506.17697", "abs": "https://arxiv.org/abs/2506.17697", "authors": ["Bohan Tang", "Dezhao Luo", "Jingxuan Chen", "Shaogang Gong", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Beyond Syntax: Action Semantics Learning for App Agents", "categories": ["cs.AI"], "comment": null, "summary": "The advent of Large Language Models (LLMs) enables the rise of App agents\nthat interpret user intent and operate smartphone Apps through actions such as\nclicking and scrolling. While prompt-based solutions with closed LLM APIs show\npromising ability, they incur heavy compute costs and external API dependency.\nFine-tuning smaller open-source LLMs solves these limitations. However, current\nfine-tuning methods use a syntax learning paradigm that forces agents to\nreproduce exactly the ground truth action strings, leading to\nout-of-distribution (OOD) vulnerability. To fill this gap, we propose Action\nSemantics Learning (ASL), a novel learning framework, where the learning\nobjective is capturing the semantics of the ground truth actions. Specifically,\ninspired by the programming language theory, we define the action semantics for\nApp agents as the state transition induced by the action in the user interface.\nWith this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a\nsemantic reward to train the App agents in generating actions aligned with the\nsemantics of ground truth actions, even when the syntactic forms differ. To\nsupport the effectiveness of ASL, we theoretically demonstrate the superior\nrobustness of ASL for the OOD problem compared with the existing syntax\nlearning paradigm. Extensive experiments on offline and online smartphone App\noperation benchmarks show that ASL significantly improves the accuracy and\ngeneralisation of App agents over existing methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using large language models for app agents that perform actions like clicking and scrolling. While it doesn't directly address trajectory prediction, the concept of action execution and state transition can be loosely related to predicting future states or actions in a trajectory. The core focus is on improving the robustness and generalization of LLM-powered agents, which is relevant to the broader field of large language models.", "keywords": ["Large Language Models", "LLMs", "App agents", "action semantics"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17811", "pdf": "https://arxiv.org/pdf/2506.17811", "abs": "https://arxiv.org/abs/2506.17811", "authors": ["Jacky Kwok", "Christopher Agia", "Rohan Sinha", "Matt Foutter", "Shulu Li", "Ion Stoica", "Azalia Mirhoseini", "Marco Pavone"], "title": "RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities\nin visuomotor control, yet ensuring their robustness in unstructured real-world\nenvironments remains a persistent challenge. In this paper, we investigate\ntest-time scaling through the lens of sampling and verification as means to\nenhance the robustness and generalization of VLAs. We first demonstrate that\nthe relationship between action error and the number of generated samples\nfollows an exponentiated power law across a range of VLAs, indicating the\nexistence of inference-time scaling laws. Building on these insights, we\nintroduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,\nRoboMonkey samples a small set of actions from a VLA, applies Gaussian\nperturbation and majority voting to construct an action proposal distribution,\nand then uses a Vision Language Model (VLM)-based verifier to select the\noptimal action. We propose a synthetic data generation pipeline for training\nsuch VLM-based action verifiers, and demonstrate that scaling the synthetic\ndataset consistently improves verification and downstream accuracy. Through\nextensive simulated and hardware experiments, we show that pairing existing\nVLAs with RoboMonkey yields significant performance gains, achieving a 25%\nabsolute improvement on out-of-distribution tasks and 8% on in-distribution\ntasks. Additionally, when adapting to new robot setups, we show that\nfine-tuning both VLAs and action verifiers yields a 7% performance increase\ncompared to fine-tuning VLAs alone.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Vision-Language-Action (VLA) models and uses a Vision Language Model (VLM) for action verification. While not directly about trajectory prediction, the \"action\" component of VLAs can relate to controlling agents that move in trajectories. The use of a VLM connects it to large language models. Therefore, there is some relevance but not a strong one.", "keywords": ["Vision-Language-Action Models", "Large Language Model", "VLM", "action"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17784", "pdf": "https://arxiv.org/pdf/2506.17784", "abs": "https://arxiv.org/abs/2506.17784", "authors": ["Song Wang", "Zhen Tan", "Zihan Chen", "Shuang Zhou", "Tianlong Chen", "Jundong Li"], "title": "AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction", "categories": ["cs.AI"], "comment": null, "summary": "Recent progress in large language model (LLM)-based multi-agent collaboration\nhighlights the power of structured communication in enabling collective\nintelligence. However, existing methods largely rely on static or graph-based\ninter-agent topologies, lacking the potential adaptability and flexibility in\ncommunication. In this work, we propose a new framework that rethinks\nmulti-agent coordination through a sequential structure rather than a graph\nstructure, offering a significantly larger topology space for multi-agent\ncommunication. Our method focuses on two key directions: (1) Next-Agent\nPrediction, which selects the most suitable agent role at each step, and (2)\nNext-Context Selection (NCS), which enables each agent to selectively access\nrelevant information from any previous step. Together, these components\nconstruct task-adaptive communication pipelines that support both role\nflexibility and global information flow. Extensive evaluations across multiple\nbenchmarks demonstrate that our approach achieves superior performance while\nsubstantially reducing communication overhead.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6458\u8981\u63d0\u5230\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534f\u4f5c\u6846\u67b6\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46next-agent prediction\u53ef\u80fd\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["large language model", "LLM", "multi-agent collaboration", "next-agent prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17545", "pdf": "https://arxiv.org/pdf/2506.17545", "abs": "https://arxiv.org/abs/2506.17545", "authors": ["Zhihao Yuan", "Shuyi Jiang", "Chun-Mei Feng", "Yaolun Zhang", "Shuguang Cui", "Zhen Li", "Na Zhao"], "title": "Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations", "categories": ["cs.CV"], "comment": null, "summary": "Currently, utilizing large language models to understand the 3D world is\nbecoming popular. Yet existing 3D-aware LLMs act as black boxes: they output\nbounding boxes or textual answers without revealing how those decisions are\nmade, and they still rely on pre-trained 3D detectors to supply object\nproposals. We introduce Scene-R1, a video-grounded framework that learns to\nreason about 3D scenes without any point-wise 3D instance supervision by\npairing reinforcement-learning-driven reasoning with a two-stage grounding\npipeline. In the temporal grounding stage, we explicitly reason about the video\nand select the video snippets most relevant to an open-ended query. In the\nsubsequent image grounding stage, we analyze the image and predict the 2D\nbounding box. After that, we track the object using SAM2 to produce\npixel-accurate masks in RGB frames, and project them back into 3D, thereby\neliminating the need for 3D detector-based proposals while capturing fine\ngeometry and material cues. Scene-R1 can also adapt to the 3D visual question\nanswering task to answer free-form questions directly from video. Our training\npipeline only needs task-level 2D boxes or textual labels without dense 3D\npoint-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on\nmultiple datasets, while delivering transparent, step-by-step rationales. These\nresults show that reinforcement-learning-based reasoning combined with RGB-D\nvideo alone offers a practical, annotation-efficient route to trustworthy 3D\nscene understanding.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c3D\u573a\u666f\u7406\u89e3\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4f7f\u7528\u4e86\u89c6\u9891\u6570\u636e\uff0c\u6d89\u53ca\u5230\u7269\u4f53\u8ddf\u8e2a\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u8bba\u6587\u660e\u786e\u63d0\u5230\u4e86large language models\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002", "keywords": ["large language models", "3D scene understanding", "video", "object tracking"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17960", "pdf": "https://arxiv.org/pdf/2506.17960", "abs": "https://arxiv.org/abs/2506.17960", "authors": ["Jiaming Wang", "Diwen Liu", "Jizhuo Chen", "Jiaxuan Da", "Nuowen Qian", "Tram Minh Man", "Harold Soh"], "title": "GeNIE: A Generalizable Navigation System for In-the-Wild Environments", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, 5 figures. Jiaming Wang, Diwen Liu, and Jizhuo Chen\n  contributed equally", "summary": "Reliable navigation in unstructured, real-world environments remains a\nsignificant challenge for embodied agents, especially when operating across\ndiverse terrains, weather conditions, and sensor configurations. In this paper,\nwe introduce GeNIE (Generalizable Navigation System for In-the-Wild\nEnvironments), a robust navigation framework designed for global deployment.\nGeNIE integrates a generalizable traversability prediction model built on SAM2\nwith a novel path fusion strategy that enhances planning stability in noisy and\nambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at\nICRA 2025, where it was evaluated across six countries spanning three\ncontinents. GeNIE took first place and achieved 79% of the maximum possible\nscore, outperforming the second-best team by 17%, and completed the entire\ncompetition without a single human intervention. These results set a new\nbenchmark for robust, generalizable outdoor robot navigation. We will release\nthe codebase, pretrained model weights, and newly curated datasets to support\nfuture research in real-world navigation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u6d89\u53ca\u5230\u8def\u5f84\u89c4\u5212\u548c\u53ef\u901a\u884c\u6027\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86SAM2\uff0c\u867d\u7136\u6ca1\u6709\u660e\u786e\u8bf4\u660e\u662f\u5426\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6697\u793a\u4e86\u5176\u53ef\u80fd\u4f7f\u7528\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u5177\u6709\u4e2d\u7b49\u7a0b\u5ea6\u7684\u76f8\u5173\u6027\u3002", "keywords": ["navigation", "path planning", "traversability prediction", "SAM2"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.18088", "pdf": "https://arxiv.org/pdf/2506.18088", "abs": "https://arxiv.org/abs/2506.18088", "authors": ["Tianxing Chen", "Zanxin Chen", "Baijun Chen", "Zijian Cai", "Yibin Liu", "Qiwei Liang", "Zixuan Li", "Xianliang Lin", "Yiheng Ge", "Zhenyu Gu", "Weiliang Deng", "Yubin Guo", "Tian Nian", "Xuanbing Xie", "Qiangyu Chen", "Kailun Su", "Tianling Xu", "Guodong Liu", "Mengkang Hu", "Huan-ang Gao", "Kaixuan Wang", "Zhixuan Liang", "Yusen Qin", "Xiaokang Yang", "Ping Luo", "Yao Mu"], "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "comment": "Project Page: https://robotwin-platform.github.io/", "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses Multimodal Large Language Models (MLLMs) to generate task-level execution code for robotic manipulation, which includes generating trajectories. While the primary focus is on robotic manipulation and data generation, the use of LLMs for trajectory generation and the mention of trajectories in the abstract indicates some relevance to both trajectory prediction and large language models.", "keywords": ["Large Language Models", "MLLMs", "trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.18178", "pdf": "https://arxiv.org/pdf/2506.18178", "abs": "https://arxiv.org/abs/2506.18178", "authors": ["Min Deng", "Bo Fu", "Lingyao Li", "Xi Wang"], "title": "Integrating LLMs and Digital Twins for Adaptive Multi-Robot Task Allocation in Construction", "categories": ["cs.RO"], "comment": null, "summary": "Multi-robot systems are emerging as a promising solution to the growing\ndemand for productivity, safety, and adaptability across industrial sectors.\nHowever, effectively coordinating multiple robots in dynamic and uncertain\nenvironments, such as construction sites, remains a challenge, particularly due\nto unpredictable factors like material delays, unexpected site conditions, and\nweather-induced disruptions. To address these challenges, this study proposes\nan adaptive task allocation framework that strategically leverages the\nsynergistic potential of Digital Twins, Integer Programming (IP), and Large\nLanguage Models (LLMs). The multi-robot task allocation problem is formally\ndefined and solved using an IP model that accounts for task dependencies, robot\nheterogeneity, scheduling constraints, and re-planning requirements. A\nmechanism for narrative-driven schedule adaptation is introduced, in which\nunstructured natural language inputs are interpreted by an LLM, and\noptimization constraints are autonomously updated, enabling human-in-the-loop\nflexibility without manual coding. A digital twin-based system has been\ndeveloped to enable real-time synchronization between physical operations and\ntheir digital representations. This closed-loop feedback framework ensures that\nthe system remains dynamic and responsive to ongoing changes on site. A case\nstudy demonstrates both the computational efficiency of the optimization\nalgorithm and the reasoning performance of several LLMs, with top-performing\nmodels achieving over 97% accuracy in constraint and parameter extraction. The\nresults confirm the practicality, adaptability, and cross-domain applicability\nof the proposed methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper integrates Large Language Models (LLMs) with Digital Twins for multi-robot task allocation. While it doesn't directly address trajectory prediction, the coordination of robots in a dynamic environment could potentially involve trajectory planning or prediction as a sub-problem. The strong focus on LLMs justifies a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "multi-robot systems", "task allocation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17629", "pdf": "https://arxiv.org/pdf/2506.17629", "abs": "https://arxiv.org/abs/2506.17629", "authors": ["Kailing Li", "Qi'ao Xu", "Tianwen Qian", "Yuqian Fu", "Yang Jiao", "Xiaoling Wang"], "title": "CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Embodied Visual Reasoning (EVR) seeks to follow complex, free-form\ninstructions based on egocentric video, enabling semantic understanding and\nspatiotemporal reasoning in dynamic environments. Despite its promising\npotential, EVR encounters significant challenges stemming from the diversity of\ncomplex instructions and the intricate spatiotemporal dynamics in long-term\negocentric videos. Prior solutions either employ Large Language Models (LLMs)\nover static video captions, which often omit critical visual details, or rely\non end-to-end Vision-Language Models (VLMs) that struggle with stepwise\ncompositional reasoning. Consider the complementary strengths of LLMs in\nreasoning and VLMs in perception, we propose CLiViS. It is a novel\ntraining-free framework that leverages LLMs for high-level task planning and\norchestrates VLM-driven open-world visual perception to iteratively update the\nscene context. Building on this synergy, the core of CLiViS is a dynamic\nCognitive Map that evolves throughout the reasoning process. This map\nconstructs a structured representation of the embodied scene, bridging\nlow-level perception and high-level reasoning. Extensive experiments across\nmultiple benchmarks demonstrate the effectiveness and generality of CLiViS,\nespecially in handling long-term visual dependencies. Code is available at\nhttps://github.com/Teacher-Tom/CLiViS.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on Embodied Visual Reasoning (EVR) using Large Language Models (LLMs) and Vision-Language Models (VLMs). While not directly trajectory prediction, EVR involves spatiotemporal reasoning in dynamic environments, which is related to understanding movement and predicting future states. The use of LLMs is a key aspect of the paper.", "keywords": ["Large Language Models", "LLMs", "Vision-Language Models", "VLMs", "spatiotemporal reasoning", "dynamic environments"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.18264", "pdf": "https://arxiv.org/pdf/2506.18264", "abs": "https://arxiv.org/abs/2506.18264", "authors": ["Jagadeswara PKV Pothuri", "Aditya Bhatt", "Prajit KrisshnaKumar", "Manaswin Oddiraju", "Souma Chowdhury"], "title": "Learning Approach to Efficient Vision-based Active Tracking of a Flying Target by an Unmanned Aerial Vehicle", "categories": ["cs.RO"], "comment": "AIAA Aviation 2025", "summary": "Autonomous tracking of flying aerial objects has important civilian and\ndefense applications, ranging from search and rescue to counter-unmanned aerial\nsystems (counter-UAS). Ground based tracking requires setting up\ninfrastructure, could be range limited, and may not be feasible in remote\nareas, crowded cities or in dense vegetation areas. Vision based active\ntracking of aerial objects from another airborne vehicle, e.g., a chaser\nunmanned aerial vehicle (UAV), promises to fill this important gap, along with\nserving aerial coordination use cases. Vision-based active tracking by a UAV\nentails solving two coupled problems: 1) compute-efficient and accurate\n(target) object detection and target state estimation; and 2) maneuver\ndecisions to ensure that the target remains in the field of view in the future\ntime-steps and favorably positioned for continued detection. As a solution to\nthe first problem, this paper presents a novel integration of standard deep\nlearning based architectures with Kernelized Correlation Filter (KCF) to\nachieve compute-efficient object detection without compromising accuracy,\nunlike standalone learning or filtering approaches. The proposed perception\nframework is validated using a lab-scale setup. For the second problem, to\nobviate the linearity assumptions and background variations limiting\neffectiveness of the traditional controllers, we present the use of\nreinforcement learning to train a neuro-controller for fast computation of\nvelocity maneuvers. New state space, action space and reward formulations are\ndeveloped for this purpose, and training is performed in simulation using\nAirSim. The trained model is also tested in AirSim with respect to complex\ntarget maneuvers, and is found to outperform a baseline PID control in terms of\ntracking up-time and average distance maintained (from the target) during\ntracking.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on the active tracking of a flying target using a UAV. While it doesn't directly involve Large Language Models, it does address the problem of trajectory prediction and control for the UAV to maintain visual contact with the target. The use of reinforcement learning for maneuver decisions also connects to the broader field of AI and prediction, although it's not explicitly trajectory prediction in the sense of predicting the target's future path, but rather the UAV's future path to maintain tracking. Therefore, it has some relevance to trajectory prediction but no relevance to Large Language Models.", "keywords": ["active tracking", "UAV", "reinforcement learning", "maneuver decisions", "state estimation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.17746", "pdf": "https://arxiv.org/pdf/2506.17746", "abs": "https://arxiv.org/abs/2506.17746", "authors": ["Sourabh Vasant Gothe", "Ayon Chattopadhyay", "Gunturi Venkata Sai Phani Kiran", "Pratik", "Vibhav Agarwal", "Jayesh Rajkumar Vachhani", "Sourav Ghosh", "Parameswaranath VM", "Barath Raj KR"], "title": "PhysID: Physics-based Interactive Dynamics from a Single-view Image", "categories": ["cs.CV"], "comment": "Published in 2025 IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP). Project page: https://physid.github.io/", "summary": "Transforming static images into interactive experiences remains a challenging\ntask in computer vision. Tackling this challenge holds the potential to elevate\nmobile user experiences, notably through interactive and AR/VR applications.\nCurrent approaches aim to achieve this either using pre-recorded video\nresponses or requiring multi-view images as input. In this paper, we present\nPhysID, that streamlines the creation of physics-based interactive dynamics\nfrom a single-view image by leveraging large generative models for 3D mesh\ngeneration and physical property prediction. This significantly reduces the\nexpertise required for engineering-intensive tasks like 3D modeling and\nintrinsic property calibration, enabling the process to be scaled with minimal\nmanual intervention. We integrate an on-device physics-based engine for\nphysically plausible real-time rendering with user interactions. PhysID\nrepresents a leap forward in mobile-based interactive dynamics, offering\nreal-time, non-deterministic interactions and user-personalization with\nefficient on-device memory consumption. Experiments evaluate the zero-shot\ncapabilities of various Multimodal Large Language Models (MLLMs) on diverse\ntasks and the performance of 3D reconstruction models. These results\ndemonstrate the cohesive functioning of all modules within the end-to-end\nframework, contributing to its effectiveness.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u867d\u7136\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u751f\u6210\u7269\u7406\u4ea4\u4e92\u52a8\u6001\uff0c\u4f46\u5b83\u4f7f\u7528\u4e86\u5927\u578b\u751f\u6210\u6a21\u578b\u8fdb\u884c3D\u7f51\u683c\u751f\u6210\u548c\u7269\u7406\u5c5e\u6027\u9884\u6d4b\uff0c\u5e76\u4e14\u63d0\u5230\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b (MLLMs) \u7684\u96f6\u6837\u672c\u80fd\u529b\u8bc4\u4f30\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u5927\u578b\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\u3002", "keywords": ["Large Language Models", "MLLMs", "large generative models", "3D mesh generation", "physical property prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.18689", "pdf": "https://arxiv.org/pdf/2506.18689", "abs": "https://arxiv.org/abs/2506.18689", "authors": ["Alessandro Saviolo", "Giuseppe Loianno"], "title": "NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous aerial target tracking in unstructured and GPS-denied environments\nremains a fundamental challenge in robotics. Many existing methods rely on\nmotion capture systems, pre-mapped scenes, or feature-based localization to\nensure safety and control, limiting their deployment in real-world conditions.\nWe introduce NOVA, a fully onboard, object-centric framework that enables\nrobust target tracking and collision-aware navigation using only a stereo\ncamera and an IMU. Rather than constructing a global map or relying on absolute\nlocalization, NOVA formulates perception, estimation, and control entirely in\nthe target's reference frame. A tightly integrated stack combines a lightweight\nobject detector with stereo depth completion, followed by histogram-based\nfiltering to infer robust target distances under occlusion and noise. These\nmeasurements feed a visual-inertial state estimator that recovers the full\n6-DoF pose of the robot relative to the target. A nonlinear model predictive\ncontroller (NMPC) plans dynamically feasible trajectories in the target frame.\nTo ensure safety, high-order control barrier functions are constructed online\nfrom a compact set of high-risk collision points extracted from depth, enabling\nreal-time obstacle avoidance without maps or dense representations. We validate\nNOVA across challenging real-world scenarios, including urban mazes, forest\ntrails, and repeated transitions through buildings with intermittent GPS loss\nand severe lighting changes that disrupt feature-based localization. Each\nexperiment is repeated multiple times under similar conditions to assess\nresilience, showing consistent and reliable performance. NOVA achieves agile\ntarget following at speeds exceeding 50 km/h. These results show that\nhigh-speed vision-based tracking is possible in the wild using only onboard\nsensing, with no reliance on external localization or environment assumptions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u4e3b\u5bfc\u822a\u548c\u76ee\u6807\u8ddf\u8e2a\uff0c\u6838\u5fc3\u6280\u672f\u5305\u62ec\u89c6\u89c9\u611f\u77e5\u3001\u72b6\u6001\u4f30\u8ba1\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u8ddf\u8e2a\u548c\u8def\u5f84\u89c4\u5212\u95ee\u9898\u53ef\u4ee5\u88ab\u89c6\u4e3a\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["target tracking", "trajectory prediction", "navigation", "model predictive control", "path planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
