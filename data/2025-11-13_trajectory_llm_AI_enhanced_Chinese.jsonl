{"id": "2511.07732", "pdf": "https://arxiv.org/pdf/2511.07732", "abs": "https://arxiv.org/abs/2511.07732", "authors": ["Sandeep Routray", "Hengkai Pan", "Unnat Jain", "Shikhar Bahl", "Deepak Pathak"], "title": "ViPRA: Video Prediction for Robot Actions", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Website: https://vipra-project.github.io", "summary": "Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u89c6\u9891\u9884\u6d4b\uff0c\u5e76\u5c06\u5176\u7528\u4e8e\u673a\u5668\u4eba\u52a8\u4f5c\u63a7\u5236\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u52a8\u4f5c\u9884\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e2d\u4f7f\u7528\u4e86video-language model\uff0c\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u7279\u5b9a\u7c7b\u578b\u7684\u5927\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["video prediction", "robot actions", "video-language model", "action prediction", "latent actions"]}, "AI": {"tldr": "ViPRA\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u672a\u6765\u89c6\u89c9\u89c2\u6d4b\u548c\u8fd0\u52a8\u4e2d\u5fc3\u6f5c\u5728\u52a8\u4f5c\uff0c\u4ece\u65e0\u52a8\u4f5c\u89c6\u9891\u4e2d\u5b66\u4e60\u8fde\u7eed\u673a\u5668\u4eba\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7f3a\u4e4f\u6807\u6ce8\u52a8\u4f5c\uff0c\u9650\u5236\u4e86\u5176\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u3002", "method": "ViPRA\u8bad\u7ec3\u4e00\u4e2a\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u89c6\u89c9\u89c2\u6d4b\u548c\u8fd0\u52a8\u4e2d\u5fc3\u6f5c\u5728\u52a8\u4f5c\uff0c\u5e76\u4f7f\u7528\u611f\u77e5\u635f\u5931\u548c\u5149\u6d41\u4e00\u81f4\u6027\u6765\u8bad\u7ec3\u8fd9\u4e9b\u6f5c\u5728\u52a8\u4f5c\u3002", "result": "ViPRA\u5728SIMPLER\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u83b7\u5f97\u4e8616%\u7684\u63d0\u5347\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u83b7\u5f97\u4e8613%\u7684\u63d0\u5347\u3002", "conclusion": "ViPRA\u907f\u514d\u4e86\u6602\u8d35\u7684\u52a8\u4f5c\u6807\u6ce8\uff0c\u652f\u6301\u8de8embodiment\u6cdb\u5316\uff0c\u5e76\u901a\u8fc7\u5206\u5757\u52a8\u4f5c\u89e3\u7801\u5b9e\u73b0\u4e86\u9ad8\u8fbe22 Hz\u7684\u5e73\u6ed1\u3001\u9ad8\u9891\u8fde\u7eed\u63a7\u5236\u3002", "summary_zh": "ViPRA\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u9884\u8bad\u7ec3-\u5fae\u8c03\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u4ece\u8fd9\u4e9b\u65e0\u52a8\u4f5c\u89c6\u9891\u4e2d\u5b66\u4e60\u8fde\u7eed\u673a\u5668\u4eba\u63a7\u5236\u3002\u8be5\u6846\u67b6\u8bad\u7ec3\u4e00\u4e2a\u89c6\u9891-\u8bed\u8a00\u6a21\u578b\u6765\u9884\u6d4b\u672a\u6765\u7684\u89c6\u89c9\u89c2\u6d4b\u548c\u4ee5\u8fd0\u52a8\u4e3a\u4e2d\u5fc3\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u8fd9\u4e9b\u6f5c\u5728\u52a8\u4f5c\u4f5c\u4e3a\u573a\u666f\u52a8\u6001\u7684\u4e2d\u95f4\u8868\u793a\u3002\u6211\u4eec\u4f7f\u7528\u611f\u77e5\u635f\u5931\u548c\u5149\u6d41\u4e00\u81f4\u6027\u6765\u8bad\u7ec3\u8fd9\u4e9b\u6f5c\u5728\u52a8\u4f5c\uff0c\u4ee5\u786e\u4fdd\u5b83\u4eec\u53cd\u6620\u7269\u7406\u57fa\u7840\u7684\u884c\u4e3a\u3002\u5bf9\u4e8e\u4e0b\u6e38\u63a7\u5236\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5206\u5757\u6d41\u52a8\u5339\u914d\u89e3\u7801\u5668\uff0c\u8be5\u89e3\u7801\u5668\u4f7f\u7528\u4ec5100\u5230200\u4e2a\u8fdc\u7a0b\u64cd\u4f5c\u6f14\u793a\u5c06\u6f5c\u5728\u52a8\u4f5c\u6620\u5c04\u5230\u7279\u5b9a\u4e8e\u673a\u5668\u4eba\u7684\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u3002"}}
{"id": "2511.07484", "pdf": "https://arxiv.org/pdf/2511.07484", "abs": "https://arxiv.org/abs/2511.07484", "authors": ["Dharmateja Priyadarshi Uddandarao", "Ravi Kiran Vadlamani"], "title": "Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs", "categories": ["cs.LG", "cs.CE", "stat.ME"], "comment": null, "summary": "This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on forecasting user behavior trajectories using generative AI models (transformer-based), which aligns with trajectory prediction. While it doesn't explicitly mention large language models, the use of transformer-based generative AI suggests a potential connection. The use of causal graphs and counterfactual analysis also strengthens the relevance to behavior prediction.", "keywords": ["trajectory prediction", "generative AI", "transformer", "forecasting", "behavior prediction", "causal graphs"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u53cd\u4e8b\u5b9e\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4e0e\u57fa\u4e8eTransformer\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u95ee\u9898\uff0c\u5e76\u91cf\u5316\u4e0d\u540c\u5e72\u9884\u63aa\u65bd\u5bf9\u7528\u6237\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "method": "\u8be5\u65b9\u6cd5\u6784\u5efa\u56e0\u679c\u56fe\u6765\u6a21\u62df\u7528\u6237\u4e92\u52a8\u3001\u91c7\u7528\u6307\u6807\u548c\u4ea7\u54c1\u7279\u5f81\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u751f\u6210\u6a21\u578b\u5728\u53cd\u4e8b\u5b9e\u6761\u4ef6\u4e0b\u751f\u6210\u903c\u771f\u7684\u884c\u4e3a\u8f68\u8ff9\u3002", "result": "\u5728\u7f51\u7edc\u4e92\u52a8\u3001\u79fb\u52a8\u5e94\u7528\u548c\u7535\u5b50\u5546\u52a1\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u9884\u6d4b\u548c\u63d0\u5347\u5efa\u6a21\u6280\u672f\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u56e0\u679c\u8def\u5f84\u53ef\u89c6\u5316\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u4ea7\u54c1\u56e2\u961f\u80fd\u591f\u5728\u90e8\u7f72\u524d\u6709\u6548\u5730\u6a21\u62df\u548c\u8bc4\u4f30\u53ef\u80fd\u7684\u5e72\u9884\u63aa\u65bd\u3002", "summary_zh": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u53cd\u4e8b\u5b9e\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u4e0e\u57fa\u4e8eTransformer\u7684\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u3002\u4e3a\u4e86\u6a21\u62df\u865a\u6784\u60c5\u5883\uff0c\u8be5\u65b9\u6cd5\u521b\u5efa\u4e86\u56e0\u679c\u56fe\uff0c\u8fd9\u4e9b\u56e0\u679c\u56fe\u63cf\u7ed8\u4e86\u7528\u6237\u4e92\u52a8\u3001\u91c7\u7528\u6307\u6807\u548c\u4ea7\u54c1\u7279\u5f81\u4e4b\u95f4\u7684\u8054\u7cfb\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u4ee5\u56e0\u679c\u53d8\u91cf\u4e3a\u6761\u4ef6\u7684\u751f\u6210\u6a21\u578b\uff0c\u751f\u6210\u53cd\u4e8b\u5b9e\u6761\u4ef6\u4e0b\u7684\u771f\u5b9e\u884c\u4e3a\u8f68\u8ff9\u3002\u901a\u8fc7\u5728\u6765\u81ea\u7f51\u7edc\u4e92\u52a8\u3001\u79fb\u52a8\u5e94\u7528\u548c\u7535\u5b50\u5546\u52a1\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u7684\u9884\u6d4b\u548c\u63d0\u5347\u5efa\u6a21\u6280\u672f\u3002\u7531\u4e8e\u8be5\u6846\u67b6\u901a\u8fc7\u56e0\u679c\u8def\u5f84\u53ef\u89c6\u5316\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u4ea7\u54c1\u56e2\u961f\u53ef\u4ee5\u5728\u90e8\u7f72\u524d\u6709\u6548\u5730\u6a21\u62df\u548c\u8bc4\u4f30\u53ef\u80fd\u7684\u5e72\u9884\u63aa\u65bd\u3002"}}
{"id": "2511.07727", "pdf": "https://arxiv.org/pdf/2511.07727", "abs": "https://arxiv.org/abs/2511.07727", "authors": ["Xiaohan Zhang", "Yan Ding", "Yohei Hayamizu", "Zainab Altaweel", "Yifeng Zhu", "Yuke Zhu", "Peter Stone", "Chris Paxton", "Shiqi Zhang"], "title": "LLM-GROP: Visually Grounded Robot Task and Motion Planning with Large Language Models", "categories": ["cs.RO"], "comment": null, "summary": "Task planning and motion planning are two of the most important problems in robotics, where task planning methods help robots achieve high-level goals and motion planning methods maintain low-level feasibility. Task and motion planning (TAMP) methods interleave the two processes of task planning and motion planning to ensure goal achievement and motion feasibility. Within the TAMP context, we are concerned with the mobile manipulation (MoMa) of multiple objects, where it is necessary to interleave actions for navigation and manipulation.\n  In particular, we aim to compute where and how each object should be placed given underspecified goals, such as ``set up dinner table with a fork, knife and plate.'' We leverage the rich common sense knowledge from large language models (LLMs), e.g., about how tableware is organized, to facilitate both task-level and motion-level planning. In addition, we use computer vision methods to learn a strategy for selecting base positions to facilitate MoMa behaviors, where the base position corresponds to the robot's ``footprint'' and orientation in its operating space. Altogether, this article provides a principled TAMP framework for MoMa tasks that accounts for common sense about object rearrangement and is adaptive to novel situations that include many objects that need to be moved. We performed quantitative experiments in both real-world settings and simulated environments. We evaluated the success rate and efficiency in completing long-horizon object rearrangement tasks. While the robot completed 84.4\\% real-world object rearrangement trials, subjective human evaluations indicated that the robot's performance is still lower than experienced human waiters.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on task and motion planning for robots, leveraging large language models for common sense reasoning about object arrangement. While it doesn't directly address trajectory prediction, the motion planning aspect has some overlap, and the use of LLMs is a significant component. Therefore, it has moderate relevance.", "keywords": ["motion planning", "large language models", "LLMs", "robotics", "task planning"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5e38\u8bc6\u77e5\u8bc6\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u7684 TAMP \u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u7269\u4f53\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u64cd\u4f5c\u4e2d\uff0c\u673a\u5668\u4eba\u5982\u4f55\u5728\u4e0d\u660e\u786e\u76ee\u6807\u4e0b\uff0c\u6839\u636e\u5e38\u8bc6\u77e5\u8bc6\u653e\u7f6e\u7269\u4f53\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5e38\u8bc6\u77e5\u8bc6\u8f85\u52a9\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u5e76\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u5b66\u4e60\u9009\u62e9\u57fa\u5ea7\u4f4d\u7f6e\u7684\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9a\u91cf\u5b9e\u9a8c\uff0c\u673a\u5668\u4eba\u5b8c\u6210\u4e86 84.4% \u7684\u771f\u5b9e\u7269\u4f53\u91cd\u6392\u8bd5\u9a8c\uff0c\u4f46\u6027\u80fd\u4ecd\u4f4e\u4e8e\u6709\u7ecf\u9a8c\u7684\u670d\u52a1\u5458\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684 TAMP \u6846\u67b6\u80fd\u591f\u5904\u7406\u591a\u7269\u4f53\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u9002\u5e94\u65b0\u60c5\u51b5\uff0c\u4f46\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "summary_zh": "\u4efb\u52a1\u89c4\u5212\u548c\u8fd0\u52a8\u89c4\u5212\u662f\u673a\u5668\u4eba\u5b66\u4e2d\u4e24\u4e2a\u6700\u91cd\u8981\u7684\u95ee\u9898\u3002\u4efb\u52a1\u89c4\u5212\u65b9\u6cd5\u5e2e\u52a9\u673a\u5668\u4eba\u5b9e\u73b0\u9ad8\u5c42\u76ee\u6807\uff0c\u800c\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u4fdd\u8bc1\u5e95\u5c42\u53ef\u884c\u6027\u3002\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212 (TAMP) \u65b9\u6cd5\u4ea4\u9519\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\u548c\u8fd0\u52a8\u89c4\u5212\u8fd9\u4e24\u4e2a\u8fc7\u7a0b\uff0c\u4ee5\u786e\u4fdd\u76ee\u6807\u8fbe\u6210\u548c\u8fd0\u52a8\u53ef\u884c\u6027\u3002\u5728 TAMP \u7684\u80cc\u666f\u4e0b\uff0c\u6211\u4eec\u5173\u6ce8\u7684\u662f\u591a\u7269\u4f53\u7684\u79fb\u52a8\u64cd\u4f5c (MoMa)\uff0c\u5176\u4e2d\u6709\u5fc5\u8981\u4ea4\u9519\u5bfc\u822a\u548c\u64cd\u4f5c\u7684\u52a8\u4f5c\u3002\u7279\u522b\u5730\uff0c\u6211\u4eec\u7684\u76ee\u6807\u662f\u8ba1\u7b97\u6bcf\u4e2a\u7269\u4f53\u5e94\u8be5\u653e\u7f6e\u5728\u54ea\u91cc\u4ee5\u53ca\u5982\u4f55\u653e\u7f6e\uff0c\u7ed9\u5b9a\u4e0d\u660e\u786e\u7684\u76ee\u6807\uff0c\u4f8b\u5982\u201c\u7528\u53c9\u5b50\u3001\u5200\u5b50\u548c\u76d8\u5b50\u6446\u653e\u9910\u684c\u201d\u3002\u6211\u4eec\u5229\u7528\u6765\u81ea\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u4e30\u5bcc\u7684\u5e38\u8bc6\u77e5\u8bc6\uff0c\u4f8b\u5982\u5173\u4e8e\u9910\u5177\u5982\u4f55\u7ec4\u7ec7\u7684\u77e5\u8bc6\uff0c\u4ee5\u4fc3\u8fdb\u4efb\u52a1\u7ea7\u522b\u548c\u8fd0\u52a8\u7ea7\u522b\u7684\u89c4\u5212\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u65b9\u6cd5\u6765\u5b66\u4e60\u9009\u62e9\u57fa\u5ea7\u4f4d\u7f6e\u7684\u7b56\u7565\uff0c\u4ee5\u65b9\u4fbf MoMa \u884c\u4e3a\uff0c\u5176\u4e2d\u57fa\u5ea7\u4f4d\u7f6e\u5bf9\u5e94\u4e8e\u673a\u5668\u4eba\u5728\u5176\u64cd\u4f5c\u7a7a\u95f4\u4e2d\u7684\u201c\u8db3\u8ff9\u201d\u548c\u65b9\u5411\u3002\u603b\u800c\u8a00\u4e4b\uff0c\u672c\u6587\u4e3a MoMa \u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u7684 TAMP \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u8003\u8651\u4e86\u5173\u4e8e\u7269\u4f53\u91cd\u65b0\u6392\u5217\u7684\u5e38\u8bc6\uff0c\u5e76\u4e14\u80fd\u591f\u9002\u5e94\u5305\u62ec\u8bb8\u591a\u9700\u8981\u79fb\u52a8\u7684\u7269\u4f53\u7684\u65b0\u60c5\u51b5\u3002\u6211\u4eec\u5728\u771f\u5b9e\u73af\u5883\u548c\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9a\u91cf\u5b9e\u9a8c\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u5b8c\u6210\u957f\u65f6\u7a0b\u7269\u4f53\u91cd\u6392\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u6548\u7387\u3002\u867d\u7136\u673a\u5668\u4eba\u5b8c\u6210\u4e86 84.4% \u7684\u771f\u5b9e\u7269\u4f53\u91cd\u6392\u8bd5\u9a8c\uff0c\u4f46\u4e3b\u89c2\u7684\u4eba\u5de5\u8bc4\u4f30\u8868\u660e\uff0c\u673a\u5668\u4eba\u7684\u6027\u80fd\u4ecd\u7136\u4f4e\u4e8e\u7ecf\u9a8c\u4e30\u5bcc\u7684\u670d\u52a1\u5458\u3002"}}
{"id": "2511.07678", "pdf": "https://arxiv.org/pdf/2511.07678", "abs": "https://arxiv.org/abs/2511.07678", "authors": ["Rohan Alur", "Bradly C. Stadie", "Daniel Kang", "Ryan Chen", "Matt McManus", "Michael Rickert", "Tyler Lee", "Michael Federici", "Richard Zhu", "Dennis Fogerty", "Hayley Williamson", "Nina Lozinski", "Aaron Linsky", "Jasjeet S. Sekhon"], "title": "AIA Forecaster: Technical Report", "categories": ["cs.AI"], "comment": null, "summary": "This technical report describes the AIA Forecaster, a Large Language Model (LLM)-based system for judgmental forecasting using unstructured data. The AIA Forecaster approach combines three core elements: agentic search over high-quality news sources, a supervisor agent that reconciles disparate forecasts for the same event, and a set of statistical calibration techniques to counter behavioral biases in large language models. On the ForecastBench benchmark (Karger et al., 2024), the AIA Forecaster achieves performance equal to human superforecasters, surpassing prior LLM baselines. In addition to reporting on ForecastBench, we also introduce a more challenging forecasting benchmark sourced from liquid prediction markets. While the AIA Forecaster underperforms market consensus on this benchmark, an ensemble combining AIA Forecaster with market consensus outperforms consensus alone, demonstrating that our forecaster provides additive information. Our work establishes a new state of the art in AI forecasting and provides practical, transferable recommendations for future research. To the best of our knowledge, this is the first work that verifiably achieves expert-level forecasting at scale.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Large Language Models (LLMs) for judgmental forecasting. While it doesn't directly address trajectory prediction, the general concept of forecasting is related, and it heavily utilizes LLMs. Therefore, it has a moderate relevance.", "keywords": ["Large Language Models", "LLM", "forecasting", "AI forecasting"]}, "AI": {"tldr": "AIA Forecaster\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9884\u6d4b\u7cfb\u7edf\uff0c\u5b83\u7ed3\u5408\u4e86\u4ee3\u7406\u641c\u7d22\u3001\u9884\u6d4b\u534f\u8c03\u548c\u7edf\u8ba1\u6821\u51c6\u6280\u672f\uff0c\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u4eba\u7c7b\u8d85\u9884\u6d4b\u8005\u7684\u6c34\u5e73\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5982\u4f55\u5229\u7528\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u522b\u7684\u5224\u65ad\u9884\u6d4b\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e09\u4e2a\u6838\u5fc3\u8981\u7d20\uff1a\u9ad8\u8d28\u91cf\u65b0\u95fb\u6765\u6e90\u4e0a\u7684\u4ee3\u7406\u641c\u7d22\u3001\u534f\u8c03\u76f8\u540c\u4e8b\u4ef6\u4e0d\u540c\u9884\u6d4b\u7684\u76d1\u7763\u4ee3\u7406\uff0c\u4ee5\u53ca\u7528\u4e8e\u5bf9\u6297\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u884c\u4e3a\u504f\u5dee\u7684\u7edf\u8ba1\u6821\u51c6\u6280\u672f\u3002", "result": "\u5728ForecastBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAIA Forecaster\u7684\u6027\u80fd\u4e0e\u4eba\u7c7b\u8d85\u9884\u6d4b\u8005\u76f8\u5f53\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684LLM\u57fa\u7ebf\u3002\u6b64\u5916\uff0c\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u9884\u6d4b\u5e02\u573a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAIA Forecaster\u867d\u7136\u4e0d\u5982\u5e02\u573a\u5171\u8bc6\uff0c\u4f46\u4e0e\u5e02\u573a\u5171\u8bc6\u7684\u96c6\u6210\u4f18\u4e8e\u5355\u72ec\u7684\u5e02\u573a\u5171\u8bc6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684AI\u9884\u6d4b\u6280\u672f\uff0c\u8fbe\u5230\u4e86\u4e13\u5bb6\u6c34\u5e73\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u5b9e\u8df5\u5efa\u8bae\u3002", "summary_zh": "\u672c\u6280\u672f\u62a5\u544a\u4ecb\u7ecd\u4e86AIA Forecaster\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cfb\u7edf\uff0c\u5b83\u4f7f\u7528\u975e\u7ed3\u6784\u5316\u6570\u636e\u8fdb\u884c\u5224\u65ad\u9884\u6d4b\u3002AIA Forecaster\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e09\u4e2a\u6838\u5fc3\u8981\u7d20\uff1a\u5728\u9ad8\u8d28\u91cf\u65b0\u95fb\u6765\u6e90\u4e0a\u8fdb\u884c\u4ee3\u7406\u641c\u7d22\uff0c\u4e00\u4e2a\u534f\u8c03\u76f8\u540c\u4e8b\u4ef6\u4e0d\u540c\u9884\u6d4b\u7684\u76d1\u7763\u4ee3\u7406\uff0c\u4ee5\u53ca\u4e00\u5957\u7528\u4e8e\u5bf9\u6297\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u884c\u4e3a\u504f\u5dee\u7684\u7edf\u8ba1\u6821\u51c6\u6280\u672f\u3002\u5728ForecastBench\u57fa\u51c6\u6d4b\u8bd5\uff08Karger et al., 2024\uff09\u4e2d\uff0cAIA Forecaster\u7684\u6027\u80fd\u4e0e\u4eba\u7c7b\u8d85\u9884\u6d4b\u8005\u76f8\u5f53\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684LLM\u57fa\u7ebf\u3002\u9664\u4e86\u62a5\u544aForecastBench\u7684\u7ed3\u679c\u5916\uff0c\u6211\u4eec\u8fd8\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u6765\u81ea\u6d41\u52a8\u9884\u6d4b\u5e02\u573a\u7684\u3001\u66f4\u5177\u6311\u6218\u6027\u7684\u9884\u6d4b\u57fa\u51c6\u3002\u867d\u7136AIA Forecaster\u5728\u8fd9\u4e2a\u57fa\u51c6\u4e0a\u8868\u73b0\u4e0d\u5982\u5e02\u573a\u5171\u8bc6\uff0c\u4f46AIA Forecaster\u4e0e\u5e02\u573a\u5171\u8bc6\u7684\u96c6\u6210\u4f18\u4e8e\u5355\u72ec\u7684\u5e02\u573a\u5171\u8bc6\uff0c\u8868\u660e\u6211\u4eec\u7684\u9884\u6d4b\u5668\u63d0\u4f9b\u4e86\u989d\u5916\u7684\u6709\u7528\u4fe1\u606f\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5efa\u7acb\u4e86\u4e00\u79cd\u65b0\u7684AI\u9884\u6d4b\u6280\u672f\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u5b9e\u8df5\u5efa\u8bae\u3002\u636e\u6211\u4eec\u6240\u77e5\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u3001\u5927\u89c4\u6a21\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u9884\u6d4b\u7684\u5de5\u4f5c\u3002"}}
{"id": "2511.07811", "pdf": "https://arxiv.org/pdf/2511.07811", "abs": "https://arxiv.org/abs/2511.07811", "authors": ["Sagar Gupta", "Thanh Vinh Nguyen", "Thieu Long Phan", "Vidul Attri", "Archit Gupta", "Niroshinie Fernando", "Kevin Lee", "Seng W. Loke", "Ronny Kutadinata", "Benjamin Champion", "Akansel Cosgun"], "title": "Virtual Traffic Lights for Multi-Robot Navigation: Decentralized Planning with Centralized Conflict Resolution", "categories": ["cs.RO"], "comment": null, "summary": "We present a hybrid multi-robot coordination framework that combines decentralized path planning with centralized conflict resolution. In our approach, each robot autonomously plans its path and shares this information with a centralized node. The centralized system detects potential conflicts and allows only one of the conflicting robots to proceed at a time, instructing others to stop outside the conflicting area to avoid deadlocks. Unlike traditional centralized planning methods, our system does not dictate robot paths but instead provides stop commands, functioning as a virtual traffic light. In simulation experiments with multiple robots, our approach increased the success rate of robots reaching their goals while reducing deadlocks. Furthermore, we successfully validated the system in real-world experiments with two quadruped robots and separately with wheeled Duckiebots.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on multi-robot navigation using decentralized planning and centralized conflict resolution, which is related to trajectory prediction and path planning for multiple agents. While it doesn't directly involve large language models, the problem of coordinating multiple robots inherently requires predicting their future trajectories to avoid collisions. The 'decentralized path planning' aspect and the need for conflict resolution based on predicted paths contribute to a moderate relevance score. The 'virtual traffic lights' concept also implicitly involves predicting the future states of the robots.", "keywords": ["multi-robot navigation", "decentralized path planning", "conflict resolution", "trajectory prediction", "path planning"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u591a\u673a\u5668\u4eba\u534f\u8c03\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u6563\u8def\u5f84\u89c4\u5212\u548c\u96c6\u4e2d\u5f0f\u51b2\u7a81\u89e3\u51b3\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u7684\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e86\u6b7b\u9501\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u534f\u8c03\u4e2d\u7684\u51b2\u7a81\u548c\u6b7b\u9501\u95ee\u9898\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u7684\u6210\u529f\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u591a\u673a\u5668\u4eba\u534f\u8c03\u6846\u67b6\uff0c\u673a\u5668\u4eba\u81ea\u4e3b\u89c4\u5212\u8def\u5f84\u5e76\u4e0e\u4e2d\u592e\u8282\u70b9\u5171\u4eab\uff0c\u4e2d\u592e\u7cfb\u7edf\u68c0\u6d4b\u6f5c\u5728\u51b2\u7a81\uff0c\u5e76\u6307\u793a\u51b2\u7a81\u673a\u5668\u4eba\u505c\u6b62\u4ee5\u907f\u514d\u6b7b\u9501\u3002", "result": "\u5728\u591a\u673a\u5668\u4eba\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u7684\u6210\u529f\u7387\uff0c\u51cf\u5c11\u4e86\u6b7b\u9501\u3002\u5728\u4e24\u8db3\u673a\u5668\u4eba\u548c\u8f6e\u5f0fDuckiebots\u7684\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6df7\u5408\u591a\u673a\u5668\u4eba\u534f\u8c03\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u51b2\u7a81\u548c\u6b7b\u9501\u95ee\u9898\uff0c\u63d0\u9ad8\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\u3002", "summary_zh": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u591a\u673a\u5668\u4eba\u534f\u8c03\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u5206\u6563\u5f0f\u8def\u5f84\u89c4\u5212\u548c\u96c6\u4e2d\u5f0f\u51b2\u7a81\u89e3\u51b3\u3002\u5728\u6211\u4eec\u7684\u65b9\u6cd5\u4e2d\uff0c\u6bcf\u4e2a\u673a\u5668\u4eba\u81ea\u4e3b\u5730\u89c4\u5212\u5176\u8def\u5f84\uff0c\u5e76\u5c06\u6b64\u4fe1\u606f\u4e0e\u4e2d\u592e\u8282\u70b9\u5171\u4eab\u3002\u4e2d\u592e\u7cfb\u7edf\u68c0\u6d4b\u6f5c\u5728\u7684\u51b2\u7a81\uff0c\u5e76\u4e14\u4e00\u6b21\u53ea\u5141\u8bb8\u4e00\u4e2a\u51b2\u7a81\u673a\u5668\u4eba\u7ee7\u7eed\u524d\u8fdb\uff0c\u6307\u793a\u5176\u4ed6\u673a\u5668\u4eba\u505c\u6b62\u5728\u51b2\u7a81\u533a\u57df\u4e4b\u5916\u4ee5\u907f\u514d\u6b7b\u9501\u3002\u4e0e\u4f20\u7edf\u7684\u96c6\u4e2d\u5f0f\u89c4\u5212\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u7684\u7cfb\u7edf\u4e0d\u6307\u793a\u673a\u5668\u4eba\u8def\u5f84\uff0c\u800c\u662f\u63d0\u4f9b\u505c\u6b62\u547d\u4ee4\uff0c\u5176\u529f\u80fd\u7c7b\u4f3c\u4e8e\u865a\u62df\u4ea4\u901a\u706f\u3002\u5728\u591a\u4e2a\u673a\u5668\u4eba\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u6b7b\u9501\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5728\u4e24\u4e2a\u56db\u8db3\u673a\u5668\u4eba\u4ee5\u53ca\u8f6e\u5f0fDuckiebots\u7684\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u6210\u529f\u9a8c\u8bc1\u4e86\u8be5\u7cfb\u7edf\u3002"}}
{"id": "2511.07690", "pdf": "https://arxiv.org/pdf/2511.07690", "abs": "https://arxiv.org/abs/2511.07690", "authors": ["Soham Hans", "Volkan Ustun", "Benjamin Nye", "James Sterrett", "Matthew Green"], "title": "Towards AI-Assisted Generation of Military Training Scenarios", "categories": ["cs.AI"], "comment": null, "summary": "Achieving expert-level performance in simulation-based training relies on the creation of complex, adaptable scenarios, a traditionally laborious and resource intensive process. Although prior research explored scenario generation for military training, pre-LLM AI tools struggled to generate sufficiently complex or adaptable scenarios. This paper introduces a multi-agent, multi-modal reasoning framework that leverages Large Language Models (LLMs) to generate critical training artifacts, such as Operations Orders (OPORDs). We structure our framework by decomposing scenario generation into a hierarchy of subproblems, and for each one, defining the role of the AI tool: (1) generating options for a human author to select from, (2) producing a candidate product for human approval or modification, or (3) generating textual artifacts fully automatically. Our framework employs specialized LLM-based agents to address distinct subproblems. Each agent receives input from preceding subproblem agents, integrating both text-based scenario details and visual information (e.g., map features, unit positions and applies specialized reasoning to produce appropriate outputs. Subsequent agents process these outputs sequentially, preserving logical consistency and ensuring accurate document generation. This multi-agent strategy overcomes the limitations of basic prompting or single-agent approaches when tackling such highly complex tasks. We validate our framework through a proof-of-concept that generates the scheme of maneuver and movement section of an OPORD while estimating map positions and movements as a precursor demonstrating its feasibility and accuracy. Our results demonstrate the potential of LLM-driven multi-agent systems to generate coherent, nuanced documents and adapt dynamically to changing conditions, advancing automation in scenario generation for military training.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using Large Language Models (LLMs) to generate military training scenarios, including aspects like Operations Orders (OPORDs) and unit movements on a map. While it doesn't directly focus on *trajectory prediction* as a core problem, the generation of movement sections of OPORDs and estimation of map positions and movements can be seen as related to path planning and movement prediction, albeit in a high-level, scenario-generation context. The strong LLM focus increases the relevance score.", "keywords": ["Large Language Models", "LLMs", "scenario generation", "multi-agent", "movement", "Operations Orders", "OPORDs"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u3001\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u590d\u6742\u4e14\u53ef\u9002\u5e94\u7684\u6a21\u62df\u8bad\u7ec3\u573a\u666f\uff0c\u4ece\u800c\u663e\u8457\u63d0\u5347\u519b\u4e8b\u8bad\u7ec3\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u57fa\u4e8e\u4eff\u771f\u7684\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u521b\u5efa\u590d\u6742\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u573a\u666f\uff0c\u4f46\u8fd9\u662f\u4e00\u4e2a\u8d39\u529b\u4e14\u8d44\u6e90\u5bc6\u96c6\u7684\u8fc7\u7a0b\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u519b\u4e8b\u8bad\u7ec3\u7684\u573a\u666f\u751f\u6210\uff0c\u4f46 pre-LLM AI \u5de5\u5177\u96be\u4ee5\u751f\u6210\u8db3\u591f\u590d\u6742\u6216\u9002\u5e94\u6027\u5f3a\u7684\u573a\u666f\u3002", "method": "\u8be5\u6846\u67b6\u5c06\u573a\u666f\u751f\u6210\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u5b50\u95ee\u9898\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5b50\u95ee\u9898\u5b9a\u4e49\u4e86 AI \u5de5\u5177\u7684\u89d2\u8272\uff1a(1) \u751f\u6210\u9009\u9879\u4f9b\u4eba\u5de5\u4f5c\u8005\u9009\u62e9\uff0c(2) \u751f\u6210\u5019\u9009\u4ea7\u54c1\u4f9b\u4eba\u5de5\u6279\u51c6\u6216\u4fee\u6539\uff0c\u6216 (3) \u5b8c\u5168\u81ea\u52a8\u751f\u6210\u6587\u672c\u5236\u54c1\u3002\u8be5\u6846\u67b6\u91c7\u7528\u57fa\u4e8e LLM \u7684\u4e13\u7528\u667a\u80fd\u4f53\u6765\u89e3\u51b3\u4e0d\u540c\u7684\u5b50\u95ee\u9898\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u63a5\u6536\u6765\u81ea\u5148\u524d\u5b50\u95ee\u9898\u667a\u80fd\u4f53\u7684\u8f93\u5165\uff0c\u6574\u5408\u57fa\u4e8e\u6587\u672c\u7684\u573a\u666f\u7ec6\u8282\u548c\u89c6\u89c9\u4fe1\u606f\uff08\u4f8b\u5982\uff0c\u5730\u56fe\u7279\u5f81\u3001\u5355\u4f4d\u4f4d\u7f6e\uff09\uff0c\u5e76\u5e94\u7528\u4e13\u95e8\u7684\u63a8\u7406\u6765\u751f\u6210\u9002\u5f53\u7684\u8f93\u51fa\u3002\u540e\u7eed\u667a\u80fd\u4f53\u6309\u987a\u5e8f\u5904\u7406\u8fd9\u4e9b\u8f93\u51fa\uff0c\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u5e76\u786e\u4fdd\u51c6\u786e\u7684\u6587\u6863\u751f\u6210\u3002", "result": "\u901a\u8fc7\u751f\u6210\u4f5c\u6218\u547d\u4ee4\uff08OPORD\uff09\u7684\u673a\u52a8\u65b9\u6848\u548c\u884c\u52a8\u90e8\u5206\uff0c\u5e76\u4f30\u8ba1\u5730\u56fe\u4f4d\u7f6e\u548c\u884c\u52a8\u4f5c\u4e3a\u5148\u51b3\u6761\u4ef6\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e LLM \u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u6f5c\u529b\u751f\u6210\u8fde\u8d2f\u3001\u7ec6\u81f4\u7684\u6587\u6863\uff0c\u5e76\u52a8\u6001\u9002\u5e94\u53d8\u5316\u7684\u73af\u5883\uff0c\u4ece\u800c\u63a8\u8fdb\u519b\u4e8b\u8bad\u7ec3\u573a\u666f\u751f\u6210\u7684\u81ea\u52a8\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e LLM \u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u751f\u6210\u590d\u6742\u4e14\u53ef\u9002\u5e94\u7684\u6a21\u62df\u8bad\u7ec3\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u519b\u4e8b\u8bad\u7ec3\u7684\u81ea\u52a8\u5316\u6c34\u5e73\uff0c\u5177\u6709\u5de8\u5927\u7684\u5e94\u7528\u6f5c\u529b\u3002", "summary_zh": "\u4e3a\u4e86\u5728\u57fa\u4e8e\u4eff\u771f\u7684\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e13\u5bb6\u7ea7\u7684\u8868\u73b0\uff0c\u9700\u8981\u521b\u5efa\u590d\u6742\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u573a\u666f\uff0c\u4f46\u8fd9\u901a\u5e38\u662f\u4e00\u4e2a\u8d39\u529b\u4e14\u8d44\u6e90\u5bc6\u96c6\u7684\u8fc7\u7a0b\u3002\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u519b\u4e8b\u8bad\u7ec3\u7684\u573a\u666f\u751f\u6210\uff0c\u4f46 pre-LLM AI \u5de5\u5177\u96be\u4ee5\u751f\u6210\u8db3\u591f\u590d\u6742\u6216\u9002\u5e94\u6027\u5f3a\u7684\u573a\u666f\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u3001\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u751f\u6210\u5173\u952e\u7684\u8bad\u7ec3\u5de5\u4ef6\uff0c\u4f8b\u5982\u4f5c\u6218\u547d\u4ee4 (OPORD)\u3002\u6211\u4eec\u901a\u8fc7\u5c06\u573a\u666f\u751f\u6210\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u7684\u5c42\u6b21\u7ed3\u6784\u6765\u6784\u5efa\u6211\u4eec\u7684\u6846\u67b6\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u5b50\u95ee\u9898\u5b9a\u4e49 AI \u5de5\u5177\u7684\u89d2\u8272\uff1a(1) \u751f\u6210\u9009\u9879\u4f9b\u4eba\u5de5\u4f5c\u8005\u9009\u62e9\uff0c(2) \u751f\u6210\u5019\u9009\u4ea7\u54c1\u4f9b\u4eba\u5de5\u6279\u51c6\u6216\u4fee\u6539\uff0c\u6216 (3) \u5b8c\u5168\u81ea\u52a8\u751f\u6210\u6587\u672c\u5236\u54c1\u3002\u6211\u4eec\u7684\u6846\u67b6\u91c7\u7528\u57fa\u4e8e LLM \u7684\u4e13\u7528\u667a\u80fd\u4f53\u6765\u89e3\u51b3\u4e0d\u540c\u7684\u5b50\u95ee\u9898\u3002\u6bcf\u4e2a\u667a\u80fd\u4f53\u63a5\u6536\u6765\u81ea\u5148\u524d\u5b50\u95ee\u9898\u667a\u80fd\u4f53\u7684\u8f93\u5165\uff0c\u6574\u5408\u57fa\u4e8e\u6587\u672c\u7684\u573a\u666f\u7ec6\u8282\u548c\u89c6\u89c9\u4fe1\u606f\uff08\u4f8b\u5982\uff0c\u5730\u56fe\u7279\u5f81\u3001\u5355\u4f4d\u4f4d\u7f6e\uff09\uff0c\u5e76\u5e94\u7528\u4e13\u95e8\u7684\u63a8\u7406\u6765\u751f\u6210\u9002\u5f53\u7684\u8f93\u51fa\u3002\u540e\u7eed\u667a\u80fd\u4f53\u6309\u987a\u5e8f\u5904\u7406\u8fd9\u4e9b\u8f93\u51fa\uff0c\u4fdd\u6301\u903b\u8f91\u4e00\u81f4\u6027\u5e76\u786e\u4fdd\u51c6\u786e\u7684\u6587\u6863\u751f\u6210\u3002\u8fd9\u79cd\u591a\u667a\u80fd\u4f53\u7b56\u7565\u514b\u670d\u4e86\u5728\u5904\u7406\u6b64\u7c7b\u9ad8\u5ea6\u590d\u6742\u7684\u4efb\u52a1\u65f6\u57fa\u672c\u63d0\u793a\u6216\u5355\u667a\u80fd\u4f53\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u6211\u4eec\u901a\u8fc7\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u6765\u9a8c\u8bc1\u6211\u4eec\u7684\u6846\u67b6\uff0c\u8be5\u9a8c\u8bc1\u751f\u6210 OPORD \u7684\u673a\u52a8\u65b9\u6848\u548c\u884c\u52a8\u90e8\u5206\uff0c\u540c\u65f6\u4f30\u8ba1\u5730\u56fe\u4f4d\u7f6e\u548c\u884c\u52a8\u4f5c\u4e3a\u5148\u51b3\u6761\u4ef6\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u884c\u6027\u548c\u51c6\u786e\u6027\u3002\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0cLLM \u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6709\u6f5c\u529b\u751f\u6210\u8fde\u8d2f\u3001\u7ec6\u81f4\u7684\u6587\u6863\uff0c\u5e76\u52a8\u6001\u9002\u5e94\u53d8\u5316\u7684\u73af\u5883\uff0c\u4ece\u800c\u63a8\u8fdb\u519b\u4e8b\u8bad\u7ec3\u573a\u666f\u751f\u6210\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2511.07820", "pdf": "https://arxiv.org/pdf/2511.07820", "abs": "https://arxiv.org/abs/2511.07820", "authors": ["Zhengyi Luo", "Ye Yuan", "Tingwu Wang", "Chenran Li", "Sirui Chen", "Fernando Casta\u00f1eda", "Zi-Ang Cao", "Jiefeng Li", "David Minor", "Qingwei Ben", "Xingye Da", "Runyu Ding", "Cyrus Hogg", "Lina Song", "Edy Lim", "Eugene Jeong", "Tairan He", "Haoru Xue", "Wenli Xiao", "Zi Wang", "Simon Yuen", "Jan Kautz", "Yan Chang", "Umar Iqbal", "Linxi \"Jim\" Fan", "Yuke Zhu"], "title": "SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.GR", "eess.SY"], "comment": "Project page: https://nvlabs.github.io/SONIC/", "summary": "Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on motion tracking for humanoid control and leverages large models (foundation models). While not directly about trajectory prediction in the traditional sense (pedestrians, vehicles), motion tracking can be considered a form of trajectory prediction for humanoids. The use of a large model trained on extensive motion capture data also connects to the large language model theme, albeit in a different modality (motion data instead of text). The mention of vision-language-action (VLA) models further strengthens the connection to the large model domain.", "keywords": ["motion tracking", "foundation models", "humanoid control", "large models", "vision-language-action models", "motion data"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.07927", "pdf": "https://arxiv.org/pdf/2511.07927", "abs": "https://arxiv.org/abs/2511.07927", "authors": ["Okan Arif Guvenkaya", "Selim Ahmet Iz", "Mustafa Unel"], "title": "Local Path Planning with Dynamic Obstacle Avoidance in Unstructured Environments", "categories": ["cs.RO", "math.DS", "math.OC"], "comment": null, "summary": "Obstacle avoidance and path planning are essential for guiding unmanned ground vehicles (UGVs) through environments that are densely populated with dynamic obstacles. This paper develops a novel approach that combines tangentbased path planning and extrapolation methods to create a new decision-making algorithm for local path planning. In the assumed scenario, a UGV has a prior knowledge of its initial and target points within the dynamic environment. A global path has already been computed, and the robot is provided with waypoints along this path. As the UGV travels between these waypoints, the algorithm aims to avoid collisions with dynamic obstacles. These obstacles follow polynomial trajectories, with their initial positions randomized in the local map and velocities randomized between O and the allowable physical velocity limit of the robot, along with some random accelerations. The developed algorithm is tested in several scenarios where many dynamic obstacles move randomly in the environment. Simulation results show the effectiveness of the proposed local path planning strategy by gradually generating a collision free path which allows the robot to navigate safely between initial and the target locations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on local path planning and dynamic obstacle avoidance, which are related to trajectory prediction. However, it does not involve large language models. The algorithm uses extrapolation methods to predict obstacle trajectories, indicating a connection to trajectory prediction, but the lack of LLMs reduces the overall relevance.", "keywords": ["path planning", "dynamic obstacle avoidance", "trajectory", "extrapolation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.07896", "pdf": "https://arxiv.org/pdf/2511.07896", "abs": "https://arxiv.org/abs/2511.07896", "authors": ["Dengcan Liu", "Jiahao Li", "Zheren Fu", "Yi Tu", "Jiajun Li", "Zhendong Mao", "Yongdong Zhang"], "title": "SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder", "categories": ["cs.AI", "cs.CL"], "comment": "15pages,11figures,AAAI-26", "summary": "Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5956\u52b1\u6a21\u578b(Reward Model, RM)\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u504f\u597d\u5efa\u6a21\u65b9\u6cd5SparseRM\u3002 \u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u8bba\u6587\u7684\u6838\u5fc3\u5728\u4e8e\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5c5e\u4e8e\u5927\u6a21\u578b\u9886\u57df\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "Reward Models", "Sparse Autoencoder", "preference modeling", "alignment"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.08001", "pdf": "https://arxiv.org/pdf/2511.08001", "abs": "https://arxiv.org/abs/2511.08001", "authors": ["Avishav Engle", "Andrey Zhitnikov", "Oren Salzman", "Omer Ben-Porat", "Kiril Solovey"], "title": "Effective Game-Theoretic Motion Planning via Nested Search", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "To facilitate effective, safe deployment in the real world, individual robots must reason about interactions with other agents, which often occur without explicit communication. Recent work has identified game theory, particularly the concept of Nash Equilibrium (NE), as a key enabler for behavior-aware decision-making. Yet, existing work falls short of fully unleashing the power of game-theoretic reasoning. Specifically, popular optimization-based methods require simplified robot dynamics and tend to get trapped in local minima due to convexification. Other works that rely on payoff matrices suffer from poor scalability due to the explicit enumeration of all possible trajectories. To bridge this gap, we introduce Game-Theoretic Nested Search (GTNS), a novel, scalable, and provably correct approach for computing NEs in general dynamical systems. GTNS efficiently searches the action space of all agents involved, while discarding trajectories that violate the NE constraint (no unilateral deviation) through an inner search over a lower-dimensional space. Our algorithm enables explicit selection among equilibria by utilizing a user-specified global objective, thereby capturing a rich set of realistic interactions. We demonstrate the approach on a variety of autonomous driving and racing scenarios where we achieve solutions in mere seconds on commodity hardware.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4e8e\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u8fd0\u52a8\u89c4\u5212\uff0c\u7279\u522b\u662f\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u884c\u4e3a\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\uff0c\u5c24\u5176\u662f\u5728\u6a21\u62df\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u7684\u573a\u666f\u4e0b\u3002\u5173\u952e\u8bcd\u4e2d\u5305\u542b\u4e86\u201cautonomous driving\u201d\u548c\u201ctrajectories\u201d\uff0c\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u5173\u8054\u3002", "keywords": ["motion planning", "autonomous driving", "trajectories", "game theory", "behavior-aware decision-making"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.07813", "pdf": "https://arxiv.org/pdf/2511.07813", "abs": "https://arxiv.org/abs/2511.07813", "authors": ["Haida Feng", "Hao Wei", "Zewen Xu", "Haolin Wang", "Chade Li", "Yihong Wu"], "title": "Sparse3DPR: Training-Free 3D Hierarchical Scene Parsing and Task-Adaptive Subgraph Reasoning from Sparse RGB Views", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have been explored widely for 3D scene understanding. Among them, training-free approaches are gaining attention for their flexibility and generalization over training-based methods. However, they typically struggle with accuracy and efficiency in practical deployment. To address the problems, we propose Sparse3DPR, a novel training-free framework for open-ended scene understanding, which leverages the reasoning capabilities of pre-trained LLMs and requires only sparse-view RGB inputs. Specifically, we introduce a hierarchical plane-enhanced scene graph that supports open vocabulary and adopts dominant planar structures as spatial anchors, which enables clearer reasoning chains and more reliable high-level inferences. Furthermore, we design a task-adaptive subgraph extraction method to filter query-irrelevant information dynamically, reducing contextual noise and improving 3D scene reasoning efficiency and accuracy. Experimental results demonstrate the superiority of Sparse3DPR, which achieves a 28.7% EM@1 improvement and a 78.2% speedup compared with ConceptGraphs on the Space3D-Bench. Moreover, Sparse3DPR obtains comparable performance to training-based methods on ScanQA, with additional real-world experiments confirming its robustness and generalization capability.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 3D scene understanding using large language models (LLMs). While it doesn't directly deal with trajectory prediction, the use of LLMs for reasoning about spatial information and scene understanding could potentially be relevant to future work in trajectory prediction, especially in complex environments where understanding the scene is crucial for predicting movement. The connection is not direct, hence the score is not very high.", "keywords": ["large language models", "LLMs", "3D scene understanding", "reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.08098", "pdf": "https://arxiv.org/pdf/2511.08098", "abs": "https://arxiv.org/abs/2511.08098", "authors": ["Sabrina Patania", "Luca Annese", "Anita Pellegrini", "Silvia Serino", "Anna Lambiase", "Luca Pallonetto", "Silvia Rossi", "Simone Colombani", "Tom Foulsham", "Azzurra Ruggeri", "Dimitri Ognibene"], "title": "PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.HC"], "comment": "Accepted at IAS19", "summary": "Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on enhancing LLM's collaboration skills through perspective taking and active vision. While it doesn't directly address trajectory prediction, it explores the application of LLMs in robotics and multi-agent systems, which are often related to trajectory planning and understanding agent behaviors. The 'active vision' aspect could potentially involve predicting future states or actions, but this isn't explicitly stated. The connection to LLMs is strong.", "keywords": ["Large Language Models (LLMs)", "foundation models", "robotics", "multi-agent systems", "active vision"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.08214", "pdf": "https://arxiv.org/pdf/2511.08214", "abs": "https://arxiv.org/abs/2511.08214", "authors": ["Yi Huang", "Zhan Qu", "Lihui Jiang", "Bingbing Liu", "Hongbo Zhang"], "title": "Prioritizing Perception-Guided Self-Supervision: A New Paradigm for Causal Modeling in End-to-End Autonomous Driving", "categories": ["cs.RO"], "comment": "Accepted at NeurIPS 2025", "summary": "End-to-end autonomous driving systems, predominantly trained through imitation learning, have demonstrated considerable effectiveness in leveraging large-scale expert driving data. Despite their success in open-loop evaluations, these systems often exhibit significant performance degradation in closed-loop scenarios due to causal confusion. This confusion is fundamentally exacerbated by the overreliance of the imitation learning paradigm on expert trajectories, which often contain unattributable noise and interfere with the modeling of causal relationships between environmental contexts and appropriate driving actions.\n  To address this fundamental limitation, we propose Perception-Guided Self-Supervision (PGS) - a simple yet effective training paradigm that leverages perception outputs as the primary supervisory signals, explicitly modeling causal relationships in decision-making. The proposed framework aligns both the inputs and outputs of the decision-making module with perception results, such as lane centerlines and the predicted motions of surrounding agents, by introducing positive and negative self-supervision for the ego trajectory. This alignment is specifically designed to mitigate causal confusion arising from the inherent noise in expert trajectories.\n  Equipped with perception-driven supervision, our method, built on a standard end-to-end architecture, achieves a Driving Score of 78.08 and a mean success rate of 48.64% on the challenging closed-loop Bench2Drive benchmark, significantly outperforming existing state-of-the-art methods, including those employing more complex network architectures and inference pipelines. These results underscore the effectiveness and robustness of the proposed PGS framework and point to a promising direction for addressing causal confusion and enhancing real-world generalization in autonomous driving.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u56e0\u4e3a\u5b83\u6d89\u53ca\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8f66\u8f86\u8f68\u8ff9\u7684\u89c4\u5212\u548c\u9884\u6d4b\uff0c\u7279\u522b\u662fego trajectory\u7684\u4f18\u5316\u3002 \u6458\u8981\u4e2d\u63d0\u5230\u4e86\"predicted motions of surrounding agents\"\uff0c\u4e5f\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u6a21\u578b\uff0c\u4f46end-to-end autonomous driving systems\u901a\u5e38\u4f1a\u53d7\u76ca\u4e8e\u66f4\u5927\u7684\u6a21\u578b\u3002", "keywords": ["trajectory prediction", "autonomous driving", "motion prediction", "ego trajectory", "perception"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.08277", "pdf": "https://arxiv.org/pdf/2511.08277", "abs": "https://arxiv.org/abs/2511.08277", "authors": ["Dehan Shen", "Changhao Chen"], "title": "X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on inertial odometry, specifically improving the accuracy of trajectory estimation using IMU data. While it doesn't directly involve Large Language Models, it is related to trajectory prediction for different platforms (pedestrians and quadruped robots). The core contribution is a novel network architecture for processing IMU data to predict displacement and estimate the trajectory. Therefore, the relevance is moderate.", "keywords": ["inertial odometry", "trajectory error", "pedestrian navigation", "trajectory estimation", "IMU", "motion prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
