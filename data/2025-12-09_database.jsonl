{"id": "2512.05203", "pdf": "https://arxiv.org/pdf/2512.05203", "abs": "https://arxiv.org/abs/2512.05203", "authors": ["Vinicius Stein Dani", "Xixi Lu", "Iris Beerepoot"], "title": "Integrating Wearable Data into Process Mining: Event, Case and Activity Enrichment", "categories": ["cs.DB", "cs.CY"], "comment": "Accepted manuscript (on August 22, 2025) to the 1st International Workshop on Personal and Human-Centric Process Mining (PHPM 2025), held in conjunction with the 7th International Conference on Process Mining (ICPM 2025)", "summary": "In this short paper, we explore the enrichment of event logs with data from wearable devices. We discuss three approaches: (1) treating wearable data as event attributes, linking them directly to individual events, (2) treating wearable data as case attributes, using aggregated day-level scores, and (3) introducing new events derived from wearable data, such as sleep episodes or physical activities. To illustrate these approaches, we use real-world data from one person, matching health data from a smartwatch with events extracted from a digital calendar application. Finally, we discuss the technical and conceptual challenges involved in integrating wearable data into process mining for personal productivity and well-being."}
{"id": "2512.05399", "pdf": "https://arxiv.org/pdf/2512.05399", "abs": "https://arxiv.org/abs/2512.05399", "authors": ["Sepanta Zeighami", "Shreya Shankar", "Aditya Parameswaran"], "title": "Featurized-Decomposition Join: Low-Cost Semantic Joins with Guarantees", "categories": ["cs.DB"], "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used within data systems to process large datasets with text fields. A broad class of such tasks involves a semantic join-joining two tables based on a natural language predicate per pair of tuples, evaluated using an LLM. Semantic joins generalize tasks such as entity matching and record categorization, as well as more complex text understanding tasks. A naive implementation is expensive as it requires invoking an LLM for every pair of rows in the cross product. Existing approaches mitigate this cost by first applying embedding-based semantic similarity to filter candidate pairs, deferring to an LLM only when similarity scores are deemed inconclusive. However, these methods yield limited gains in practice, since semantic similarity may not reliably predict the join outcome. We propose Featurized-Decomposition Join (FDJ for short), a novel approach for performing semantic joins that significantly reduces cost while preserving quality. FDJ automatically extracts features and combines them into a logical expression in conjunctive normal form that we call a featurized decomposition to effectively prune out non-matching pairs. A featurized decomposition extracts key information from text records and performs inexpensive comparisons on the extracted features. We show how to use LLMs to automatically extract reliable features and compose them into logical expressions while providing statistical guarantees on the output result-an inherently challenging problem due to dependencies among features. Experiments on real-world datasets show up to 10 times reduction in cost compared with the state-of-the-art while providing the same quality guarantees."}
{"id": "2512.05417", "pdf": "https://arxiv.org/pdf/2512.05417", "abs": "https://arxiv.org/abs/2512.05417", "authors": ["Jinghe Song", "Zongyu Zuo", "Xuelian Lin", "Yang Wang", "Shuai Ma"], "title": "PETGraphDB: A Property Evolution Temporal Graph Data Management System", "categories": ["cs.DB"], "comment": null, "summary": "Temporal graphs are graphs whose nodes and edges, together with their associated properties, continuously change over time. With the development of Internet of Things (IoT) systems, a subclass of the temporal graph, i.e., Property Evolution Temporal Graph, in which the value of properties on nodes or edges changes frequently while the graph's topology barely changes, is growing rapidly. However, existing temporal graph management solutions are not oriented to the Property Evolution Temporal Graph data, which leads to highly complex data modeling and low-performance query processing of temporal graph queries. To solve these problems, we developed PETGraph, a data management system for Property Evolution Temporal Graph data. PETGraph adopts a valid-time temporal property graph data model to facilitate data modeling, supporting ACID features with transactions. To improve temporal graph query performance, we designed a space-efficient temporal property storage and a fine-granularity multi-level locking mechanism. Experimental results show that PETGraph requires, on average, only 33% of the storage space needed by the current best data management solution. Additionally, it achieves an average of 58.8 times higher transaction throughput in HTAP workloads compared to the best current solutions and outperforms them by an average of 267 times in query latency."}
{"id": "2512.05453", "pdf": "https://arxiv.org/pdf/2512.05453", "abs": "https://arxiv.org/abs/2512.05453", "authors": ["Luc Moreau", "Alfred Rossi", "Sophie Stalla-Bourdillon"], "title": "Parajudica: An RDF-Based Reasoner and Metamodel for Multi-Framework Context-Dependent Data Compliance Assessments", "categories": ["cs.DB", "cs.AI", "cs.CY", "cs.LO"], "comment": "17 pages, 8 figures. Code and examples available at https://github.com/alfredr/parajudica", "summary": "Motivated by the challenges of implementing policy-based data access control (PBAC) under multiple simultaneously applicable compliance frameworks, we present Parajudica, an open, modular, and extensible RDF/SPARQL-based rule system for evaluating context-dependent data compliance status. We demonstrate the utility of this resource and accompanying metamodel through application to existing legal frameworks and industry standards, offering insights for comparative framework analysis. Applications include compliance policy enforcement, compliance monitoring, data discovery, and risk assessment."}
{"id": "2512.05525", "pdf": "https://arxiv.org/pdf/2512.05525", "abs": "https://arxiv.org/abs/2512.05525", "authors": ["Nils Strassenburg", "Boris Glavic", "Tilmann Rabl"], "title": "Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks."}
{"id": "2512.05374", "pdf": "https://arxiv.org/pdf/2512.05374", "abs": "https://arxiv.org/abs/2512.05374", "authors": ["Charlie Summers", "Haneen Mohammed", "Eugene Wu"], "title": "Please Don't Kill My Vibe: Empowering Agents with Data Flow Control", "categories": ["cs.CR", "cs.AI", "cs.DB"], "comment": "7 pages, 7 figures, CIDR 2026", "summary": "The promise of Large Language Model (LLM) agents is to perform complex, stateful tasks. This promise is stunted by significant risks - policy violations, process corruption, and security flaws - that stem from the lack of visibility and mechanisms to manage undesirable data flows produced by agent actions. Today, agent workflows are responsible for enforcing these policies in ad hoc ways. Just as data validation and access controls shifted from the application to the DBMS, freeing application developers from these concerns, we argue that systems should support Data Flow Controls (DFCs) and enforce DFC policies natively. This paper describes early work developing a portable instance of DFC for DBMSes and outlines a broader research agenda toward DFC for agent ecosystems."}
