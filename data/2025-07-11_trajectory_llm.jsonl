{"id": "2507.06564", "pdf": "https://arxiv.org/pdf/2507.06564", "abs": "https://arxiv.org/abs/2507.06564", "authors": ["Tianshun Li", "Tianyi Huai", "Zhen Li", "Yichun Gao", "Haoang Li", "Xinhu Zheng"], "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "8 pages, 9 figures, has been accepted by IROS 2025", "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper directly integrates Large Language Models (LLMs) for vision-and-language navigation (VLN) of UAVs, which involves trajectory planning and control (NMPC). The use of LLMs for interpreting instructions and navigating dynamic environments connects it to both trajectory prediction and large language models.", "keywords": ["Large Language Models", "LLMs", "vision-and-language navigation", "trajectory tracking", "Nonlinear Model Predictive Control", "NMPC", "UAV", "navigation"]}}
{"id": "2507.06441", "pdf": "https://arxiv.org/pdf/2507.06441", "abs": "https://arxiv.org/abs/2507.06441", "authors": ["Shanting Wang", "Panagiotis Typaldos", "Chenjun Li", "Andreas A. Malikopoulos"], "title": "VisioPath: Vision-Language Enhanced Model Predictive Control for Safe Autonomous Navigation in Mixed Traffic", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "In this paper, we introduce VisioPath, a novel framework combining\nvision-language models (VLMs) with model predictive control (MPC) to enable\nsafe autonomous driving in dynamic traffic environments. The proposed approach\nleverages a bird's-eye view video processing pipeline and zero-shot VLM\ncapabilities to obtain structured information about surrounding vehicles,\nincluding their positions, dimensions, and velocities. Using this rich\nperception output, we construct elliptical collision-avoidance potential fields\naround other traffic participants, which are seamlessly integrated into a\nfinite-horizon optimal control problem for trajectory planning. The resulting\ntrajectory optimization is solved via differential dynamic programming with an\nadaptive regularization scheme and is embedded in an event-triggered MPC loop.\nTo ensure collision-free motion, a safety verification layer is incorporated in\nthe framework that provides an assessment of potential unsafe trajectories.\nExtensive simulations in Simulation of Urban Mobility (SUMO) demonstrate that\nVisioPath outperforms conventional MPC baselines across multiple metrics. By\ncombining modern AI-driven perception with the rigorous foundation of optimal\ncontrol, VisioPath represents a significant step forward in safe trajectory\nplanning for complex traffic systems.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant as it combines vision-language models (VLMs), a type of large model, with model predictive control (MPC) for trajectory planning in autonomous driving. It explicitly uses VLMs for perception and incorporates the output into a trajectory optimization framework.", "keywords": ["trajectory prediction", "vision-language models", "VLMs", "model predictive control", "MPC", "autonomous navigation", "trajectory planning"]}}
{"id": "2507.05116", "pdf": "https://arxiv.org/pdf/2507.05116", "abs": "https://arxiv.org/abs/2507.05116", "authors": ["Juyi Lin", "Amir Taherin", "Arash Akbari", "Arman Akbari", "Lei Lu", "Guangyu Chen", "Taskin Padir", "Xiaomeng Yang", "Weiwei Chen", "Yiqian Li", "Xue Lin", "David Kaeli", "Pu Zhao", "Yanzhi Wang"], "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35$\\times$ faster inference and 145 Hz\nthroughput. All the details and codes will be open-sourced.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on Vision-Language-Action (VLA) models for robotic manipulation. While it doesn't explicitly mention 'trajectory prediction', the 'action prediction' aspect, especially in the context of robotic manipulation, is closely related. The paper also utilizes and optimizes large-scale vision language action models, indicating relevance to large language models. The ensemble voting strategy for action sampling also hints at a prediction aspect. Therefore, the paper has a moderate degree of relevance.", "keywords": ["large language models", "VLA models", "action prediction", "robotic manipulation", "ensemble voting"]}}
{"id": "2507.06747", "pdf": "https://arxiv.org/pdf/2507.06747", "abs": "https://arxiv.org/abs/2507.06747", "authors": ["Daojie Peng", "Jiahang Cao", "Qiang Zhang", "Jun Ma"], "title": "LOVON: Legged Open-Vocabulary Object Navigator", "categories": ["cs.RO", "cs.CV"], "comment": "9 pages, 10 figures; Project Page:\n  https://daojiepeng.github.io/LOVON/", "summary": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper uses large language models for hierarchical task planning in object navigation, which involves planning and executing trajectories. While not directly focused on trajectory *prediction*, the navigation aspect implies trajectory generation and control. The use of LLMs is a major component, making it relevant to the prompt.", "keywords": ["large language models", "LLMs", "object navigation", "task planning", "long-range navigation"]}}
{"id": "2507.06531", "pdf": "https://arxiv.org/pdf/2507.06531", "abs": "https://arxiv.org/abs/2507.06531", "authors": ["Mingjin Zeng", "Nan Ouyang", "Wenkang Wan", "Lei Ao", "Qing Cai", "Kai Sheng"], "title": "ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture", "categories": ["cs.CV"], "comment": null, "summary": "Trajectory prediction for multi-agent interaction scenarios is a crucial\nchallenge. Most advanced methods model agent interactions by efficiently\nfactorized attention based on the temporal and agent axes. However, this static\nand foward modeling lacks explicit interactive spatio-temporal coordination,\ncapturing only obvious and immediate behavioral intentions. Alternatively, the\nmodern trajectory prediction framework refines the successive predictions by a\nfixed-anchor selection strategy, which is difficult to adapt in different\nfuture environments. It is acknowledged that human drivers dynamically adjust\ninitial driving decisions based on further assumptions about the intentions of\nsurrounding vehicles. Motivated by human driving behaviors, this paper proposes\nILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)\nattention and Dynamic Anchor Selection (DAS) module. IL Attention employs an\ninverse learning paradigm to model interactions at neighboring moments,\nintroducing proposed intentions to dynamically encode the spatio-temporal\ncoordination of interactions, thereby enhancing the model's ability to capture\ncomplex interaction patterns. Then, the learnable DAS module is proposed to\nextract multiple trajectory change keypoints as anchors in parallel with almost\nno increase in parameters. Experimental results show that the ILNet achieves\nstate-of-the-art performance on the INTERACTION and Argoverse motion\nforecasting datasets. Particularly, in challenged interaction scenarios, ILNet\nachieves higher accuracy and more multimodal distributions of trajectories over\nfewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u573a\u666f\u4e0b\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5bc6\u5207\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "multi-agent interaction", "attention", "motion forecasting"]}}
{"id": "2507.06830", "pdf": "https://arxiv.org/pdf/2507.06830", "abs": "https://arxiv.org/abs/2507.06830", "authors": ["Tao Feng", "Xianbing Zhao", "Zhenhua Chen", "Tien Tsin Wong", "Hamid Rezatofighi", "Gholamreza Haffari", "Lizhen Qu"], "title": "Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in diffusion-based and autoregressive video generation models\nhave achieved remarkable visual realism. However, these models typically lack\naccurate physical alignment, failing to replicate real-world dynamics in object\nmotion. This limitation arises primarily from their reliance on learned\nstatistical correlations rather than capturing mechanisms adhering to physical\nlaws. To address this issue, we introduce a novel framework that integrates\nsymbolic regression (SR) and trajectory-guided image-to-video (I2V) models for\nphysics-grounded video forecasting. Our approach extracts motion trajectories\nfrom input videos, uses a retrieval-based pre-training mechanism to enhance\nsymbolic regression, and discovers equations of motion to forecast physically\naccurate future trajectories. These trajectories then guide video generation\nwithout requiring fine-tuning of existing models. Evaluated on scenarios in\nClassical Mechanics, including spring-mass, pendulums, and projectile motions,\nour method successfully recovers ground-truth analytical equations and improves\nthe physical alignment of generated videos over baseline methods.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on trajectory prediction for video generation, specifically using equations of motion derived from observed trajectories. While it doesn't directly use or mention Large Language Models, the concept of \"forecasting trajectories\" and using \"retrieval-based pre-training\" for symbolic regression are relevant to trajectory prediction and could potentially be combined with LLMs in future work. The connection to large models is weaker but present through the \"retrieval-based pre-training\" aspect.", "keywords": ["trajectory prediction", "motion forecasting", "trajectory-guided", "equation discovery", "symbolic regression", "retrieval-based pre-training"]}}
{"id": "2507.06952", "pdf": "https://arxiv.org/pdf/2507.06952", "abs": "https://arxiv.org/abs/2507.06952", "authors": ["Keyon Vafa", "Peter G. Chang", "Ashesh Rambachan", "Sendhil Mullainathan"], "title": "What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models", "categories": ["cs.LG", "cs.AI"], "comment": "To appear in ICML 2025", "summary": "Foundation models are premised on the idea that sequence prediction can\nuncover deeper domain understanding, much like how Kepler's predictions of\nplanetary motion later led to the discovery of Newtonian mechanics. However,\nevaluating whether these models truly capture deeper structure remains a\nchallenge. We develop a technique for evaluating foundation models that\nexamines how they adapt to synthetic datasets generated from some postulated\nworld model. Our technique measures whether the foundation model's inductive\nbias aligns with the world model, and so we refer to it as an inductive bias\nprobe. Across multiple domains, we find that foundation models can excel at\ntheir training tasks yet fail to develop inductive biases towards the\nunderlying world model when adapted to new tasks. We particularly find that\nfoundation models trained on orbital trajectories consistently fail to apply\nNewtonian mechanics when adapted to new physics tasks. Further analysis reveals\nthat these models behave as if they develop task-specific heuristics that fail\nto generalize.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper explores the ability of foundation models to learn underlying world models from sequence prediction, specifically using orbital trajectories as a case study. While it doesn't directly focus on trajectory prediction as in pedestrian or vehicle prediction, it does investigate how foundation models learn from trajectory data and their ability to generalize. It also directly mentions foundation models and sequence prediction, linking it to the large language model area. The connection to trajectory prediction is through the use of orbital trajectories as a test case.", "keywords": ["foundation models", "sequence prediction", "orbital trajectories", "inductive bias", "world models"]}}
{"id": "2507.06404", "pdf": "https://arxiv.org/pdf/2507.06404", "abs": "https://arxiv.org/abs/2507.06404", "authors": ["Matteo Tiezzi", "Tommaso Apicella", "Carlos Cardenas-Perez", "Giovanni Fregonese", "Stefano Dafarra", "Pietro Morerio", "Daniele Pucci", "Alessio Del Bue"], "title": "Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Evaluating and comparing the performance of autonomous Humanoid Robots is\nchallenging, as success rate metrics are difficult to reproduce and fail to\ncapture the complexity of robot movement trajectories, critical in Human-Robot\nInteraction and Collaboration (HRIC). To address these challenges, we propose a\ngeneral evaluation framework that measures the quality of Imitation Learning\n(IL) methods by focusing on trajectory performance. We devise the Neural Meta\nEvaluator (NeME), a deep learning model trained to classify actions from robot\njoint trajectories. NeME serves as a meta-evaluator to compare the performance\nof robot control policies, enabling policy evaluation without requiring human\ninvolvement in the loop. We validate our framework on ergoCub, a humanoid\nrobot, using teleoperation data and comparing IL methods tailored to the\navailable platform. The experimental results indicate that our method is more\naligned with the success rate obtained on the robot than baselines, offering a\nreproducible, systematic, and insightful means for comparing the performance of\nmultimodal imitation learning approaches in complex HRI tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u673a\u5668\u4eba\u8f68\u8ff9\u8bc4\u4f30\u548c\u6a21\u4eff\u5b66\u4e60\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7528\u4e8e\u8bc4\u4f30\u8f68\u8ff9\u8d28\u91cf\uff0c\u8868\u660e\u4e86\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u7684\u76f8\u5173\u6027\u3002\u8bba\u6587\u6ca1\u6709\u63d0\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["trajectory performance", "Imitation Learning", "robot joint trajectories", "policy evaluation"]}}
{"id": "2507.06993", "pdf": "https://arxiv.org/pdf/2507.06993", "abs": "https://arxiv.org/abs/2507.06993", "authors": ["Jieren Deng", "Aleksandar Cvetkovic", "Pak Kiu Chung", "Dragomir Yankov", "Chiqun Zhang"], "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u63d0\u5230\u4e86\u4f7f\u7528LLM\u6765\u589e\u5f3a\u89c4\u5212\u3001\u5bfc\u822a\u548c\u52a8\u6001\u9002\u5e94\u6027\u3002\u867d\u7136\u6ca1\u6709\u660e\u786e\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5bfc\u822a\u548c\u8def\u5f84\u89c4\u5212\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u4f7f\u7528\u4e86LLM\u8fdb\u884c\u65c5\u884c\u89c4\u5212\u548c\u5bfc\u822a\uff0c\u5e76\u6d89\u53ca\u52a8\u6001\u9002\u5e94\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["LLM", "Large Language Models", "navigation", "planning", "Retrieval-Augmented Generation", "RAG"]}}
{"id": "2507.06605", "pdf": "https://arxiv.org/pdf/2507.06605", "abs": "https://arxiv.org/abs/2507.06605", "authors": ["Xinyu Wu"], "title": "Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration", "categories": ["cs.RO"], "comment": null, "summary": "Classical sampling-based motion planners like the RRTs suffer from\ninefficiencies, particularly in cluttered or high-dimensional spaces, due to\ntheir reliance on undirected, random sampling. This paper introduces the\nEpisodic RRT, a novel hybrid planning framework that replaces the primitive of\na random point with a learned, multi-step \"exploratory episode\" generated by a\nDeep Reinforcement Learning agent. By making the DRL agent the engine of\nexploration, ERRT transforms the search process from a diffuse, volumetric\nexpansion into a directed, branch-like growth. This paradigm shift yields key\nadvantages: it counters the curse of dimensionality with focused exploration,\nminimizes expensive collision checks by proactively proposing locally valid\npaths, and improves connectivity by generating inherently connected path\nsegments. We demonstrate through extensive empirical evaluation across 2D, 3D,\nand 6D environments that ERRT and its variants consistently and significantly\noutperform their classical counterparts. In a challenging 6D robotic arm\nscenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to\n107x faster, reduces collision checks by over 99.6%, and finds initial paths\nthat are nearly 50% shorter. Furthermore, its asymptotically optimal variant,\nERRT*, demonstrates vastly superior anytime performance, refining solutions to\nnear-optimality up to 29x faster than standard RRT* in 3D environments. Code:\nhttps://xinyuwuu.github.io/Episodic_RRT/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving RRT (Rapidly-exploring Random Tree) algorithms for motion planning using Deep Reinforcement Learning. While it doesn't directly address trajectory prediction in the sense of predicting future trajectories of agents, it does deal with generating paths for agents, which is related to trajectory generation and path planning. It doesn't involve Large Language Models.", "keywords": ["motion planning", "RRT", "path planning", "reinforcement learning", "exploration"]}}
{"id": "2507.06710", "pdf": "https://arxiv.org/pdf/2507.06710", "abs": "https://arxiv.org/abs/2507.06710", "authors": ["Zhenyang Liu", "Yikai Wang", "Kuanning Wang", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "title": "Spatial-Temporal Aware Visuomotor Diffusion Policy Learning", "categories": ["cs.RO"], "comment": null, "summary": "Visual imitation learning is effective for robots to learn versatile tasks.\nHowever, many existing methods rely on behavior cloning with supervised\nhistorical trajectories, limiting their 3D spatial and 4D spatiotemporal\nawareness. Consequently, these methods struggle to capture the 3D structures\nand 4D spatiotemporal relationships necessary for real-world deployment. In\nthis work, we propose 4D Diffusion Policy (DP4), a novel visual imitation\nlearning method that incorporates spatiotemporal awareness into diffusion-based\npolicies. Unlike traditional approaches that rely on trajectory cloning, DP4\nleverages a dynamic Gaussian world model to guide the learning of 3D spatial\nand 4D spatiotemporal perceptions from interactive environments. Our method\nconstructs the current 3D scene from a single-view RGB-D observation and\npredicts the future 3D scene, optimizing trajectory generation by explicitly\nmodeling both spatial and temporal dependencies. Extensive experiments across\n17 simulation tasks with 173 variants and 3 real-world robotic tasks\ndemonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,\nimproving the average simulation task success rate by 16.4% (Adroit), 14%\n(DexArt), and 6.45% (RLBench), and the average real-world robotic task success\nrate by 8.6%.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on learning visuomotor policies for robots using a diffusion-based approach. While it doesn't directly use or mention large language models, it does deal with trajectory generation and prediction in the context of robot learning, making it moderately relevant to trajectory prediction. The use of a diffusion model also connects to generative models, which are sometimes related to large language models in the broader context of AI.", "keywords": ["trajectory generation", "diffusion policy", "imitation learning", "spatiotemporal awareness", "robot learning"]}}
{"id": "2507.06466", "pdf": "https://arxiv.org/pdf/2507.06466", "abs": "https://arxiv.org/abs/2507.06466", "authors": ["Aaron Dharna", "Cong Lu", "Jeff Clune"], "title": "Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models", "categories": ["cs.LG", "cs.AI"], "comment": "67 pages, accepted to RLC 2025", "summary": "Multi-agent interactions have long fueled innovation, from natural\npredator-prey dynamics to the space race. Self-play (SP) algorithms try to\nharness these dynamics by pitting agents against ever-improving opponents,\nthereby creating an implicit curriculum toward learning high-quality solutions.\nHowever, SP often fails to produce diverse solutions and can get stuck in\nlocally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a\nnew direction that leverages the code-generation capabilities and vast\nknowledge of foundation models (FMs) to overcome these challenges by leaping\nacross local optima in policy space. We propose a family of approaches: (1)\n\\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent\npolicies via competitive self-play; (2) \\textbf{Novelty-Search Self-Play\n(NSSP)} builds a diverse population of strategies, ignoring performance; and\n(3) the most promising variant, \\textbf{Quality-Diveristy Self-Play (QDSP)},\ncreates a diverse set of high-quality policies by combining the diversity of\nNSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a\ncontinuous-control pursuer-evader setting, and in Gandalf, a simple AI safety\nsimulation in which an attacker tries to jailbreak an LLM's defenses. In Car\nTag, FMSPs explore a wide variety of reinforcement learning, tree search, and\nheuristic-based methods, to name just a few. In terms of discovered policy\nquality, \\ouralgo and vFMSP surpass strong human-designed strategies. In\nGandalf, FMSPs can successfully automatically red-team an LLM, breaking through\nand jailbreaking six different, progressively stronger levels of defense.\nFurthermore, FMSPs can automatically proceed to patch the discovered\nvulnerabilities. Overall, FMSPs represent a promising new research frontier of\nimproving self-play with foundation models, opening fresh paths toward more\ncreative and open-ended strategy discovery", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using foundation models for self-play in various environments, including a continuous control pursuer-evader setting (Car Tag). While Car Tag can be interpreted as a simplified form of trajectory prediction problem (evasion), the primary focus is on self-play and strategy innovation using LLMs. The connection to trajectory prediction is not explicit or central, but the 'Car Tag' environment hints at a potential link. The paper also mentions jailbreaking LLMs, further strengthening the LLM relevance.", "keywords": ["foundation models", "LLMs", "self-play", "Car Tag", "continuous control"]}}
{"id": "2507.06502", "pdf": "https://arxiv.org/pdf/2507.06502", "abs": "https://arxiv.org/abs/2507.06502", "authors": ["Yiwen Liu", "Chenyu Zhang", "Junjie Song", "Siqi Chen", "Sun Yin", "Zihan Wang", "Lingming Zeng", "Yuji Cao", "Junming Jiao"], "title": "MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As a prominent data modality task, time series forecasting plays a pivotal\nrole in diverse applications. With the remarkable advancements in Large\nLanguage Models (LLMs), the adoption of LLMs as the foundational architecture\nfor time series modeling has gained significant attention. Although existing\nmodels achieve some success, they rarely both model time and frequency\ncharacteristics in a pretraining-finetuning paradigm leading to suboptimal\nperformance in predictions of complex time series, which requires both modeling\nperiodicity and prior pattern knowledge of signals. We propose MoFE-Time, an\ninnovative time series forecasting model that integrates time and frequency\ndomain features within a Mixture of Experts (MoE) network. Moreover, we use the\npretraining-finetuning paradigm as our training framework to effectively\ntransfer prior pattern knowledge across pretraining and finetuning datasets\nwith different periodicity distributions. Our method introduces both frequency\nand time cells as experts after attention modules and leverages the MoE routing\nmechanism to construct multidimensional sparse representations of input\nsignals. In experiments on six public benchmarks, MoFE-Time has achieved new\nstate-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared\nto the representative methods Time-MoE. Beyond the existing evaluation\nbenchmarks, we have developed a proprietary dataset, NEV-sales, derived from\nreal-world business scenarios. Our method achieves outstanding results on this\ndataset, underscoring the effectiveness of the MoFE-Time model in practical\ncommercial applications.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u867d\u7136\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u4f46\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u4f5c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u7684\u57fa\u7840\u67b6\u6784\uff0c\u5e76\u4f7f\u7528\u4e86\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u5f0f\u3002\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u8054\u6027\uff0c\u5e76\u4e14\u660e\u786e\u6d89\u53ca\u5230\u4e86\u5927\u6a21\u578b\u3002", "keywords": ["Large Language Models", "LLMs", "time series forecasting", "pretraining-finetuning"]}}
{"id": "2507.06543", "pdf": "https://arxiv.org/pdf/2507.06543", "abs": "https://arxiv.org/abs/2507.06543", "authors": ["Taekyung Kim", "Dongyoon Han", "Byeongho Heo", "Jeongeun Park", "Sangdoo Yun"], "title": "Token Bottleneck: One Token to Remember Dynamics", "categories": ["cs.CV"], "comment": "17 pages, 9 figures, 8 tables, project page:\n  https://token-bottleneck.github.io, code: https://github.com/naver-ai/tobo", "summary": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on learning representations for dynamic scenes, which is relevant to trajectory prediction. It mentions applications in robotic manipulation and visual tracking, which are often related to predicting future states or trajectories. While it doesn't explicitly mention trajectory prediction or large language models, the core idea of learning temporal dynamics and using it for sequential tasks connects to the broader field of trajectory forecasting. The 'Token Bottleneck' could potentially be a mechanism to integrate with larger models, although this isn't directly explored in the abstract.", "keywords": ["dynamic scenes", "visual tracking", "robotic manipulation", "temporal dynamics", "sequential scene understanding"]}}
{"id": "2507.06336", "pdf": "https://arxiv.org/pdf/2507.06336", "abs": "https://arxiv.org/abs/2507.06336", "authors": ["Adam J Riesselman", "Evan M Cofer", "Therese LaRue", "Wim Meeussen"], "title": "Self-supervised learning predicts plant growth trajectories from multi-modal industrial greenhouse data", "categories": ["q-bio.QM", "cs.LG", "cs.RO"], "comment": null, "summary": "Quantifying organism-level phenotypes, such as growth dynamics and biomass\naccumulation, is fundamental to understanding agronomic traits and optimizing\ncrop production. However, quality growing data of plants at scale is difficult\nto generate. Here we use a mobile robotic platform to capture high-resolution\nenvironmental sensing and phenotyping measurements of a large-scale hydroponic\nleafy greens system. We describe a self-supervised modeling approach to build a\nmap from observed growing data to the entire plant growth trajectory. We\ndemonstrate our approach by forecasting future plant height and harvest mass of\ncrops in this system. This approach represents a significant advance in\ncombining robotic automation and machine learning, as well as providing\nactionable insights for agronomic research and operational efficiency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on predicting plant growth trajectories using self-supervised learning from multi-modal data. While it involves trajectory prediction (plant growth), it does not explicitly mention or utilize large language models. The connection to trajectory prediction is present but not in the typical context of human or vehicle movement. The use of self-supervised learning increases the relevance slightly.", "keywords": ["trajectory prediction", "self-supervised learning", "growth trajectories"]}}
{"id": "2507.06590", "pdf": "https://arxiv.org/pdf/2507.06590", "abs": "https://arxiv.org/abs/2507.06590", "authors": ["Yin Wang", "Mu li", "Zhiying Leng", "Frederick W. B. Li", "Xiaohui Liang"], "title": "MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf\ninteraction, aimed at addressing the persistent challenge of generating human\nmotion from rare language prompts. While previous approaches struggle with\ncoarse-grained matching and overlook important semantic cues due to motion\nredundancy, our key insight lies in leveraging fine-grained clip relationships\nto mitigate these issues. MOST's retrieval stage presents the first formulation\nof its kind - temporal clip Banzhaf interaction - which precisely quantifies\ntextual-motion coherence at the clip level. This facilitates direct,\nfine-grained text-to-motion clip matching and eliminates prevalent redundancy.\nIn the generation stage, a motion prompt module effectively utilizes retrieved\nmotion clips to produce semantically consistent movements. Extensive\nevaluations confirm that MOST achieves state-of-the-art text-to-motion\nretrieval and generation performance by comprehensively addressing previous\nchallenges, as demonstrated through quantitative and qualitative results\nhighlighting its effectiveness, especially for rare prompts.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating human motion from text prompts using a diffusion model. While it involves motion generation, which is related to trajectory prediction (specifically, human motion prediction), it doesn't explicitly deal with trajectory prediction in the traditional sense (e.g., predicting future locations). The use of language prompts connects it to the broader domain of large language models, although it doesn't directly utilize or evaluate LLMs themselves. The temporal clip Banzhaf interaction could be seen as a form of temporal reasoning relevant to trajectory understanding.", "keywords": ["motion generation", "diffusion model", "text-to-motion", "human motion"]}}
{"id": "2507.06719", "pdf": "https://arxiv.org/pdf/2507.06719", "abs": "https://arxiv.org/abs/2507.06719", "authors": ["Zhenyang Liu", "Sixiao Zheng", "Siyu Chen", "Cairong Zhao", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c3D\u89c6\u89c9\u5b9a\u4f4d\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u89c6\u89c9\u573a\u666f\u7406\u89e3\u548c\u7a7a\u95f4\u63a8\u7406\uff0c\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u81ea\u4e3b\u5bfc\u822a\uff0c\u4f46\u8bba\u6587\u7684\u6838\u5fc3\u8d21\u732e\u5728\u4e8e\u5229\u7528LLM\u8fdb\u884c\u7a7a\u95f4\u5173\u7cfb\u63a8\u7406\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e3D\u89c6\u89c9\u5b9a\u4f4d\u4efb\u52a1\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u76f4\u63a5\u5173\u8054\u8f83\u5f31\u3002", "keywords": ["Large Language Models", "LLM", "3D visual grounding", "spatial reasoning", "autonomous navigation"]}}
{"id": "2507.07012", "pdf": "https://arxiv.org/pdf/2507.07012", "abs": "https://arxiv.org/abs/2507.07012", "authors": ["Chengyuan Zhang", "Zhengbing He", "Cathy Wu", "Lijun Sun"], "title": "When Context Is Not Enough: Modeling Unexplained Variability in Car-Following Behavior", "categories": ["stat.AP", "cs.LG", "cs.RO"], "comment": null, "summary": "Modeling car-following behavior is fundamental to microscopic traffic\nsimulation, yet traditional deterministic models often fail to capture the full\nextent of variability and unpredictability in human driving. While many modern\napproaches incorporate context-aware inputs (e.g., spacing, speed, relative\nspeed), they frequently overlook structured stochasticity that arises from\nlatent driver intentions, perception errors, and memory effects -- factors that\nare not directly observable from context alone. To fill the gap, this study\nintroduces an interpretable stochastic modeling framework that captures not\nonly context-dependent dynamics but also residual variability beyond what\ncontext can explain. Leveraging deep neural networks integrated with\nnonstationary Gaussian processes (GPs), our model employs a scenario-adaptive\nGibbs kernel to learn dynamic temporal correlations in acceleration decisions,\nwhere the strength and duration of correlations between acceleration decisions\nevolve with the driving context. This formulation enables a principled,\ndata-driven quantification of uncertainty in acceleration, speed, and spacing,\ngrounded in both observable context and latent behavioral variability.\nComprehensive experiments on the naturalistic vehicle trajectory dataset\ncollected from the German highway, i.e., the HighD dataset, demonstrate that\nthe proposed stochastic simulation method within this framework surpasses\nconventional methods in both predictive performance and interpretable\nuncertainty quantification. The integration of interpretability and accuracy\nmakes this framework a promising tool for traffic analysis and safety-critical\napplications.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5efa\u6a21car-following\u884c\u4e3a\u6765\u63d0\u9ad8\u4ea4\u901a\u4eff\u771f\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002 \u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u4f7f\u7528\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u4e14\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u7684\u4e3b\u9898\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b", "car-following behavior", "deep neural networks", "uncertainty quantification"]}}
