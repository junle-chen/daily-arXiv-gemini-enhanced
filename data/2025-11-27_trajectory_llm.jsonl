{"id": "2511.20610", "pdf": "https://arxiv.org/pdf/2511.20610", "abs": "https://arxiv.org/abs/2511.20610", "authors": ["Gaspard Merten", "Mahmoud Sakr", "Gilles Dejaegere"], "title": "Building a Foundation Model for Trajectory from Scratch", "categories": ["cs.AI"], "comment": null, "summary": "Foundation models are transformative in artificial intelligence, but building them from scratch, especially for mobility trajectories, is not yet clear or documented. This tutorial bridges this gap by demonstrating the steps and code of a minimal implementation of a trajectory-focused foundation model starting from GPT-2. Through a concise, step-by-step, code-driven process, we demonstrate adapting GPT-2 for spatiotemporal data. We then review and compare representative trajectory foundation models, such as TrajFM and TrajGPT, highlighting their architectural innovations and differences. Additionally, we introduce complementary techniques from related domains, like TimesFM's patching approach. Targeted at researchers and practitioners, this tutorial aims to explain the concepts and terminology of foundation models, at the implementation level. We find it timely and indispensable to create this educational material in order to support the SIGSPATIAL community in building and evaluating mobility foundation models, enhancing both research clarity and peer-review effectiveness in mobility AI.", "relevance_analysis": {"relevance_score": 1.0, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u6307\u51fa\u5176\u76ee\u6807\u662f\u6784\u5efa\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u57fa\u4e8eGPT-2\u8fdb\u884c\u5b9e\u73b0\u3002\u8bba\u6587\u8fd8\u8ba8\u8bba\u4e86\u5176\u4ed6\u8f68\u8ff9\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u5982TrajFM\u548cTrajGPT\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "foundation model", "large language model", "GPT-2", "TrajFM", "TrajGPT", "mobility trajectories", "spatiotemporal data"]}}
{"id": "2511.19912", "pdf": "https://arxiv.org/pdf/2511.19912", "abs": "https://arxiv.org/abs/2511.19912", "authors": ["Dapeng Zhang", "Zhenlong Yuan", "Zhangquan Chen", "Chih-Ting Liao", "Yinda Chen", "Fei Shen", "Qingguo Zhou", "Tat-Seng Chua"], "title": "Reasoning-VLA: A Fast and General Vision-Language-Action Reasoning Model for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models have recently shown strong decision-making capabilities in autonomous driving. However, existing VLAs often struggle with achieving efficient inference and generalizing to novel autonomous vehicle configurations and driving scenarios. In this paper, we propose Reasoning-VLA, a general and fast action-generation VLA framework. The proposed model employs a set of learnable action queries, initialized via Gaussian sampling from ground-truth trajectories within the training corpus. These learnable queries interact with reasoning-enhanced vision-language features to generate continuous action trajectories in parallel. To promote robust generalization, we consolidate eight publicly available autonomous driving datasets into a standardized, Chain-of-Thought reasoning-based, and easy-to-use data format for model training. Leveraging both supervised learning and reinforcement learning fine-tuning, extensive empirical evaluations across multiple benchmarks demonstrate that Reasoning-VLA achieves state-of-the-art performance, superior generalization capability, and the excellent inference speed reported to date.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper discusses a Vision-Language-Action model for autonomous driving, which directly relates to trajectory prediction (action trajectories) and utilizes vision-language models, which can be considered related to large language models. The use of 'Chain-of-Thought reasoning' also suggests a connection to techniques used in large language models.", "keywords": ["trajectory prediction", "autonomous driving", "vision-language models", "action trajectories", "reasoning"]}}
{"id": "2511.19663", "pdf": "https://arxiv.org/pdf/2511.19663", "abs": "https://arxiv.org/abs/2511.19663", "authors": ["Ahmed Awadallah", "Yash Lara", "Raghav Magazine", "Hussein Mozannar", "Akshay Nambi", "Yash Pandya", "Aravind Rajeswaran", "Corby Rosset", "Alexey Taymanov", "Vibhav Vineet", "Spencer Whitehead", "Andrew Zhao"], "title": "Fara-7B: An Efficient Agentic Model for Computer Use", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper discusses using a large language model (Fara-7B) for computer use agents. While it doesn't directly focus on trajectory prediction in the traditional sense (e.g., pedestrian or vehicle movement), it involves predicting and executing actions (coordinates) on a computer screen, which can be viewed as a form of trajectory prediction in a digital space. The paper explicitly mentions LLMs and the generation of trajectories, albeit in the context of computer interaction.", "keywords": ["Large Language Models", "LLMs", "agentic model", "trajectories", "coordinate prediction"]}}
{"id": "2511.19859", "pdf": "https://arxiv.org/pdf/2511.19859", "abs": "https://arxiv.org/abs/2511.19859", "authors": ["Xiangkai Ma", "Lekai Xing", "Han Zhang", "Wenzhong Li", "Sanglu Lu"], "title": "Unifying Perception and Action: A Hybrid-Modality Pipeline with Implicit Visual Chain-of-Thought for Robotic Action Generation", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models built upon Chain-of-Thought (CoT) have achieved remarkable success in advancing general-purpose robotic agents, owing to its significant perceptual comprehension. Recently, since text-only CoT struggles to adequately capture scene details in complex spatial environments, a highly promising strategy involves leveraging visual priors to guide robotic action generation. Nevertheless, these strategies face two inherent challenges: (i) a modality gap between visual observations and low-level actions, and (ii) unstable training due to competing objectives between visual prediction and action generation. To address these challenges, we propose a Vision-Integrated Trajectory Alignment (VITA) framework that learns a shared discrete latent space for vision and action, enabling joint modeling of perception and motor control. VITA introduces a implicit visual CoT: autoregressively generated tokens is simultaneously decoded into future frames predictions and robot actions, thereby internalizing visual dynamics as an inductive bias for motion planning. Extensive experiments on simulated and real-world environments demonstrate state-of-the-art performance. VITA improves 14.5\\%, 9.6\\% and 12.1\\% over existing baselines on CALVIN, LIBERO and SimplerEnv. Furthermore, VITA attains an average success rate of 80.5\\% across six real-world tasks, demonstrating its potential as a generalist robotic manipulation model.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on robotic action generation using a vision-language-action model. While it doesn't explicitly mention trajectory prediction in the title, the abstract describes generating future frames and robot actions, which implicitly involves predicting the trajectory of the robot. The use of a large model architecture (VLA) and chain-of-thought reasoning further contributes to its relevance. The keyword 'motion planning' also directly links to trajectory prediction.", "keywords": ["Vision-Language-Action models", "Chain-of-Thought", "robotic action generation", "motion planning", "visual dynamics"]}}
{"id": "2511.19914", "pdf": "https://arxiv.org/pdf/2511.19914", "abs": "https://arxiv.org/abs/2511.19914", "authors": ["Dapeng Zhang", "Fei Shen", "Rui Zhao", "Yinda Chen", "Peng Zhi", "Chenyang Li", "Rui Zhou", "Qingguo Zhou"], "title": "CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model", "categories": ["cs.RO"], "comment": null, "summary": "Autonomous driving represents a prominent application of artificial intelligence. Recent approaches have shifted from focusing solely on common scenarios to addressing complex, long-tail situations such as subtle human behaviors, traffic accidents, and non-compliant driving patterns. Given the demonstrated capabilities of large language models (LLMs) in understanding visual and natural language inputs and following instructions, recent methods have integrated LLMs into autonomous driving systems to enhance reasoning, interpretability, and performance across diverse scenarios. However, existing methods typically rely either on real-world data, which is suitable for industrial deployment, or on simulation data tailored to rare or hard case scenarios. Few approaches effectively integrate the complementary advantages of both data sources. To address this limitation, we propose a novel VLM-guided, end-to-end adversarial transfer framework for autonomous driving that transfers long-tail handling capabilities from simulation to real-world deployment, named CoC-VLA. The framework comprises a teacher VLM model, a student VLM model, and a discriminator. Both the teacher and student VLM models utilize a shared base architecture, termed the Chain-of-Causality Visual-Language Model (CoC VLM), which integrates temporal information via an end-to-end text adapter. This architecture supports chain-of-thought reasoning to infer complex driving logic. The teacher and student VLM models are pre-trained separately on simulated and real-world datasets. The discriminator is trained adversarially to facilitate the transfer of long-tail handling capabilities from simulated to real-world environments by the student VLM model, using a novel backpropagation strategy.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper discusses autonomous driving, which often involves trajectory prediction. The abstract explicitly mentions the use of large language models (LLMs) to enhance reasoning and performance in autonomous driving systems. While not directly focused on trajectory prediction methods, the integration of LLMs within autonomous driving and the mention of handling complex scenarios suggests a connection to trajectory prediction within those scenarios.", "keywords": ["Large Language Models", "LLMs", "Autonomous Driving", "Visual-Language Model", "VLM"]}}
{"id": "2511.19528", "pdf": "https://arxiv.org/pdf/2511.19528", "abs": "https://arxiv.org/abs/2511.19528", "authors": ["Rushuai Yang", "Zhiyuan Feng", "Tianxiang Zhang", "Kaixin Wang", "Chuheng Zhang", "Li Zhao", "Xiu Su", "Yi Chen", "Jiang Bian"], "title": "Discover, Learn, and Reinforce: Scaling Vision-Language-Action Pretraining with Diverse RL-Generated Trajectories", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Scaling vision-language-action (VLA) model pre-training requires large volumes of diverse, high-quality manipulation trajectories. Most current data is obtained via human teleoperation, which is expensive and difficult to scale. Reinforcement learning (RL) methods learn useful skills through autonomous exploration, making them a viable approach for generating data. However, standard RL training collapses to a narrow execution pattern, limiting its utility for large-scale pre-training. We propose Discover, Lea rn and Reinforce (DLR), an information-theoretic pattern discovery framework that generates multiple distinct, high-success behavioral patterns for VLA pretraining. Empirically, DLR generates a markedly more diverse trajectory corpus on LIBERO. Specifically, it learns multiple distinct, high-success strategies for the same task where standard RL discovers only one, and hence it covers substantially broader regions of the state-action space. When adapted to unseen downstream task suites, VLA models pretrained on our diverse RL data surpass counterparts trained on equal-sized standard RL datasets. Moreover, DLR exhibits positive data-scaling behavior that single-pattern RL lacks. These results position multi-pattern RL as a practical, scalable data engine for embodied foundation models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on generating diverse trajectories using reinforcement learning for pre-training vision-language-action models, which are related to large models/foundation models. While it doesn't directly deal with trajectory prediction, the generation of trajectories for pre-training large models connects to the broader theme of trajectory data and its utilization in large-scale AI.", "keywords": ["trajectories", "reinforcement learning", "vision-language-action models", "pre-training", "foundation models"]}}
{"id": "2511.19465", "pdf": "https://arxiv.org/pdf/2511.19465", "abs": "https://arxiv.org/abs/2511.19465", "authors": ["Theo Demessance", "Chongke Bi", "Sonia Djebali", "Guillaume Guerard"], "title": "Hidden markov model to predict tourists visited place", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Nowadays, social networks are becoming a popular way of analyzing tourist behavior, thanks to the digital traces left by travelers during their stays on these networks. The massive amount of data generated; by the propensity of tourists to share comments and photos during their trip; makes it possible to model their journeys and analyze their behavior. Predicting the next movement of tourists plays a key role in tourism marketing to understand demand and improve decision support. In this paper, we propose a method to understand and to learn tourists' movements based on social network data analysis to predict future movements. The method relies on a machine learning grammatical inference algorithm. A major contribution in this paper is to adapt the grammatical inference algorithm to the context of big data. Our method produces a hidden Markov model representing the movements of a group of tourists. The hidden Markov model is flexible and editable with new data. The capital city of France, Paris is selected to demonstrate the efficiency of the proposed methodology.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\u9884\u6d4b\u6e38\u5ba2\u7684\u8f68\u8ff9\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u3002\u867d\u7136\u63d0\u5230\u4e86\u5927\u6570\u636e\uff0c\u4f46\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6216\u57fa\u7840\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b", "\u79fb\u52a8\u7269\u4f53\u8def\u5f84\u89c4\u5212", "\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b"]}}
{"id": "2511.19577", "pdf": "https://arxiv.org/pdf/2511.19577", "abs": "https://arxiv.org/abs/2511.19577", "authors": ["Abhay Goyal", "Navin Kumar", "Kimberly DiMeola", "Rafael Trujillo", "Soorya Ram Shimgekar", "Christian Poellabauer", "Pi Zonooz", "Ermonda Gjoni-Markaj", "Declan Barry", "Lynn Madden"], "title": "Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on using wearable devices and AI models, including LLMs, to predict and understand pain spikes in patients with opioid use disorder and chronic pain. While it doesn't directly address trajectory prediction, it does involve predicting pain events over time, which can be considered a form of time-series prediction. The mention of LLMs also contributes to the relevance, although their performance was limited in this specific application.", "keywords": ["Large Language Models", "LLMs", "AI models", "prediction"]}}
{"id": "2511.19653", "pdf": "https://arxiv.org/pdf/2511.19653", "abs": "https://arxiv.org/abs/2511.19653", "authors": ["Mahmud Suhaimi Ibrahim", "Shantanu Rahman", "Muhammad Samin Hasan", "Minhaj Uddin Ahmad", "Abdullah Abrar"], "title": "Flow-Based Path Planning for Multiple Homogenous UAVs for Outdoor Formation-Flying", "categories": ["cs.RO"], "comment": "9 pages, 15 figures, conference", "summary": "Collision-free path planning is the most crucial component in multi-UAV formation-flying (MFF). We use unlabeled homogenous quadcopters (UAVs) to demonstrate the use of a flow network to create complete (inter-UAV) collision-free paths. This procedure has three main parts: 1) Creating a flow network graph from physical GPS coordinates, 2) Finding a path of minimum cost (least distance) using any graph-based path-finding algorithm, and 3) Implementing the Ford-Fulkerson Method to find the paths with the maximum flow (no collision). Simulations of up to 64 UAVs were conducted for various formations, followed by a practical experiment with 3 quadcopters for testing physical plausibility and feasibility. The results of these tests show the efficacy of this method's ability to produce safe, collision-free paths.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on path planning for UAVs, which is related to trajectory prediction. However, it does not mention or utilize Large Language Models. The connection to trajectory prediction is through the path planning aspect.", "keywords": ["path planning", "UAV", "collision-free paths"]}}
{"id": "2511.19655", "pdf": "https://arxiv.org/pdf/2511.19655", "abs": "https://arxiv.org/abs/2511.19655", "authors": ["Shantanu Rahman", "Nayeb Hasin", "Mainul Islam"], "title": "Development of a Testbed for Autonomous Vehicles: Integrating MPC Control with Monocular Camera Lane Detection", "categories": ["cs.RO"], "comment": "49 pages, 23 figures", "summary": "Autonomous vehicles are becoming popular day by day not only for autonomous road traversal but also for industrial automation, farming and military. Most of the standard vehicles follow the Ackermann style steering mechanism. This has become to de facto standard for large and long faring vehicles. The local planner of an autonomous vehicle controls the low-level vehicle movement upon which the vehicle will perform its motor actuation. In our work, we focus on autonomous vehicles in road and perform experiments to analyze the effect of low-level controllers in the simulation and a real environment. To increase the precision and stability of trajectory tracking in autonomous cars, a novel method that combines lane identification with Model Predictive Control (MPC) is presented. The research focuses on camera-equipped autonomous vehicles and uses methods like edge recognition, sliding window-based straight-line identification for lane line extraction, and dynamic region of interest (ROI) extraction. Next, to follow the identified lane line, an MPC built on a bicycle vehicle dynamics model is created. A single-lane road simulation model is built using ROS Gazebo and tested in order to verify the controller's performance. The root mean square error between the optimal tracking trajectory and the target trajectory was reduced by 27.65% in the simulation results, demonstrating the high robustness and flexibility of the developed controller.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autonomous vehicle control using Model Predictive Control (MPC) and lane detection. While it involves trajectory tracking and control (related to trajectory prediction), it doesn't involve Large Language Models. The connection to trajectory prediction is through MPC-based trajectory tracking, but it's not a direct trajectory prediction problem.", "keywords": ["trajectory tracking", "Model Predictive Control", "autonomous vehicles", "lane detection"]}}
{"id": "2511.19691", "pdf": "https://arxiv.org/pdf/2511.19691", "abs": "https://arxiv.org/abs/2511.19691", "authors": ["Thomas Marshall Vielmetti", "Devansh R Agrawal", "Dimitra Panagou"], "title": "Multi-Agent gatekeeper: Safe Flight Planning and Formation Control for Urban Air Mobility", "categories": ["cs.RO"], "comment": "13 pages, 4 figures, to appear AIAA SciTech 2026", "summary": "We present Multi-Agent gatekeeper, a framework that provides provable safety guarantees for leader-follower formation control in cluttered 3D environments. Existing methods face a trad-off: online planners and controllers lack formal safety guarantees, while offline planners lack adaptability to changes in the number of agents or desired formation. To address this gap, we propose a hybrid architecture where a single leader tracks a pre-computed, safe trajectory, which serves as a shared trajectory backup set for all follower agents. Followers execute a nominal formation-keeping tracking controller, and are guaranteed to remain safe by always possessing a known-safe backup maneuver along the leader's path. We formally prove this method ensures collision avoidance with both static obstacles and other agents. The primary contributions are: (1) the multi-agent gatekeeper algorithm, which extends our single-agent gatekeeper framework to multi-agent systems; (2) the trajectory backup set for provably safe inter-agent coordination for leader-follower formation control; and (3) the first application of the gatekeeper framework in a 3D environment. We demonstrate our approach in a simulated 3D urban environment, where it achieved a 100% collision-avoidance success rate across 100 randomized trials, significantly outperforming baseline CBF and NMPC methods. Finally, we demonstrate the physical feasibility of the resulting trajectories on a team of quadcopters.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u98de\u884c\u89c4\u5212\u548c\u7f16\u961f\u63a7\u5236\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u548c\u8def\u5f84\u89c4\u5212\u9886\u57df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u4e2d\u6d89\u53ca\u7684\u8f68\u8ff9\u89c4\u5212\u3001\u907f\u969c\u7b49\u6280\u672f\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5bc6\u5207\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "path planning", "formation control", "collision avoidance", "multi-agent systems"]}}
{"id": "2511.20180", "pdf": "https://arxiv.org/pdf/2511.20180", "abs": "https://arxiv.org/abs/2511.20180", "authors": ["Ryohei Kobayashi", "Kosei Isomoto", "Kosei Yamao", "Soma Fumoto", "Koshun Arimura", "Naoki Yamaguchi", "Akinobu Mizutani", "Tomoya Shiba", "Kouki Kimizuka", "Yuta Ohno", "Ryo Terashima", "Hiromasa Yamaguchi", "Tomoaki Fujino", "Ryoga Maruno", "Wataru Yoshimura", "Kazuhito Mine", "Tang Phu Thien Nhan", "Yuga Yano", "Yuichiro Tanaka", "Takeshi Nishida", "Takashi Morie", "Hakaru Tamukoh"], "title": "Hibikino-Musashi@Home 2025 Team Description Paper", "categories": ["cs.RO"], "comment": null, "summary": "This paper provides an overview of the techniques employed by Hibikino-Musashi@Home, which intends to participate in the domestic standard platform league. The team developed a dataset generator for training a robot vision system and an open-source development environment running on a Human Support Robot simulator. The large-language-model-powered task planner selects appropriate primitive skills to perform the task requested by the user. Moreover, the team has focused on research involving brain-inspired memory models for adaptation to individual home environments. This approach aims to provide intuitive and personalized assistance. Additionally, the team contributed to the reusability of the navigation system developed by Pumas in RoboCup2024. The team aimed to design a home service robot to assist humans in their homes and continuously attend competitions to evaluate and improve the developed system.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper describes a home service robot that utilizes a large-language-model-powered task planner. While the primary focus isn't trajectory prediction, the mention of a navigation system and the overall context of robot movement within a home environment suggest a potential, albeit indirect, relevance. The use of LLMs for task planning directly relates to the large language model aspect.", "keywords": ["large-language-model", "navigation system", "robot vision"]}}
{"id": "2511.20394", "pdf": "https://arxiv.org/pdf/2511.20394", "abs": "https://arxiv.org/abs/2511.20394", "authors": ["Shiqian Liu", "Azlan Mohd Zain", "Le-le Mao"], "title": "Improved adaptive wind driven optimization algorithm for real-time path planning", "categories": ["cs.RO"], "comment": "23 pages, 4 figures", "summary": "Recently, path planning has achieved remarkable progress in enhancing global search capability and convergence accuracy through heuristic and learning-inspired optimization frameworks. However, real-time adaptability in dynamic environments remains a critical challenge for autonomous navigation, particularly when robots must generate collision-free, smooth, and efficient trajectories under complex constraints. By analyzing the difficulties in dynamic path planning, the Wind Driven Optimization (WDO) algorithm emerges as a promising framework owing to its physically interpretable search dynamics. Motivated by these observations, this work revisits the WDO principle and proposes a variant formulation, Multi-hierarchical adaptive wind driven optimization(MAWDO), that improves adaptability and robustness in time-varying environments. To mitigate instability and premature convergence, a hierarchical-guidance mechanism divides the population into multiple groups guided by individual, regional, and global leaders to balance exploration and exploitation. Extensive evaluations on sixteen benchmark functions show that MAWDO achieves superior optimization accuracy, convergence stability, and adaptability over state-of-the art metaheuristics. In dynamic path planning, MAWDO shortens the path length to 469.28 pixels, improving over Multi-strategy ensemble wind driven optimization(MEWDO), Adaptive wind driven optimization(AWDO) and WDO by 3.51\\%, 11.63\\% and 14.93\\%, and achieves the smallest optimality gap (1.01) with smoothness 0.71 versus 13.50 and 15.67 for AWDO and WDO, leading to smoother, shorter, and collision-free trajectories that confirm its effectiveness for real-time path planning in complex environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on path planning using an improved optimization algorithm. While it does not directly involve large language models, it is related to trajectory prediction through the path planning aspect. The abstract mentions generating trajectories and path planning in dynamic environments, which falls under the umbrella of trajectory prediction.", "keywords": ["path planning", "trajectory", "dynamic environments", "optimization"]}}
{"id": "2511.20593", "pdf": "https://arxiv.org/pdf/2511.20593", "abs": "https://arxiv.org/abs/2511.20593", "authors": ["Allen Emmanuel Binny", "Mahathi Anand", "Hugo T. M. Kussaba", "Lingyun Chen", "Shreenabh Agrawal", "Fares J. Abu-Dakka", "Abdalla Swikir"], "title": "Safe and Stable Neural Network Dynamical Systems for Robot Motion Planning", "categories": ["cs.RO", "eess.SY"], "comment": null, "summary": "Learning safe and stable robot motions from demonstrations remains a challenge, especially in complex, nonlinear tasks involving dynamic, obstacle-rich environments. In this paper, we propose Safe and Stable Neural Network Dynamical Systems S$^2$-NNDS, a learning-from-demonstration framework that simultaneously learns expressive neural dynamical systems alongside neural Lyapunov stability and barrier safety certificates. Unlike traditional approaches with restrictive polynomial parameterizations, S$^2$-NNDS leverages neural networks to capture complex robot motions providing probabilistic guarantees through split conformal prediction in learned certificates. Experimental results on various 2D and 3D datasets -- including LASA handwriting and demonstrations recorded kinesthetically from the Franka Emika Panda robot -- validate S$^2$-NNDS effectiveness in learning robust, safe, and stable motions from potentially unsafe demonstrations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on learning safe and stable robot motions using neural network dynamical systems. While it doesn't explicitly use Large Language Models, it falls under the broader umbrella of trajectory prediction and motion planning, especially considering the use of neural networks for learning dynamical systems. The connection to trajectory prediction is evident in the context of robot motion planning.", "keywords": ["robot motion planning", "trajectory prediction", "neural networks", "dynamical systems"]}}
{"id": "2511.20633", "pdf": "https://arxiv.org/pdf/2511.20633", "abs": "https://arxiv.org/abs/2511.20633", "authors": ["Jiahui Zhang", "Ze Huang", "Chun Gu", "Zipei Ma", "Li Zhang"], "title": "Reinforcing Action Policies by Prophesying", "categories": ["cs.RO"], "comment": "https://LogosRoboticsGroup.github.io/ProphRL", "summary": "Vision-Language-Action (VLA) policies excel in aligning language, perception, and robot control. However, most VLAs are trained purely by imitation, which overfits to demonstrations, and is brittle under distribution shift. Reinforcement learning (RL) directly optimizes task reward and thus addresses this misalignment, but real-robot interaction is expensive and conventional simulators are hard to engineer and transfer. We address both data efficiency and optimization stability in VLA post-training via a learned world model and an RL procedure tailored to flow-based action heads. Specifically, we introduce Prophet, a unified action-to-video robot actuation pretrained across large-scale, heterogeneous robot data to learn reusable action-outcome dynamics. It is able to few-shot adapt to new robots, objects, and environments, yielding a rollout-ready simulator. Upon Prophet, we reinforce action policies with Flow-action-GRPO (FA-GRPO), which adapts Flow-GRPO to operate on VLA actions, and with FlowScale, a stepwise reweighting that rescales per-step gradients in the flow head. Together, Prophet, FA-GRPO, and FlowScale constitute ProphRL, a practical, data- and compute-efficient path to VLA post-training. Experiments show 5-17% success gains on public benchmarks and 24-30% gains on real robots across different VLA variants.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on reinforcing action policies for robots using a learned world model. While it doesn't explicitly mention trajectory prediction, the core idea of predicting action outcomes and optimizing robot control is related to trajectory planning and action prediction. The use of a large-scale pretrained model (Prophet) also connects to the theme of large models, although it's not a language model. The connection to trajectory prediction is through the implied prediction of future states based on actions.", "keywords": ["action policies", "robot control", "world model", "pretrained model", "action prediction"]}}
{"id": "2511.19861", "pdf": "https://arxiv.org/pdf/2511.19861", "abs": "https://arxiv.org/abs/2511.19861", "authors": ["GigaWorld Team", "Angen Ye", "Boyuan Wang", "Chaojun Ni", "Guan Huang", "Guosheng Zhao", "Haoyun Li", "Jiagang Zhu", "Kerui Li", "Mengyuan Xu", "Qiuping Deng", "Siting Wang", "Wenkang Qin", "Xinze Chen", "Xiaofeng Wang", "Yankai Wang", "Yu Cao", "Yifan Chang", "Yuan Xu", "Yun Ye", "Yang Wang", "Yukun Zhou", "Zhengyuan Zhang", "Zhehao Dong", "Zheng Zhu"], "title": "GigaWorld-0: World Models as Data Engine to Empower Embodied AI", "categories": ["cs.CV", "cs.RO"], "comment": "Project Page: https://gigaworld0.github.io/", "summary": "World models are emerging as a foundational paradigm for scalable, data-efficient embodied AI. In this work, we present GigaWorld-0, a unified world model framework designed explicitly as a data engine for Vision-Language-Action (VLA) learning. GigaWorld-0 integrates two synergistic components: GigaWorld-0-Video, which leverages large-scale video generation to produce diverse, texture-rich, and temporally coherent embodied sequences under fine-grained control of appearance, camera viewpoint, and action semantics; and GigaWorld-0-3D, which combines 3D generative modeling, 3D Gaussian Splatting reconstruction, physically differentiable system identification, and executable motion planning to ensure geometric consistency and physical realism. Their joint optimization enables the scalable synthesis of embodied interaction data that is visually compelling, spatially coherent, physically plausible, and instruction-aligned. Training at scale is made feasible through our efficient GigaTrain framework, which exploits FP8-precision and sparse attention to drastically reduce memory and compute requirements. We conduct comprehensive evaluations showing that GigaWorld-0 generates high-quality, diverse, and controllable data across multiple dimensions. Critically, VLA model (e.g., GigaBrain-0) trained on GigaWorld-0-generated data achieve strong real-world performance, significantly improving generalization and task success on physical robots without any real-world interaction during training.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5177\u8eab\u667a\u80fd\u7684\u6570\u636e\u5f15\u64ce\uff0c\u63d0\u5230\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528\u5927\u578b\u89c6\u9891\u751f\u6210\u548c3D\u751f\u6210\u5efa\u6a21\u6765\u5408\u6210\u6570\u636e\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u4e2d\u7684\u52a8\u4f5c\u8bed\u4e49\u63a7\u5236\u548c\u8fd0\u52a8\u89c4\u5212\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002\u540c\u65f6\uff0c\u8bba\u6587\u6d89\u53ca\u5927\u578b\u6a21\u578b\uff08GigaBrain-0\uff09\u7684\u8bad\u7ec3\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["world models", "Vision-Language-Action learning", "large-scale video generation", "3D generative modeling", "motion planning", "large models", "foundation models"]}}
{"id": "2511.20156", "pdf": "https://arxiv.org/pdf/2511.20156", "abs": "https://arxiv.org/abs/2511.20156", "authors": ["Bin Hu", "Zijian Lu", "Haicheng Liao", "Chengran Yuan", "Bin Rao", "Yongkang Li", "Guofa Li", "Zhiyong Cui", "Cheng-zhong Xu", "Zhenning Li"], "title": "Map-World: Masked Action planning and Path-Integral World Model for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Motion planning for autonomous driving must handle multiple plausible futures while remaining computationally efficient. Recent end-to-end systems and world-model-based planners predict rich multi-modal trajectories, but typically rely on handcrafted anchors or reinforcement learning to select a single best mode for training and control. This selection discards information about alternative futures and complicates optimization. We propose MAP-World, a prior-free multi-modal planning framework that couples masked action planning with a path-weighted world model. The Masked Action Planning (MAP) module treats future ego motion as masked sequence completion: past waypoints are encoded as visible tokens, future waypoints are represented as mask tokens, and a driving-intent path provides a coarse scaffold. A compact latent planning state is expanded into multiple trajectory queries with injected noise, yielding diverse, temporally consistent modes without anchor libraries or teacher policies. A lightweight world model then rolls out future BEV semantics conditioned on each candidate trajectory. During training, semantic losses are computed as an expectation over modes, using trajectory probabilities as discrete path weights, so the planner learns from the full distribution of plausible futures instead of a single selected path. On NAVSIM, our method matches anchor-based approaches and achieves state-of-the-art performance among world-model-based methods, while avoiding reinforcement learning and maintaining real-time inference latency.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on motion planning for autonomous driving, which falls under the umbrella of trajectory prediction. While it doesn't directly utilize Large Language Models, it explores techniques related to world models and masked action planning, which share conceptual similarities with sequence completion tasks often addressed by LLMs. The connection to LLMs is indirect, but the core problem of predicting future trajectories is highly relevant.", "keywords": ["trajectory prediction", "motion planning", "autonomous driving", "world model", "masked action planning"]}}
{"id": "2511.20431", "pdf": "https://arxiv.org/pdf/2511.20431", "abs": "https://arxiv.org/abs/2511.20431", "authors": ["Dohun Lim", "Minji Kim", "Jaewoon Lim", "Sungchan Kim"], "title": "BRIC: Bridging Kinematic Plans and Physical Control at Test Time", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on human motion generation using diffusion models (which can be considered a type of large model) and reinforcement learning for physical control. While it doesn't explicitly mention trajectory prediction, the long-term motion generation aspect is closely related. The use of diffusion models also connects it to the broader field of large-scale AI models.", "keywords": ["human motion generation", "diffusion models", "reinforcement learning", "motion planning", "long-term tasks"]}}
