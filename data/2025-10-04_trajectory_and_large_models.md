# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-10-04

## 目录

- [人工智能 (Artificial Intelligence) (5)](#cs-ai)
- [计算机视觉 (Computer Vision) (3)](#cs-cv)
- [机器学习 (Machine Learning) (3)](#cs-lg)
- [机器人学 (Robotics) (7)](#cs-ro)

## 人工智能 (Artificial Intelligence) [cs.AI]
### [1] [Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective](https://arxiv.org/abs/2510.01639)
*Thinh Hung Truong, Jey Han Lau, Jianzhong Qi*

Main category: cs.AI

TL;DR: 本文探讨了大型语言模型（LLMs）的地理空间推理能力，通过轨迹恢复任务验证了LLMs在理解道路网络和执行导航方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索大型语言模型（LLMs）是否具备阅读道路网络地图并执行导航的能力。

Method: 本文将轨迹恢复定义为一个代理任务，要求模型重建被掩盖的GPS轨迹，并引入了包含超过4,000条真实世界轨迹的数据集GLOBALTRACE。利用道路网络作为上下文，本文的提示框架使LLMs能够生成有效的路径，而无需访问任何外部导航工具。

Result: 实验结果表明，LLMs优于现成的基线模型和专门的轨迹恢复模型，并具有强大的零样本泛化能力。细粒度分析表明，LLMs对道路网络和坐标系统有很强的理解能力，但也存在关于区域和运输方式的系统性偏差。

Conclusion: 本文证明了LLMs可以通过以灵活的方式推理地图来增强导航体验，从而整合用户偏好。

Abstract: 本文探讨了大型语言模型（LLMs）的地理空间推理能力，特别是LLMs是否能够读取道路网络地图并执行导航。我们将轨迹恢复定义为一个代理任务，该任务要求模型重建被掩盖的GPS轨迹，并引入了GLOBALTRACE，这是一个包含超过4,000条跨不同区域和交通方式的真实世界轨迹的数据集。通过使用道路网络作为上下文，我们的提示框架使LLMs能够生成有效的路径，而无需访问任何外部导航工具。实验表明，LLMs优于现成的基线模型和专门的轨迹恢复模型，并具有强大的零样本泛化能力。细粒度分析表明，LLMs对道路网络和坐标系统有很强的理解能力，但也存在关于区域和运输方式的系统性偏差。最后，我们展示了LLMs如何通过以灵活的方式推理地图来增强导航体验，从而整合用户偏好。

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01639) | **Categories:** cs.AI

---

### [2] [Modeling Others' Minds as Code](https://arxiv.org/abs/2510.01272)
*Kunal Jha, Aydan Yuenan Huang, Eric Ye, Natasha Jaques, Max Kleiman-Weiner*

Main category: cs.AI

TL;DR: 本文提出了一种名为ROTE的新算法，通过将日常社交互动建模为行为程序，利用大型语言模型和概率推理，实现了对人类和AI行为的准确预测。


<details>
  <summary>Details</summary>
Motivation: 现有的行为建模方法通常需要大量数据且脆弱，因为它们要么对理性做出不切实际的假设，要么计算量过大而无法快速适应。

Method: 提出ROTE算法，该算法利用大型语言模型合成行为程序的假设空间，并利用概率推理来推理该空间的不确定性。

Result: 在网格世界任务和大型具身家庭模拟器中测试ROTE，结果表明ROTE在样本内准确率和样本外泛化方面优于包括行为克隆和基于LLM的方法在内的竞争基线，高达50%。

Conclusion: 通过将动作理解视为程序合成问题，ROTE为AI系统高效有效地预测现实世界中的人类行为开辟了一条道路。

Abstract: 为了实现稳健和安全的人机协作，准确预测人类行为至关重要。然而，现有的建模方法通常需要大量数据且脆弱，因为它们要么对理性做出不切实际的假设，要么计算量过大而无法快速适应。我们的关键洞察是，许多日常社交互动可能遵循可预测的模式；高效的“脚本”可以最大限度地减少参与者和观察者的认知负荷，例如“等待绿灯，然后前进”。我们建议将这些例程建模为在计算机代码中实例化的行为程序，而不是以信念和愿望为条件的策略。我们介绍了一种名为ROTE的新算法，该算法利用大型语言模型（LLM）来合成行为程序的假设空间，并利用概率推理来推理该空间的不确定性。我们在一套网格世界任务和一个大型具身家庭模拟器中测试ROTE。ROTE可以根据稀疏的观察结果预测人类和AI行为，在样本内准确率和样本外泛化方面优于包括行为克隆和基于LLM的方法在内的竞争基线，高达50%。通过将动作理解视为程序合成问题，ROTE为AI系统高效有效地预测现实世界中的人类行为开辟了一条道路。

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01272) | **Categories:** cs.AI, cs.LG

---

### [3] [Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents](https://arxiv.org/abs/2510.01398)
*Yang Liu, Zaid Abulawi, Abhiram Garimidi, Doyeong Lim*

Main category: cs.AI

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets. Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications. In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks. We evaluate two LLM-agent frameworks: a multi-agent system featuring specialized collaborative agents, and a single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both frameworks autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset. Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts. These outcomes underscore the significant potential of LLM-based agents to automate complex engineering modeling tasks, greatly reducing human workload while meeting or exceeding existing standards of predictive performance.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01398) | **Categories:** cs.AI

---

### [4] [Information Seeking for Robust Decision Making under Partial Observability](https://arxiv.org/abs/2510.01531)
*Djengo Cyun-Jyun Fang, Tsung-Wei Ke*

Main category: cs.AI

TL;DR: InfoSeeker 通过整合任务导向规划和信息搜寻，显著提升了 LLM 在不确定环境下的决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 规划智能体忽略了其内部动态与实际环境之间的差异，无法有效应对不完整信息和动态变化。

Method: InfoSeeker 框架提示 LLM 主动收集信息，通过规划行动来验证其理解、检测环境变化或测试假设，从而生成或修订任务导向计划。

Result: 实验表明，InfoSeeker 在性能上比现有方法提高了 74%，并且在机器人操作和网页导航等基准测试中表现优异。

Conclusion: 在部分可观察环境中，紧密整合规划和信息搜寻对于实现稳健行为至关重要。

Abstract: 在信息不完整和动态变化的环境中，人类解决问题需要明确的信息搜寻。当无法直接观察到真实环境状态时，人类会寻求信息来更新其内部动态，并为未来的决策提供信息。虽然现有的大型语言模型（LLM）规划智能体已经解决了观测不确定性问题，但它们经常忽略其内部动态与实际环境之间的差异。我们引入了信息搜寻决策规划器（InfoSeeker），这是一个 LLM 决策框架，它整合了任务导向规划和信息搜寻，以在智能体观测和环境动态的不确定性下对齐内部动态并做出最佳决策。InfoSeeker 提示 LLM 通过规划行动来主动收集信息，以验证其理解、检测环境变化或在生成或修改任务导向计划之前测试假设。为了评估 InfoSeeker，我们引入了一个新的基准测试套件，其中包含具有不完整观测和不确定动态的部分可观察环境。实验表明，InfoSeeker 的性能比现有方法提高了 74%，且不牺牲样本效率。此外，InfoSeeker 可以在 LLM 之间推广，并且在机器人操作和 Web 导航等已建立的基准测试中优于基线。这些发现强调了紧密整合规划和信息搜寻对于在部分可观察环境中实现稳健行为的重要性。项目页面位于 https://infoseekerllm.github.io

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01531) | **Categories:** cs.AI, cs.CL, cs.RO

---

### [5] [AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.01586)
*Zhenyu Pan, Yiting Zhang, Zhuo Liu, Yolo Yunlong Tang, Zeliang Zhang, Haozheng Luo, Yuwei Han, Jianshu Zhang, Dennis Wu, Hong-Yu Chen, Haoran Lu, Haoyang Fang, Manling Li, Chenliang Xu, Philip S. Yu, Han Liu*

Main category: cs.AI

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01586) | **Categories:** cs.AI

---


## 计算机视觉 (Computer Vision) [cs.CV]
### [1] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas, Dang Nguyen, Nesihan Bulut, Mohammadhossein Bateni, Vahab Mirrokni, Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Data-efficient learning aims to eliminate redundancy in large training datasets by training models on smaller subsets of the most informative examples. While data selection has been extensively explored for vision models and large language models (LLMs), it remains underexplored for Large Vision-Language Models (LVLMs). Notably, none of existing methods can outperform random selection at different subset sizes. In this work, we propose the first principled method for data-efficient instruction tuning of LVLMs. We prove that examples with similar cross-modal attention matrices during instruction tuning have similar gradients. Thus, they influence model parameters in a similar manner and convey the same information to the model during training. Building on this insight, we propose XMAS, which clusters examples based on the trajectories of the top singular values of their attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a balanced subset from these clusters, XMAS effectively removes redundancy in large-scale LVLM training data. Extensive experiments show that XMAS can discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and speeding up its training by 1.2x. This is 30% more data reduction compared to the best baseline for LLaVA-665k. The project's website can be found at https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01454) | **Categories:** cs.CV, cs.LG

---

### [2] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, Zheng Zhu*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01623) | **Categories:** cs.CV, cs.RO

---

### [3] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu, Can Wang, Zhenghao Chen, Dong Xu*

Main category: cs.CV

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01991) | **Categories:** cs.CV

---


## 机器学习 (Machine Learning) [cs.LG]
### [1] [RSTGCN: Railway-centric Spatio-Temporal Graph Convolutional Network for Train Delay Prediction](https://arxiv.org/abs/2510.01262)
*Koyena Chowdhury, Paramita Koley, Abhijnan Chakraborty, Saptarshi Ghosh*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Accurate prediction of train delays is critical for efficient railway operations, enabling better scheduling and dispatching decisions. While earlier approaches have largely focused on forecasting the exact delays of individual trains, recent studies have begun exploring station-level delay prediction to support higher-level traffic management. In this paper, we propose the Railway-centric Spatio-Temporal Graph Convolutional Network (RSTGCN), designed to forecast average arrival delays of all the incoming trains at railway stations for a particular time period. Our approach incorporates several architectural innovations and novel feature integrations, including train frequency-aware spatial attention, which significantly enhances predictive performance. To support this effort, we curate and release a comprehensive dataset for the entire Indian Railway Network (IRN), spanning 4,735 stations across 17 zones - the largest and most diverse railway network studied to date. We conduct extensive experiments using multiple state-of-the-art baselines, demonstrating consistent improvements across standard metrics. Our work not only advances the modeling of average delay prediction in large-scale rail networks but also provides an open dataset to encourage further research in this critical domain.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01262) | **Categories:** cs.LG, cs.AI

---

### [2] [Predictive Preference Learning from Human Interventions](https://arxiv.org/abs/2510.01545)
*Haoyuan Cai, Zhenghao Peng, Bolei Zhou*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent's action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01545) | **Categories:** cs.LG, cs.AI, cs.RO

---

### [3] [TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis](https://arxiv.org/abs/2510.01538)
*Haokun Zhao, Xiang Zhang, Jiaqi Wei, Yiwei Xu, Yuting He, Siqi Sun, Chenyu You*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01538) | **Categories:** cs.LG

---


## 机器人学 (Robotics) [cs.RO]
### [1] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang, Xiangyun Meng, Luca Calliari, Dong-Ki Kim, Shayegan Omidshafiei, Joydeep Biswas, Ali Agha, Amirreza Shaban*

Main category: cs.RO

TL;DR: VENTURA通过微调图像扩散模型生成路径掩码，实现视觉语言导航，并在真实环境中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型难以适应机器人导航任务，因为动作空间和预训练目标存在差异，阻碍了迁移性。

Method: VENTURA系统微调了互联网预训练的图像扩散模型用于路径规划，生成路径掩码作为视觉计划，并通过轻量级的行为克隆策略将视觉计划转化为可执行的轨迹。

Result: 在真实环境评估中，VENTURA在物体抓取、避障和地形偏好任务上优于现有模型，成功率提高33%，碰撞减少54%，并展现出组合泛化能力。

Conclusion: VENTURA通过视觉语言导航系统，显著提高了机器人在复杂环境中的导航性能和泛化能力。

Abstract: 机器人必须适应多样化的人类指令，并在非结构化的开放世界环境中安全运行。 近期视觉-语言模型 (VLMs) 为语言和感知提供了强大的先验，但由于动作空间和预训练目标的差异，它们仍然难以用于导航，这阻碍了它们向机器人任务的转移。 为了解决这个问题，我们引入了 VENTURA，一个视觉语言导航系统，它对互联网预训练的图像扩散模型进行微调，以进行路径规划。 VENTURA 没有直接预测低级动作，而是在图像空间中生成路径掩码（即视觉计划），该路径掩码捕获细粒度的、上下文感知的导航行为。 轻量级的行为克隆策略将这些视觉计划转化为可执行的轨迹，从而产生一个遵循自然语言指令以生成各种机器人行为的界面。 为了扩大训练规模，我们对从自监督跟踪模型和 VLM 增强的字幕中导出的路径掩码进行监督，避免了手动像素级注释或高度工程化的数据收集设置。 在广泛的真实世界评估中，VENTURA 在物体到达、避障和地形偏好任务上优于最先进的基础模型基线，在可见和不可见场景中，成功率提高了 33%，碰撞减少了 54%。 值得注意的是，我们发现 VENTURA 可以推广到不同任务的未见组合，从而揭示了新兴的组合能力。 视频、代码和其他材料：https://venturapath.github.io

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01388) | **Categories:** cs.RO, cs.CV

---

### [2] [Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels](https://arxiv.org/abs/2510.01357)
*Alejandro Gonzalez-Garcia, Wei Xiao, Wei Wang, Alejandro Astudillo, Wilm Decré, Jan Swevers, Carlo Ratti, Daniela Rus*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Safe motion planning is essential for autonomous vessel operations, especially in challenging spaces such as narrow inland waterways. However, conventional motion planning approaches are often computationally intensive or overly conservative. This paper proposes a safe motion planning strategy combining Model Predictive Control (MPC) and Control Barrier Functions (CBFs). We introduce a time-varying inflated ellipse obstacle representation, where the inflation radius is adjusted depending on the relative position and attitude between the vessel and the obstacle. The proposed adaptive inflation reduces the conservativeness of the controller compared to traditional fixed-ellipsoid obstacle formulations. The MPC solution provides an approximate motion plan, and high-order CBFs ensure the vessel's safety using the varying inflation radius. Simulation and real-world experiments demonstrate that the proposed strategy enables the fully-actuated autonomous robot vessel to navigate through narrow spaces in real time and resolve potential deadlocks, all while ensuring safety.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01357) | **Categories:** cs.RO

---

### [3] [VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs](https://arxiv.org/abs/2510.01483)
*Mohamad Al Mdfaa, Svetlana Lukina, Timur Akhtyamov, Arthur Nigmatzyanov, Dmitrii Nalberskii, Sergey Zagoruyko, Gonzalo Ferrer*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Vision-language models (VLMs) have shown potential for robot navigation but encounter fundamental limitations: they lack persistent scene memory, offer limited spatial reasoning, and do not scale effectively with video duration for real-time application. We present VL-KnG, a Visual Scene Understanding system that tackles these challenges using spatiotemporal knowledge graph construction and computationally efficient query processing for navigation goal identification. Our approach processes video sequences in chunks utilizing modern VLMs, creates persistent knowledge graphs that maintain object identity over time, and enables explainable spatial reasoning through queryable graph structures. We also introduce WalkieKnowledge, a new benchmark with about 200 manually annotated questions across 8 diverse trajectories spanning approximately 100 minutes of video data, enabling fair comparison between structured approaches and general-purpose VLMs. Real-world deployment on a differential drive robot demonstrates practical applicability, with our method achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5 Pro performance while providing explainable reasoning supported by the knowledge graph, computational efficiency for real-time deployment across different tasks, such as localization, navigation and planning. Code and dataset will be released after acceptance.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01483) | **Categories:** cs.RO, cs.AI

---

### [4] [FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models](https://arxiv.org/abs/2510.01642)
*Zijun Lin, Jiafei Duan, Haoquan Fang, Dieter Fox, Ranjay Krishna, Cheston Tan, Bihan Wen*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Recent advances in robotic manipulation have integrated low-level robotic control into Vision-Language Models (VLMs), extending them into Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve strong performance in downstream robotic applications, supported by large-scale crowd-sourced robot training data, they still inevitably encounter failures during execution. Enabling robots to reason about and recover from unpredictable and abrupt failures remains a critical challenge. Existing robotic manipulation datasets, collected in either simulation or the real world, primarily provide only ground-truth trajectories, leaving robots unable to recover once failures occur. Moreover, the few datasets that address failure detection typically offer only textual explanations, which are difficult to utilize directly in VLA models. To address this gap, we introduce FailSafe, a novel failure generation and recovery system that automatically produces diverse failure cases paired with executable recovery actions. FailSafe can be seamlessly applied to any manipulation task in any simulator, enabling scalable creation of failure-action data. To demonstrate its effectiveness, we fine-tune LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results show that FailSafe-VLM successfully helps robotic arm detect and recover from potential failures, improving the performance of three state-of-the-art VLA models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different spatial configurations, camera viewpoints, and robotic embodiments. We plan to release the FailSafe code to the community.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01642) | **Categories:** cs.RO

---

### [5] [Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving](https://arxiv.org/abs/2510.01795)
*Haibo Hu, Lianming Huang, Xinyu Wang, Yufei Cui, Nan Guan, Chun Jason Xue*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Vision-Language Models (VLMs) are increasingly applied in autonomous driving for unified perception and reasoning, but high inference latency hinders real-time deployment. Early-exit reduces latency by terminating inference at intermediate layers, yet its task-dependent nature limits generalization across diverse scenarios. We observe that this limitation aligns with autonomous driving: navigation systems can anticipate upcoming contexts (e.g., intersections, traffic lights), indicating which tasks will be required. We propose Nav-EE, a navigation-guided early-exit framework that precomputes task-specific exit layers offline and dynamically applies them online based on navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE achieves accuracy comparable to full inference while reducing latency by up to 63.9%. Real-vehicle integration with Autoware Universe further demonstrates reduced inference latency (600ms to 300ms), supporting faster decision-making in complex scenarios. These results suggest that coupling navigation foresight with early-exit offers a viable path toward efficient deployment of large models in autonomous systems. Code and data are available at our anonymous repository: https://anonymous.4open.science/r/Nav-EE-BBC4

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01795) | **Categories:** cs.RO, cs.AI

---

### [6] [Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots](https://arxiv.org/abs/2510.01843)
*Wanyue Li, Ji Ma, Minghao Lu, Peng Lu*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Humanoid robot soccer presents several challenges, particularly in maintaining system stability during aggressive kicking motions while achieving precise ball trajectory control. Current solutions, whether traditional position-based control methods or reinforcement learning (RL) approaches, exhibit significant limitations. Model predictive control (MPC) is a prevalent approach for ordinary quadruped and biped robots. While MPC has demonstrated advantages in legged robots, existing studies often oversimplify the leg swing progress, relying merely on simple trajectory interpolation methods. This severely constrains the foot's environmental interaction capability, hindering tasks such as ball kicking. This study innovatively adapts the spatial-temporal trajectory planning method, which has been successful in drone applications, to bipedal robotic systems. The proposed approach autonomously generates foot trajectories that satisfy constraints on target kicking position, velocity, and acceleration while simultaneously optimizing swing phase duration. Experimental results demonstrate that the optimized trajectories closely mimic human kicking behavior, featuring a backswing motion. Simulation and hardware experiments confirm the algorithm's efficiency, with trajectory planning times under 1 ms, and its reliability, achieving nearly 100 % task completion accuracy when the soccer goal is within the range of -90{\deg} to 90{\deg}.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01843) | **Categories:** cs.RO, I.2.9; I.2.8; G.1.6

---

### [7] [TACOS: Task Agnostic COordinator of a multi-drone System](https://arxiv.org/abs/2510.01869)
*Alessandro Nazzari, Roberto Rubinacci, Marco Lovera*

Main category: cs.RO

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: When a single pilot is responsible for managing a multi-drone system, the task demands varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real-world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system in real-world multi-drone system and conduct an ablation study to assess the contribution of each module.

</details>

[**[PDF]**](https://arxiv.org/pdf/2510.01869) | **Categories:** cs.RO, cs.AI, cs.MA

---
