{"id": "2511.03393", "pdf": "https://arxiv.org/pdf/2511.03393", "abs": "https://arxiv.org/abs/2511.03393", "authors": ["Chiara Rucco", "Motaz Saad", "Antonella Longo"], "title": "Formalizing ETLT and ELTL Design Patterns and Proposing Enhanced Variants: A Systematic Framework for Modern Data Engineering", "categories": ["cs.DB"], "comment": null, "summary": "Traditional ETL and ELT design patterns struggle to meet modern requirements\nof scalability, governance, and real-time data processing. Hybrid approaches\nsuch as ETLT (Extract-Transform-Load-Transform) and ELTL\n(Extract-Load-Transform-Load) are already used in practice, but the literature\nlacks best practices and formal recognition of these approaches as design\npatterns. This paper formalizes ETLT and ELTL as reusable design patterns by\ncodifying implicit best practices and introduces enhanced variants, ETLT++ and\nELTL++, to address persistent gaps in governance, quality assurance, and\nobservability. We define ETLT and ELTL patterns systematically within a design\npattern framework, outlining their structure, trade-offs, and use cases.\nBuilding on this foundation, we extend them into ETLT++ and ELTL++ by embedding\nexplicit contracts, versioning, semantic curation, and continuous monitoring as\nmandatory design obligations. The proposed framework offers practitioners a\nstructured roadmap to build auditable, scalable, and cost-efficient pipelines,\nunifying quality enforcement, lineage, and usability across multi-cloud and\nreal-time contexts. By formalizing ETLT and ELTL, and enhancing them through\nETLT++ and ELTL++, this work bridges the gap between ad hoc practice and\nsystematic design, providing a reusable foundation for modern, trustworthy data\nengineering."}
{"id": "2511.03437", "pdf": "https://arxiv.org/pdf/2511.03437", "abs": "https://arxiv.org/abs/2511.03437", "authors": ["Md Mizanur Rahaman Nayan", "Zheyu Li", "Flavio Ponzina", "Sumukh Pinge", "Tajana Rosing", "Azad J. Naeemi"], "title": "HERP: Hardware for Energy Efficient and Realtime DB Search and Cluster Expansion in Proteomics", "categories": ["cs.DB", "cs.ET"], "comment": null, "summary": "Database (DB) search and clustering are fundamental in proteomics but\nconventional full clustering and search approaches demand high resources and\nincur long latency. We propose a lightweight incremental clustering and highly\nparallelizable DB search platform tailored for resource-constrained\nenvironments, delivering low energy and latency without compromising\nperformance. By leveraging mass-spectrometry insights, we employ bucket-wise\nparallelization and query scheduling to reduce latency. A one-time hardware\ninitialization with pre-clustered proteomics data enables continuous DB search\nand local re-clustering, offering a more practical and efficient alternative to\nclustering from scratch. Heuristics from pre-clustered data guide incremental\nclustering, accelerating the process by 20x with only a 0.3% increase in\nclustering error. DB search results overlap by 96% with state-of-the-art tools,\nvalidating search quality. The hardware leverages a 3T 2M T J SOT-CAM at the\n7nm node with a compute-in-memory design. For the human genome draft dataset\n(131GB), setup requires 1.19mJ for 2M spectra, while a 1000 query search\nconsumes 1.1{\\mu}J. Bucket-wise parallelization further achieves 100x speedup."}
{"id": "2511.03480", "pdf": "https://arxiv.org/pdf/2511.03480", "abs": "https://arxiv.org/abs/2511.03480", "authors": ["Khalid Belhajjame", "Haroun Mezrioui", "Yuyan Zhao"], "title": "In-Memory Indexing and Querying of Provenance in Data Preparation Pipelines", "categories": ["cs.DB"], "comment": null, "summary": "Data provenance has numerous applications in the context of data preparation\npipelines. It can be used for debugging faulty pipelines, interpreting results,\nverifying fairness, and identifying data quality issues, which may affect the\nsources feeding the pipeline execution. In this paper, we present an indexing\nmechanism to efficiently capture and query pipeline provenance. Our solution\nleverages tensors to capture fine-grained provenance of data processing\noperations, using minimal memory. In addition to record-level lineage\nrelationships, we provide finer granularity at the attribute level. This is\nachieved by augmenting tensors, which capture retrospective provenance, with\nprospective provenance information, drawing connections between input and\noutput schemas of data processing operations. We demonstrate how these two\ntypes of provenance (retrospective and prospective) can be combined to answer a\nbroad range of provenance queries efficiently, and show effectiveness through\nevaluation exercises using both real and synthetic data."}
{"id": "2511.03489", "pdf": "https://arxiv.org/pdf/2511.03489", "abs": "https://arxiv.org/abs/2511.03489", "authors": ["Daniel Kang"], "title": "Analytical Queries for Unstructured Data", "categories": ["cs.DB"], "comment": null, "summary": "Unstructured data, in the form of text, images, video, and audio, is produced\nat exponentially higher rates. In tandem, machine learning (ML) methods have\nbecome increasingly powerful at analyzing unstructured data. Modern ML methods\ncan now detect objects in images, understand actions in videos, and even\nclassify complex legal texts based on legal intent. Combined, these trends make\nit increasingly feasible for analysts and researchers to automatically\nunderstand the \"real world.\" However, there are major challenges in deploying\nthese techniques: 1) executing queries efficiently given the expense of ML\nmethods, 2) expressing queries over bespoke forms of data, and 3) handling\nerrors in ML methods.\n  In this monograph, we discuss challenges and advances in data management\nsystems for unstructured data using ML, with a particular focus on video\nanalytics. Using ML to answer queries introduces new challenges.First, even\nturning user intent into queries can be challenging: it is not obvious how to\nexpress a query of the form \"select instances of cars turning left.\" Second, ML\nmodels can be orders of magnitude more expensive compared to processing\ntraditional structured data. Third, ML models and the methods to accelerate\nanalytics with ML models can be error-prone.\n  Recent work in the data management community has aimed to address all of\nthese challenges. Users can now express queries via user-defined functions,\nopaquely through standard structured schemas, and even by providing examples.\nGiven a query, recent work focuses on optimizing queries by approximating\nexpensive \"gold\" methods with varying levels of guarantees. Finally, to handle\nerrors in ML models, recent work has focused on applying outlier and drift\ndetection to data analytics with ML."}
