{"id": "2506.21784", "pdf": "https://arxiv.org/pdf/2506.21784", "abs": "https://arxiv.org/abs/2506.21784", "authors": ["Yifan Liu", "Xishun Liao", "Haoxuan Ma", "Jonathan Liu", "Rohan Jadhav", "Jiaqi Ma"], "title": "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Understanding and modeling human mobility patterns is crucial for effective\ntransportation planning and urban development. Despite significant advances in\nmobility research, there remains a critical gap in simulation platforms that\nallow for algorithm development, policy implementation, and comprehensive\nevaluation at scale. Traditional activity-based models require extensive data\ncollection and manual calibration, machine learning approaches struggle with\nadaptation to dynamic conditions, and treding agent-based Large Language Models\n(LLMs) implementations face computational constraints with large-scale\nsimulations. To address these challenges, we propose MobiVerse, a hybrid\nframework leverages the efficiency of lightweight domain-specific generator for\ngenerating base activity chains with the adaptability of LLMs for context-aware\nmodifications. A case study was conducted in Westwood, Los Angeles, where we\nefficiently generated and dynamically adjusted schedules for the whole\npopulation of approximately 53,000 agents on a standard PC. Our experiments\ndemonstrate that MobiVerse successfully enables agents to respond to\nenvironmental feedback, including road closures, large gathering events like\nfootball games, and congestion, through our hybrid framework. Its modular\ndesign facilitates testing various mobility algorithms at both transportation\nsystem and agent levels. Results show our approach maintains computational\nefficiency while enhancing behavioral realism. MobiVerse bridges the gap in\nmobility simulation by providing a customizable platform for mobility systems\nplanning and operations with benchmark algorithms. Code and videos are\navailable at https://github.com/ucla-mobility/MobiVerse.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u7684\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ee5\u53ca\u57ce\u5e02\u79fb\u52a8\u6a21\u62df\uff0c\u800c\u57ce\u5e02\u79fb\u52a8\u6a21\u62df\u901a\u5e38\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u5229\u7528LLMs\u7684\u9002\u5e94\u6027\u6765\u6539\u8fdb\u79fb\u52a8\u6a21\u5f0f\u7684\u751f\u6210\u548c\u8c03\u6574\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u79fb\u52a8\u6a21\u5f0f\u7684\u751f\u6210\u548c\u4eff\u771f\uff0c\u4f46\u5176\u6838\u5fc3\u4e0e\u7406\u89e3\u548c\u9884\u6d4b\u4e2a\u4f53\u79fb\u52a8\u8f68\u8ff9\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u660e\u786e\u4f7f\u7528\u4e86LLM\u3002\u56e0\u6b64\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "mobility simulation", "agent-based", "urban mobility", "activity chains"]}}
{"id": "2506.21805", "pdf": "https://arxiv.org/pdf/2506.21805", "abs": "https://arxiv.org/abs/2506.21805", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper utilizes large language models to simulate human behavior in urban environments, including generating daily schedules and navigation, which is related to trajectory prediction. While the main focus is on urban simulation, the agent navigation aspect connects to trajectory prediction, and the use of LLMs is central to the work.", "keywords": ["Large Language Models", "LLMs", "agent navigation", "urban simulation", "human behavior modeling"]}}
{"id": "2506.21980", "pdf": "https://arxiv.org/pdf/2506.21980", "abs": "https://arxiv.org/abs/2506.21980", "authors": ["Biao Wang", "Wenwen Li"], "title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning", "categories": ["cs.CV"], "comment": "7 pages, 2 figures", "summary": "Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u6307\u51fa\u4f7f\u7528\u4e86MLLMs\uff08\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\uff0c\u800c\u89c6\u89c9\u76ee\u6807\u8ddf\u8e2a\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\u3002\u8bba\u6587\u76f4\u63a5\u5e94\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u89c6\u89c9\u8ddf\u8e2a\u95ee\u9898\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["MLLMs", "Large Language Models", "Visual Object Tracking", "Reinforcement Learning", "Template Matching", "Qwen2.5-VL"]}}
{"id": "2506.21853", "pdf": "https://arxiv.org/pdf/2506.21853", "abs": "https://arxiv.org/abs/2506.21853", "authors": ["Dewei Wang", "Chenjia Ba", "Chenhui Li", "Jiyuan Shi", "Yan Ding", "Chi Zhang", "Bin Zhao"], "title": "Skill-Nav: Enhanced Navigation with Versatile Quadrupedal Locomotion via Waypoint Interface", "categories": ["cs.RO"], "comment": "17pages, 6 figures", "summary": "Quadrupedal robots have demonstrated exceptional locomotion capabilities\nthrough Reinforcement Learning (RL), including extreme parkour maneuvers.\nHowever, integrating locomotion skills with navigation in quadrupedal robots\nhas not been fully investigated, which holds promise for enhancing\nlong-distance movement capabilities. In this paper, we propose Skill-Nav, a\nmethod that incorporates quadrupedal locomotion skills into a hierarchical\nnavigation framework using waypoints as an interface. Specifically, we train a\nwaypoint-guided locomotion policy using deep RL, enabling the robot to\nautonomously adjust its locomotion skills to reach targeted positions while\navoiding obstacles. Compared with direct velocity commands, waypoints offer a\nsimpler yet more flexible interface for high-level planning and low-level\ncontrol. Utilizing waypoints as the interface allows for the application of\nvarious general planning tools, such as large language models (LLMs) and path\nplanning algorithms, to guide our locomotion policy in traversing terrains with\ndiverse obstacles. Extensive experiments conducted in both simulated and\nreal-world scenarios demonstrate that Skill-Nav can effectively traverse\ncomplex terrains and complete challenging navigation tasks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper describes a navigation method for quadrupedal robots that uses waypoints as an interface between high-level planning and low-level control. The abstract explicitly mentions the potential application of large language models (LLMs) for guiding the locomotion policy, indicating a connection to large models. While the primary focus is on robot navigation and locomotion skills, the mention of LLMs and the use of waypoints (which can be interpreted as a form of trajectory planning) gives it a moderate relevance to the specified topics.", "keywords": ["navigation", "locomotion", "waypoints", "large language models", "LLMs", "path planning"]}}
{"id": "2506.22056", "pdf": "https://arxiv.org/pdf/2506.22056", "abs": "https://arxiv.org/abs/2506.22056", "authors": ["Xuan Zhang", "Ziyan Jiang", "Rui Meng", "Yifei Leng", "Zhenbang Xiao", "Zora Zhiruo Wang", "Yanyi Shang", "Dehan Kong"], "title": "Universal Retrieval for Multimodal Trajectory Modeling", "categories": ["cs.AI"], "comment": "18 pages, 3 figures, accepted by Workshop on Computer-use Agents @\n  ICML 2025", "summary": "Trajectory data, capturing human actions and environmental states across\nvarious modalities, holds significant potential for enhancing AI agent\ncapabilities, particularly in GUI environments. However, how to model the\nrepresentation of trajectory-level data presents a significant challenge that\nhas not been systematically addressed amid explosive trajectory data growth. In\nthis work, we introduce Multimodal Trajectory Retrieval, bridging the gap\nbetween universal retrieval and agent-centric trajectory modeling. We construct\nthe Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and\nstates across diverse real-world scenarios. Based on this, we present\nGAE-Bench, a benchmark containing a large number of trajectory-based retrieval\npairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework\nthat adopts vision-language models and incorporates optimized contrastive\nlearning through a token selection and the GradCache mechanism. Comprehensive\nevaluations across multiple datasets show that GAE-Retriever consistently\noutperforms strong baselines in retrieval recall, highlighting its\neffectiveness in advancing multimodal trajectory retrieval.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u8f68\u8ff9\u5efa\u6a21\u548c\u68c0\u7d22\uff0c\u660e\u786e\u63d0\u5230\u4e86\u8f68\u8ff9\u6570\u636e\u548cAI agent\u80fd\u529b\u63d0\u5347\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u4fee\u6539\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86vision-language models\uff0c\u5e76\u4e14\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u5229\u7528\u5927\u578b\u6a21\u578b\u7406\u89e3\u548c\u751f\u6210\u8f68\u8ff9\u6570\u636e\u6709\u6f5c\u5728\u5173\u8054\u3002", "keywords": ["trajectory prediction", "multimodal trajectory modeling", "vision-language models", "retrieval", "agent"]}}
{"id": "2506.21976", "pdf": "https://arxiv.org/pdf/2506.21976", "abs": "https://arxiv.org/abs/2506.21976", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "comment": "Accepted to CVPR 2025", "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u57ce\u5e02\u89c4\u6a21\u7684\u4ea4\u901a\u4eff\u771f\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u4e16\u754c\u6a21\u578b\u6765\u6a21\u62df\u4ea4\u901a\u573a\u666f\uff0c\u6d89\u53ca\u52a8\u6001agent\u7684\u884c\u4e3a\u5efa\u6a21\u548c\u8f68\u8ff9\u751f\u6210\u3002\u867d\u7136\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u751f\u6210\u5f0f\u6a21\u578b\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u5927\u578b\u6a21\u578b\uff0c\u5e76\u4e14agent\u884c\u4e3a\u5efa\u6a21\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["traffic simulation", "generative world model", "agent behavior modeling", "dynamic agents", "scene generation", "trajectory prediction"]}}
{"id": "2506.21982", "pdf": "https://arxiv.org/pdf/2506.21982", "abs": "https://arxiv.org/abs/2506.21982", "authors": ["Akshay Jaitly", "Jack Cline", "Siavash Farzan"], "title": "A MILP-Based Solution to Multi-Agent Motion Planning and Collision Avoidance in Constrained Environments", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted to 2025 IEEE International Conference on Automation Science\n  and Engineering (CASE 2025)", "summary": "We propose a mixed-integer linear program (MILP) for multi-agent motion\nplanning that embeds Polytopic Action-based Motion Planning (PAAMP) into a\nsequence-then-solve pipeline. Region sequences confine each agent to adjacent\nconvex polytopes, while a big-M hyperplane model enforces inter-agent\nseparation. Collision constraints are applied only to agents sharing or\nneighboring a region, which reduces binary variables exponentially compared\nwith naive formulations. An L1 path-length-plus-acceleration cost yields smooth\ntrajectories. We prove finite-time convergence and demonstrate on\nrepresentative multi-agent scenarios with obstacles that our formulation\nproduces collision-free trajectories an order of magnitude faster than an\nunstructured MILP baseline.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u548c\u907f\u969c\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u76f8\u5173\u9886\u57df\uff08\u7279\u522b\u662f\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff09\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u8fd0\u52a8\u89c4\u5212\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u5173\u952e\u8bcd\u5305\u62ec\u8fd0\u52a8\u89c4\u5212\u548c\u8f68\u8ff9\u3002", "keywords": ["motion planning", "trajectory"]}}
{"id": "2506.22087", "pdf": "https://arxiv.org/pdf/2506.22087", "abs": "https://arxiv.org/abs/2506.22087", "authors": ["Armand Jordana", "Jianghan Zhang", "Joseph Amigo", "Ludovic Righetti"], "title": "An Introduction to Zero-Order Optimization Techniques for Robotics", "categories": ["cs.RO"], "comment": null, "summary": "Zero-order optimization techniques are becoming increasingly popular in\nrobotics due to their ability to handle non-differentiable functions and escape\nlocal minima. These advantages make them particularly useful for trajectory\noptimization and policy optimization. In this work, we propose a mathematical\ntutorial on random search. It offers a simple and unifying perspective for\nunderstanding a wide range of algorithms commonly used in robotics. Leveraging\nthis viewpoint, we classify many trajectory optimization methods under a common\nframework and derive novel competitive RL algorithms.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses zero-order optimization techniques and their application in robotics, specifically mentioning trajectory optimization. Although it does not directly involve Large Language Models, the focus on trajectory optimization warrants a moderate relevance score. It does not discuss large language models.", "keywords": ["trajectory optimization", "robotics"]}}
{"id": "2506.21885", "pdf": "https://arxiv.org/pdf/2506.21885", "abs": "https://arxiv.org/abs/2506.21885", "authors": ["Chuheng Wei", "Ziye Qin", "Ziyan Zhang", "Guoyuan Wu", "Matthew J. Barth"], "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles", "categories": ["cs.CV", "cs.MM", "cs.RO"], "comment": "Accepted by IEEE IV 2025", "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper reviews multi-sensor fusion techniques for autonomous driving. While the primary focus is on sensor fusion, it mentions the integration of Vision-Language Models (VLMs) and Large Language Models (LLMs) and their potential to enhance system adaptability and robustness in autonomous driving. This connects to the large language model aspect. Although trajectory prediction isn't explicitly mentioned, it's implicitly related to autonomous driving, which requires predicting the future trajectories of surrounding objects.", "keywords": ["Large Language Models", "LLMs", "Vision-Language Models", "VLMs", "autonomous driving"]}}
{"id": "2506.22039", "pdf": "https://arxiv.org/pdf/2506.22039", "abs": "https://arxiv.org/abs/2506.22039", "authors": ["Lu Han", "Yu Liu", "Qiwen Deng", "Jian Jiang", "Yinbo Sun", "Zhe Yu", "Binfeng Wang", "Xingyu Lu", "Lintao Ma", "Han-Jia Ye", "De-Chuan Zhan"], "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time Series Foundation Models (TSFMs) have achieved remarkable success\nthrough large-scale pretraining. However, their design primarily targets\nreal-valued series, limiting their ability to handle general forecasting tasks\ninvolving diverse and often heterogeneous covariates--such as categorical\nvariables and multimodal data (e.g., images, text)--which are typically\ntask-specific and difficult to leverage during pretraining. To address this\ngap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge\nTSFMs with general covariate-aware forecasting. UniCA first performs covariate\nhomogenization to transform heterogeneous covariates into high-level\nhomogeneous series representations and then fuses them via a unified\nattention-based fusion mechanism. UniCA is compatible and universal for\nadaptation with both homogeneous and heterogeneous covariates, incorporating\nextra covariate information while preserving the generalization ability of\nTSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware\nforecasting benchmarks demonstrate the superiority of UniCA, highlighting the\npromise of covariate-aware TSFM adaptation in real-world forecasting scenarios.\nCodes are released on https://github.com/hanlu-nju/UniCA.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u4e86\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff08TSFMs\uff09\uff0c\u8fd9\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u5e7f\u4e49\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u5904\u7406\u5f02\u6784\u534f\u53d8\u91cf\u7684\u80fd\u529b\uff0c\u6697\u793a\u4e86\u4e0e\u5927\u6a21\u578b\u548c\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\u7684\u76f8\u5173\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u9884\u6d4b\u6846\u67b6\u548c\u5bf9\u65f6\u95f4\u5e8f\u5217\u7684\u5173\u6ce8\u4f7f\u5176\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Time Series Foundation Models", "forecasting", "large-scale pretraining", "foundation models"]}}
{"id": "2506.21562", "pdf": "https://arxiv.org/pdf/2506.21562", "abs": "https://arxiv.org/abs/2506.21562", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": null, "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses floorplan generation, drawing inspiration from the 'next token prediction' mechanism used in large language models. While not directly related to trajectory prediction, the use of LLM concepts in a spatial prediction task gives it some relevance. The 'next room prediction' paradigm is conceptually similar to predicting the next location in a trajectory.", "keywords": ["Large Language Models", "next token prediction", "autoregressive", "prediction"]}}
{"id": "2506.21912", "pdf": "https://arxiv.org/pdf/2506.21912", "abs": "https://arxiv.org/abs/2506.21912", "authors": ["Xinghan Wang", "Kun Xu", "Fei Li", "Cao Sheng", "Jiazhong Yu", "Yadong Mu"], "title": "Generating Attribute-Aware Human Motions from Textual Prompt", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Text-driven human motion generation has recently attracted considerable\nattention, allowing models to generate human motions based on textual\ndescriptions. However, current methods neglect the influence of human\nattributes (such as age, gender, weight, and height) which are key factors\nshaping human motion patterns. This work represents a pilot exploration for\nbridging this gap. We conceptualize each motion as comprising both attribute\ninformation and action semantics, where textual descriptions align exclusively\nwith action semantics. To achieve this, a new framework inspired by Structural\nCausal Models is proposed to decouple action semantics from human attributes,\nenabling text-to-semantics prediction and attribute-controlled generation. The\nresulting model is capable of generating realistic, attribute-aware motion\naligned with the user's text and attribute inputs. For evaluation, we introduce\nHumanAttr, a comprehensive dataset containing attribute annotations for\ntext-motion pairs, setting the first benchmark for attribute-aware\ntext-to-motion generation. Extensive experiments on the new dataset validate\nour model's effectiveness.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating human motions from text, which can be considered a form of trajectory prediction (specifically, predicting the future positions/poses of a human). While it doesn't explicitly mention trajectory prediction or directly use large language models, the use of textual prompts and the generation of motion sequences suggests a connection. The focus on attributes influencing motion also ties into potential applications in realistic trajectory prediction scenarios. However, the absence of explicit mention of LLMs or trajectory prediction methodologies lowers the score.", "keywords": ["human motion generation", "text-to-motion", "action semantics", "attribute-aware", "motion prediction"]}}
{"id": "2506.21570", "pdf": "https://arxiv.org/pdf/2506.21570", "abs": "https://arxiv.org/abs/2506.21570", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper explores the use of language models for time series forecasting. While not directly related to trajectory prediction, time series forecasting shares similarities with trajectory prediction, especially in the context of predicting future states based on historical data. The paper also focuses on the use of large language models (LMs), which is one of the core topics. The connection to trajectory prediction is indirect but present.", "keywords": ["language models", "LMs", "time series forecasting", "transfer learning"]}}
{"id": "2506.21924", "pdf": "https://arxiv.org/pdf/2506.21924", "abs": "https://arxiv.org/abs/2506.21924", "authors": ["Zhao Jin", "Rong-Cheng Tu", "Jingyi Liao", "Wenhao Sun", "Xiao Luo", "Shunyu Liu", "Dacheng Tao"], "title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding", "categories": ["cs.CV"], "comment": null, "summary": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene\nbased on natural language queries. To alleviate the reliance on costly 3D\ntraining data, recent studies have explored zero-shot 3DVG by leveraging the\nextensive knowledge and powerful reasoning capabilities of pre-trained LLMs and\nVLMs. However, existing paradigms tend to emphasize either spatial (3D-based)\nor semantic (2D-based) understanding, limiting their effectiveness in complex\nreal-world applications. In this work, we introduce SPAZER - a VLM-driven agent\nthat combines both modalities in a progressive reasoning framework. It first\nholistically analyzes the scene and produces a 3D rendering from the optimal\nviewpoint. Based on this, anchor-guided candidate screening is conducted to\nperform a coarse-level localization of potential objects. Furthermore,\nleveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is\nefficiently performed to determine the best-matching object. By bridging\nspatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot\ngrounding without training on 3D-labeled data. Extensive experiments on\nScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms\nprevious state-of-the-art zero-shot methods, achieving notable gains of 9.0%\nand 10.9% in accuracy.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses 3D visual grounding using Large Language Models (LLMs). While it doesn't directly address trajectory prediction, it leverages LLMs for spatial reasoning and object localization in 3D scenes, which can be relevant to understanding scene context for future trajectory prediction tasks. The connection is not direct, but the use of LLMs in spatial understanding warrants a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "3D Visual Grounding", "spatial reasoning", "zero-shot"]}}
{"id": "2506.22007", "pdf": "https://arxiv.org/pdf/2506.22007", "abs": "https://arxiv.org/abs/2506.22007", "authors": ["Liudi Yang", "Yang Bai", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Dong Chen", "Soumajit Majumder", "Ziyuan Liu", "Gitta Kutyniok", "Abhinav Valada"], "title": "RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "We address the problem of generating long-horizon videos for robotic\nmanipulation tasks. Text-to-video diffusion models have made significant\nprogress in photorealism, language understanding, and motion generation but\nstruggle with long-horizon robotic tasks. Recent works use video diffusion\nmodels for high-quality simulation data and predictive rollouts in robot\nplanning. However, these works predict short sequences of the robot achieving\none task and employ an autoregressive paradigm to extend to the long horizon,\nleading to error accumulations in the generated video and in the execution. To\novercome these limitations, we propose a novel pipeline that bypasses the need\nfor autoregressive generation. We achieve this through a threefold\ncontribution: 1) we first decompose the high-level goals into smaller atomic\ntasks and generate keyframes aligned with these instructions. A second\ndiffusion model then interpolates between each of the two generated frames,\nachieving the long-horizon video. 2) We propose a semantics preserving\nattention module to maintain consistency between the keyframes. 3) We design a\nlightweight policy model to regress the robot joint states from generated\nvideos. Our approach achieves state-of-the-art results on two benchmarks in\nvideo quality and consistency while outperforming previous policy models on\nlong-horizon tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on long-horizon video generation for robot manipulation. While it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting the future trajectory of pedestrians or vehicles), it does involve generating a sequence of future states (videos) of a robot performing a task. The use of diffusion models connects it to the broader field of large models, even though it's not a Large Language Model. The connection to trajectory prediction is through the generation of a sequence of future states representing the robot's \"trajectory\" of actions.", "keywords": ["video generation", "diffusion models", "long-horizon", "robot manipulation", "policy model"]}}
