{"id": "2510.15368", "pdf": "https://arxiv.org/pdf/2510.15368", "abs": "https://arxiv.org/abs/2510.15368", "authors": ["Renrui Li", "Qingzhi Ma", "Jiajie Xu", "Lei Zhao", "An Liu"], "title": "TKHist: Cardinality Estimation for Join Queries via Histograms with Dominant Attribute Correlation Finding", "categories": ["cs.DB"], "comment": "CIKM2025", "summary": "Cardinality estimation has long been crucial for cost-based database\noptimizers in identifying optimal query execution plans, attracting significant\nattention over the past decades. While recent advancements have significantly\nimproved the accuracy of multi-table join query estimations, these methods\nintroduce challenges such as higher space overhead, increased latency, and\ngreater complexity, especially when integrated with the binary join framework.\nIn this paper, we introduce a novel cardinality estimation method named TKHist,\nwhich addresses these challenges by relaxing the uniformity assumption in\nhistograms. TKHist captures bin-wise non-uniformity information, enabling\naccurate cardinality estimation for join queries without filter predicates.\nFurthermore, we explore the attribute independent assumption, which can lead to\nsignificant over-estimation rather than under-estimation in multi-table join\nqueries. To address this issue, we propose the dominating join path correlation\ndiscovery algorithm to highlight and manage correlations between join keys and\nfilter predicates. Our extensive experiments on popular benchmarks demonstrate\nthat TKHist reduces error variance by 2-3 orders of magnitude compared to SOTA\nmethods, while maintaining comparable or lower memory usage."}
{"id": "2510.15445", "pdf": "https://arxiv.org/pdf/2510.15445", "abs": "https://arxiv.org/abs/2510.15445", "authors": ["Gregory", "Weintraub"], "title": "Optimizing Data Lakes' Queries", "categories": ["cs.DB"], "comment": null, "summary": "Cloud data lakes provide a modern solution for managing large volumes of\ndata. The fundamental principle behind these systems is the separation of\ncompute and storage layers. In this architecture, inexpensive cloud storage is\nutilized for data storage, while compute engines are employed to perform\nanalytics on this data in an \"on-demand\" mode. However, to execute any\ncalculations on the data, it must be transferred from the storage layer to the\ncompute layer over the network for each query. This transfer can negatively\nimpact calculation performance and requires significant network bandwidth. In\nthis thesis, we examine various strategies to enhance query performance within\na cloud data lake architecture. We begin by formalizing the problem and\nproposing a straightforward yet robust theoretical framework that clearly\noutlines the associated trade-offs. Central to our framework is the concept of\na \"query coverage set,\" which is defined as the collection of files that need\nto be accessed from storage to fulfill a specific query. Our objective is to\nidentify the minimal coverage set for each query and execute the query\nexclusively on this subset of files. This approach enables us to significantly\nimprove query performance."}
{"id": "2510.15413", "pdf": "https://arxiv.org/pdf/2510.15413", "abs": "https://arxiv.org/abs/2510.15413", "authors": ["Po-Yu Tseng", "Po-Chu Hsu", "Shih-Wei Liao"], "title": "FHE-SQL: Fully Homomorphic Encrypted SQL Database", "categories": ["cs.CR", "cs.DB"], "comment": "12 pages, 1 figures, Keywords: Fully Homomorphic Encryption, Private\n  Information Retrieval, Encrypted Databases, Privacy-Preserving Systems", "summary": "FHE-SQL is a privacy-preserving database system that enables secure query\nprocessing on encrypted data using Fully Homomorphic Encryption (FHE),\nproviding privacy guaranties where an untrusted server can execute encrypted\nqueries without learning either the query contents or the underlying data.\nUnlike property-preserving encryption-based systems such as CryptDB, which rely\non deterministic or order-preserving encryption and are vulnerable to\nfrequency, order, and equality-pattern inference attacks, FHE-SQL performs\ncomputations entirely under encryption, eliminating these leakage channels.\nCompared to trusted-hardware approaches such as TrustedDB, which depend on a\nhardware security module and thus inherit its trust and side-channel\nlimitations, our design achieves end-to-end cryptographic protection without\nrequiring trusted execution environments. In contrast to high-performance\nFHE-based engines-Hermes, which target specialized workloads such as vector\nsearch, FHE-SQL supports general SQL query semantics with schema-aware,\ntype-safe definitions suitable for relational data management. FHE-SQL\nmitigates the high cost of ciphertext space by using an indirection\narchitecture that separates metadata in RocksDB from large ciphertexts in blob\nstorage. It supports oblivious selection via homomorphic boolean masks,\nmulti-tier caching, and garbage collection, with security proven under the\nUniversal Composability framework."}
{"id": "2510.15485", "pdf": "https://arxiv.org/pdf/2510.15485", "abs": "https://arxiv.org/abs/2510.15485", "authors": ["D\u0101vis Ka\u017eemaks", "Laurens Versluis", "Burcu Kulahcioglu Ozkan", "J\u00e9r\u00e9mie Decouchant"], "title": "Balancing Fairness and Performance in Multi-User Spark Workloads with Dynamic Scheduling (extended version)", "categories": ["cs.DC", "cs.DB", "cs.SY", "eess.SY"], "comment": "This paper is an extended version of a paper accepted at the ACM\n  Symposium on Cloud Computing (SoCC'25) that contains a proof of correctness", "summary": "Apache Spark is a widely adopted framework for large-scale data processing.\nHowever, in industrial analytics environments, Spark's built-in schedulers,\nsuch as FIFO and fair scheduling, struggle to maintain both user-level fairness\nand low mean response time, particularly in long-running shared applications.\nExisting solutions typically focus on job-level fairness which unintentionally\nfavors users who submit more jobs. Although Spark offers a built-in fair\nscheduler, it lacks adaptability to dynamic user workloads and may degrade\noverall job performance. We present the User Weighted Fair Queuing (UWFQ)\nscheduler, designed to minimize job response times while ensuring equitable\nresource distribution across users and their respective jobs. UWFQ simulates a\nvirtual fair queuing system and schedules jobs based on their estimated finish\ntimes under a bounded fairness model. To further address task skew and reduce\npriority inversions, which are common in Spark workloads, we introduce runtime\npartitioning, a method that dynamically refines task granularity based on\nexpected runtime. We implement UWFQ within the Spark framework and evaluate its\nperformance using multi-user synthetic workloads and Google cluster traces. We\nshow that UWFQ reduces the average response time of small jobs by up to 74%\ncompared to existing built-in Spark schedulers and to state-of-the-art fair\nscheduling algorithms."}
{"id": "2510.15560", "pdf": "https://arxiv.org/pdf/2510.15560", "abs": "https://arxiv.org/abs/2510.15560", "authors": ["Jiayuan Bai", "Xuan-guang Pan", "Chongyang Tao", "Shuai Ma"], "title": "JudgeSQL: Reasoning over SQL Candidates with Weighted Consensus Tournament", "categories": ["cs.AI", "cs.DB"], "comment": "13 pages", "summary": "Text-to-SQL is a pivotal task that bridges natural language understanding and\nstructured data access, yet it remains fundamentally challenging due to\nsemantic ambiguity and complex compositional reasoning. While large language\nmodels (LLMs) have greatly advanced SQL generation though prompting, supervised\nfinetuning and reinforced tuning, the shift toward test-time scaling exposes a\nnew bottleneck: selecting the correct query from a diverse candidate pool.\nExisting selection approaches, such as self-consistency or best-of-$N$\ndecoding, provide only shallow signals, making them prone to inconsistent\nscoring, fragile reasoning chains, and a failure to capture fine-grained\nsemantic distinctions between closely related SQL candidates. To this end, we\nintroduce JudgeSQL, a principled framework that redefines SQL candidate\nselection through structured reasoning and weighted consensus tournament\nmechanism. JudgeSQL develops a reasoning-based SQL judge model that distills\nreasoning traces with reinforcement learning guided by verifiable rewards,\nenabling accurate and interpretable judgments. Building on this, a weighted\nconsensus tournament integrates explicit reasoning preferences with implicit\ngenerator confidence, yielding selections that are both more reliable and more\nefficient. Extensive experiments on the BIRD benchmark demonstrate that\nJudgeSQL exhibits superior SQL judgment capabilities and good cross-scale\ngeneralization and robustness to generator capacity."}
{"id": "2510.15727", "pdf": "https://arxiv.org/pdf/2510.15727", "abs": "https://arxiv.org/abs/2510.15727", "authors": ["Sai Yashwant", "Anurag Dubey", "Praneeth Paikray", "Gantala Thulsiram"], "title": "Invoice Information Extraction: Methods and Performance Evaluation", "categories": ["cs.AI", "cs.DB"], "comment": null, "summary": "This paper presents methods for extracting structured information from\ninvoice documents and proposes a set of evaluation metrics (EM) to assess the\naccuracy of the extracted data against annotated ground truth. The approach\ninvolves pre-processing scanned or digital invoices, applying Docling and\nLlamaCloud Services to identify and extract key fields such as invoice number,\ndate, total amount, and vendor details. To ensure the reliability of the\nextraction process, we establish a robust evaluation framework comprising\nfield-level precision, consistency check failures, and exact match accuracy.\nThe proposed metrics provide a standardized way to compare different extraction\nmethods and highlight strengths and weaknesses in field-specific performance."}
