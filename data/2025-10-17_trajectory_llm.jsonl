{"id": "2510.12992", "pdf": "https://arxiv.org/pdf/2510.12992", "abs": "https://arxiv.org/abs/2510.12992", "authors": ["Neel P. Bhatt", "Po-han Li", "Kushagra Gupta", "Rohan Siva", "Daniel Milan", "Alexander T. Hogue", "Sandeep P. Chinchali", "David Fridovich-Keil", "Zhangyang Wang", "Ufuk Topcu"], "title": "UNCAP: Uncertainty-Guided Planning Using Natural Language Communication for Cooperative Autonomous Vehicles", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.MA"], "comment": null, "summary": "Safe large-scale coordination of multiple cooperative connected autonomous\nvehicles (CAVs) hinges on communication that is both efficient and\ninterpretable. Existing approaches either rely on transmitting high-bandwidth\nraw sensor data streams or neglect perception and planning uncertainties\ninherent in shared data, resulting in systems that are neither scalable nor\nsafe. To address these limitations, we propose Uncertainty-Guided Natural\nLanguage Cooperative Autonomous Planning (UNCAP), a vision-language model-based\nplanning approach that enables CAVs to communicate via lightweight natural\nlanguage messages while explicitly accounting for perception uncertainty in\ndecision-making. UNCAP features a two-stage communication protocol: (i) an ego\nCAV first identifies the subset of vehicles most relevant for information\nexchange, and (ii) the selected CAVs then transmit messages that quantitatively\nexpress their perception uncertainty. By selectively fusing messages that\nmaximize mutual information, this strategy allows the ego vehicle to integrate\nonly the most relevant signals into its decision-making, improving both the\nscalability and reliability of cooperative planning. Experiments across diverse\ndriving scenarios show a 63% reduction in communication bandwidth with a 31%\nincrease in driving safety score, a 61% reduction in decision uncertainty, and\na four-fold increase in collision distance margin during near-miss events.\nProject website: https://uncap-project.github.io/", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u5c06\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\uff08\u901a\u8fc7vision-language\u6a21\u578b\u5b9e\u73b0\uff09\u5e94\u7528\u4e8e\u534f\u540c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u89c4\u5212\uff0c\u91cd\u70b9\u5728\u4e8e\u89e3\u51b3\u901a\u4fe1\u6548\u7387\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u534f\u540c\u89c4\u5212\uff0c\u4f46\u5176\u4e2d\u6d89\u53ca\u4e86\u8f66\u8f86\u7684\u51b3\u7b56\u548c\u89c4\u5212\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u89c6\u4e3a\u8f68\u8ff9\u9884\u6d4b\u7684\u6269\u5c55\u5e94\u7528\u3002\u5e76\u4e14\u4f7f\u7528\u4e86vision-language\u6a21\u578b\uff0c\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002", "keywords": ["autonomous vehicles", "planning", "vision-language model", "perception uncertainty", "cooperative planning"]}}
{"id": "2510.13108", "pdf": "https://arxiv.org/pdf/2510.13108", "abs": "https://arxiv.org/abs/2510.13108", "authors": ["Jingyu Song", "Zhenxin Li", "Shiyi Lan", "Xinglong Sun", "Nadine Chang", "Maying Shen", "Joshua Chen", "Katherine A. Skinner", "Jose M. Alvarez"], "title": "DriveCritic: Towards Context-Aware, Human-Aligned Evaluation for Autonomous Driving with Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "9 pages, 3 figures", "summary": "Benchmarking autonomous driving planners to align with human judgment remains\na critical challenge, as state-of-the-art metrics like the Extended Predictive\nDriver Model Score (EPDMS) lack context awareness in nuanced scenarios. To\naddress this, we introduce DriveCritic, a novel framework featuring two key\ncontributions: the DriveCritic dataset, a curated collection of challenging\nscenarios where context is critical for correct judgment and annotated with\npairwise human preferences, and the DriveCritic model, a Vision-Language Model\n(VLM) based evaluator. Fine-tuned using a two-stage supervised and\nreinforcement learning pipeline, the DriveCritic model learns to adjudicate\nbetween trajectory pairs by integrating visual and symbolic context.\nExperiments show DriveCritic significantly outperforms existing metrics and\nbaselines in matching human preferences and demonstrates strong context\nawareness. Overall, our work provides a more reliable, human-aligned foundation\nto evaluating autonomous driving systems.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528 Vision-Language Models (VLMs) \u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\uff0c\u800c\u81ea\u52a8\u9a7e\u9a76\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u3002\u8bba\u6587\u5c06\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\u7ed3\u5408\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8f68\u8ff9\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u90fd\u76f8\u5173\u3002", "keywords": ["autonomous driving", "trajectory", "Vision-Language Models", "VLMs", "evaluation"]}}
{"id": "2510.13002", "pdf": "https://arxiv.org/pdf/2510.13002", "abs": "https://arxiv.org/abs/2510.13002", "authors": ["Boyou Chen", "Gerui Xu", "Zifei Wang", "Huizhong Guo", "Ananna Ahmed", "Zhaonan Sun", "Zhen Hu", "Kaihan Zhang", "Shan Bao"], "title": "From Narratives to Probabilistic Reasoning: Predicting and Interpreting Drivers' Hazardous Actions in Crashes Using Large Language Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Vehicle crashes involve complex interactions between road users, split-second\ndecisions, and challenging environmental conditions. Among these, two-vehicle\ncrashes are the most prevalent, accounting for approximately 70% of roadway\ncrashes and posing a significant challenge to traffic safety. Identifying\nDriver Hazardous Action (DHA) is essential for understanding crash causation,\nyet the reliability of DHA data in large-scale databases is limited by\ninconsistent and labor-intensive manual coding practices. Here, we present an\ninnovative framework that leverages a fine-tuned large language model to\nautomatically infer DHAs from textual crash narratives, thereby improving the\nvalidity and interpretability of DHA classifications. Using five years of\ntwo-vehicle crash data from MTCF, we fine-tuned the Llama 3.2 1B model on\ndetailed crash narratives and benchmarked its performance against conventional\nmachine learning classifiers, including Random Forest, XGBoost, CatBoost, and a\nneural network. The fine-tuned LLM achieved an overall accuracy of 80%,\nsurpassing all baseline models and demonstrating pronounced improvements in\nscenarios with imbalanced data. To increase interpretability, we developed a\nprobabilistic reasoning approach, analyzing model output shifts across original\ntest sets and three targeted counterfactual scenarios: variations in driver\ndistraction and age. Our analysis revealed that introducing distraction for one\ndriver substantially increased the likelihood of \"General Unsafe Driving\";\ndistraction for both drivers maximized the probability of \"Both Drivers Took\nHazardous Actions\"; and assigning a teen driver markedly elevated the\nprobability of \"Speed and Stopping Violations.\" Our framework and analytical\nmethods provide a robust and interpretable solution for large-scale automated\nDHA detection, offering new opportunities for traffic safety analysis and\nintervention.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5206\u6790\u8f66\u8f86\u78b0\u649e\u4e8b\u6545\u7684\u6587\u672c\u53d9\u8ff0\uff0c\u4ee5\u63a8\u65ad\u9a7e\u9a76\u5458\u7684\u5371\u9669\u884c\u4e3a\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u5229\u7528LLM\u5bf9\u4e0e\u4e8b\u6545\u76f8\u5173\u7684\u9a7e\u9a76\u884c\u4e3a\u8fdb\u884c\u5efa\u6a21\u548c\u9884\u6d4b\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002\u540c\u65f6\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86Llama 3.2 1B\u6a21\u578b\uff0c\u5c5e\u4e8e\u5927\u6a21\u578b\u8303\u7574\u3002", "keywords": ["Large Language Model", "LLM", "Driver Hazardous Action", "crash", "probabilistic reasoning"]}}
{"id": "2510.12979", "pdf": "https://arxiv.org/pdf/2510.12979", "abs": "https://arxiv.org/abs/2510.12979", "authors": ["Wei Fan", "Wenlin Yao", "Zheng Li", "Feng Yao", "Xin Liu", "Liang Qiu", "Qingyu Yin", "Yangqiu Song", "Bing Yin"], "title": "DeepPlanner: Scaling Planning Capability for Deep Research Agents via Advantage Shaping", "categories": ["cs.AI", "cs.CL"], "comment": "Under Review", "summary": "Large language models (LLMs) augmented with multi-step reasoning and action\ngeneration abilities have shown promise in leveraging external tools to tackle\ncomplex tasks that require long-horizon planning. However, existing approaches\neither rely on implicit planning in the reasoning stage or introduce explicit\nplanners without systematically addressing how to optimize the planning stage.\nAs evidence, we observe that under vanilla reinforcement learning (RL),\nplanning tokens exhibit significantly higher entropy than other action tokens,\nrevealing uncertain decision points that remain under-optimized. To address\nthis, we propose DeepPlanner, an end-to-end RL framework that effectively\nenhances the planning capabilities of deep research agents. Our approach shapes\ntoken-level advantage with an entropy-based term to allocate larger updates to\nhigh entropy tokens, and selectively upweights sample-level advantages for\nplanning-intensive rollouts. Extensive experiments across seven deep research\nbenchmarks demonstrate that DeepPlanner improves planning quality and achieves\nstate-of-the-art results under a substantially lower training budget.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving the planning capabilities of deep research agents using large language models and reinforcement learning. While it doesn't directly address trajectory prediction, the concept of planning is related to trajectory generation and the use of LLMs is a central theme. Therefore, it has moderate relevance.", "keywords": ["Large language models", "LLMs", "planning", "reinforcement learning", "deep research agents"]}}
{"id": "2510.12847", "pdf": "https://arxiv.org/pdf/2510.12847", "abs": "https://arxiv.org/abs/2510.12847", "authors": ["Liangwei Nathan Zheng", "Wenhao Liang", "Wei Emma Zhang", "Miao Xu", "Olaf Maennel", "Weitong Chen"], "title": "Lifting Manifolds to Mitigate Pseudo-Alignment in LLM4TS", "categories": ["cs.LG"], "comment": null, "summary": "Pseudo-Alignment is a pervasive challenge in many large language models for\ntime series (LLM4TS) models, often causing them to underperform compared to\nlinear models or randomly initialised backbones. However, there is limited\ndiscussion in the community for the reasons that pseudo-alignment occurs. In\nthis work, we conduct a thorough investigation into the root causes of\npseudo-alignment in LLM4TS and build a connection of pseudo-alignment to the\ncone effect in LLM. We demonstrate that pseudo-alignment arises from the\ninterplay of cone effect within pretrained LLM components and the intrinsically\nlow-dimensional manifold of time-series data. In addition, we also introduce\n\\textit{\\textbf{TimeSUP}}, a novel technique designed to mitigate this issue\nand improve forecast performance in existing LLM4TS approaches. TimeSUP\naddresses this by increasing the time series manifold to more closely match the\nintrinsic dimension of language embeddings, allowing the model to distinguish\ntemporal signals clearly while still capturing shared structures across\nmodalities. As a result, representations for time and language tokens remain\ndistinct yet exhibit high cosine similarity, signifying that the model\npreserves each modality unique features while learning their commonalities in a\nunified embedding space. Empirically, TimeSUP consistently outperforms\nstate-of-the-art LLM4TS methods and other lightweight baselines on long-term\nforecasting performance. Furthermore, it can be seamlessly integrated into four\nexisting LLM4TS pipelines and delivers significant improvements in forecasting\nperformance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6982\u5ff5\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u8bba\u6587\u8ba8\u8bba\u4e86LLM\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u4f2a\u5bf9\u9f50\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86TimeSUP\u65b9\u6cd5\u6765\u6539\u8fdb\u9884\u6d4b\u6027\u80fd\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u5927\u6a21\u578b\u76f8\u5173\uff0c\u5e76\u4e14\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5177\u6709\u4e00\u5b9a\u7684\u95f4\u63a5\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLM4TS", "time series", "forecasting"]}}
{"id": "2510.12924", "pdf": "https://arxiv.org/pdf/2510.12924", "abs": "https://arxiv.org/abs/2510.12924", "authors": ["Pavel Pochobradsk\u00fd", "Ond\u0159ej Proch\u00e1zka", "Robert P\u011bni\u010dka", "Vojt\u011bch Von\u00e1sek", "Martin Saska"], "title": "Geometric Model Predictive Path Integral for Agile UAV Control with Online Collision Avoidance", "categories": ["cs.RO"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In this letter, we introduce Geometric Model Predictive Path Integral\n(GMPPI), a sampling-based controller capable of tracking agile trajectories\nwhile avoiding obstacles. In each iteration, GMPPI generates a large number of\ncandidate rollout trajectories and then averages them to create a nominal\ncontrol to be followed by the Unmanned Aerial Vehicle (UAV). We propose using\ngeometric SE(3) control to generate part of the rollout trajectories,\nsignificantly increasing precision in agile flight. Furthermore, we introduce\nvarying rollout simulation time step length and dynamic cost and noise\nparameters, vastly improving tracking performance of smooth and low-speed\ntrajectories over an existing Model Predictive Path Integral (MPPI)\nimplementation. Finally, we propose an integration of GMPPI with a stereo depth\ncamera, enabling online obstacle avoidance at high speeds, a crucial step\ntowards autonomous UAV flights in complex environments. The proposed controller\ncan track simulated agile reference trajectories with position error similar to\nthe geometric SE(3) controller. However, the same configuration of the proposed\ncontroller can avoid obstacles in a simulated forest environment at speeds of\nup to 13m/s, surpassing the performance of a state-of-the-art obstacle-aware\nplanner. In real-world experiments, GMPPI retains the capability to track agile\ntrajectories and avoids obstacles at speeds of up to 10m/s.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory planning and control for UAVs, specifically using Model Predictive Path Integral (MPPI) methods. While it does not directly involve large language models, it falls within the broader domain of trajectory prediction and path planning, hence a moderate relevance score. The core focus is on improving trajectory tracking and collision avoidance for UAVs.", "keywords": ["trajectory prediction", "path planning", "UAV control", "collision avoidance", "Model Predictive Path Integral", "MPPI"]}}
{"id": "2510.12985", "pdf": "https://arxiv.org/pdf/2510.12985", "abs": "https://arxiv.org/abs/2510.12985", "authors": ["Simon Sinong Zhan", "Yao Liu", "Philip Wang", "Zinan Wang", "Qineng Wang", "Zhian Ruan", "Xiangyu Shi", "Xinyu Cao", "Frank Yang", "Kangrui Wang", "Huajie Shao", "Manling Li", "Qi Zhu"], "title": "SENTINEL: A Multi-Level Formal Framework for Safety Evaluation of LLM-based Embodied Agents", "categories": ["cs.AI"], "comment": null, "summary": "We present Sentinel, the first framework for formally evaluating the physical\nsafety of Large Language Model(LLM-based) embodied agents across the semantic,\nplan, and trajectory levels. Unlike prior methods that rely on heuristic rules\nor subjective LLM judgments, Sentinel grounds practical safety requirements in\nformal temporal logic (TL) semantics that can precisely specify state\ninvariants, temporal dependencies, and timing constraints. It then employs a\nmulti-level verification pipeline where (i) at the semantic level, intuitive\nnatural language safety requirements are formalized into TL formulas and the\nLLM agent's understanding of these requirements is probed for alignment with\nthe TL formulas; (ii) at the plan level, high-level action plans and subgoals\ngenerated by the LLM agent are verified against the TL formulas to detect\nunsafe plans before execution; and (iii) at the trajectory level, multiple\nexecution trajectories are merged into a computation tree and efficiently\nverified against physically-detailed TL specifications for a final safety\ncheck. We apply Sentinel in VirtualHome and ALFRED, and formally evaluate\nmultiple LLM-based embodied agents against diverse safety requirements. Our\nexperiments show that by grounding physical safety in temporal logic and\napplying verification methods across multiple levels, Sentinel provides a\nrigorous foundation for systematically evaluating LLM-based embodied agents in\nphysical environments, exposing safety violations overlooked by previous\nmethods and offering insights into their failure modes.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u5173\u6ce8LLM-based embodied agents\u7684\u5b89\u5168\u6027\u8bc4\u4f30\uff0c\u5176\u4e2d trajectory level \u7684\u9a8c\u8bc1\u90e8\u5206\u4e0e\u8f68\u8ff9\u76f8\u5173\uff0c\u540c\u65f6\u660e\u786e\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\uff0c\u4f46\u4e3b\u8981\u4fa7\u91cd\u4e8e\u5b89\u5168\u6027\u9a8c\u8bc1\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u3002", "keywords": ["Large Language Models", "LLM-based embodied agents", "trajectory level"]}}
{"id": "2510.12962", "pdf": "https://arxiv.org/pdf/2510.12962", "abs": "https://arxiv.org/abs/2510.12962", "authors": ["Michal Mina\u0159\u00edk", "Vojt\u011bch Von\u00e1sek", "Robert P\u011bni\u010dka"], "title": "Enhancing Sampling-based Planning with a Library of Paths", "categories": ["cs.RO"], "comment": null, "summary": "Path planning for 3D solid objects is a challenging problem, requiring a\nsearch in a six-dimensional configuration space, which is, nevertheless,\nessential in many robotic applications such as bin-picking and assembly. The\ncommonly used sampling-based planners, such as Rapidly-exploring Random Trees,\nstruggle with narrow passages where the sampling probability is low, increasing\nthe time needed to find a solution. In scenarios like robotic bin-picking,\nvarious objects must be transported through the same environment. However,\ntraditional planners start from scratch each time, losing valuable information\ngained during the planning process. We address this by using a library of past\nsolutions, allowing the reuse of previous experiences even when planning for a\nnew, previously unseen object. Paths for a set of objects are stored, and when\nplanning for a new object, we find the most similar one in the library and use\nits paths as approximate solutions, adjusting for possible mutual\ntransformations. The configuration space is then sampled along the approximate\npaths. Our method is tested in various narrow passage scenarios and compared\nwith state-of-the-art methods from the OMPL library. Results show significant\nspeed improvements (up to 85% decrease in the required time) of our method,\noften finding a solution in cases where the other planners fail. Our\nimplementation of the proposed method is released as an open-source package.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8def\u5f84\u89c4\u5212\uff0c\u7279\u522b\u662f\u9488\u5bf93D\u7269\u4f53\u7684\u91c7\u6837\u89c4\u5212\u3002\u867d\u7136\u6d89\u53ca\u5230\u8def\u5f84\u89c4\u5212\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u5e7f\u4e49\u8303\u7574\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u884c\u4eba\u6216\u8f66\u8f86\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4e5f\u6ca1\u6709\u6d89\u53ca\u5927\u8bed\u8a00\u6a21\u578b\u3002\u5173\u952e\u8bcd\u5982\u201cPath planning\u201d\u548c\u201csampling-based planners\u201d\u8868\u660e\u5176\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u4f46\u5173\u8054\u6027\u76f8\u5bf9\u8f83\u4f4e\u3002", "keywords": ["Path planning", "sampling-based planners"]}}
{"id": "2510.13029", "pdf": "https://arxiv.org/pdf/2510.13029", "abs": "https://arxiv.org/abs/2510.13029", "authors": ["Xinlei Wang", "Mingtian Tan", "Jing Qiu", "Junhua Zhao", "Jinjin Gu"], "title": "Toward Reasoning-Centric Time-Series Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Traditional time series analysis has long relied on pattern recognition,\ntrained on static and well-established benchmarks. However, in real-world\nsettings -- where policies shift, human behavior adapts, and unexpected events\nunfold -- effective analysis must go beyond surface-level trends to uncover the\nactual forces driving them. The recent rise of Large Language Models (LLMs)\npresents new opportunities for rethinking time series analysis by integrating\nmultimodal inputs. However, as the use of LLMs becomes popular, we must remain\ncautious, asking why we use LLMs and how to exploit them effectively. Most\nexisting LLM-based methods still employ their numerical regression ability and\nignore their deeper reasoning potential. This paper argues for rethinking time\nseries with LLMs as a reasoning task that prioritizes causal structure and\nexplainability. This shift brings time series analysis closer to human-aligned\nunderstanding, enabling transparent and context-aware insights in complex\nreal-world environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses time series analysis using Large Language Models (LLMs) for reasoning and explainability. While it doesn't directly address trajectory prediction, the use of LLMs in analyzing time-series data, especially in real-world settings with evolving behaviors, has potential relevance to trajectory prediction where understanding underlying patterns and reasoning about future movements is crucial.", "keywords": ["Large Language Models", "LLMs", "time series analysis", "reasoning", "explainability"]}}
{"id": "2510.13044", "pdf": "https://arxiv.org/pdf/2510.13044", "abs": "https://arxiv.org/abs/2510.13044", "authors": ["Jungbin Cho", "Minsu Kim", "Jisoo Kim", "Ce Zheng", "Laszlo A. Jeni", "Ming-Hsuan Yang", "Youngjae Yu", "Seonjoo Kim"], "title": "SceneAdapt: Scene-aware Adaptation of Human Motion Diffusion", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages", "summary": "Human motion is inherently diverse and semantically rich, while also shaped\nby the surrounding scene. However, existing motion generation approaches\naddress either motion semantics or scene-awareness in isolation, since\nconstructing large-scale datasets with both rich text--motion coverage and\nprecise scene interactions is extremely challenging. In this work, we introduce\nSceneAdapt, a framework that injects scene awareness into text-conditioned\nmotion models by leveraging disjoint scene--motion and text--motion datasets\nthrough two adaptation stages: inbetweening and scene-aware inbetweening. The\nkey idea is to use motion inbetweening, learnable without text, as a proxy task\nto bridge two distinct datasets and thereby inject scene-awareness to\ntext-to-motion models. In the first stage, we introduce keyframing layers that\nmodulate motion latents for inbetweening while preserving the latent manifold.\nIn the second stage, we add a scene-conditioning layer that injects scene\ngeometry by adaptively querying local context through cross-attention.\nExperimental results show that SceneAdapt effectively injects scene awareness\ninto text-to-motion models, and we further analyze the mechanisms through which\nthis awareness emerges. Code and models will be released.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on human motion generation conditioned on text and scene context. While it doesn't directly involve trajectory prediction in the sense of predicting future locations, it does deal with motion generation, which is related. It also touches upon text-to-motion models which are related to the use of large language models, even though the paper doesn't explicitly mention them. The use of cross-attention suggests a connection to transformer-based models which are often used in large language models.", "keywords": ["motion generation", "scene-awareness", "text-to-motion", "cross-attention"]}}
{"id": "2510.13215", "pdf": "https://arxiv.org/pdf/2510.13215", "abs": "https://arxiv.org/abs/2510.13215", "authors": ["Joy Jia Yin Lim", "Ye He", "Jifan Yu", "Xin Cong", "Daniel Zhang-Li", "Zhiyuan Liu", "Huiqin Liu", "Lei Hou", "Juanzi Li", "Bin Xu"], "title": "Personalized Learning Path Planning with Goal-Driven Learner State Modeling", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Personalized Learning Path Planning (PLPP) aims to design adaptive learning\npaths that align with individual goals. While large language models (LLMs) show\npotential in personalizing learning experiences, existing approaches often lack\nmechanisms for goal-aligned planning. We introduce Pxplore, a novel framework\nfor PLPP that integrates a reinforcement-based training paradigm and an\nLLM-driven educational architecture. We design a structured learner state model\nand an automated reward function that transforms abstract objectives into\ncomputable signals. We train the policy combining supervised fine-tuning (SFT)\nand Group Relative Policy Optimization (GRPO), and deploy it within a\nreal-world learning platform. Extensive experiments validate Pxplore's\neffectiveness in producing coherent, personalized, and goal-driven learning\npaths. We release our code and dataset to facilitate future research.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses personalized learning path planning, which shares conceptual similarities with trajectory prediction. More importantly, the abstract explicitly mentions the use of Large Language Models (LLMs) for personalizing learning experiences. Therefore, while not directly focused on trajectory prediction in the traditional sense, the combination of path planning and LLMs warrants a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "path planning", "personalized learning"]}}
{"id": "2510.13443", "pdf": "https://arxiv.org/pdf/2510.13443", "abs": "https://arxiv.org/abs/2510.13443", "authors": ["Mojtaba Mollahossein", "Gholamreza Vossoughi", "Mohammad Hossein Rohban"], "title": "Real-Time Knee Angle Prediction Using EMG and Kinematic Data with an Attention-Based CNN-LSTM Network and Transfer Learning Across Multiple Datasets", "categories": ["cs.RO"], "comment": null, "summary": "Electromyography (EMG) signals are widely used for predicting body joint\nangles through machine learning (ML) and deep learning (DL) methods. However,\nthese approaches often face challenges such as limited real-time applicability,\nnon-representative test conditions, and the need for large datasets to achieve\noptimal performance. This paper presents a transfer-learning framework for knee\njoint angle prediction that requires only a few gait cycles from new subjects.\nThree datasets - Georgia Tech, the University of California Irvine (UCI), and\nthe Sharif Mechatronic Lab Exoskeleton (SMLE) - containing four EMG channels\nrelevant to knee motion were utilized. A lightweight attention-based CNN-LSTM\nmodel was developed and pre-trained on the Georgia Tech dataset, then\ntransferred to the UCI and SMLE datasets. The proposed model achieved\nNormalized Mean Absolute Errors (NMAE) of 6.8 percent and 13.7 percent for\none-step and 50-step predictions on abnormal subjects using EMG inputs alone.\nIncorporating historical knee angles reduced the NMAE to 3.1 percent and 3.5\npercent for normal subjects, and to 2.8 percent and 7.5 percent for abnormal\nsubjects. When further adapted to the SMLE exoskeleton with EMG, kinematic, and\ninteraction force inputs, the model achieved 1.09 percent and 3.1 percent NMAE\nfor one- and 50-step predictions, respectively. These results demonstrate\nrobust performance and strong generalization for both short- and long-term\nrehabilitation scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on predicting knee joint angles, which can be seen as a form of trajectory prediction (specifically, the trajectory of the knee joint). It uses EMG and kinematic data and a CNN-LSTM network. While it doesn't directly involve large language models, the use of deep learning for prediction and the mention of transfer learning across multiple datasets suggest some relevance to the general field of prediction and modeling, pushing the relevance score slightly higher than a purely unrelated paper. The mention of long-term prediction also aligns with trajectory prediction concepts.", "keywords": ["trajectory prediction", "prediction", "CNN-LSTM", "transfer learning", "EMG", "kinematic data", "time-series prediction", "long-term prediction"]}}
{"id": "2510.13727", "pdf": "https://arxiv.org/pdf/2510.13727", "abs": "https://arxiv.org/abs/2510.13727", "authors": ["Ravi Pandya", "Madison Bland", "Duy P. Nguyen", "Changliu Liu", "Jaime Fern\u00e1ndez Fisac", "Andrea Bajcsy"], "title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails", "categories": ["cs.AI"], "comment": null, "summary": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses generative AI systems and their safety in practical settings like autonomous cars, which relates to trajectory prediction. It also focuses on using LLMs as agents and building guardrails for them. The connection to trajectory prediction is through the simulated driving experiments and the mention of autonomous cars, although the core focus is on the control and safety of LLMs.", "keywords": ["Large Language Models", "LLMs", "Generative AI", "autonomous cars", "simulated driving"]}}
{"id": "2510.13235", "pdf": "https://arxiv.org/pdf/2510.13235", "abs": "https://arxiv.org/abs/2510.13235", "authors": ["Yukuan Zhang", "Jiarui Zhao", "Shangqing Nie", "Jin Kuang", "Shengsheng Wang"], "title": "EPIPTrack: Rethinking Prompt Modeling with Explicit and Implicit Prompts for Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal semantic cues, such as textual descriptions, have shown strong\npotential in enhancing target perception for tracking. However, existing\nmethods rely on static textual descriptions from large language models, which\nlack adaptability to real-time target state changes and prone to\nhallucinations. To address these challenges, we propose a unified multimodal\nvision-language tracking framework, named EPIPTrack, which leverages explicit\nand implicit prompts for dynamic target modeling and semantic alignment.\nSpecifically, explicit prompts transform spatial motion information into\nnatural language descriptions to provide spatiotemporal guidance. Implicit\nprompts combine pseudo-words with learnable descriptors to construct\nindividualized knowledge representations capturing appearance attributes. Both\nprompts undergo dynamic adjustment via the CLIP text encoder to respond to\nchanges in target state. Furthermore, we design a Discriminative Feature\nAugmentor to enhance visual and cross-modal representations. Extensive\nexperiments on MOT17, MOT20, and DanceTrack demonstrate that EPIPTrack\noutperforms existing trackers in diverse scenarios, exhibiting robust\nadaptability and superior performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u76ee\u6807\u8ddf\u8e2a\uff08Multi-Object Tracking\uff09\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6587\u672c\u63cf\u8ff0\u4f5c\u4e3a\u63d0\u793a\u4fe1\u606f\u6765\u589e\u5f3a\u76ee\u6807\u611f\u77e5\u3002\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4e0e\u8fd0\u52a8\u4fe1\u606f\u76f8\u5173\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u5927\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Multi-Object Tracking", "large language models", "prompt modeling", "motion information"]}}
{"id": "2510.13778", "pdf": "https://arxiv.org/pdf/2510.13778", "abs": "https://arxiv.org/abs/2510.13778", "authors": ["Xinyi Chen", "Yilun Chen", "Yanwei Fu", "Ning Gao", "Jiaya Jia", "Weiyang Jin", "Hao Li", "Yao Mu", "Jiangmiao Pang", "Yu Qiao", "Yang Tian", "Bin Wang", "Bolun Wang", "Fangjing Wang", "Hanqing Wang", "Tai Wang", "Ziqin Wang", "Xueyuan Wei", "Chao Wu", "Shuai Yang", "Jinhui Ye", "Junqiu Yu", "Jia Zeng", "Jingjing Zhang", "Jinyu Zhang", "Shi Zhang", "Feng Zheng", "Bowen Zhou", "Yangkun Zhu"], "title": "InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Technical report", "summary": "We introduce InternVLA-M1, a unified framework for spatial grounding and\nrobot control that advances instruction-following robots toward scalable,\ngeneral-purpose intelligence. Its core idea is spatially guided\nvision-language-action training, where spatial grounding serves as the critical\nlink between instructions and robot actions. InternVLA-M1 employs a two-stage\npipeline: (i) spatial grounding pre-training on over 2.3M spatial reasoning\ndata to determine ``where to act'' by aligning instructions with visual,\nembodiment-agnostic positions, and (ii) spatially guided action post-training\nto decide ``how to act'' by generating embodiment-aware actions through\nplug-and-play spatial prompting. This spatially guided training recipe yields\nconsistent gains: InternVLA-M1 outperforms its variant without spatial guidance\nby +14.6% on SimplerEnv Google Robot, +17% on WidowX, and +4.3% on LIBERO\nFranka, while demonstrating stronger spatial reasoning capability in box,\npoint, and trace prediction. To further scale instruction following, we built a\nsimulation engine to collect 244K generalizable pick-and-place episodes,\nenabling a 6.2% average improvement across 200 tasks and 3K+ objects. In\nreal-world clustered pick-and-place, InternVLA-M1 improved by 7.3%, and with\nsynthetic co-training, achieved +20.6% on unseen objects and novel\nconfigurations. Moreover, in long-horizon reasoning-intensive scenarios, it\nsurpassed existing works by over 10%. These results highlight spatially guided\ntraining as a unifying principle for scalable and resilient generalist robots.\nCode and models are available at\nhttps://github.com/InternRobotics/InternVLA-M1.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4f7f\u7528\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u5e76\u6d89\u53ca\u7a7a\u95f4\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u4e2d\u5173\u4e8e\u7a7a\u95f4\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\u7684\u63cf\u8ff0\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u5173\u8054\u3002\u540c\u65f6\uff0c\u8be5\u8bba\u6587\u6d89\u53ca\u5927\u89c4\u6a21\u6570\u636e\u8bad\u7ec3\uff0c\u6697\u793a\u53ef\u80fd\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\u3002", "keywords": ["vision-language-action", "robot control", "spatial reasoning", "action generation", "large-scale training", "foundation models"]}}
{"id": "2510.13114", "pdf": "https://arxiv.org/pdf/2510.13114", "abs": "https://arxiv.org/abs/2510.13114", "authors": ["Zhuoyuan Wang", "Tongyao Jia", "Pharuj Rajborirug", "Neeraj Ramesh", "Hiroyuki Okuda", "Tatsuya Suzuki", "Soummya Kar", "Yorie Nakahira"], "title": "Safe Driving in Occluded Environments", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Ensuring safe autonomous driving in the presence of occlusions poses a\nsignificant challenge in its policy design. While existing model-driven control\ntechniques based on set invariance can handle visible risks, occlusions create\nlatent risks in which safety-critical states are not observable. Data-driven\ntechniques also struggle to handle latent risks because direct mappings from\nrisk-critical objects in sensor inputs to safe actions cannot be learned\nwithout visible risk-critical objects. Motivated by these challenges, in this\npaper, we propose a probabilistic safety certificate for latent risk. Our key\ntechnical enabler is the application of probabilistic invariance: It relaxes\nthe strict observability requirements imposed by set-invariance methods that\ndemand the knowledge of risk-critical states. The proposed techniques provide\nlinear action constraints that confine the latent risk probability within\ntolerance. Such constraints can be integrated into model predictive controllers\nor embedded in data-driven policies to mitigate latent risks. The proposed\nmethod is tested using the CARLA simulator and compared with a few existing\ntechniques. The theoretical and empirical analysis jointly demonstrate that the\nproposed methods assure long-term safety in real-time control in occluded\nenvironments without being overly conservative and with transparency to exposed\nrisks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on safe autonomous driving in occluded environments, which involves predicting the behavior of other agents and planning a safe trajectory. While it doesn't explicitly mention large language models, the problem of handling uncertainty and latent risks could potentially benefit from LLMs for learning more robust representations or generating plausible scenarios. The core focus is trajectory planning and risk assessment, making it relevant to trajectory prediction, but lacking direct connection to large language models.", "keywords": ["autonomous driving", "trajectory planning", "risk assessment", "occluded environments", "model predictive control"]}}
{"id": "2510.13461", "pdf": "https://arxiv.org/pdf/2510.13461", "abs": "https://arxiv.org/abs/2510.13461", "authors": ["Yangye Jiang", "Jiachen Wang", "Daofei Li"], "title": "Physics-Informed Neural Network Modeling of Vehicle Collision Dynamics in Precision Immobilization Technique Maneuvers", "categories": ["eess.SY", "cs.RO", "cs.SY"], "comment": null, "summary": "Accurate prediction of vehicle collision dynamics is crucial for advanced\nsafety systems and post-impact control applications, yet existing methods face\ninherent trade-offs among computational efficiency, prediction accuracy, and\ndata requirements. This paper proposes a dual Physics-Informed Neural Network\nframework addressing these challenges through two complementary networks. The\nfirst network integrates Gaussian Mixture Models with PINN architecture to\nlearn impact force distributions from finite element analysis data while\nenforcing momentum conservation and energy consistency constraints. The second\nnetwork employs an adaptive PINN with dynamic constraint weighting to predict\npost-collision vehicle dynamics, featuring an adaptive physics guard layer that\nprevents unrealistic predictions whil e preserving data-driven learning\ncapabilities. The framework incorporates uncertainty quantification through\ntime-varying parameters and enables rapid adaptation via fine-tuning\nstrategies. Validation demonstrates significant improvements: the impact force\nmodel achieves relative errors below 15.0% for force prediction on finite\nelement analysis (FEA) datasets, while the vehicle dynamics model reduces\naverage trajectory prediction error by 63.6% compared to traditional\nfour-degree-of-freedom models in scaled vehicle experiments. The integrated\nsystem maintains millisecond-level computational efficiency suitable for\nreal-time applications while providing probabilistic confidence bounds\nessential for safety-critical control. Comprehensive validation through FEA\nsimulation, dynamic modeling, and scaled vehicle experiments confirms the\nframework's effectiveness for Precision Immobilization Technique scenarios and\ngeneral collision dynamics prediction.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u78b0\u649e\u52a8\u529b\u5b66\u7684\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86Physics-Informed Neural Networks (PINNs)\u3002\u867d\u7136\u6d89\u53ca\u795e\u7ecf\u7f51\u7edc\u548c\u8f66\u8f86\u52a8\u529b\u5b66\u9884\u6d4b\uff0c\u4f46\u4e0e\u4f20\u7edf\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u548c\u5927\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002PINN\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u79cd\u7279\u5b9a\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u4f46\u4e0e\u5927\u6a21\u578b\u901a\u5e38\u6307\u7684LLM\u5728\u89c4\u6a21\u548c\u5e94\u7528\u573a\u666f\u4e0a\u5dee\u5f02\u5f88\u5927\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u504f\u4f4e\u3002", "keywords": ["vehicle dynamics", "collision dynamics", "trajectory prediction", "Physics-Informed Neural Networks", "PINN"]}}
