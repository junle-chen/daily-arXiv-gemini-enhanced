{"id": "2511.17053", "pdf": "https://arxiv.org/pdf/2511.17053", "abs": "https://arxiv.org/abs/2511.17053", "authors": ["Teng Fu", "Mengyang Zhao", "Ke Niu", "Kaixin Peng", "Bin Li"], "title": "OmniPT: Unleashing the Potential of Large Vision Language Models for Pedestrian Tracking and Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "AAAI 2026", "summary": "LVLMs have been shown to perform excellently in image-level tasks such as VQA and caption. However, in many instance-level tasks, such as visual grounding and object detection, LVLMs still show performance gaps compared to previous expert models. Meanwhile, although pedestrian tracking is a classical task, there have been a number of new topics in combining object tracking and natural language, such as Referring MOT, Cross-view Referring MOT, and Semantic MOT. These tasks emphasize that models should understand the tracked object at an advanced semantic level, which is exactly where LVLMs excel. In this paper, we propose a new unified Pedestrian Tracking framework, namely OmniPT, which can track, track based on reference and generate semantic understanding of tracked objects interactively. We address two issues: how to model the tracking task into a task that foundation models can perform, and how to make the model output formatted answers. To this end, we implement a training phase consisting of RL-Mid Training-SFT-RL. Based on the pre-trained weights of the LVLM, we first perform a simple RL phase to enable the model to output fixed and supervisable bounding box format. Subsequently, we conduct a mid-training phase using a large number of pedestrian-related datasets. Finally, we perform supervised fine-tuning on several pedestrian tracking datasets, and then carry out another RL phase to improve the model's tracking performance and enhance its ability to follow instructions. We conduct experiments on tracking benchmarks and the experimental results demonstrate that the proposed method can perform better than the previous methods.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u8868\u660e\u5176\u4f7f\u7528\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u8fdb\u884c\u884c\u4eba\u8ddf\u8e2a\uff0c\u8fd9\u7ed3\u5408\u4e86\u8f68\u8ff9\u9884\u6d4b\uff08\u884c\u4eba\u8ddf\u8e2a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e24\u4e2a\u4e3b\u9898\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aOmniPT\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5229\u7528LVLM\u7684\u6f5c\u529b\u6765\u7406\u89e3\u548c\u8ddf\u8e2a\u884c\u4eba\u3002\u867d\u7136\u91cd\u70b9\u662f\u8ddf\u8e2a\u800c\u975e\u9884\u6d4b\uff0c\u4f46\u8ddf\u8e2a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u57fa\u7840\u3002", "keywords": ["Large Vision Language Models", "LVLM", "Pedestrian Tracking", "Tracking", "foundation models", "Referring MOT", "Cross-view Referring MOT", "Semantic MOT"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u884c\u4eba\u8ddf\u8e2a\u6846\u67b6OmniPT\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4ea4\u4e92\u5f0f\u5730\u8ddf\u8e2a\u3001\u57fa\u4e8e\u53c2\u8003\u8fdb\u884c\u8ddf\u8e2a\u5e76\u751f\u6210\u88ab\u8ddf\u8e2a\u5bf9\u8c61\u7684\u8bed\u4e49\u7406\u89e3\u3002", "motivation": "\u73b0\u6709LVLM\u5728\u5b9e\u4f8b\u7ea7\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u5b9a\u4f4d\u548c\u76ee\u6807\u68c0\u6d4b\uff09\u4e2d\u8868\u73b0\u4e0d\u5982\u4e13\u5bb6\u6a21\u578b\uff1b\u540c\u65f6\uff0c\u884c\u4eba\u8ddf\u8e2a\u9886\u57df\u6d8c\u73b0\u4e86\u7ed3\u5408\u5bf9\u8c61\u8ddf\u8e2a\u548c\u81ea\u7136\u8bed\u8a00\u7684\u65b0\u8bfe\u9898\uff0c\u5f3a\u8c03\u6a21\u578b\u5bf9\u88ab\u8ddf\u8e2a\u5bf9\u8c61\u7684\u9ad8\u7ea7\u8bed\u4e49\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5305\u542bRL-Mid Training-SFT-RL\u7684\u8bad\u7ec3\u9636\u6bb5\u3002\u9996\u5148\uff0c\u6267\u884c\u4e00\u4e2a\u7b80\u5355\u7684RL\u9636\u6bb5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8f93\u51fa\u56fa\u5b9a\u7684\u3001\u53ef\u76d1\u7763\u7684\u8fb9\u754c\u6846\u683c\u5f0f\u3002\u5176\u6b21\uff0c\u4f7f\u7528\u5927\u91cf\u7684\u884c\u4eba\u76f8\u5173\u6570\u636e\u96c6\u8fdb\u884c\u4e2d\u95f4\u8bad\u7ec3\u9636\u6bb5\u3002\u6700\u540e\uff0c\u5728\u51e0\u4e2a\u884c\u4eba\u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u8fdb\u884c\u53e6\u4e00\u4e2aRL\u9636\u6bb5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u8ddf\u8e2a\u6027\u80fd\u5e76\u589e\u5f3a\u5176\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\u3002", "result": "\u5728\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684OmniPT\u6846\u67b6\u5728\u884c\u4eba\u8ddf\u8e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u5730\u7ed3\u5408\u89c6\u89c9\u4fe1\u606f\u548c\u8bed\u4e49\u7406\u89e3\u3002", "summary_zh": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u56fe\u50cf\u7ea7\u522b\u7684\u4efb\u52a1\uff08\u5982\u89c6\u89c9\u95ee\u7b54\u548c\u56fe\u50cf\u63cf\u8ff0\uff09\u4e2d\u8868\u73b0\u51fa\u8272\u3002\u7136\u800c\uff0c\u5728\u8bb8\u591a\u5b9e\u4f8b\u7ea7\u522b\u7684\u4efb\u52a1\u4e2d\uff0c\u4f8b\u5982\u89c6\u89c9\u5b9a\u4f4d\u548c\u76ee\u6807\u68c0\u6d4b\uff0c\u4e0e\u4e4b\u524d\u7684\u4e13\u5bb6\u6a21\u578b\u76f8\u6bd4\uff0cLVLM\u4ecd\u7136\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002\u540c\u65f6\uff0c\u867d\u7136\u884c\u4eba\u8ddf\u8e2a\u662f\u4e00\u4e2a\u7ecf\u5178\u7684\u4efb\u52a1\uff0c\u4f46\u5728\u7ed3\u5408\u5bf9\u8c61\u8ddf\u8e2a\u548c\u81ea\u7136\u8bed\u8a00\u65b9\u9762\u51fa\u73b0\u4e86\u4e00\u4e9b\u65b0\u7684\u8bfe\u9898\uff0c\u4f8b\u5982Referring MOT\u3001Cross-view Referring MOT\u548cSemantic MOT\u3002\u8fd9\u4e9b\u4efb\u52a1\u5f3a\u8c03\u6a21\u578b\u5e94\u8be5\u5728\u9ad8\u7ea7\u8bed\u4e49\u5c42\u9762\u7406\u89e3\u88ab\u8ddf\u8e2a\u7684\u5bf9\u8c61\uff0c\u800c\u8fd9\u6b63\u662fLVLM\u6240\u64c5\u957f\u7684\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7edf\u4e00\u7684\u884c\u4eba\u8ddf\u8e2a\u6846\u67b6\uff0c\u5373OmniPT\uff0c\u5b83\u53ef\u4ee5\u4ea4\u4e92\u5f0f\u5730\u8ddf\u8e2a\u3001\u57fa\u4e8e\u53c2\u8003\u8fdb\u884c\u8ddf\u8e2a\uff0c\u5e76\u751f\u6210\u88ab\u8ddf\u8e2a\u5bf9\u8c61\u7684\u8bed\u4e49\u7406\u89e3\u3002\u6211\u4eec\u89e3\u51b3\u4e86\u4e24\u4e2a\u95ee\u9898\uff1a\u5982\u4f55\u5c06\u8ddf\u8e2a\u4efb\u52a1\u5efa\u6a21\u6210\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u6267\u884c\u7684\u4efb\u52a1\uff0c\u4ee5\u53ca\u5982\u4f55\u4f7f\u6a21\u578b\u8f93\u51fa\u683c\u5f0f\u5316\u7684\u7b54\u6848\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5305\u542bRL-Mid Training-SFT-RL\u7684\u8bad\u7ec3\u9636\u6bb5\u3002\u57fa\u4e8eLVLM\u7684\u9884\u8bad\u7ec3\u6743\u91cd\uff0c\u6211\u4eec\u9996\u5148\u6267\u884c\u4e00\u4e2a\u7b80\u5355\u7684RL\u9636\u6bb5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u8f93\u51fa\u56fa\u5b9a\u4e14\u53ef\u76d1\u7763\u7684\u8fb9\u754c\u6846\u683c\u5f0f\u3002\u968f\u540e\uff0c\u6211\u4eec\u4f7f\u7528\u5927\u91cf\u7684\u884c\u4eba\u76f8\u5173\u6570\u636e\u96c6\u8fdb\u884c\u4e2d\u95f4\u8bad\u7ec3\u9636\u6bb5\u3002\u6700\u540e\uff0c\u6211\u4eec\u5728\u51e0\u4e2a\u884c\u4eba\u8ddf\u8e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u7136\u540e\u8fdb\u884c\u53e6\u4e00\u4e2aRL\u9636\u6bb5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u8ddf\u8e2a\u6027\u80fd\u5e76\u589e\u5f3a\u5176\u9075\u5faa\u6307\u4ee4\u7684\u80fd\u529b\u3002\u6211\u4eec\u5728\u8ddf\u8e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53ef\u4ee5\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2511.17150", "pdf": "https://arxiv.org/pdf/2511.17150", "abs": "https://arxiv.org/abs/2511.17150", "authors": ["Liuhan Yin", "Runkun Ju", "Guodong Guo", "Erkang Cheng"], "title": "DiffRefiner: Coarse to Fine Trajectory Planning via Diffusion Refinement with Semantic Interaction for End to End Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted to AAAI 2026", "summary": "Unlike discriminative approaches in autonomous driving that predict a fixed set of candidate trajectories of the ego vehicle, generative methods, such as diffusion models, learn the underlying distribution of future motion, enabling more flexible trajectory prediction. However, since these methods typically rely on denoising human-crafted trajectory anchors or random noise, there remains significant room for improvement. In this paper, we propose DiffRefiner, a novel two-stage trajectory prediction framework. The first stage uses a transformer-based Proposal Decoder to generate coarse trajectory predictions by regressing from sensor inputs using predefined trajectory anchors. The second stage applies a Diffusion Refiner that iteratively denoises and refines these initial predictions. In this way, we enhance the performance of diffusion-based planning by incorporating a discriminative trajectory proposal module, which provides strong guidance for the generative refinement process. Furthermore, we design a fine-grained denoising decoder to enhance scene compliance, enabling more accurate trajectory prediction through enhanced alignment with the surrounding environment. Experimental results demonstrate that DiffRefiner achieves state-of-the-art performance, attaining 87.4 EPDMS on NAVSIM v2, and 87.1 DS along with 71.4 SR on Bench2Drive, thereby setting new records on both public benchmarks. The effectiveness of each component is validated via ablation studies as well.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper focuses on trajectory prediction for autonomous driving using a diffusion model. While it doesn't directly utilize large language models, it leverages a generative approach (diffusion models) for trajectory planning, making it relevant to the trajectory prediction aspect. The use of a transformer-based decoder further strengthens the connection to modern deep learning techniques often associated with large models.", "keywords": ["trajectory prediction", "autonomous driving", "diffusion model", "trajectory planning", "transformer"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17013", "pdf": "https://arxiv.org/pdf/2511.17013", "abs": "https://arxiv.org/abs/2511.17013", "authors": ["Yiwen Ying", "Hanjing Ye", "Senzi Luo", "Luyao Liu", "Yu Zhan", "Li He", "Hong Zhang"], "title": "MfNeuPAN: Proactive End-to-End Navigation in Dynamic Environments via Direct Multi-Frame Point Constraints", "categories": ["cs.RO"], "comment": "6 pages, 9 figures, accepted at IEEE ROBIO 2025", "summary": "Obstacle avoidance in complex and dynamic environments is a critical challenge for real-time robot navigation. Model-based and learning-based methods often fail in highly dynamic scenarios because traditional methods assume a static environment and cannot adapt to real-time changes, while learning-based methods rely on single-frame observations for motion constraint estimation, limiting their adaptability. To overcome these limitations, this paper proposes a novel framework that leverages multi-frame point constraints, including current and future frames predicted by a dedicated module, to enable proactive end-to-end navigation. By incorporating a prediction module that forecasts the future path of moving obstacles based on multi-frame observations, our method allows the robot to proactively anticipate and avoid potential dangers. This proactive planning capability significantly enhances navigation robustness and efficiency in unknown dynamic environments. Simulations and real-world experiments validate the effectiveness of our approach.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on proactive robot navigation in dynamic environments using multi-frame point constraints and a prediction module for forecasting obstacle paths. While it doesn't directly involve large language models, the use of a prediction module for future trajectory estimation aligns with the theme of trajectory prediction. The connection to LLMs is weak but potentially there are ways that future path prediction can be improved with LLMs.", "keywords": ["trajectory prediction", "navigation", "obstacle avoidance", "dynamic environments", "motion prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17225", "pdf": "https://arxiv.org/pdf/2511.17225", "abs": "https://arxiv.org/abs/2511.17225", "authors": ["Shanshan Li", "Da Huang", "Yu He", "Yanwei Fu", "Yu-Gang Jiang", "Xiangyang Xue"], "title": "TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Accepted at NeurIPS 2025", "summary": "In daily life, people often move through spaces to find objects that meet their needs, posing a key challenge in embodied AI. Traditional Demand-Driven Navigation (DDN) handles one need at a time but does not reflect the complexity of real-world tasks involving multiple needs and personal choices. To bridge this gap, we introduce Task-Preferenced Multi-Demand-Driven Navigation (TP-MDDN), a new benchmark for long-horizon navigation involving multiple sub-demands with explicit task preferences. To solve TP-MDDN, we propose AWMSystem, an autonomous decision-making system composed of three key modules: BreakLLM (instruction decomposition), LocateLLM (goal selection), and StatusMLLM (task monitoring). For spatial memory, we design MASMap, which combines 3D point cloud accumulation with 2D semantic mapping for accurate and efficient environmental understanding. Our Dual-Tempo action generation framework integrates zero-shot planning with policy-based fine control, and is further supported by an Adaptive Error Corrector that handles failure cases in real time. Experiments demonstrate that our approach outperforms state-of-the-art baselines in both perception accuracy and navigation robustness.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6d89\u53ca\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u5bfc\u822a\u4efb\u52a1\uff0c\u5176\u4e2d\u5bfc\u822a\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\u9884\u6d4b\u7684\u5f62\u5f0f\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u4e86\u4f7f\u7528LLM\u8fdb\u884c\u6307\u4ee4\u5206\u89e3\u3001\u76ee\u6807\u9009\u62e9\u548c\u4efb\u52a1\u76d1\u63a7\uff0c\u5e76\u7ed3\u5408\u4e86\u7a7a\u95f4\u8bb0\u5fc6\u548c\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u5bfc\u822a\u800c\u975e\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4f7f\u7528\u4e86LLM\u5e76\u4e0e\u79fb\u52a8\u8def\u5f84\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["navigation", "LLM", "Large Language Models", "autonomous decision-making", "instruction decomposition", "goal selection", "task monitoring"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17384", "pdf": "https://arxiv.org/pdf/2511.17384", "abs": "https://arxiv.org/abs/2511.17384", "authors": ["Yifan Li", "Lichi Li", "Anh Dao", "Xinyu Zhou", "Yicheng Qiao", "Zheda Mai", "Daeun Lee", "Zichen Chen", "Zhen Tan", "Mohit Bansal", "Yu Kong"], "title": "IndustryNav: Exploring Spatial Reasoning of Embodied Agents in Dynamic Industrial Navigation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "While Visual Large Language Models (VLLMs) show great promise as embodied agents, they continue to face substantial challenges in spatial reasoning. Existing embodied benchmarks largely focus on passive, static household environments and evaluate only isolated capabilities, failing to capture holistic performance in dynamic, real-world complexity. To fill this gap, we present IndustryNav, the first dynamic industrial navigation benchmark for active spatial reasoning. IndustryNav leverages 12 manually created, high-fidelity Unity warehouse scenarios featuring dynamic objects and human movement. Our evaluation employs a PointGoal navigation pipeline that effectively combines egocentric vision with global odometry to assess holistic local-global planning. Crucially, we introduce the \"collision rate\" and \"warning rate\" metrics to measure safety-oriented behaviors and distance estimation. A comprehensive study of nine state-of-the-art VLLMs (including models such as GPT-5-mini, Claude-4.5, and Gemini-2.5) reveals that closed-source models maintain a consistent advantage; however, all agents exhibit notable deficiencies in robust path planning, collision avoidance and active exploration. This highlights a critical need for embodied research to move beyond passive perception and toward tasks that demand stable planning, active exploration, and safe behavior in dynamic, real-world environment.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper discusses the spatial reasoning capabilities of Visual Large Language Models (VLLMs) in a dynamic industrial navigation setting. It evaluates VLLMs on a new benchmark, IndustryNav, which involves navigating dynamic environments with moving objects and humans. While the paper doesn't directly focus on trajectory prediction algorithms, the navigation task inherently requires predicting future states and planning trajectories, especially for collision avoidance. The use of VLLMs and the focus on dynamic environments contribute to its relevance.", "keywords": ["Large Language Models", "VLLMs", "embodied agents", "spatial reasoning", "dynamic navigation", "collision avoidance", "path planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17496", "pdf": "https://arxiv.org/pdf/2511.17496", "abs": "https://arxiv.org/abs/2511.17496", "authors": ["Zhiyu Huang", "Zewei Zhou", "Tianhui Cai", "Yun Zhang", "Jiaqi Ma"], "title": "MDG: Masked Denoising Generation for Multi-Agent Behavior Modeling in Traffic Environments", "categories": ["cs.RO", "cs.MA"], "comment": null, "summary": "Modeling realistic and interactive multi-agent behavior is critical to autonomous driving and traffic simulation. However, existing diffusion and autoregressive approaches are limited by iterative sampling, sequential decoding, or task-specific designs, which hinder efficiency and reuse. We propose Masked Denoising Generation (MDG), a unified generative framework that reformulates multi-agent behavior modeling as the reconstruction of independently noised spatiotemporal tensors. Instead of relying on diffusion time steps or discrete tokenization, MDG applies continuous, per-agent and per-timestep noise masks that enable localized denoising and controllable trajectory generation in a single or few forward passes. This mask-driven formulation generalizes across open-loop prediction, closed-loop simulation, motion planning, and conditional generation within one model. Trained on large-scale real-world driving datasets, MDG achieves competitive closed-loop performance on the Waymo Sim Agents and nuPlan Planning benchmarks, while providing efficient, consistent, and controllable open-loop multi-agent trajectory generation. These results position MDG as a simple yet versatile paradigm for multi-agent behavior modeling.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u5efa\u6a21\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMDG\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u884c\u4e3a\u6a21\u62df\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\uff0c\u4e14\u63d0\u51fa\u7684\u751f\u6210\u6a21\u578b\u5177\u6709\u4e00\u5b9a\u7684\u901a\u7528\u6027\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u4e3a\u4ea4\u901a\u573a\u666f\u8bbe\u8ba1\u7684\u7279\u5b9a\u9886\u57df\u6a21\u578b\u3002", "keywords": ["trajectory prediction", "multi-agent behavior modeling", "traffic environments", "autonomous driving", "motion planning", "generative framework"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17045", "pdf": "https://arxiv.org/pdf/2511.17045", "abs": "https://arxiv.org/abs/2511.17045", "authors": ["Linfeng Dong", "Yuchen Yang", "Hao Wu", "Wei Wang", "Yuenan HouZhihang Zhong", "Xiao Sun"], "title": "RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted to AAAI 2026 (Oral)", "summary": "We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper is relevant to trajectory prediction as it focuses on predictive ball trajectory forecasting. While it doesn't directly involve Large Language Models, it explores trajectory prediction within the context of sports analytics and proposes a novel dataset and benchmark for this task. The keywords 'trajectory prediction' and 'motion forecasting' indicate a strong connection to the topic.", "keywords": ["trajectory prediction", "motion forecasting", "ball tracking"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16929", "pdf": "https://arxiv.org/pdf/2511.16929", "abs": "https://arxiv.org/abs/2511.16929", "authors": ["Rui Xue", "Dan He", "Fengmei Jin", "Chen Zhang", "Xiaofang Zhou"], "title": "CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection", "categories": ["cs.LG", "cs.DB"], "comment": "18 pages, 4 figures, will be submitted to VLDBJ", "summary": "Detecting trajectory anomalies is a vital task in modern Intelligent Transportation Systems (ITS), enabling the identification of unsafe, inefficient, or irregular travel behaviours. While deep learning has emerged as the dominant approach, several key challenges remain unresolved. First, sub-trajectory anomaly detection, capable of pinpointing the precise segments where anomalies occur, remains underexplored compared to whole-trajectory analysis. Second, many existing methods depend on carefully tuned thresholds, limiting their adaptability in real-world applications. Moreover, the irregular sampling of trajectory data and the presence of noise in training sets further degrade model performance, making it difficult to learn reliable representations of normal routes. To address these challenges, we propose a contrastive reinforcement learning framework for online trajectory anomaly detection, CroTad. Our method is threshold-free and robust to noisy, irregularly sampled data. By incorporating contrastive learning, CroTad learns to extract diverse normal travel patterns for different itineraries and effectively distinguish anomalous behaviours at both sub-trajectory and point levels. The detection module leverages deep reinforcement learning to perform online, real-time anomaly scoring, enabling timely and fine-grained identification of abnormal segments. Extensive experiments on two real-world datasets demonstrate the effectiveness and robustness of our framework across various evaluation scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory anomaly detection, which is related to trajectory prediction. While it doesn't directly use or discuss large language models, the underlying techniques could potentially be combined with LLMs for enhanced anomaly detection or prediction capabilities. The paper utilizes deep reinforcement learning, which is a relevant area in the broader context of AI and trajectory analysis.", "keywords": ["trajectory anomaly detection", "trajectory", "reinforcement learning", "Intelligent Transportation Systems", "deep learning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16916", "pdf": "https://arxiv.org/pdf/2511.16916", "abs": "https://arxiv.org/abs/2511.16916", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "title": "Hybrid Differential Reward: Combining Temporal Difference and Action Gradients for Efficient Multi-Agent Reinforcement Learning in Cooperative Driving", "categories": ["cs.AI"], "comment": null, "summary": "In multi-vehicle cooperative driving tasks involving high-frequency continuous control, traditional state-based reward functions suffer from the issue of vanishing reward differences. This phenomenon results in a low signal-to-noise ratio (SNR) for policy gradients, significantly hindering algorithm convergence and performance improvement. To address this challenge, this paper proposes a novel Hybrid Differential Reward (HDR) mechanism. We first theoretically elucidate how the temporal quasi-steady nature of traffic states and the physical proximity of actions lead to the failure of traditional reward signals. Building on this analysis, the HDR framework innovatively integrates two complementary components: (1) a Temporal Difference Reward (TRD) based on a global potential function, which utilizes the evolutionary trend of potential energy to ensure optimal policy invariance and consistency with long-term objectives; and (2) an Action Gradient Reward (ARG), which directly measures the marginal utility of actions to provide a local guidance signal with a high SNR. Furthermore, we formulate the cooperative driving problem as a Multi-Agent Partially Observable Markov Game (POMDPG) with a time-varying agent set and provide a complete instantiation scheme for HDR within this framework. Extensive experiments conducted using both online planning (MCTS) and Multi-Agent Reinforcement Learning (QMIX, MAPPO, MADDPG) algorithms demonstrate that the HDR mechanism significantly improves convergence speed and policy stability. The results confirm that HDR guides agents to learn high-quality cooperative policies that effectively balance traffic efficiency and safety.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u534f\u540c\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\uff0c\u6d89\u53ca\u8f66\u8f86\u8f68\u8ff9\u7684\u89c4\u5212\u548c\u63a7\u5236\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u9884\u6d4b\u548c\u51b3\u7b56\uff0c\u4e0e\u5229\u7528\u5927\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u5177\u6709\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002", "keywords": ["multi-agent reinforcement learning", "cooperative driving", "trajectory planning", "action gradients", "policy gradients"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16949", "pdf": "https://arxiv.org/pdf/2511.16949", "abs": "https://arxiv.org/abs/2511.16949", "authors": ["Junseo Kim", "Guido Dumont", "Xinyu Gao", "Gang Chen", "Holger Caesar", "Javier Alonso-Mora"], "title": "MobileOcc: A Human-Aware Semantic Occupancy Dataset for Mobile Robots", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Dense 3D semantic occupancy perception is critical for mobile robots operating in pedestrian-rich environments, yet it remains underexplored compared to its application in autonomous driving. To address this gap, we present MobileOcc, a semantic occupancy dataset for mobile robots operating in crowded human environments. Our dataset is built using an annotation pipeline that incorporates static object occupancy annotations and a novel mesh optimization framework explicitly designed for human occupancy modeling. It reconstructs deformable human geometry from 2D images and subsequently refines and optimizes it using associated LiDAR point data. Using MobileOcc, we establish benchmarks for two tasks, i) Occupancy prediction and ii) Pedestrian velocity prediction, using different methods including monocular, stereo, and panoptic occupancy, with metrics and baseline implementations for reproducible comparison. Beyond occupancy prediction, we further assess our annotation method on 3D human pose estimation datasets. Results demonstrate that our method exhibits robust performance across different datasets.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on semantic occupancy prediction and pedestrian velocity prediction, which are related to trajectory prediction. While it doesn't directly involve large language models, it establishes benchmarks for pedestrian velocity prediction, a sub-area of trajectory prediction. The inclusion of human occupancy modeling further strengthens its relevance to human-aware trajectory prediction, even though LLMs are not used.", "keywords": ["pedestrian velocity prediction", "occupancy prediction", "human occupancy modeling"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16825", "pdf": "https://arxiv.org/pdf/2511.16825", "abs": "https://arxiv.org/abs/2511.16825", "authors": ["Dilin Wang", "Hyunyoung Jung", "Tom Monnier", "Kihyuk Sohn", "Chuhang Zou", "Xiaoyu Xiang", "Yu-Ying Yeh", "Di Liu", "Zixuan Huang", "Thu Nguyen-Phuoc", "Yuchen Fan", "Sergiu Oprea", "Ziyan Wang", "Roman Shapovalov", "Nikolaos Sarafianos", "Thibault Groueix", "Antoine Toisoul", "Prithviraj Dhar", "Xiao Chu", "Minghao Chen", "Geon Yeong Park", "Mahima Gupta", "Yassir Azziz", "Rakesh Ranjan", "Andrea Vedaldi"], "title": "WorldGen: From Text to Traversable and Interactive 3D Worlds", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce WorldGen, a system that enables the automatic creation of large-scale, interactive 3D worlds directly from text prompts. Our approach transforms natural language descriptions into traversable, fully textured environments that can be immediately explored or edited within standard game engines. By combining LLM-driven scene layout reasoning, procedural generation, diffusion-based 3D generation, and object-aware scene decomposition, WorldGen bridges the gap between creative intent and functional virtual spaces, allowing creators to design coherent, navigable worlds without manual modeling or specialized 3D expertise. The system is fully modular and supports fine-grained control over layout, scale, and style, producing worlds that are geometrically consistent, visually rich, and efficient to render in real time. This work represents a step towards accessible, generative world-building at scale, advancing the frontier of 3D generative AI for applications in gaming, simulation, and immersive social environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper describes a system, WorldGen, that generates interactive 3D worlds from text prompts using LLMs and other generative AI techniques. While it doesn't directly focus on trajectory prediction, the generated worlds could be used for simulation environments where trajectory prediction algorithms are tested or trained. The use of LLMs is a strong indicator of relevance.", "keywords": ["LLM", "Large Language Models", "3D generative AI", "simulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17178", "pdf": "https://arxiv.org/pdf/2511.17178", "abs": "https://arxiv.org/abs/2511.17178", "authors": ["Kento Kawaharazuka", "Yoshiki Obinata", "Naoaki Kanazawa", "Haoyu Jia", "Kei Okada"], "title": "Efficient Robot Design with Multi-Objective Black-Box Optimization and Large Language Models", "categories": ["cs.RO"], "comment": null, "summary": "Various methods for robot design optimization have been developed so far. These methods are diverse, ranging from numerical optimization to black-box optimization. While numerical optimization is fast, it is not suitable for cases involving complex structures or discrete values, leading to frequent use of black-box optimization instead. However, black-box optimization suffers from low sampling efficiency and takes considerable sampling iterations to obtain good solutions. In this study, we propose a method to enhance the efficiency of robot body design based on black-box optimization by utilizing large language models (LLMs). In parallel with the sampling process based on black-box optimization, sampling is performed using LLMs, which are provided with problem settings and extensive feedback. We demonstrate that this method enables more efficient exploration of design solutions and discuss its characteristics and limitations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on robot design optimization using black-box optimization techniques and large language models (LLMs). While it doesn't directly address trajectory prediction, the use of LLMs for robot design, which could potentially influence trajectory planning or control, gives it some relevance. The connection to trajectory prediction is indirect, hence the moderate score.", "keywords": ["Large Language Models", "LLMs", "robot design optimization", "black-box optimization"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16857", "pdf": "https://arxiv.org/pdf/2511.16857", "abs": "https://arxiv.org/abs/2511.16857", "authors": ["Vineet Bhat", "Sungsu Kim", "Valts Blukis", "Greg Heinrich", "Prashanth Krishnamurthy", "Ramesh Karri", "Stan Birchfield", "Farshad Khorrami", "Jonathan Tremblay"], "title": "BOP-ASK: Object-Interaction Reasoning for Vision-Language Models", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Vision Language Models (VLMs) have achieved impressive performance on spatial reasoning benchmarks, yet these evaluations mask critical weaknesses in understanding object interactions. Current benchmarks test high level relationships ('left of,' 'behind', etc.) but ignore fine-grained spatial understanding needed for real world applications: precise 3D localization, physical compatibility between objects, object affordances and multi step spatial planning. In this work, we present BOP-ASK, a novel large scale dataset for object interaction reasoning for both training and benchmarking. Our data generation pipeline leverages 6D object poses from the Benchmark for Object Pose Estimation (BOP) datasets from which we derive fine grained annotations such as grasp poses, referred object poses, path planning trajectories, relative spatial and depth relationships, and object-to-object relationships. BOP-ASK comprises over 150k images and 33M question answer pairs spanning six tasks (four novel), providing a rich resource for training and evaluating VLMs. We evaluate proprietary and open sourced VLMs, and conduct human evaluations on BOP-ASK-core, a contributed test benchmark. We also release BOP-ASK-lab, an out-of-distribution benchmark with images not sourced from BOP, enabling testing of generalization. Our experiments demonstrate that models trained on BOP-ASK outperform baselines and exhibit emergent capabilities such as precise object and grasp pose estimation, trajectory planning, and fine-grained object-centric spatial reasoning in cluttered environments. We will publicly release our datasets and dataset generation pipeline.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on object interaction reasoning for vision-language models (VLMs). While it doesn't directly address trajectory prediction using LLMs, it does involve path planning trajectories as part of its dataset generation and evaluation. It also uses Large Language Models. Therefore, there's a moderate level of relevance due to the inclusion of trajectory planning and LLMs.", "keywords": ["Vision Language Models", "VLMs", "large scale dataset", "object interaction reasoning", "path planning trajectories", "object affordances"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16901", "pdf": "https://arxiv.org/pdf/2511.16901", "abs": "https://arxiv.org/abs/2511.16901", "authors": ["Lu Zhu", "Tiantian Geng", "Yangye Chen", "Teng Wang", "Ping Lu", "Feng Zheng"], "title": "R-AVST: Empowering Video-LLMs with Fine-Grained Spatio-Temporal Reasoning in Complex Audio-Visual Scenarios", "categories": ["cs.CV"], "comment": "Accepted by AAAI 2026. Project page: https://github.com/zhlllau/R-AVST", "summary": "Recently, rapid advancements have been made in multimodal large language models (MLLMs), especially in video understanding tasks. However, current research focuses on simple video scenarios, failing to reflect the complex and diverse nature of real-world audio-visual events in videos. To bridge this gap, we firstly introduce R-AVST, a dataset for audio-visual reasoning featuring fine-grained spatio-temporal annotations. In constructing this, we design a pipeline consisting of LLM-based key object extraction, automatic spatial annotation and manual quality inspection, resulting in over 5K untrimmed videos with 27K objects across 100 types of audio-visual events. Building on this dataset, we define three core tasks for spatio-temporal reasoning in audio-visual scenes and generate more than 8K high-quality, evenly distributed question-answer pairs to effectively benchmark model performance. To further enhance reasoning, we propose AVST-Zero, a reinforcement learning-based model that avoids intermediate supervision, directly optimizing behavior via carefully designed multi-dimensional rewards. Extensive experiments validate the effectiveness of our R-AVST in advancing audio-visual spatio-temporal reasoning, upon which AVST-Zero demonstrates competitive performance compared to existing models. To the best of our knowledge, R-AVST is the first dataset designed for real-world audio-visual spatio-temporal reasoning, and AVST-Zero offers a novel perspective for tackling future challenges in this domain.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on spatio-temporal reasoning in video using large language models. While it doesn't explicitly mention trajectory prediction, spatio-temporal reasoning is a closely related concept. The use of LLMs is also a key indicator of relevance.", "keywords": ["large language models", "LLMs", "spatio-temporal reasoning", "video understanding"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17335", "pdf": "https://arxiv.org/pdf/2511.17335", "abs": "https://arxiv.org/abs/2511.17335", "authors": ["Chiori Hori", "Yoshiki Masuyama", "Siddarth Jain", "Radu Corcodel", "Devesh Jha", "Diego Romeres", "Jonathan Le Roux"], "title": "Robot Confirmation Generation and Action Planning Using Long-context Q-Former Integrated with Multimodal LLM", "categories": ["cs.RO", "cs.CL", "cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted to ASRU 2025", "summary": "Human-robot collaboration towards a shared goal requires robots to understand human action and interaction with the surrounding environment. This paper focuses on human-robot interaction (HRI) based on human-robot dialogue that relies on the robot action confirmation and action step generation using multimodal scene understanding. The state-of-the-art approach uses multimodal transformers to generate robot action steps aligned with robot action confirmation from a single clip showing a task composed of multiple micro steps. Although actions towards a long-horizon task depend on each other throughout an entire video, the current approaches mainly focus on clip-level processing and do not leverage long-context information. This paper proposes a long-context Q-former incorporating left and right context dependency in full videos. Furthermore, this paper proposes a text-conditioning approach to feed text embeddings directly into the LLM decoder to mitigate the high abstraction of the information in text by Q-former. Experiments with the YouCook2 corpus show that the accuracy of confirmation generation is a major factor in the performance of action planning. Furthermore, we demonstrate that the long-context Q-former improves the confirmation and action planning by integrating VideoLLaMA3.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on human-robot interaction and action planning using multimodal LLMs. While it doesn't directly predict trajectories in the traditional sense, it involves predicting robot actions based on human actions and environmental understanding, which is related to action prediction. The use of LLMs is a key component, making it relevant to the large language model aspect.", "keywords": ["Large Language Models", "LLM", "action planning", "action prediction", "human-robot interaction", "multimodal"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16937", "pdf": "https://arxiv.org/pdf/2511.16937", "abs": "https://arxiv.org/abs/2511.16937", "authors": ["Hong Gao", "Jingyu Wu", "Xiangkai Xu", "Kangni Xie", "Yunchen Zhang", "Bin Zhong", "Xurui Gao", "Min-Ling Zhang"], "title": "OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\\_tIoU and m\\_vIoU on OmniGround with consistent gains across four benchmarks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65f6\u7a7a\u89c6\u9891\u5b9a\u4f4d\uff08STVG\uff09\uff0c\u76ee\u6807\u662f\u5728\u89c6\u9891\u4e2d\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5b9a\u4f4d\u76ee\u6807\u5bf9\u8c61\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u70b9\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u6d89\u53ca\u65f6\u7a7a\u63a8\u7406\u548c\u76ee\u6807\u8ddf\u8e2a\uff0c\u8fd9\u4e9b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u6b64\u5916\uff0c\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Multimodal Large Language Models\uff09\uff0c\u8868\u660e\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Spatio-Temporal Video Grounding", "Multimodal Large Language Models", "temporal grounding", "spatio-temporal propagation", "tracking"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.16951", "pdf": "https://arxiv.org/pdf/2511.16951", "abs": "https://arxiv.org/abs/2511.16951", "authors": ["Xin Shen", "Rui Zhu", "Lei Shen", "Xinyu Wang", "Kaihao Zhang", "Tianqing Zhu", "Shuchen Wu", "Chenxi Miao", "Weikang Li", "Yang Li", "Deguo Xia", "Jizhou Huang", "Xin Yu"], "title": "FingerCap: Fine-grained Finger-level Hand Motion Captioning", "categories": ["cs.CV"], "comment": null, "summary": "Understanding fine-grained human hand motion is fundamental to visual perception, embodied intelligence, and multimodal communication. In this work, we propose Fine-grained Finger-level Hand Motion Captioning (FingerCap), which aims to generate textual descriptions that capture detailed finger-level semantics of hand actions. To support this task, we curate FingerCap-40K, a large-scale corpus of 40K paired hand-motion videos and captions spanning two complementary sources: concise instruction-style finger motions and diverse, naturalistic hand-object interactions. To enable effective evaluation, we employ HandJudge, a LLM-based rubric that measures finger-level correctness and motion completeness. Temporal sparsity remains a fundamental bottleneck for current Video-MLLMs, since sparse RGB sampling is insufficient to capture the subtle, high-frequency dynamics underlying fine finger motions. As a simple and compute-friendly remedy, we introduce FiGOP (Finger Group-of-Pictures), which pairs each RGB keyframe with subsequent hand keypoints until the next keyframe. A lightweight temporal encoder converts the keypoints into motion embeddings and integrates them with RGB features. FiGOP adapts the classic GOP concept to finger motion, recovering fine temporal cues without increasing RGB density. Experiments on FingerCap-40K show that strong open- and closed-source Video-MLLMs still struggle with finger-level reasoning, while our FiGOP-augmented model yield consistent gains under HandJudge and human studies.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on fine-grained hand motion captioning, which is related to trajectory prediction in the sense that it involves understanding and describing the movement of hands and fingers. It also leverages LLMs for evaluation (HandJudge). While not directly addressing traditional trajectory prediction tasks (e.g., pedestrian or vehicle trajectory prediction), the movement analysis and use of LLMs contribute to a moderate level of relevance. The use of LLMs is primarily for evaluation purposes, not for generating trajectories themselves.", "keywords": ["Large Language Models", "LLM", "hand motion", "motion captioning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17497", "pdf": "https://arxiv.org/pdf/2511.17497", "abs": "https://arxiv.org/abs/2511.17497", "authors": ["Yuezhan Tao", "Dexter Ong", "Fernando Cladera", "Jason Hughes", "Camillo J. Taylor", "Pratik Chaudhari", "Vijay Kumar"], "title": "HALO: High-Altitude Language-Conditioned Monocular Aerial Exploration and Navigation", "categories": ["cs.RO"], "comment": null, "summary": "We demonstrate real-time high-altitude aerial metric-semantic mapping and exploration using a monocular camera paired with a global positioning system (GPS) and an inertial measurement unit (IMU). Our system, named HALO, addresses two key challenges: (i) real-time dense 3D reconstruction using vision at large distances, and (ii) mapping and exploration of large-scale outdoor environments with accurate scene geometry and semantics. We demonstrate that HALO can plan informative paths that exploit this information to complete missions with multiple tasks specified in natural language. In simulation-based evaluation across large-scale environments of size up to 78,000 sq. m., HALO consistently completes tasks with less exploration time and achieves up to 68% higher competitive ratio in terms of the distance traveled compared to the state-of-the-art semantic exploration baseline. We use real-world experiments on a custom quadrotor platform to demonstrate that (i) all modules can run onboard the robot, and that (ii) in diverse environments HALO can support effective autonomous execution of missions covering up to 24,600 sq. m. area at an altitude of 40 m. Experiment videos and more details can be found on our project page: https://tyuezhan.github.io/halo/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on aerial exploration and navigation conditioned on natural language. While it doesn't explicitly deal with trajectory prediction in the traditional sense (like predicting the future trajectory of an agent), it does involve path planning and control based on language instructions, suggesting a connection to trajectory generation and potentially utilizing large language models for understanding and interpreting the language commands. The 'language-conditioned' aspect and the use of natural language for mission specification are the primary reasons for a non-zero relevance score. However, the core focus is on mapping and exploration, not directly on predicting trajectories of other agents or objects.", "keywords": ["language-conditioned", "navigation", "path planning", "natural language"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17502", "pdf": "https://arxiv.org/pdf/2511.17502", "abs": "https://arxiv.org/abs/2511.17502", "authors": ["Jun Cen", "Siteng Huang", "Yuqian Yuan", "Hangjie Yuan", "Chaohui Yu", "Yuming Jiang", "Jiayan Guo", "Kehan Li", "Hao Luo", "Fan Wang", "Xin Li", "Deli Zhao", "Hao Chen"], "title": "RynnVLA-002: A Unified Vision-Language-Action and World Model", "categories": ["cs.RO"], "comment": null, "summary": "We introduce RynnVLA-002, a unified Vision-Language-Action (VLA) and world model. The world model leverages action and visual inputs to predict future image states, learning the underlying physics of the environment to refine action generation. Conversely, the VLA model produces subsequent actions from image observations, enhancing visual understanding and supporting the world model's image generation. The unified framework of RynnVLA-002 enables joint learning of environmental dynamics and action planning. Our experiments show that RynnVLA-002 surpasses individual VLA and world models, demonstrating their mutual enhancement. We evaluate RynnVLA-002 in both simulation and real-world robot tasks. RynnVLA-002 achieves 97.4% success rate on the LIBERO simulation benchmark without pretraining, while in real-world LeRobot experiments, its integrated world model boosts the overall success rate by 50%.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper describes a unified Vision-Language-Action (VLA) and world model, which learns to predict future image states based on action and visual inputs. While it doesn't explicitly mention trajectory prediction, the concept of predicting future states is related. The use of a large model (though not explicitly stated as a Large Language Model) is implied by the mention of a 'world model' and 'VLA model' and the ability to handle vision, language, and action. The connection to trajectory prediction is indirect through the prediction of future states and action planning.", "keywords": ["world model", "action planning", "future image states", "VLA model", "Vision-Language-Action"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17435", "pdf": "https://arxiv.org/pdf/2511.17435", "abs": "https://arxiv.org/abs/2511.17435", "authors": ["Zengyu Zou", "Jingyuan Wang", "Yixuan Huang", "Junjie Wu"], "title": "Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems", "categories": ["cs.LG"], "comment": "15 pages", "summary": "This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on multi-vehicle dynamic pickup and delivery problems, which is related to trajectory prediction and path planning for multiple agents. While it uses a Transformer-based architecture, it doesn't explicitly leverage large language models. The connection to trajectory prediction is through the optimization of vehicle routes and spatio-temporal system optimization.", "keywords": ["vehicle routing problem", "spatio-temporal system optimization", "Transformer", "reinforcement learning", "multi-vehicle dynamic pickup and delivery"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2511.17094", "pdf": "https://arxiv.org/pdf/2511.17094", "abs": "https://arxiv.org/abs/2511.17094", "authors": ["He Huang", "Zixuan Hu", "Dongxiao Li", "Yao Xiao", "Ling-Yu Duan"], "title": "Sparse Reasoning is Enough: Biological-Inspired Framework for Video Anomaly Detection with Large Pre-trained Models", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection (VAD) plays a vital role in real-world applications such as security surveillance, autonomous driving, and industrial monitoring. Recent advances in large pre-trained models have opened new opportunities for training-free VAD by leveraging rich prior knowledge and general reasoning capabilities. However, existing studies typically rely on dense frame-level inference, incurring high computational costs and latency. This raises a fundamental question: Is dense reasoning truly necessary when using powerful pre-trained models in VAD systems? To answer this, we propose ReCoVAD, a novel framework inspired by the dual reflex and conscious pathways of the human nervous system, enabling selective frame processing to reduce redundant computation. ReCoVAD consists of two core pathways: (i) a Reflex pathway that uses a lightweight CLIP-based module to fuse visual features with prototype prompts and produce decision vectors, which query a dynamic memory of past frames and anomaly scores for fast response; and (ii) a Conscious pathway that employs a medium-scale vision-language model to generate textual event descriptions and refined anomaly scores for novel frames. It continuously updates the memory and prototype prompts, while an integrated large language model periodically reviews accumulated descriptions to identify unseen anomalies, correct errors, and refine prototypes. Extensive experiments show that ReCoVAD achieves state-of-the-art training-free performance while processing only 28.55\\% and 16.04\\% of the frames used by previous methods on the UCF-Crime and XD-Violence datasets, demonstrating that sparse reasoning is sufficient for effective large-model-based VAD.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f46\u6458\u8981\u63d0\u5230\u4e86\u81ea\u4e3b\u9a7e\u9a76\uff08autonomous driving\uff09\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e5f\u4f7f\u7528\u4e86\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff08large pre-trained models\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08large language model\uff09\u3002\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u7814\u7a76\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u76f8\u5173\u6027\u5b58\u5728\u3002", "keywords": ["autonomous driving", "large pre-trained models", "large language model"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
