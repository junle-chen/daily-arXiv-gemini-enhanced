{"id": "2509.25264", "pdf": "https://arxiv.org/pdf/2509.25264", "abs": "https://arxiv.org/abs/2509.25264", "authors": ["Shuyang Hou", "Haoyue Jiao", "Ziqi Liu", "Lutong Xie", "Guanyu Chen", "Shaowen Wu", "Xuefeng Guan", "Huayi Wu"], "title": "From NL2SQL to NL2GeoSQL: GeoSQL-Eval for automated evaluation of LLMs on PostGIS queries", "categories": ["cs.DB", "cs.AI", "cs.LG", "cs.SE"], "comment": null, "summary": "In recent years, large language models (LLMs) have achieved remarkable\nprogress in natural language understanding and structured query generation\n(NL2SQL). However, extending these advances to GeoSQL tasks in the PostGIS\nenvironment remains challenging due to the complexity of spatial functions,\ngeometric data types, and execution semantics. Existing evaluations primarily\nfocus on general relational databases or Google Earth Engine code generation,\nleaving a lack of systematic benchmarks tailored to spatial databases. To\naddress this gap, this study introduces GeoSQL-Eval, the first end-to-end\nautomated evaluation framework for PostGIS query generation. Built upon Webb's\nDepth of Knowledge (DOK) model, the framework encompasses four cognitive\ndimensions, five proficiency levels, and twenty task categories, providing a\ncomprehensive assessment of model performance in terms of knowledge\nacquisition, syntactic generation, semantic alignment, execution accuracy, and\nrobustness. In parallel, we developed GeoSQL-Bench, a benchmark dataset\ncomprising 14178 questions that span three task types, 340 PostGIS functions,\nand 82 domain-specific databases. Leveraging this framework, we systematically\nevaluated 24 representative models across six categories, applying\nentropy-weighting and statistical analyses to reveal differences in\nperformance, error distributions, and resource consumption patterns.\nFurthermore, we established a public GeoSQL-Eval leaderboard that enables\nglobal research teams to conduct ongoing testing and comparison. These\ncontributions not only extend the boundaries of NL2SQL applications but also\nprovide a standardized, interpretable, and scalable framework for evaluating\nLLM performance in spatial database contexts, offering valuable insights for\nmodel optimization and applications in geographic information science, urban\nstudies, and spatial analysis.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.25285", "pdf": "https://arxiv.org/pdf/2509.25285", "abs": "https://arxiv.org/abs/2509.25285", "authors": ["Jun Kawasaki"], "title": "ActorDB: A Unified Database Model Integrating Single-Writer Actors, Incremental View Maintenance, and Zero-Trust Messaging", "categories": ["cs.DB", "cs.CL", "cs.DC"], "comment": "7 pages, 1 table, 1 figures. Code and data available at\n  https://github.com/com-junkawasaki/dekigoto", "summary": "This paper presents ActorDB ( Dekigoto ) , a novel database architecture that\ntightly integrates a single-writer actor model for writes, Incremental View\nMaintenance (IVM), and a zero-trust security model as a core component. The\nprimary contribution of this work is the unification of these powerful but\ncomplex concepts into a single, cohesive system designed to reduce\narchitectural complexity for developers of modern, data-intensive applications.\nWe argue that by providing these capabilities out-of-the-box, ActorDB can offer\na more robust, secure, and developer-friendly platform compared to solutions\nthat require manual integration of separate systems for actor persistence,\nstream processing, and security. We present the core architecture, discuss the\ncritical trade-offs in its design, and define the performance criteria for a\nMinimum Viable Product (MVP) to validate our approach.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.25907", "pdf": "https://arxiv.org/pdf/2509.25907", "abs": "https://arxiv.org/abs/2509.25907", "authors": ["Jian Fu", "Xixian Han", "Xiaolong Wan", "Wenjian Wang"], "title": "PAT: Pattern-Perceptive Transformer for Error Detection in Relational Databases", "categories": ["cs.DB"], "comment": null, "summary": "Error detection in relational databases is critical for maintaining data\nquality and is fundamental to tasks such as data cleaning and assessment.\nCurrent error detection studies mostly employ the multi-detector approach to\nhandle heterogeneous attributes in databases, incurring high costs.\nAdditionally, their data preprocessing strategies fail to leverage the\nvariable-length characteristic of data sequences, resulting in reduced\naccuracy. In this paper, we propose an attribute-wise PAttern-perceptive\nTransformer (PAT) framework for error detection in relational databases. First,\nPAT introduces a learned pattern module that captures attribute-specific data\ndistributions through learned embeddings during model training. Second, the\nQuasi-Tokens Arrangement (QTA) tokenizer is designed to divide the cell\nsequence based on its length and word types, and then generate the\nword-adaptive data tokens, meanwhile providing compact hyperparameters to\nensure efficiency. By interleaving data tokens with the attribute-specific\npattern tokens, PAT jointly learns shared data features across different\nattributes and pattern features that are distinguishable and unique in each\nspecified attribute. Third, PAT visualizes the attention map to interpret its\nerror detection mechanism. Extensive experiments show that PAT achieves\nexcellent F1 scores compared to state-of-the-art data error detection methods.\nMoreover, PAT significantly reduces the model parameters and FLOPs when\napplying the compact QTA tokenizer.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.26102", "pdf": "https://arxiv.org/pdf/2509.26102", "abs": "https://arxiv.org/abs/2509.26102", "authors": ["Genoveva Vargas-Solar", "Umberto Costa", "J\u00e9r\u00f4me Darmont", "Javier Espinosa-Oviedo", "Carmem Hara", "Sabine Loudcher", "Regina Motz", "Martin A. Musicante", "Jos\u00e9-Luis Zechinelli-Martini"], "title": "Experiversum: an Ecosystem for Curating and Enhancing Data-Driven Experimental Science", "categories": ["cs.DB"], "comment": null, "summary": "This paper introduces Experiversum, a lakehouse-based ecosystem that supports\nthe curation, documentation and reproducibility of exploratory experiments.\nExperiversum enables structured research through iterative data cycles, while\ncapturing metadata and collaborative decisions. Demonstrated through case\nstudies in Earth, Life and Political Sciences, Experiversum promotes\ntransparent workflows and multi-perspective result interpretation. Experiversum\nbridges exploratory and reproducible research, encouraging accountable and\nrobust data-driven practices across disciplines.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.26434", "pdf": "https://arxiv.org/pdf/2509.26434", "abs": "https://arxiv.org/abs/2509.26434", "authors": ["Lars Vogt", "Barend Mons"], "title": "The Grammar of FAIR: A Granular Architecture of Semantic Units for FAIR Semantics, Inspired by Biology and Linguistics", "categories": ["cs.DB"], "comment": null, "summary": "The FAIR Principles aim to make data and knowledge Findable, Accessible,\nInteroperable, and Reusable, yet current digital infrastructures often lack a\nunifying semantic framework that bridges human cognition and\nmachine-actionability. In this paper, we introduce the Grammar of FAIR: a\ngranular and modular architecture for FAIR semantics built on the concept of\nsemantic units. Semantic units, comprising atomic statement units and composite\ncompound units, implement the principle of semantic modularisation, decomposing\ndata and knowledge into independently identifiable, semantically meaningful,\nand machine-actionable units. A central metaphor guiding our approach is the\nanalogy between the hierarchy of level of organisation in biological systems\nand the hierarchy of levels of organisation in information systems: both are\nstructured by granular building blocks that mediate across multiple\nperspectives while preserving functional unity. Drawing further inspiration\nfrom concept formation and natural language grammar, we show how these building\nblocks map to FAIR Digitial Objects (FDOs), enabling format-agnostic semantic\ntransitivity from natural language token models to schema-based\nrepresentations. This dual biological-linguistic analogy provides a\nsemantics-first foundation for evolving cross-ecosystem infrastructures, paving\nthe way for the Internet of FAIR Data and Services (IFDS) and a future of\nmodular, AI-ready, and citation-granular scholarly communication.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.25487", "pdf": "https://arxiv.org/pdf/2509.25487", "abs": "https://arxiv.org/abs/2509.25487", "authors": ["Dingyi Kang", "Dongming Jiang", "Hanshen Yang", "Hang Liu", "Bingzhe Li"], "title": "Scalable Disk-Based Approximate Nearest Neighbor Search with Page-Aligned Graph", "categories": ["cs.LG", "cs.DB", "cs.IR"], "comment": null, "summary": "Approximate Nearest Neighbor Search (ANNS), as the core of vector databases\n(VectorDBs), has become widely used in modern AI and ML systems, powering\napplications from information retrieval to bio-informatics. While graph-based\nANNS methods achieve high query efficiency, their scalability is constrained by\nthe available host memory. Recent disk-based ANNS approaches mitigate memory\nusage by offloading data to Solid-State Drives (SSDs). However, they still\nsuffer from issues such as long I/O traversal path, misalignment with storage\nI/O granularity, and high in-memory indexing overhead, leading to significant\nI/O latency and ultimately limiting scalability for large-scale vector search.\n  In this paper, we propose PageANN, a disk-based approximate nearest neighbor\nsearch (ANNS) framework designed for high performance and scalability. PageANN\nintroduces a page-node graph structure that aligns logical graph nodes with\nphysical SSD pages, thereby shortening I/O traversal paths and reducing I/O\noperations. Specifically, similar vectors are clustered into page nodes, and a\nco-designed disk data layout leverages this structure with a merging technique\nto store only representative vectors and topology information, avoiding\nunnecessary reads. To further improve efficiency, we design a memory management\nstrategy that combines lightweight indexing with coordinated memory-disk data\nallocation, maximizing host memory utilization while minimizing query latency\nand storage overhead. Experimental results show that PageANN significantly\noutperforms state-of-the-art (SOTA) disk-based ANNS methods, achieving\n1.85x-10.83x higher throughput and 51.7%-91.9% lower latency across different\ndatasets and memory budgets, while maintaining comparable high recall accuracy.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.25672", "pdf": "https://arxiv.org/pdf/2509.25672", "abs": "https://arxiv.org/abs/2509.25672", "authors": ["Hasan Alp Cafero\u011flu", "Mehmet Serhat \u00c7elik", "\u00d6zg\u00fcr Ulusoy"], "title": "SING-SQL: A Synthetic Data Generation Framework for In-Domain Text-to-SQL Translation", "categories": ["cs.AI", "cs.DB"], "comment": null, "summary": "Translating natural language questions into SQL has become a core challenge\nin enabling non-technical users to query databases. While recent work has\nexplored large-scale synthetic data generation to improve model performance\nthrough post-training, most efforts emphasize cross-domain generalization. This\nleaves a gap for real-world enterprise scenarios, where models need to\nspecialize to a single database schema and organizations require to be able to\nevaluate their Text-to-SQL systems on their own databases. To address this, we\nintroduce SING-SQL, a fully automated two-stage framework for generating\nhigh-quality, high-coverage synthetic Text-to-SQL data for any target database,\nwithout relying on SQL logs or manual annotations. Our approach hierarchically\npartitions a database schema into sub-schemas, synthesizes SQL queries across\nmultiple complexity levels, and applies a quality-aware pipeline that includes\nLLM-as-a-judge validation, executability checks, automatic repair, and column\nbalancing. We further release SingSQL-LM, a family of compact language models\nfine-tuned on the synthetic data, achieving strong in-domain generalization. On\nthe subset of the BIRD benchmark, SingSQL-LM-3B-R64 reaches 82.87% Soft F1 and\n73.03% EX upper bound with 32 candidates, outperforming the best 3B-scale\nbaseline by +16.21 in Soft F1 and +12.36 in EX. At the 1.5B scale,\nSingSQL-LM-1.5B-R64 improves over prior systems by +9.30 in Soft F1 and +4.49\nin EX. On synthetic evaluation sets, SingSQL-LMs exceed prior systems by wide\nmargins, establishing state-of-the-art performance among open models at\ncomparable scales. Our study of context management strategies reveals that\nschema-free fine-tuning combined with schema-only inference provides the most\nrobust results. These findings establish SING-SQL as a scalable,\ndatabase-agnostic paradigm for producing and evaluating enterprise-grade\nText-to-SQL systems.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.25839", "pdf": "https://arxiv.org/pdf/2509.25839", "abs": "https://arxiv.org/abs/2509.25839", "authors": ["Han Zhang", "Dongfang Zhao"], "title": "RAE: A Neural Network Dimensionality Reduction Method for Nearest Neighbors Preservation in Vector Search", "categories": ["cs.IR", "cs.AI", "cs.DB"], "comment": "submitted to ICLR 2026", "summary": "While high-dimensional embedding vectors are being increasingly employed in\nvarious tasks like Retrieval-Augmented Generation and Recommendation Systems,\npopular dimensionality reduction (DR) methods such as PCA and UMAP have rarely\nbeen adopted for accelerating the retrieval process due to their inability of\npreserving the nearest neighbor (NN) relationship among vectors. Empowered by\nneural networks' optimization capability and the bounding effect of Rayleigh\nquotient, we propose a Regularized Auto-Encoder (RAE) for k-NN preserving\ndimensionality reduction. RAE constrains the network parameter variation\nthrough regularization terms, adjusting singular values to control embedding\nmagnitude changes during reduction, thus preserving k-NN relationships. We\nprovide a rigorous mathematical analysis demonstrating that regularization\nestablishes an upper bound on the norm distortion rate of transformed vectors,\nthereby offering provable guarantees for k-NN preservation. With modest\ntraining overhead, RAE achieves superior k-NN recall compared to existing DR\napproaches while maintaining fast retrieval efficiency.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
