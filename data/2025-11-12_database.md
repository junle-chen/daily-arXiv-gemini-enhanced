# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-11-12

## 目录

- [计算语言学 (Computation and Language) (1)](#cs-cl)
- [cs.CY (1)](#cs-cy)
- [cs.DB (4)](#cs-db)

## 计算语言学 (Computation and Language) [cs.CL]
### [1] [Future of AI Models: A Computational perspective on Model collapse](https://arxiv.org/abs/2511.05535)
*Trivikram Satharasi, S Sitharama Iyengar*

Main category: cs.CL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Artificial Intelligence, especially Large Language Models (LLMs), has transformed domains such as software engineering, journalism, creative writing, academia, and media (Naveed et al. 2025; arXiv:2307.06435). Diffusion models like Stable Diffusion generate high-quality images and videos from text. Evidence shows rapid expansion: 74.2% of newly published webpages now contain AI-generated material (Ryan Law 2025), 30-40% of the active web corpus is synthetic (Spennemann 2025; arXiv:2504.08755), 52% of U.S. adults use LLMs for writing, coding, or research (Staff 2025), and audits find AI involvement in 18% of financial complaints and 24% of press releases (Liang et al. 2025). The underlying neural architectures, including Transformers (Vaswani et al. 2023; arXiv:1706.03762), RNNs, LSTMs, GANs, and diffusion networks, depend on large, diverse, human-authored datasets (Shi & Iyengar 2019). As synthetic content dominates, recursive training risks eroding linguistic and semantic diversity, producing Model Collapse (Shumailov et al. 2024; arXiv:2307.15043; Dohmatob et al. 2024; arXiv:2402.07712). This study quantifies and forecasts collapse onset by examining year-wise semantic similarity in English-language Wikipedia (filtered Common Crawl) from 2013 to 2025 using Transformer embeddings and cosine similarity metrics. Results reveal a steady rise in similarity before public LLM adoption, likely driven by early RNN/LSTM translation and text-normalization pipelines, though modest due to a smaller scale. Observed fluctuations reflect irreducible linguistic diversity, variable corpus size across years, finite sampling error, and an exponential rise in similarity after the public adoption of LLM models. These findings provide a data-driven estimate of when recursive AI contamination may significantly threaten data richness and model generalization.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.05535) | **Categories:** cs.CL, cs.DB, cs.IT

---


## cs.CY [cs.CY]
### [1] [AgriTrust: a Federated Semantic Governance Framework for Trusted Agricultural Data Sharing](https://arxiv.org/abs/2511.05572)
*Ivan Bergier*

Main category: cs.CY

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: The potential of agricultural data (AgData) to drive efficiency and sustainability is stifled by the "AgData Paradox": a pervasive lack of trust and interoperability that locks data in silos, despite its recognized value. This paper introduces AgriTrust, a federated semantic governance framework designed to resolve this paradox. AgriTrust integrates a multi-stakeholder governance model, built on pillars of Data Sovereignty, Transparent Data Contracts, Equitable Value Sharing, and Regulatory Compliance, with a semantic digital layer. This layer is realized through the AgriTrust Core Ontology, a formal OWL ontology that provides a shared vocabulary for tokenization, traceability, and certification, enabling true semantic interoperability across independent platforms. A key innovation is a blockchain-agnostic, multi-provider architecture that prevents vendor lock-in. The framework's viability is demonstrated through case studies across three critical Brazilian supply chains: coffee (for EUDR compliance), soy (for mass balance), and beef (for animal tracking). The results show that AgriTrust successfully enables verifiable provenance, automates compliance, and creates new revenue streams for data producers, thereby transforming data sharing from a trust-based dilemma into a governed, automated operation. This work provides a foundational blueprint for a more transparent, efficient, and equitable agricultural data economy.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.05572) | **Categories:** cs.CY, cs.CE, cs.CR, cs.DB, cs.HC

---


## cs.DB [cs.DB]
### [1] [OntoTune: Ontology-Driven Learning for Query Optimization with Convolutional Models](https://arxiv.org/abs/2511.06780)
*Songhui Yue, Yang Shao, Sean Hayes*

Main category: cs.DB

TL;DR: 本文提出了一种基于本体的平台OntoTune，用于增强查询优化的学习。


<details>
  <summary>Details</summary>
Motivation: 查询优化可以使用机器学习、强化学习以及最近的基于图的卷积网络进行研究。本体作为一种结构化的、信息丰富的知识表示，可以提供上下文，特别是在学习问题中。

Method: 本文提出了一种基于本体的平台OntoTune，通过连接SQL查询、数据库元数据和统计信息，该研究中开发的本体在捕获关系和查询性能的重要决定因素方面很有前景。此外，该研究还开发了一种嵌入本体的方法，在将其输入到基于树和基于图的卷积网络等学习算法之前，尽可能多地保留关系和关键信息。

Result: 案例研究表明，与数据库系统默认查询执行相比，OntoTune的本体驱动学习带来了性能提升。

Conclusion: OntoTune平台利用本体论增强了查询优化的学习效果，并通过案例研究验证了其性能优势。

Abstract: 查询优化领域已经开始利用机器学习、强化学习以及基于图的卷积网络等技术。本体作为一种结构化且信息丰富的知识表示方法，能够为学习问题提供有价值的上下文信息。本文介绍了一种名为OntoTune的基于本体的平台，旨在提升查询优化的学习效果。通过连接SQL查询、数据库元数据和统计信息，本研究构建的本体在捕捉查询性能的关键关系和决定因素方面展现出潜力。此外，本研究还提出了一种本体嵌入方法，该方法在尽可能保留关系和关键信息的前提下，将本体信息输入到诸如基于树和基于图的卷积网络等学习算法中。一个案例研究表明，相较于数据库系统的默认查询执行方式，OntoTune的本体驱动学习能够带来性能上的提升。

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.06780) | **Categories:** cs.DB, cs.AI, cs.LG

---

### [2] [Trading Vector Data in Vector Databases](https://arxiv.org/abs/2511.07139)
*Jin Cheng, Xiangxiang Dai, Ningning Ding, John C. S. Lui, Jianwei Huang*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Vector data trading is essential for cross-domain learning with vector databases, yet it remains largely unexplored. We study this problem under online learning, where sellers face uncertain retrieval costs and buyers provide stochastic feedback to posted prices. Three main challenges arise: (1) heterogeneous and partial feedback in configuration learning, (2) variable and complex feedback in pricing learning, and (3) inherent coupling between configuration and pricing decisions.   We propose a hierarchical bandit framework that jointly optimizes retrieval configurations and pricing. Stage I employs contextual clustering with confidence-based exploration to learn effective configurations with logarithmic regret. Stage II adopts interval-based price selection with local Taylor approximation to estimate buyer responses and achieve sublinear regret. We establish theoretical guarantees with polynomial time complexity and validate the framework on four real-world datasets, demonstrating consistent improvements in cumulative reward and regret reduction compared with existing methods.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.07139) | **Categories:** cs.DB, cs.LG

---

### [3] [MemoriesDB: A Temporal-Semantic-Relational Database for Long-Term Agent Memory / Modeling Experience as a Graph of Temporal-Semantic Surfaces](https://arxiv.org/abs/2511.06179)
*Joel Ward*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: We introduce MemoriesDB, a unified data architecture designed to avoid decoherence across time, meaning, and relation in long-term computational memory. Each memory is a time-semantic-relational entity-a structure that simultaneously encodes when an event occurred, what it means, and how it connects to other events. Built initially atop PostgreSQL with pgvector extensions, MemoriesDB combines the properties of a time-series datastore, a vector database, and a graph system within a single append-only schema. Each memory is represented as a vertex uniquely labeled by its microsecond timestamp and accompanied by low- and high-dimensional normalized embeddings that capture semantic context. Directed edges between memories form labeled relations with per-edge metadata, enabling multiple contextual links between the same vertices. Together these constructs form a time-indexed stack of temporal-semantic surfaces, where edges project as directional arrows in a 1+1-dimensional similarity field, tracing the evolution of meaning through time while maintaining cross-temporal coherence. This formulation supports efficient time-bounded retrieval, hybrid semantic search, and lightweight structural reasoning in a single query path. A working prototype demonstrates scalable recall and contextual reinforcement using standard relational infrastructure, and we discuss extensions toward a columnar backend, distributed clustering, and emergent topic modeling.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.06179) | **Categories:** cs.DB, cs.AI, cs.IR

---

### [4] [A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs](https://arxiv.org/abs/2511.06455)
*Milena Trajanoska, Riste Stojanov, Dimitar Trajanov*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Enterprises often maintain multiple databases for storing critical business data in siloed systems, resulting in inefficiencies and challenges with data interoperability. A key to overcoming these challenges lies in integrating disparate data sources, enabling businesses to unlock the full potential of their data. Our work presents a novel approach for integrating multiple databases using knowledge graphs, focusing on the application of large language models as semantic agents for mapping and connecting structured data across systems by leveraging existing vocabularies. The proposed methodology introduces a semantic layer above tables in relational databases, utilizing a system comprising multiple LLM agents that map tables and columns to Schema.org terms. Our approach achieves a mapping accuracy of over 90% in multiple domains.

</details>

[**[PDF]**](https://arxiv.org/pdf/2511.06455) | **Categories:** cs.DB, cs.AI

---
