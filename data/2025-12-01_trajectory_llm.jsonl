{"id": "2511.20729", "pdf": "https://arxiv.org/pdf/2511.20729", "abs": "https://arxiv.org/abs/2511.20729", "authors": ["Sean Bin Yang", "Ying Sun", "Yunyao Cheng", "Yan Lin", "Kristian Torp", "Jilin Hu"], "title": "Spatio-Temporal Trajectory Foundation Model - Recent Advances and Future Directions", "categories": ["cs.LG", "cs.AI"], "comment": "This paper has been accepted by CIKM 2025 STIntelligence Workshop", "summary": "Foundation models (FMs) have emerged as a powerful paradigm, enabling a diverse range of data analytics and knowledge discovery tasks across scientific fields. Inspired by the success of FMs, particularly large language models, researchers have recently begun to explore spatio-temporal foundation models (STFMs) to improve adaptability and generalization across a wide spectrum of spatio-temporal (ST) tasks. Despite rapid progress, a systematic investigation of trajectory foundation models (TFMs), a crucial subclass of STFMs, is largely lacking. This tutorial addresses this gap by offering a comprehensive overview of recent advances in TFMs, including a taxonomy of existing methodologies and a critical analysis of their strengths and limitations. In addition, the tutorial highlights open challenges and outlines promising research directions to advance spatio-temporal general intelligence through the development of robust, responsible, and transferable TFMs.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u660e\u786e\u63d0\u53ca\u4e86\u65f6\u7a7a\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\uff08Spatio-Temporal Trajectory Foundation Model\uff09\uff0c\u6458\u8981\u4e2d\u4e5f\u63d0\u5230\u4e86\u57fa\u7840\u6a21\u578b\uff08Foundation Models\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08large language models\uff09\u7684\u542f\u53d1\uff0c\u5e76\u91cd\u70b9\u5173\u6ce8\u8f68\u8ff9\u57fa\u7840\u6a21\u578b\uff08Trajectory Foundation Models\uff09\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "foundation models", "large language models", "spatio-temporal foundation models", "trajectory foundation models"]}}
{"id": "2511.21135", "pdf": "https://arxiv.org/pdf/2511.21135", "abs": "https://arxiv.org/abs/2511.21135", "authors": ["Ziyi Chen", "Yingnan Guo", "Zedong Chu", "Minghua Luo", "Yanfen Shen", "Mingchao Sun", "Junjun Hu", "Shichao Xie", "Kuan Yang", "Pei Shi", "Zhining Gu", "Lu Liu", "Honglin Han", "Xiaolong Wu", "Mu Xu", "Yu Zhang"], "title": "SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Embodied navigation that adheres to social norms remains an open research challenge. Our \\textbf{SocialNav} is a foundational model for socially-aware navigation with a hierarchical \"brain-action\" architecture, capable of understanding high-level social norms and generating low-level, socially compliant trajectories. To enable such dual capabilities, we construct the SocNav Dataset, a large-scale collection of 7 million samples, comprising (1) a Cognitive Activation Dataset providing social reasoning signals such as chain-of-thought explanations and social traversability prediction, and (2) an Expert Trajectories Pyramid aggregating diverse navigation demonstrations from internet videos, simulated environments, and real-world robots. A multi-stage training pipeline is proposed to gradually inject and refine navigation intelligence: we first inject general navigation skills and social norms understanding into the model via imitation learning, and then refine such skills through a deliberately designed Socially-Aware Flow Exploration GRPO (SAFE-GRPO), the first flow-based reinforcement learning framework for embodied navigation that explicitly rewards socially compliant behaviors. SocialNav achieves +38% success rate and +46% social compliance rate compared to the state-of-the-art method, demonstrating strong gains in both navigation performance and social compliance. Our project page: https://amap-eai.github.io/SocialNav/", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86 socially-aware navigation\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u6458\u8981\u4e2d\u4e5f\u63d0\u5230\u4e86 foundational model\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u8bf4\u662f Large Language Model\uff0c\u4f46\u6697\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u8f83\u5927\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4f7f\u7528\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "socially-aware navigation", "foundation model", "embodied navigation", "reinforcement learning"]}}
{"id": "2511.20726", "pdf": "https://arxiv.org/pdf/2511.20726", "abs": "https://arxiv.org/abs/2511.20726", "authors": ["Yuhang Wang", "Heye Huang", "Zhenhua Xu", "Kailai Sun", "Baoshen Guo", "Jinhua Zhao"], "title": "Learning from Risk: LLM-Guided Generation of Safety-Critical Scenarios with Prior Knowledge", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages, 6 figures", "summary": "Autonomous driving faces critical challenges in rare long-tail events and complex multi-agent interactions, which are scarce in real-world data yet essential for robust safety validation. This paper presents a high-fidelity scenario generation framework that integrates a conditional variational autoencoder (CVAE) with a large language model (LLM). The CVAE encodes historical trajectories and map information from large-scale naturalistic datasets to learn latent traffic structures, enabling the generation of physically consistent base scenarios. Building on this, the LLM acts as an adversarial reasoning engine, parsing unstructured scene descriptions into domain-specific loss functions and dynamically guiding scenario generation across varying risk levels. This knowledge-driven optimization balances realism with controllability, ensuring that generated scenarios remain both plausible and risk-sensitive. Extensive experiments in CARLA and SMARTS demonstrate that our framework substantially increases the coverage of high-risk and long-tail events, improves consistency between simulated and real-world traffic distributions, and exposes autonomous driving systems to interactions that are significantly more challenging than those produced by existing rule- or data-driven methods. These results establish a new pathway for safety validation, enabling principled stress-testing of autonomous systems under rare but consequential events.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u6307\u5bfc\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u5e76\u4e14\u573a\u666f\u751f\u6210\u4e0e\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\uff0c\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u548c\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u3002\u8bba\u6587\u5c06LLM\u7528\u4e8e\u98ce\u9669\u8bc4\u4f30\u548c\u573a\u666f\u751f\u6210\uff0c\u4ee5\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u9ad8\u5ea6\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "large language model", "LLM", "autonomous driving", "scenario generation", "multi-agent interactions", "risk assessment"]}}
{"id": "2511.20720", "pdf": "https://arxiv.org/pdf/2511.20720", "abs": "https://arxiv.org/abs/2511.20720", "authors": ["Haibo HU", "Lianming Huang", "Nan Guan", "Chun Jason Xue"], "title": "DeeAD: Dynamic Early Exit of Vision-Language Action for Efficient Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": null, "summary": "Vision-Language Action (VLA) models unify perception, reasoning, and trajectory generation for autonomous driving, but suffer from significant inference latency due to deep transformer stacks. We present DeeAD, a training-free, action-guided early-exit framework that accelerates VLA planning by evaluating the physical feasibility of intermediate trajectories. Instead of relying on confidence scores, DeeAD terminates inference when predicted trajectories align with lightweight planning priors (e.g., Navigation or Low-precision Planning) within a tolerable deviation (<2m). To improve efficiency, we introduce a multi-hop controller that adaptively skips redundant layers based on the change rate of scores. DeeAD integrates into existing VLA models, such as ORION, without requiring retraining. Experiments on the Bench2Drive benchmark demonstrate up to 28% transformer-layer sparsity and 29% latency reduction, while preserving planning quality and safety.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant as it focuses on trajectory generation for autonomous driving, a core aspect of trajectory prediction, and utilizes a Vision-Language Action (VLA) model, which can be considered a form of large model, particularly in the context of vision-language tasks. The paper also addresses efficiency concerns related to the deep transformer stacks often found in large models.", "keywords": ["trajectory generation", "autonomous driving", "Vision-Language Action models", "VLA", "transformer", "planning"]}}
{"id": "2511.20713", "pdf": "https://arxiv.org/pdf/2511.20713", "abs": "https://arxiv.org/abs/2511.20713", "authors": ["Minhui Zhang", "Prahar Ijner", "Yoav Wald", "Elliot Creager"], "title": "Active Slice Discovery in Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted for presentation at NeurIPS 2025 - Reliable ML Workshop", "summary": "Large Language Models (LLMs) often exhibit systematic errors on specific subsets of data, known as error slices. For instance, a slice can correspond to a certain demographic, where a model does poorly in identifying toxic comments regarding that demographic. Identifying error slices is crucial to understanding and improving models, but it is also challenging. An appealing approach to reduce the amount of manual annotation required is to actively group errors that are likely to belong to the same slice, while using limited access to an annotator to verify whether the chosen samples share the same pattern of model mistake. In this paper, we formalize this approach as Active Slice Discovery and explore it empirically on a problem of discovering human-defined slices in toxicity classification. We examine the efficacy of active slice discovery under different choices of feature representations and active learning algorithms. On several slices, we find that uncertainty-based active learning algorithms are most effective, achieving competitive accuracy using 2-10% of the available slice membership information, while significantly outperforming baselines.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u9519\u8bef\u5207\u7247\u7684\u53d1\u73b0\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4e0e\u5927\u6a21\u578b\u4e3b\u9898\u9ad8\u5ea6\u76f8\u5173\uff0c\u5e76\u4e14\u53ef\u4ee5\u5e94\u7528\u4e8e\u6539\u8fdb\u5927\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5305\u62ec\u53ef\u80fd\u7528\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u5927\u6a21\u578b\u3002", "keywords": ["Large Language Models", "LLMs", "foundation models", "toxicity classification", "active learning"]}}
{"id": "2511.21690", "pdf": "https://arxiv.org/pdf/2511.21690", "abs": "https://arxiv.org/abs/2511.21690", "authors": ["Seungjae Lee", "Yoonkyo Jung", "Inkook Chun", "Yao-Chih Lee", "Zikui Cai", "Hongjia Huang", "Aayush Talreja", "Tan Dat Dao", "Yongyuan Liang", "Jia-Bin Huang", "Furong Huang"], "title": "TraceGen: World Modeling in 3D Trace Space Enables Learning from Cross-Embodiment Videos", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "Learning new robot tasks on new platforms and in new scenes from only a handful of demonstrations remains challenging. While videos of other embodiments - humans and different robots - are abundant, differences in embodiment, camera, and environment hinder their direct use. We address the small-data problem by introducing a unifying, symbolic representation - a compact 3D \"trace-space\" of scene-level trajectories - that enables learning from cross-embodiment, cross-environment, and cross-task videos. We present TraceGen, a world model that predicts future motion in trace-space rather than pixel space, abstracting away appearance while retaining the geometric structure needed for manipulation. To train TraceGen at scale, we develop TraceForge, a data pipeline that transforms heterogeneous human and robot videos into consistent 3D traces, yielding a corpus of 123K videos and 1.8M observation-trace-language triplets. Pretraining on this corpus produces a transferable 3D motion prior that adapts efficiently: with just five target robot videos, TraceGen attains 80% success across four tasks while offering 50-600x faster inference than state-of-the-art video-based world models. In the more challenging case where only five uncalibrated human demonstration videos captured on a handheld phone are available, it still reaches 67.5% success on a real robot, highlighting TraceGen's ability to adapt across embodiments without relying on object detectors or heavy pixel-space generation.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u7684\u6838\u5fc3\u662f\u5b66\u4e60\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u5e76\u63d0\u51fa\u4e86TraceGen\uff0c\u4e00\u4e2a\u57283D\u8f68\u8ff9\u7a7a\u95f4\u4e2d\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\u7684\u4e16\u754c\u6a21\u578b\u3002 \u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u6458\u8981\u63d0\u5230\u4e86\u4f7f\u7528\u5305\u542b\u8bed\u8a00\u63cf\u8ff0\u7684observation-trace-language triplets\u8fdb\u884c\u9884\u8bad\u7ec3\u3002 \u8f68\u8ff9\u9884\u6d4b\u662f\u4e3b\u8981\u5173\u6ce8\u70b9\uff0c\u4f46\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u4f7f\u5176\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "world model", "3D trace space", "motion prediction", "cross-embodiment", "language triplets"]}}
{"id": "2511.20965", "pdf": "https://arxiv.org/pdf/2511.20965", "abs": "https://arxiv.org/abs/2511.20965", "authors": ["Md Adnan Arefeen", "Biplob Debnath", "Srimat Chakradhar"], "title": "TrafficLens: Multi-Camera Traffic Video Analysis Using LLMs", "categories": ["cs.CV", "cs.CL"], "comment": "2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)", "summary": "Traffic cameras are essential in urban areas, playing a crucial role in intelligent transportation systems. Multiple cameras at intersections enhance law enforcement capabilities, traffic management, and pedestrian safety. However, efficiently managing and analyzing multi-camera feeds poses challenges due to the vast amount of data. Analyzing such huge video data requires advanced analytical tools. While Large Language Models (LLMs) like ChatGPT, equipped with retrieval-augmented generation (RAG) systems, excel in text-based tasks, integrating them into traffic video analysis demands converting video data into text using a Vision-Language Model (VLM), which is time-consuming and delays the timely utilization of traffic videos for generating insights and investigating incidents. To address these challenges, we propose TrafficLens, a tailored algorithm for multi-camera traffic intersections. TrafficLens employs a sequential approach, utilizing overlapping coverage areas of cameras. It iteratively applies VLMs with varying token limits, using previous outputs as prompts for subsequent cameras, enabling rapid generation of detailed textual descriptions while reducing processing time. Additionally, TrafficLens intelligently bypasses redundant VLM invocations through an object-level similarity detector. Experimental results with real-world datasets demonstrate that TrafficLens reduces video-to-text conversion time by up to $4\\times$ while maintaining information accuracy.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper discusses using Large Language Models (LLMs) for analyzing multi-camera traffic video. While it doesn't directly focus on trajectory prediction, the context of traffic video analysis often involves understanding and predicting the movement of vehicles and pedestrians. The paper explicitly mentions using LLMs and Vision-Language Models (VLMs), making it relevant to the topic of large models. The connection to trajectory prediction is implicit but present due to the domain.", "keywords": ["Large Language Models", "LLMs", "Vision-Language Model", "VLM", "traffic video analysis", "intelligent transportation systems"]}}
{"id": "2511.20848", "pdf": "https://arxiv.org/pdf/2511.20848", "abs": "https://arxiv.org/abs/2511.20848", "authors": ["Tasha Kim", "Yingke Wang", "Hanvit Cho", "Alex Hodges"], "title": "NOIR 2.0: Neural Signal Operated Intelligent Robots for Everyday Activities", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.LG", "eess.SY"], "comment": "Conference on Robot Learning (CoRL 2024), CoRoboLearn", "summary": "Neural Signal Operated Intelligent Robots (NOIR) system is a versatile brain-robot interface that allows humans to control robots for daily tasks using their brain signals. This interface utilizes electroencephalography (EEG) to translate human intentions regarding specific objects and desired actions directly into commands that robots can execute. We present NOIR 2.0, an enhanced version of NOIR. NOIR 2.0 includes faster and more accurate brain decoding algorithms, which reduce task completion time by 46%. NOIR 2.0 uses few-shot robot learning algorithms to adapt to individual users and predict their intentions. The new learning algorithms leverage foundation models for more sample-efficient learning and adaptation (15 demos vs. a single demo), significantly reducing overall human time by 65%.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper describes a brain-robot interface (NOIR 2.0) that uses brain signals to control robots for daily tasks. While the core focus isn't directly on trajectory prediction, it does involve predicting human intentions and translating them into robot actions. The use of foundation models (a type of large model) for few-shot learning and adaptation is also a relevant aspect. The connection to trajectory prediction is indirect, as the robot needs to plan and execute actions based on predicted intentions, but trajectory prediction isn't explicitly mentioned. The use of foundation models increases the relevance.", "keywords": ["foundation models", "brain-robot interface", "intention prediction", "few-shot learning"]}}
{"id": "2511.21053", "pdf": "https://arxiv.org/pdf/2511.21053", "abs": "https://arxiv.org/abs/2511.21053", "authors": ["Chenglizhao Chen", "Shaofeng Liang", "Runwei Guan", "Xiaolou Sun", "Haocheng Zhao", "Haiyun Jiang", "Tao Huang", "Henghui Ding", "Qing-Long Han"], "title": "AerialMind: Towards Referring Multi-Object Tracking in UAV Scenarios", "categories": ["cs.RO", "cs.CV"], "comment": "AAAI 2026", "summary": "Referring Multi-Object Tracking (RMOT) aims to achieve precise object detection and tracking through natural language instructions, representing a fundamental capability for intelligent robotic systems. However, current RMOT research remains mostly confined to ground-level scenarios, which constrains their ability to capture broad-scale scene contexts and perform comprehensive tracking and path planning. In contrast, Unmanned Aerial Vehicles (UAVs) leverage their expansive aerial perspectives and superior maneuverability to enable wide-area surveillance. Moreover, UAVs have emerged as critical platforms for Embodied Intelligence, which has given rise to an unprecedented demand for intelligent aerial systems capable of natural language interaction. To this end, we introduce AerialMind, the first large-scale RMOT benchmark in UAV scenarios, which aims to bridge this research gap. To facilitate its construction, we develop an innovative semi-automated collaborative agent-based labeling assistant (COALA) framework that significantly reduces labor costs while maintaining annotation quality. Furthermore, we propose HawkEyeTrack (HETrack), a novel method that collaboratively enhances vision-language representation learning and improves the perception of UAV scenarios. Comprehensive experiments validated the challenging nature of our dataset and the effectiveness of our method.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on referring multi-object tracking in UAV scenarios, which is related to trajectory prediction and path planning. While it doesn't explicitly use large language models, the mention of natural language interaction and vision-language representation learning suggests a potential connection to LLMs, albeit indirectly. The core focus is on tracking, which is a precursor to trajectory prediction.", "keywords": ["multi-object tracking", "path planning", "natural language interaction", "vision-language representation learning"]}}
{"id": "2511.21280", "pdf": "https://arxiv.org/pdf/2511.21280", "abs": "https://arxiv.org/abs/2511.21280", "authors": ["Jamal Raiyn"], "title": "Improvement of Collision Avoidance in Cut-In Maneuvers Using Time-to-Collision Metrics", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "This paper proposes a new strategy for collision avoidance system leveraging Time-to-Collision (TTC) metrics for handling cut-in scenarios, which are particularly challenging for autonomous vehicles (AVs). By integrating a deep learning with TTC calculations, the system predicts potential collisions and determines appropriate evasive actions compared to traditional TTC -based approaches.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on collision avoidance for autonomous vehicles using Time-to-Collision (TTC) metrics and integrates a deep learning component for predicting potential collisions. While it touches upon trajectory prediction implicitly through collision prediction and avoidance, it doesn't explicitly mention or heavily rely on large language models. The core focus is on collision avoidance strategies using deep learning within the context of autonomous vehicle maneuvers.", "keywords": ["collision avoidance", "autonomous vehicles", "deep learning", "Time-to-Collision", "trajectory prediction"]}}
{"id": "2511.21542", "pdf": "https://arxiv.org/pdf/2511.21542", "abs": "https://arxiv.org/abs/2511.21542", "authors": ["Zhihao Zhan", "Jiaying Zhou", "Likui Zhang", "Qinhan Lv", "Hao Liu", "Jusheng Zhang", "Weizheng Li", "Ziliang Chen", "Tianshui Chen", "Keze Wang", "Liang Lin", "Guangrun Wang"], "title": "$\\mathcal{E}_0$: Enhancing Generalization and Fine-Grained Control in VLA Models via Continuized Discrete Diffusion", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models offer a unified framework for robotic manipulation by integrating visual perception, language understanding, and control generation. Yet existing VLA models still struggle to generalize across diverse tasks, scenes, and camera viewpoints, and often produce coarse or unstable actions. We introduce E0, a continuized discrete diffusion framework that formulates action generation as iterative denoising over quantized action tokens. Compared with continuous diffusion policies, E0 offers two key advantages: (1) discrete action tokens align naturally with the symbolic structure of pretrained VLM/VLA backbones, enabling stronger semantic conditioning; and 2. discrete diffusion matches the true quantized nature of real-world robot control-whose hardware constraints (e.g., encoder resolution, control frequency, actuation latency) inherently discretize continuous signals-and therefore benefits from a Bayes-optimal denoiser that models the correct discrete action distribution, leading to stronger generalization. Compared with discrete autoregressive and mask-based discrete diffusion models, E0 supports a significantly larger and finer-grained action vocabulary and avoids the distributional mismatch introduced by masking-based corruptions-yielding more accurate fine-grained action control. We further introduce a spherical viewpoint perturbation augmentation method to improve robustness to camera shifts without additional data. Experiments on LIBERO, VLABench, and ManiSkill show that E0 achieves state-of-the-art performance across 14 diverse environments, outperforming strong baselines by 10.7% on average. Real-world evaluation on a Franka arm confirms that E0 delivers precise, robust, and transferable manipulation, establishing discrete diffusion as a promising direction for generalizable VLA policy learning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8Vision-Language-Action (VLA)\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\uff0c\u4f7f\u7528\u4e86diffusion\u6a21\u578b\u8fdb\u884c\u52a8\u4f5c\u751f\u6210\uff0c\u5e76\u5229\u7528\u4e86\u9884\u8bad\u7ec3\u7684VLM/VLA backbone\uff0c\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002\u867d\u7136\u8bba\u6587\u63d0\u5230\u4e86\u52a8\u4f5c\u751f\u6210\uff0c\u4f46\u5176\u6838\u5fc3\u662f\u63a7\u5236\uff0c\u800c\u4e0d\u662f\u8f68\u8ff9\u9884\u6d4b\u3002\u4e0d\u8fc7\uff0c\u52a8\u4f5c\u7684\u8fde\u7eed\u6267\u884c\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u8f68\u8ff9\uff0c\u6240\u4ee5\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Vision-Language-Action models", "VLA models", "diffusion", "VLM", "robot control"]}}
{"id": "2511.21584", "pdf": "https://arxiv.org/pdf/2511.21584", "abs": "https://arxiv.org/abs/2511.21584", "authors": ["Haohong Lin", "Yunzhi Zhang", "Wenhao Ding", "Jiajun Wu", "Ding Zhao"], "title": "Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "Published at NeurIPS 2025: https://openreview.net/forum?id=4OLbpaTKJe", "summary": "End-to-end (E2E) autonomous driving models have demonstrated strong performance in open-loop evaluations but often suffer from cascading errors and poor generalization in closed-loop settings. To address this gap, we propose Model-based Policy Adaptation (MPA), a general framework that enhances the robustness and safety of pretrained E2E driving agents during deployment. MPA first generates diverse counterfactual trajectories using a geometry-consistent simulation engine, exposing the agent to scenarios beyond the original dataset. Based on this generated data, MPA trains a diffusion-based policy adapter to refine the base policy's predictions and a multi-step Q value model to evaluate long-term outcomes. At inference time, the adapter proposes multiple trajectory candidates, and the Q value model selects the one with the highest expected utility. Experiments on the nuScenes benchmark using a photorealistic closed-loop simulator demonstrate that MPA significantly improves performance across in-domain, out-of-domain, and safety-critical scenarios. We further investigate how the scale of counterfactual data and inference-time guidance strategies affect overall effectiveness.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autonomous driving, specifically using model-based policy adaptation to improve the robustness and safety of end-to-end driving agents. While it doesn't directly involve Large Language Models, it does heavily deal with trajectory prediction and policy adaptation, especially in the context of generating counterfactual trajectories and refining policy predictions. The use of a diffusion-based policy adapter also suggests a connection to generative modeling techniques often associated with large models, albeit not necessarily LLMs themselves.", "keywords": ["trajectory prediction", "autonomous driving", "policy adaptation", "diffusion-based policy adapter", "counterfactual trajectories"]}}
{"id": "2511.21460", "pdf": "https://arxiv.org/pdf/2511.21460", "abs": "https://arxiv.org/abs/2511.21460", "authors": ["Junjian Wang", "Lidan Zhao", "Xi Sheryl Zhang"], "title": "MADRA: Multi-Agent Debate for Risk-Aware Embodied Planning", "categories": ["cs.AI"], "comment": null, "summary": "Ensuring the safety of embodied AI agents during task planning is critical for real-world deployment, especially in household environments where dangerous instructions pose significant risks. Existing methods often suffer from either high computational costs due to preference alignment training or over-rejection when using single-agent safety prompts. To address these limitations, we propose MADRA, a training-free Multi-Agent Debate Risk Assessment framework that leverages collective reasoning to enhance safety awareness without sacrificing task performance. MADRA employs multiple LLM-based agents to debate the safety of a given instruction, guided by a critical evaluator that scores responses based on logical soundness, risk identification, evidence quality, and clarity. Through iterative deliberation and consensus voting, MADRA significantly reduces false rejections while maintaining high sensitivity to dangerous tasks. Additionally, we introduce a hierarchical cognitive collaborative planning framework that integrates safety, memory, planning, and self-evolution mechanisms to improve task success rates through continuous learning. We also contribute SafeAware-VH, a benchmark dataset for safety-aware task planning in VirtualHome, containing 800 annotated instructions. Extensive experiments on AI2-THOR and VirtualHome demonstrate that our approach achieves over 90% rejection of unsafe tasks while ensuring that safe-task rejection is low, outperforming existing methods in both safety and execution efficiency. Our work provides a scalable, model-agnostic solution for building trustworthy embodied agents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses LLMs for risk assessment and planning in embodied AI agents. While it doesn't directly focus on trajectory prediction, the context of embodied agents performing tasks implies path planning and movement, which is indirectly related to trajectory prediction. The significant use of LLMs justifies a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "embodied AI", "planning"]}}
{"id": "2511.21471", "pdf": "https://arxiv.org/pdf/2511.21471", "abs": "https://arxiv.org/abs/2511.21471", "authors": ["Peiran Xu", "Sudong Wang", "Yao Zhu", "Jianing Li", "Yunjian Zhang"], "title": "SpatialBench: Benchmarking Multimodal Large Language Models for Spatial Cognition", "categories": ["cs.AI"], "comment": null, "summary": "Spatial cognition is fundamental to real-world multimodal intelligence, allowing models to effectively interact with the physical environment. While multimodal large language models (MLLMs) have made significant strides, existing benchmarks often oversimplify spatial cognition, reducing it to a single-dimensional metric, which fails to capture the hierarchical structure and interdependence of spatial abilities. To address this gap, we propose a hierarchical spatial cognition framework that decomposes spatial intelligence into five progressively complex levels from basic observation to high-level planning. Building upon this taxonomy, we construct SpatialBench, a large-scale, fine-grained benchmark covering 15 tasks aligned with these cognitive levels. To provide a unified evaluation across heterogeneous tasks, we further introduce a high-level capability-oriented metric that reliably assesses a model's overall spatial reasoning ability. Extensive experiments over massive MLLMs reveal distinct performance stratification across cognitive levels: models exhibit strong perceptual grounding yet remain limited in symbolic reasoning, causal inference, and planning. Additional human tests demonstrate that humans perform selective, goal-directed abstraction, while MLLMs tend to over-attend to surface details without coherent spatial intent. Our work establishes the first systematic framework for measuring hierarchical spatial cognition in MLLMs, laying the foundation for future spatially intelligent systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on spatial cognition and benchmarks for Multimodal Large Language Models (MLLMs). While it doesn't directly address trajectory prediction, the concept of spatial reasoning and planning, especially in the context of interacting with the physical environment, is relevant to trajectory prediction. The link is not explicit but present.", "keywords": ["Large Language Models", "MLLMs", "spatial cognition", "planning", "spatial reasoning"]}}
{"id": "2511.20993", "pdf": "https://arxiv.org/pdf/2511.20993", "abs": "https://arxiv.org/abs/2511.20993", "authors": ["Shanwei Fan"], "title": "Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game \"Crafter\" demonstrate the effectiveness of our proposed method.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses Large Language Models (LLMs) for high-level planning in reinforcement learning, specifically for decomposing tasks into subgoals. While it doesn't directly address trajectory prediction, the concept of subgoals can be related to planning a sequence of actions, which indirectly relates to trajectory generation. The LLM aspect is prominent. The core of the paper is about using LLMs for planning and refining subgoals for reinforcement learning, which has a tangential connection to trajectory prediction through planning.", "keywords": ["Large language models", "LLMs", "Reinforcement learning", "Planning", "Subgoals"]}}
{"id": "2511.21678", "pdf": "https://arxiv.org/pdf/2511.21678", "abs": "https://arxiv.org/abs/2511.21678", "authors": ["Weihao Bo", "Shan Zhang", "Yanpeng Sun", "Jingjing Wu", "Qunyi Xie", "Xiao Tan", "Kunbin Chen", "Wei He", "Xiaofan Li", "Na Zhao", "Jingdong Wang", "Zechao Li"], "title": "Agentic Learner with Grow-and-Refine Multimodal Semantic Memory", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "MLLMs exhibit strong reasoning on isolated queries, yet they operate de novo -- solving each problem independently and often repeating the same mistakes. Existing memory-augmented agents mainly store past trajectories for reuse. However, trajectory-based memory suffers from brevity bias, gradually losing essential domain knowledge. More critically, even in truly multimodal problem-solving settings, it records only a single-modality trace of past behavior, failing to preserve how visual attention and logical reasoning jointly contributed to the solution. This is fundamentally misaligned with human cognition: semantic memory is both multimodal and integrated, preserving visual and abstract knowledge through coordinated but distinct representational streams. We thus introduce ViLoMem, a dual-stream memory framework that constructs compact, schema-based memory. It separately encodes visual distraction patterns and logical reasoning errors, enabling MLLMs to learn from their successful and failed experiences. Following a grow-and-refine principle, the system incrementally accumulates and updates multimodal semantic knowledge -- preserving stable, generalizable strategies while avoiding catastrophic forgetting. Across six multimodal benchmarks, ViLoMem consistently improves pass@1 accuracy and substantially reduces repeated visual and logical errors. Ablations confirm the necessity of dual-stream memory with explicit distraction--hallucination separation, demonstrating the value of error-aware multimodal memory for lifelong and cross-domain agentic learning. Our project page will be available at https://weihao-bo.github.io/ViLoMeo-page.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684agentic learning\u548c\u8bb0\u5fc6\u589e\u5f3a\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u89c6\u89c9\u548c\u903b\u8f91\u4fe1\u606f\u7684\u7ed3\u5408\u6765\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u6838\u5fc3\u601d\u60f3\uff0c\u5373\u5b66\u4e60\u548c\u8bb0\u5fc6agent\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u95ee\u9898\u4e2d\u5b66\u4e60\u548c\u9884\u6d4b\u7269\u4f53\u8fd0\u52a8\u8f68\u8ff9\u6709\u76f8\u4f3c\u4e4b\u5904\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u7684\u201c\u8fc7\u53bb\u8f68\u8ff9\u7684\u91cd\u7528\u201d\u6697\u793a\u4e86\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u95f4\u63a5\u8054\u7cfb\u3002\u5173\u952e\u8bcd\u5305\u62ecMLLMs, agentic learning, multimodal memory\uff0c\u8fd9\u4e9b\u90fd\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002", "keywords": ["MLLMs", "Large Language Models", "agentic learning", "multimodal memory", "foundation models"]}}
{"id": "2511.21054", "pdf": "https://arxiv.org/pdf/2511.21054", "abs": "https://arxiv.org/abs/2511.21054", "authors": ["Jiaming Guo", "Rui Zhang", "Zerun Li", "Yunkai Gao", "Shaohui Peng", "Siming Lan", "Xing Hu", "Zidong Du", "Xishan Zhang", "Ling Li"], "title": "Efficient Diffusion Planning with Temporal Diffusion", "categories": ["cs.LG"], "comment": "Accepted by the AAAI26 Conference Main Track", "summary": "Diffusion planning is a promising method for learning high-performance policies from offline data. To avoid the impact of discrepancies between planning and reality on performance, previous works generate new plans at each time step. However, this incurs significant computational overhead and leads to lower decision frequencies, and frequent plan switching may also affect performance. In contrast, humans might create detailed short-term plans and more general, sometimes vague, long-term plans, and adjust them over time. Inspired by this, we propose the Temporal Diffusion Planner (TDP) which improves decision efficiency by distributing the denoising steps across the time dimension. TDP begins by generating an initial plan that becomes progressively more vague over time. At each subsequent time step, rather than generating an entirely new plan, TDP updates the previous one with a small number of denoising steps. This reduces the average number of denoising steps, improving decision efficiency. Additionally, we introduce an automated replanning mechanism to prevent significant deviations between the plan and reality. Experiments on D4RL show that, compared to previous works that generate new plans every time step, TDP improves the decision-making frequency by 11-24.8 times while achieving higher or comparable performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving the efficiency of diffusion planning, a method used for learning policies from offline data. While it doesn't explicitly mention trajectory prediction or large language models, diffusion planning is related to motion planning and decision-making, which can be applied to trajectory prediction tasks. The paper does not involve large language models directly, but the planning aspect has some relevance.", "keywords": ["diffusion planning", "offline data", "motion planning", "decision-making"]}}
{"id": "2511.21105", "pdf": "https://arxiv.org/pdf/2511.21105", "abs": "https://arxiv.org/abs/2511.21105", "authors": ["Pushkal Mishra", "Kshitiz Bansal", "Dinesh Bharadia"], "title": "Scaling Foundation Models for Radar Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Radar sensors provide reliable perception across adverse weather, lighting, and long-range conditions. Recent advances in foundation models have transformed visual and language understanding, yet their integration with radar sensing remains largely underexplored. Existing radar approaches are fragmented and task-specific; each downstream task employs distinct architectures and training objectives, preventing transfer across tasks. In this work, we introduce RadarFM: a radar foundation model that learns unified scene-level representations through structured spatial language supervision. We make two key contributions: (1) a structured caption framework that encodes vehicle distributions in native radar coordinates, and (2) a hash-aware contrastive learning objective that quantifies continuous scene similarity rather than binary matching, enabling fine-grained spatial reasoning. Leveraging the CARLA simulator, we generate large-scale, well-annotated radar datasets across diverse driving scenarios. We also propose localization-aware metrics that assess spatial accuracy beyond traditional detection measures.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on foundation models (a type of large model) for radar scene understanding. While it doesn't explicitly mention trajectory prediction, radar data is often used for that purpose, and the ability to understand radar scenes is a crucial component for accurate trajectory prediction. The paper introduces a radar foundation model (RadarFM) and explores its capabilities in understanding spatial relationships, which could indirectly contribute to trajectory prediction tasks.", "keywords": ["foundation models", "large language models", "radar scene understanding", "spatial reasoning"]}}
