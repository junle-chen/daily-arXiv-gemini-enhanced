{"id": "2506.06532", "pdf": "https://arxiv.org/pdf/2506.06532", "abs": "https://arxiv.org/abs/2506.06532", "authors": ["Zijiang Yan", "Hao Zhou", "Jianhua Pei", "Hina Tabassum"], "title": "Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks", "categories": ["cs.LG", "cs.AI", "cs.NI", "cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted in ICML 2025 Workshop on Machine Learning for Wireless\n  Communication and Networks (ML4Wireless)", "summary": "Unmanned aerial vehicles (UAVs) have been widely adopted in various\nreal-world applications. However, the control and optimization of multi-UAV\nsystems remain a significant challenge, particularly in dynamic and constrained\nenvironments. This work explores the joint motion and communication control of\nmultiple UAVs operating within integrated terrestrial and non-terrestrial\nnetworks that include high-altitude platform stations (HAPS). Specifically, we\nconsider an aerial highway scenario in which UAVs must accelerate, decelerate,\nand change lanes to avoid collisions and maintain overall traffic flow.\nDifferent from existing studies, we propose a novel hierarchical and\ncollaborative method based on large language models (LLMs). In our approach, an\nLLM deployed on the HAPS performs UAV access control, while another LLM onboard\neach UAV handles motion planning and control. This LLM-based framework\nleverages the rich knowledge embedded in pre-trained models to enable both\nhigh-level strategic planning and low-level tactical decisions. This\nknowledge-driven paradigm holds great potential for the development of\nnext-generation 3D aerial highway systems. Experimental results demonstrate\nthat our proposed collaborative LLM-based method achieves higher system\nrewards, lower operational costs, and significantly reduced UAV collision rates\ncompared to baseline approaches.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper explicitly combines Large Language Models (LLMs) with the motion planning and control of UAVs, which directly relates to trajectory prediction for multiple agents in a dynamic environment. The abstract mentions motion planning, collision avoidance, and control, all of which are relevant to trajectory prediction. The use of LLMs for both high-level strategic planning and low-level tactical decisions further strengthens the relevance.", "keywords": ["Large Language Models", "LLMs", "motion planning", "UAV", "collision avoidance", "control", "trajectory prediction"]}}
{"id": "2506.06854", "pdf": "https://arxiv.org/pdf/2506.06854", "abs": "https://arxiv.org/abs/2506.06854", "authors": ["Markus Knoche", "Daan de Geus", "Bastian Leibe"], "title": "DONUT: A Decoder-Only Model for Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Predicting the motion of other agents in a scene is highly relevant for\nautonomous driving, as it allows a self-driving car to anticipate. Inspired by\nthe success of decoder-only models for language modeling, we propose DONUT, a\nDecoder-Only Network for Unrolling Trajectories. Different from existing\nencoder-decoder forecasting models, we encode historical trajectories and\npredict future trajectories with a single autoregressive model. This allows the\nmodel to make iterative predictions in a consistent manner, and ensures that\nthe model is always provided with up-to-date information, enhancing the\nperformance. Furthermore, inspired by multi-token prediction for language\nmodeling, we introduce an 'overprediction' strategy that gives the network the\nauxiliary task of predicting trajectories at longer temporal horizons. This\nallows the model to better anticipate the future, and further improves the\nperformance. With experiments, we demonstrate that our decoder-only approach\noutperforms the encoder-decoder baseline, and achieves new state-of-the-art\nresults on the Argoverse 2 single-agent motion forecasting benchmark.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u6307\u51fa\u5176\u7814\u7a76\u65b9\u5411\u662f\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u4e14\u91c7\u7528\u4e86decoder-only\u6a21\u578b\uff0c\u53d7\u5230\u8bed\u8a00\u6a21\u578b\u7684\u542f\u53d1\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u5747\u76f8\u5173\u3002", "keywords": ["trajectory prediction", "decoder-only model", "autonomous driving", "language modeling", "autoregressive model", "motion forecasting"]}}
{"id": "2506.06694", "pdf": "https://arxiv.org/pdf/2506.06694", "abs": "https://arxiv.org/abs/2506.06694", "authors": ["Yuan Yuan", "Yukun Liu", "Chonghua Han", "Jie Feng", "Yong Li"], "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper explicitly discusses building \"mobility foundation models,\" which falls under the umbrella of large models. It also focuses on mobility data and trajectory generation, indicating relevance to trajectory prediction. The use of generative continual learning for mobility data further strengthens the connection between the two areas.", "keywords": ["foundation models", "generative continual learning", "mobility", "trajectories", "transformer", "mobility data"]}}
{"id": "2506.06301", "pdf": "https://arxiv.org/pdf/2506.06301", "abs": "https://arxiv.org/abs/2506.06301", "authors": ["Muhammad Monjurul Karim", "Yan Shi", "Shucheng Zhang", "Bingzhang Wang", "Mehrdad Nasri", "Yinhai Wang"], "title": "Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review", "categories": ["cs.AI"], "comment": null, "summary": "Roadway safety and mobility remain critical challenges for modern\ntransportation systems, demanding innovative analytical frameworks capable of\naddressing complex, dynamic, and heterogeneous environments. While traditional\nengineering methods have made progress, the complexity and dynamism of\nreal-world traffic necessitate more advanced analytical frameworks. Large\nLanguage Models (LLMs), with their unprecedented capabilities in natural\nlanguage understanding, knowledge integration, and reasoning, represent a\npromising paradigm shift. This paper comprehensively reviews the application\nand customization of LLMs for enhancing roadway safety and mobility. A key\nfocus is how LLMs are adapted -- via architectural, training, prompting, and\nmultimodal strategies -- to bridge the \"modality gap\" with transportation's\nunique spatio-temporal and physical data. The review systematically analyzes\ndiverse LLM applications in mobility (e.g., traffic flow prediction, signal\ncontrol) and safety (e.g., crash analysis, driver behavior assessment,).\nEnabling technologies such as V2X integration, domain-specific foundation\nmodels, explainability frameworks, and edge computing are also examined.\nDespite significant potential, challenges persist regarding inherent LLM\nlimitations (hallucinations, reasoning deficits), data governance (privacy,\nbias), deployment complexities (sim-to-real, latency), and rigorous safety\nassurance. Promising future research directions are highlighted, including\nadvanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI\ncollaboration, continuous learning, and the development of efficient,\nverifiable systems. This review provides a structured roadmap of current\ncapabilities, limitations, and opportunities, underscoring LLMs' transformative\npotential while emphasizing the need for responsible innovation to realize\nsafer, more intelligent transportation systems.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u660e\u786e\u63d0\u53caLarge Language Models (LLMs)\u53ca\u5176\u5728\u4ea4\u901a\u5b89\u5168\u548c\u79fb\u52a8\u6027\u589e\u5f3a\u65b9\u9762\u7684\u5e94\u7528\u3002\u6458\u8981\u4e2d\u63d0\u5230LLMs\u88ab\u5e94\u7528\u4e8e\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u7b49\u9886\u57df\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u884c\u4eba/\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u8be5\u8bba\u6587\u540c\u65f6\u6d89\u53caLLMs\u548c\u4ea4\u901a\u9886\u57df\u5e94\u7528\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "traffic flow prediction", "roadway safety", "mobility enhancement", "spatio-temporal"]}}
{"id": "2506.07725", "pdf": "https://arxiv.org/pdf/2506.07725", "abs": "https://arxiv.org/abs/2506.07725", "authors": ["Shadi Hamdan", "Chonghao Sima", "Zetong Yang", "Hongyang Li", "Fatma G\u00fcney"], "title": "ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 submission. For code, see\n  https://github.com/opendrivelab/ETA", "summary": "How can we benefit from large models without sacrificing inference speed, a\ncommon dilemma in self-driving systems? A prevalent solution is a dual-system\narchitecture, employing a small model for rapid, reactive decisions and a\nlarger model for slower but more informative analyses. Existing dual-system\ndesigns often implement parallel architectures where inference is either\ndirectly conducted using the large model at each current frame or retrieved\nfrom previously stored inference results. However, these works still struggle\nto enable large models for a timely response to every online frame. Our key\ninsight is to shift intensive computations of the current frame to previous\ntime steps and perform a batch inference of multiple time steps to make large\nmodels respond promptly to each time step. To achieve the shifting, we\nintroduce Efficiency through Thinking Ahead (ETA), an asynchronous system\ndesigned to: (1) propagate informative features from the past to the current\nframe using future predictions from the large model, (2) extract current frame\nfeatures using a small model for real-time responsiveness, and (3) integrate\nthese dual features via an action mask mechanism that emphasizes\naction-critical image regions. Evaluated on the Bench2Drive CARLA\nLeaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with\na driving score of 69.53 while maintaining a near-real-time inference speed at\n50 ms.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u63d0\u5230\u4e86Large Models\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u4e14\u63d0\u5230\u4e86\u5229\u7528\u5927\u6a21\u578b\u8fdb\u884c\u672a\u6765\u9884\u6d4b\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u201c\u8f68\u8ff9\u9884\u6d4b\u201d\uff0c\u4f46\u662f\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u548c\u51b3\u7b56\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Models", "self-driving", "inference", "future predictions", "action prediction"]}}
{"id": "2506.06472", "pdf": "https://arxiv.org/pdf/2506.06472", "abs": "https://arxiv.org/abs/2506.06472", "authors": ["Ziqi Yuan", "Haoyang Zhang", "Yirui Eric Zhou", "Apoorve Mohan", "I-Hsin Chung", "Seetharami Seelam", "Jian Huang"], "title": "Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "We present the design and implementation of a new lifetime-aware tensor\noffloading framework for GPU memory expansion using low-cost PCIe-based\nsolid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for\nlarge language model (LLM) training with multiple GPUs and multiple SSDs. Its\ndesign is driven by our observation that the active tensors take only a small\nfraction (1.7% on average) of allocated GPU memory in each LLM training\niteration, the inactive tensors are usually large and will not be used for a\nlong period of time, creating ample opportunities for offloading/prefetching\ntensors to/from slow SSDs without stalling the GPU training process. TERAIO\naccurately estimates the lifetime (active period of time in GPU memory) of each\ntensor with the profiling of the first few iterations in the training process.\nWith the tensor lifetime analysis, TERAIO will generate an optimized tensor\noffloading/prefetching plan and integrate it into the compiled LLM program via\nPyTorch. TERAIO has a runtime tensor migration engine to execute the\noffloading/prefetching plan via GPUDirect storage, which allows direct tensor\nmigration between GPUs and SSDs for alleviating the CPU bottleneck and\nmaximizing the SSD bandwidth utilization. In comparison with state-of-the-art\nstudies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves\nthe training performance of various LLMs by 1.47x on average, and achieves\n80.7% of the ideal performance assuming unlimited GPU memory.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8bad\u7ec3\uff0c\u7279\u522b\u662f\u5982\u4f55\u901a\u8fc7\u4f18\u5316GPU\u5185\u5b58\u7ba1\u7406\u6765\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5b83\u4e0e\u5927\u6a21\u578b\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u53ef\u80fd\u5bf9\u5c06\u6765\u5728\u5927\u6a21\u578b\u4e0a\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\u6709\u95f4\u63a5\u5f71\u54cd\u3002", "keywords": ["large language model", "LLM", "GPU", "training", "tensor offloading"]}}
{"id": "2506.06487", "pdf": "https://arxiv.org/pdf/2506.06487", "abs": "https://arxiv.org/abs/2506.06487", "authors": ["Zibo Zhou", "Yue Hu", "Lingkai Zhang", "Zonglin Li", "Siheng Chen"], "title": "BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation", "categories": ["cs.RO"], "comment": null, "summary": "Zero-shot object navigation (ZSON) allows robots to find target objects in\nunfamiliar environments using natural language instructions, without relying on\npre-built maps or task-specific training. Recent general-purpose models, such\nas large language models (LLMs) and vision-language models (VLMs), equip agents\nwith semantic reasoning abilities to estimate target object locations in a\nzero-shot manner. However, these models often greedily select the next goal\nwithout maintaining a global understanding of the environment and are\nfundamentally limited in the spatial reasoning necessary for effective\nnavigation. To overcome these limitations, we propose a novel 3D voxel-based\nbelief map that estimates the target's prior presence distribution within a\nvoxelized 3D space. This approach enables agents to integrate semantic priors\nfrom LLMs and visual embeddings with hierarchical spatial structure, alongside\nreal-time observations, to build a comprehensive 3D global posterior belief of\nthe target's location. Building on this 3D voxel map, we introduce\nBeliefMapNav, an efficient navigation system with two key advantages: i)\ngrounding LLM semantic reasoning within the 3D hierarchical semantics voxel\nspace for precise target position estimation, and ii) integrating sequential\npath planning to enable efficient global navigation decisions. Experiments on\nHM3D, MP3D, and HSSD benchmarks show that BeliefMapNav achieves\nstate-of-the-art (SOTA) Success Rate (SR) and Success weighted by Path Length\n(SPL), with a notable 46.4% SPL improvement over the previous best SR method,\nvalidating its effectiveness and efficiency.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on zero-shot object navigation, which involves path planning and decision-making, making it related to trajectory prediction. It also explicitly uses Large Language Models (LLMs) to enhance the navigation process. Therefore, it combines both trajectory prediction (through navigation) and large language models.", "keywords": ["object navigation", "large language models", "LLMs", "path planning", "spatial reasoning", "navigation"]}}
{"id": "2506.06570", "pdf": "https://arxiv.org/pdf/2506.06570", "abs": "https://arxiv.org/abs/2506.06570", "authors": ["Aryaman Gupta", "Yusuf Umut Ciftci", "Somil Bansal"], "title": "Enhancing Robot Safety via MLLM-Based Semantic Interpretation of Failure Data", "categories": ["cs.RO"], "comment": null, "summary": "As robotic systems become increasingly integrated into real-world\nenvironments, ranging from autonomous vehicles to household assistants, they\ninevitably encounter diverse and unstructured scenarios that lead to failures.\nWhile such failures pose safety and reliability challenges, they also provide\nrich perceptual data for improving future performance. However, manually\nanalyzing large-scale failure datasets is impractical. In this work, we present\na method for automatically organizing large-scale robotic failure data into\nsemantically meaningful clusters, enabling scalable learning from failure\nwithout human supervision. Our approach leverages the reasoning capabilities of\nMultimodal Large Language Models (MLLMs), trained on internet-scale data, to\ninfer high-level failure causes from raw perceptual trajectories and discover\ninterpretable structure within uncurated failure logs. These semantic clusters\nreveal latent patterns and hypothesized causes of failure, enabling scalable\nlearning from experience. We demonstrate that the discovered failure modes can\nguide targeted data collection for policy refinement, accelerating iterative\nimprovement in agent policies and overall safety. Additionally, we show that\nthese semantic clusters can be employed for online failure detection, offering\na lightweight yet powerful safeguard for real-time adaptation. We demonstrate\nthat this framework enhances robot learning and robustness by transforming\nreal-world failures into actionable and interpretable signals for adaptation.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on using Multimodal Large Language Models (MLLMs) to analyze robot failure data, which includes perceptual trajectories. While not directly predicting trajectories, it uses LLMs to understand and interpret them in the context of robot failures. This connects to both trajectory prediction (through the analysis of trajectories) and large language models.", "keywords": ["Multimodal Large Language Models", "MLLMs", "perceptual trajectories", "robot failures"]}}
{"id": "2506.06659", "pdf": "https://arxiv.org/pdf/2506.06659", "abs": "https://arxiv.org/abs/2506.06659", "authors": ["Wenhao Yao", "Zhenxin Li", "Shiyi Lan", "Zi Wang", "Xinglong Sun", "Jose M. Alvarez", "Zuxuan Wu"], "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "15 pages, 6 figures", "summary": "In complex driving environments, autonomous vehicles must navigate safely.\nRelying on a single predicted path, as in regression-based approaches, usually\ndoes not explicitly assess the safety of the predicted trajectory.\nSelection-based methods address this by generating and scoring multiple\ntrajectory candidates and predicting the safety score for each, but face\noptimization challenges in precisely selecting the best option from thousands\nof possibilities and distinguishing subtle but safety-critical differences,\nespecially in rare or underrepresented scenarios. We propose DriveSuprim to\novercome these challenges and advance the selection-based paradigm through a\ncoarse-to-fine paradigm for progressive candidate filtering, a rotation-based\naugmentation method to improve robustness in out-of-distribution scenarios, and\na self-distillation framework to stabilize training. DriveSuprim achieves\nstate-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS\nin NAVSIM v2 without extra data, demonstrating superior safetycritical\ncapabilities, including collision avoidance and compliance with rules, while\nmaintaining high trajectory quality in various driving scenarios.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f68\u8ff9\u9009\u62e9\u65b9\u6cd5\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u76ee\u6807\u662f\u63d0\u9ad8\u8f68\u8ff9\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u9ad8\u5ea6\u76f8\u5173\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u751f\u6210\u548c\u8bc4\u4f30\u591a\u4e2a\u8f68\u8ff9\u5019\u9009\uff0c\u4ee5\u53ca\u4f18\u5316\u9009\u62e9\u6700\u4f73\u8f68\u8ff9\uff0c\u8fd9\u4e9b\u90fd\u662f\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\u3002", "keywords": ["trajectory prediction", "autonomous vehicles", "trajectory selection", "collision avoidance", "driving scenarios"]}}
{"id": "2506.06664", "pdf": "https://arxiv.org/pdf/2506.06664", "abs": "https://arxiv.org/abs/2506.06664", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Joshua Chen", "Nadine Chang", "Maying Shen", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "categories": ["cs.RO", "cs.CV"], "comment": "The 1st place solution of the End-to-end Driving Track at the CVPR\n  2025 Autonomous Grand Challenge", "summary": "End-to-end multi-modal planning is a promising paradigm in autonomous\ndriving, enabling decision-making with diverse trajectory candidates. A key\ncomponent is a robust trajectory scorer capable of selecting the optimal\ntrajectory from these candidates. While recent trajectory scorers focus on\nscoring either large sets of static trajectories or small sets of dynamically\ngenerated ones, both approaches face significant limitations in generalization.\nStatic vocabularies provide effective coarse discretization but struggle to\nmake fine-grained adaptation, while dynamic proposals offer detailed precision\nbut fail to capture broader trajectory distributions. To overcome these\nchallenges, we propose GTRS (Generalized Trajectory Scoring), a unified\nframework for end-to-end multi-modal planning that combines coarse and\nfine-grained trajectory evaluation. GTRS consists of three complementary\ninnovations: (1) a diffusion-based trajectory generator that produces diverse\nfine-grained proposals; (2) a vocabulary generalization technique that trains a\nscorer on super-dense trajectory sets with dropout regularization, enabling its\nrobust inference on smaller subsets; and (3) a sensor augmentation strategy\nthat enhances out-of-domain generalization while incorporating refinement\ntraining for critical trajectory discrimination. As the winning solution of the\nNavsim v2 Challenge, GTRS demonstrates superior performance even with\nsub-optimal sensor inputs, approaching privileged methods that rely on\nground-truth perception. Code will be available at\nhttps://github.com/NVlabs/GTRS.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u7aef\u5230\u7aef\u7684\u6a21\u6001\u89c4\u5212\u548c\u8f68\u8ff9\u8bc4\u5206\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u8f68\u8ff9\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u662f\u5176\u76ee\u6807\u662f\u63d0\u9ad8\u8f68\u8ff9\u9884\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u9ad8\u5ea6\u76f8\u5173\u3002\u8bba\u6587\u4e2d\u63d0\u5230\u7684diffusion model\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u751f\u6210\u6a21\u578b\u7684\u4e00\u79cd\uff0c\u4f46\u4e0eLLM\u5173\u7cfb\u4e0d\u5927\u3002", "keywords": ["trajectory prediction", "trajectory scoring", "multi-modal planning", "diffusion model", "autonomous driving"]}}
{"id": "2506.06690", "pdf": "https://arxiv.org/pdf/2506.06690", "abs": "https://arxiv.org/abs/2506.06690", "authors": ["Hao Wang", "Chengkai Hou", "Xianglong Li", "Yankai Fu", "Chenxuan Li", "Ning Chen", "Gaole Dai", "Jiaming Liu", "Tiejun Huang", "Shanghang Zhang"], "title": "SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning to control high-speed objects in the real world remains a\nchallenging frontier in robotics. Table tennis serves as an ideal testbed for\nthis problem, demanding both rapid interception of fast-moving balls and\nprecise adjustment of their trajectories. This task presents two fundamental\nchallenges: it requires a high-precision vision system capable of accurately\npredicting ball trajectories, and it necessitates intelligent strategic\nplanning to ensure precise ball placement to target regions. The dynamic nature\nof table tennis, coupled with its real-time response requirements, makes it\nparticularly well-suited for advancing robotic control capabilities in\nfast-paced, precision-critical domains. In this paper, we present\nSpikePingpong, a novel system that integrates spike-based vision with imitation\nlearning for high-precision robotic table tennis. Our approach introduces two\nkey attempts that directly address the aforementioned challenges: SONIC, a\nspike camera-based module that achieves millimeter-level precision in\nball-racket contact prediction by compensating for real-world uncertainties\nsuch as air resistance and friction; and IMPACT, a strategic planning module\nthat enables accurate ball placement to targeted table regions. The system\nharnesses a 20 kHz spike camera for high-temporal resolution ball tracking,\ncombined with efficient neural network models for real-time trajectory\ncorrection and stroke planning. Experimental results demonstrate that\nSpikePingpong achieves a remarkable 91% success rate for 30 cm accuracy target\narea and 71% in the more challenging 20 cm accuracy task, surpassing previous\nstate-of-the-art approaches by 38% and 37% respectively. These significant\nperformance improvements enable the robust implementation of sophisticated\ntactical gameplay strategies, providing a new research perspective for robotic\ncontrol in high-speed dynamic tasks.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528spike vision\u8fdb\u884c\u9ad8\u901f\u8fd0\u52a8\u7269\u4f53\u7684\u63a7\u5236\uff0c\u7279\u522b\u662f\u4e52\u4e53\u7403\u8fd0\u52a8\u4e2d\u7403\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u51fb\u6253\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u8f68\u8ff9\u9884\u6d4b\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "ball trajectory", "robot learning", "imitation learning", "strategic planning", "spike camera", "neural network models"]}}
{"id": "2506.06862", "pdf": "https://arxiv.org/pdf/2506.06862", "abs": "https://arxiv.org/abs/2506.06862", "authors": ["Chenguang Huang", "Oier Mees", "Andy Zeng", "Wolfram Burgard"], "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "accepted to International Journal of Robotics Research (IJRR). 24\n  pages, 18 figures. The paper contains texts from VLMaps(arXiv:2210.05714) and\n  AVLMaps(arXiv:2303.07522). The project page is https://mslmaps.github.io/", "summary": "Grounding language to a navigating agent's observations can leverage\npretrained multimodal foundation models to match perceptions to object or event\ndescriptions. However, previous approaches remain disconnected from environment\nmapping, lack the spatial precision of geometric maps, or neglect additional\nmodality information beyond vision. To address this, we propose multimodal\nspatial language maps as a spatial map representation that fuses pretrained\nmultimodal features with a 3D reconstruction of the environment. We build these\nmaps autonomously using standard exploration. We present two instances of our\nmaps, which are visual-language maps (VLMaps) and their extension to\naudio-visual-language maps (AVLMaps) obtained by adding audio information. When\ncombined with large language models (LLMs), VLMaps can (i) translate natural\nlanguage commands into open-vocabulary spatial goals (e.g., \"in between the\nsofa and TV\") directly localized in the map, and (ii) be shared across\ndifferent robot embodiments to generate tailored obstacle maps on demand.\nBuilding upon the capabilities above, AVLMaps extend VLMaps by introducing a\nunified 3D spatial representation integrating audio, visual, and language cues\nthrough the fusion of features from pretrained multimodal foundation models.\nThis enables robots to ground multimodal goal queries (e.g., text, images, or\naudio snippets) to spatial locations for navigation. Additionally, the\nincorporation of diverse sensory inputs significantly enhances goal\ndisambiguation in ambiguous environments. Experiments in simulation and\nreal-world settings demonstrate that our multimodal spatial language maps\nenable zero-shot spatial and multimodal goal navigation and improve recall by\n50% in ambiguous scenarios. These capabilities extend to mobile robots and\ntabletop manipulators, supporting navigation and interaction guided by visual,\naudio, and spatial cues.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u7814\u7a76\u7684\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u5b58\u5728\u5173\u8054\u3002\u6b64\u5916\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u5c06\u5b83\u4eec\u4e0e\u7a7a\u95f4\u5730\u56fe\u8fdb\u884c\u878d\u5408\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u7684\u5bfc\u822a\u3002\u56e0\u6b64\uff0c\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["large language models", "LLMs", "multimodal foundation models", "robot navigation", "spatial language maps"]}}
{"id": "2506.07509", "pdf": "https://arxiv.org/pdf/2506.07509", "abs": "https://arxiv.org/abs/2506.07509", "authors": ["Shoon Kit Lim", "Melissa Jia Ying Chong", "Jing Huey Khor", "Ting Yang Ling"], "title": "Taking Flight with Dialogue: Enabling Natural Language Control for PX4-based Drone Agent", "categories": ["cs.RO", "I.2.7; I.2.9; I.2.10"], "comment": "Source code available at:\n  https://github.com/limshoonkit/ros2-agent-ws", "summary": "Recent advances in agentic and physical artificial intelligence (AI) have\nlargely focused on ground-based platforms such as humanoid and wheeled robots,\nleaving aerial robots relatively underexplored. Meanwhile, state-of-the-art\nunmanned aerial vehicle (UAV) multimodal vision-language systems typically rely\non closed-source models accessible only to well-resourced organizations. To\ndemocratize natural language control of autonomous drones, we present an\nopen-source agentic framework that integrates PX4-based flight control, Robot\nOperating System 2 (ROS 2) middleware, and locally hosted models using Ollama.\nWe evaluate performance both in simulation and on a custom quadcopter platform,\nbenchmarking four large language model (LLM) families for command generation\nand three vision-language model (VLM) families for scene understanding.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper focuses on controlling drones with natural language using large language models. While not directly addressing trajectory prediction, it involves controlling the drone's movement and path, which is related. It also explicitly utilizes and benchmarks LLMs.", "keywords": ["large language model", "LLM", "natural language control", "drones", "UAV", "flight control"]}}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382", "abs": "https://arxiv.org/abs/2506.06382", "authors": ["Micha\u0142 P. Karpowicz"], "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": null, "summary": "This paper explains \\textbf{why it is impossible to create large language\nmodels that do not hallucinate and what are the trade-offs we should be looking\nfor}. It presents a formal \\textbf{impossibility theorem} demonstrating that no\ninference mechanism can simultaneously satisfy four fundamental properties:\n\\textbf{truthful (non-hallucinatory) generation, semantic information\nconservation, relevant knowledge revelation, and knowledge-constrained\noptimality}. By modeling LLM inference as an \\textbf{auction of ideas} where\nneural components compete to contribute to responses, we prove the\nimpossibility using the Green-Laffont theorem. That mathematical framework\nprovides a rigorous foundation for understanding the nature of inference\nprocess, with implications for model architecture, training objectives, and\nevaluation methods.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u8ba8\u8bba\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u5173\u6ce8\u7684\u4e3b\u9898\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u8bba\u6587\u5173\u952e\u8bcd\u4e5f\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002", "keywords": ["Large Language Models", "LLM", "hallucination"]}}
{"id": "2506.07860", "pdf": "https://arxiv.org/pdf/2506.07860", "abs": "https://arxiv.org/abs/2506.07860", "authors": ["Ivan Alberico", "Marco Cannici", "Giovanni Cioffi", "Davide Scaramuzza"], "title": "Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction", "categories": ["cs.CV"], "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), Nashville (TN), USA, 2025; 5th International Workshop on\n  Event-Based Vision", "summary": "In this paper, we present a real-time egocentric trajectory prediction system\nfor table tennis using event cameras. Unlike standard cameras, which suffer\nfrom high latency and motion blur at fast ball speeds, event cameras provide\nhigher temporal resolution, allowing more frequent state updates, greater\nrobustness to outliers, and accurate trajectory predictions using just a short\ntime window after the opponent's impact. We collect a dataset of ping-pong game\nsequences, including 3D ground-truth trajectories of the ball, synchronized\nwith sensor data from the Meta Project Aria glasses and event streams. Our\nsystem leverages foveated vision, using eye-gaze data from the glasses to\nprocess only events in the viewer's fovea. This biologically inspired approach\nimproves ball detection performance and significantly reduces computational\nlatency, as it efficiently allocates resources to the most perceptually\nrelevant regions, achieving a reduction factor of 10.81 on the collected\ntrajectories. Our detection pipeline has a worst-case total latency of 4.5 ms,\nincluding computation and perception - significantly lower than a frame-based\n30 FPS system, which, in the worst case, takes 66 ms solely for perception.\nFinally, we fit a trajectory prediction model to the estimated states of the\nball, enabling 3D trajectory forecasting in the future. To the best of our\nknowledge, this is the first approach to predict table tennis trajectories from\nan egocentric perspective using event cameras.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on trajectory prediction, specifically for ping pong balls. While it doesn't directly involve large language models, the core topic of trajectory prediction is present. The use of event cameras and real-time prediction systems further supports its relevance, although the lack of any LLM component reduces the score.", "keywords": ["trajectory prediction", "event cameras", "real-time", "3D trajectory", "forecasting"]}}
{"id": "2506.07424", "pdf": "https://arxiv.org/pdf/2506.07424", "abs": "https://arxiv.org/abs/2506.07424", "authors": ["Kyeonghyun Kim", "Jinhee Jang", "Juhwan Choi", "Yoonji Lee", "Kyohoon Jin", "YoungBin Kim"], "title": "Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main conference", "summary": "Large language models (LLMs) are renowned for their extensive linguistic\nknowledge and strong generalization capabilities, but their high computational\ndemands make them unsuitable for resource-constrained environments. In\ncontrast, small language models (SLMs) are computationally efficient but often\nlack the broad generalization capacity of LLMs. To bridge this gap, we propose\nPiFi, a novel framework that combines the strengths of both LLMs and SLMs to\nachieve high performance while maintaining efficiency. PiFi integrates a single\nfrozen layer from an LLM into a SLM and fine-tunes the combined model for\nspecific tasks, boosting performance without a significant increase in\ncomputational cost. We show that PiFi delivers consistent performance\nimprovements across a range of natural language processing tasks, including\nboth natural language understanding and generation. Moreover, our findings\ndemonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing\ngeneralization to unseen domains and facilitating the transfer of linguistic\nabilities.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on bridging the gap between small and large language models, specifically using a plug-in and fine-tuning approach. While it doesn't directly address trajectory prediction, its core focus on large language models makes it relevant. The techniques discussed could potentially be applied to trajectory prediction tasks using LLMs.", "keywords": ["Large Language Models", "LLMs", "Small Language Models", "SLMs", "fine-tuning", "generalization"]}}
{"id": "2506.06560", "pdf": "https://arxiv.org/pdf/2506.06560", "abs": "https://arxiv.org/abs/2506.06560", "authors": ["Mihir Dharmadhikari", "Kostas Alexis"], "title": "Semantics-aware Predictive Inspection Path Planning", "categories": ["cs.RO"], "comment": "Accepted at IEEE Transactions on Field Robotics", "summary": "This paper presents a novel semantics-aware inspection path planning paradigm\ncalled \"Semantics-aware Predictive Planning\" (SPP). Industrial environments\nthat require the inspection of specific objects or structures (called\n\"semantics\"), such as ballast water tanks inside ships, often present\nstructured and repetitive spatial arrangements of the semantics of interest.\nMotivated by this, we first contribute an algorithm that identifies spatially\nrepeating patterns of semantics - exact or inexact - in a semantic scene graph\nrepresentation and makes predictions about the evolution of the graph in the\nunseen parts of the environment using these patterns. Furthermore, two\ninspection path planning strategies, tailored to ballast water tank inspection,\nthat exploit these predictions are proposed. To assess the performance of the\nnovel predictive planning paradigm, both simulation and experimental\nevaluations are performed. First, we conduct a simulation study comparing the\nmethod against relevant state-of-the-art techniques and further present tests\nshowing its ability to handle imperfect patterns. Second, we deploy our method\nonboard a collision-tolerant aerial robot operating inside the ballast tanks of\ntwo real ships. The results, both in simulation and field experiments,\ndemonstrate significant improvement over the state-of-the-art in terms of\ninspection time while maintaining equal or better semantic surface coverage. A\nset of videos describing the different parts of the method and the field\ndeployments is available at https://tinyurl.com/spp-videos. The code for this\nwork is made available at https://github.com/ntnu-arl/predictive_planning_ros.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u63d0\u5230\u4e86\"predictive planning\" \u548c \"path planning\"\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\"semantic scene graph\"\u548c\"predictions about the evolution of the graph\"\u53ef\u80fd\u6697\u793a\u4f7f\u7528\u4e86\u67d0\u79cd\u9884\u6d4b\u6a21\u578b\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["predictive planning", "path planning", "predictions", "semantic scene graph"]}}
{"id": "2506.06677", "pdf": "https://arxiv.org/pdf/2506.06677", "abs": "https://arxiv.org/abs/2506.06677", "authors": ["Songhao Han", "Boxiang Qiu", "Yue Liao", "Siyuan Huang", "Chen Gao", "Shuicheng Yan", "Si Liu"], "title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "categories": ["cs.RO", "cs.CV"], "comment": "23 pages, 18 figures", "summary": "Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on robotic manipulation and uses VLMs (Vision-Language Models) for high-level planning in a hierarchical framework. While it doesn't directly address trajectory prediction in the conventional sense (e.g., predicting pedestrian or vehicle trajectories), the 'trajectories' mentioned in the abstract refer to the action sequences of the robot. The use of large language models (GPT) for task instruction generation and VLMs for planning connects it to the 'Large Models' aspect. The connection to trajectory prediction is weaker but present through the robotic action sequences.", "keywords": ["vision-language models", "VLMs", "GPT", "robotic manipulation", "long-horizon planning", "trajectories"]}}
{"id": "2506.06683", "pdf": "https://arxiv.org/pdf/2506.06683", "abs": "https://arxiv.org/abs/2506.06683", "authors": ["Shiying Duan", "Pei Ren", "Nanxiang Jiang", "Zhengping Che", "Jian Tang", "Yifan Sun", "Zhaoxin Fan", "Wenjun Wu"], "title": "RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Dual-arm robots play a crucial role in improving efficiency and flexibility\nin complex multitasking scenarios. While existing methods have achieved\npromising results in task planning, they often fail to fully optimize task\nparallelism, limiting the potential of dual-arm collaboration. To address this\nissue, we propose RoboPARA, a novel large language model (LLM)-driven framework\nfor dual-arm task parallelism planning. RoboPARA employs a two-stage process:\n(1) Dependency Graph-based Planning Candidates Generation, which constructs\ndirected acyclic graphs (DAGs) to model task dependencies and eliminate\nredundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which\noptimizes DAG traversal to maximize parallelism while maintaining task\ncoherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task\ndataset (X-DAPT dataset), the first dataset specifically designed to evaluate\ndual-arm task parallelism across diverse scenarios and difficulty levels.\nExtensive experiments on the X-DAPT dataset demonstrate that RoboPARA\nsignificantly outperforms existing methods, achieving higher efficiency and\nreliability, particularly in complex task combinations. The code and dataset\nwill be released upon acceptance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u53cc\u81c2\u673a\u5668\u4eba\u7684\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u63d0\u9ad8\u4efb\u52a1\u7684\u5e76\u884c\u6548\u7387\u3002\u867d\u7136\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\uff0c\u4f46\u6d89\u53ca\u4e86\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u548cLLM\u7684\u5e94\u7528\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Model", "LLM", "task planning", "robot planning"]}}
{"id": "2506.06725", "pdf": "https://arxiv.org/pdf/2506.06725", "abs": "https://arxiv.org/abs/2506.06725", "authors": ["Guillaume Levy", "Cedric Colas", "Pierre-Yves Oudeyer", "Thomas Carta", "Clement Romac"], "title": "WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) possess general world knowledge but often\nstruggle to generate precise predictions in structured, domain-specific\ncontexts such as simulations. These limitations arise from their inability to\nground their broad, unstructured understanding in specific environments. To\naddress this, we present WorldLLM, a framework that enhances LLM-based world\nmodeling by combining Bayesian inference and autonomous active exploration with\nreinforcement learning. WorldLLM leverages the in-context learning abilities of\nLLMs to guide an LLM-based world model's predictions using natural language\nhypotheses given in its prompt. These hypotheses are iteratively refined\nthrough a Bayesian inference framework that leverages a second LLM as the\nproposal distribution given collected evidence. This evidence is collected\nusing a curiosity-driven reinforcement learning policy that explores the\nenvironment to find transitions with a low log-likelihood under our LLM-based\npredictive model using the current hypotheses. By alternating between refining\nhypotheses and collecting new evidence, our framework autonomously drives\ncontinual improvement of the predictions. Our experiments demonstrate the\neffectiveness of WorldLLM in a textual game environment that requires agents to\nmanipulate and combine objects. The framework not only enhances predictive\naccuracy, but also generates human-interpretable theories of environment\ndynamics.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses improving LLMs' world modeling capabilities, which could be relevant to trajectory prediction if the 'world' being modeled involves predicting the future states or trajectories of objects. The paper focuses on improving the predictive accuracy of LLMs in structured environments using Bayesian inference and reinforcement learning, which are relevant to trajectory prediction. However, it doesn't explicitly mention trajectory prediction or related tasks. The connection is more about improving general prediction capabilities of LLMs which could be applied to trajectory prediction.", "keywords": ["Large Language Models", "LLMs", "world modeling", "prediction", "Bayesian inference", "reinforcement learning"]}}
{"id": "2506.07004", "pdf": "https://arxiv.org/pdf/2506.07004", "abs": "https://arxiv.org/abs/2506.07004", "authors": ["Zhe Huang", "Ye-Ji Mun", "Fatemeh Cheraghi Pouria", "Katherine Driggs-Campbell"], "title": "Hierarchical Intention Tracking with Switching Trees for Real-Time Adaptation to Dynamic Human Intentions during Collaboration", "categories": ["cs.RO"], "comment": "15 pages, 10 figures", "summary": "During collaborative tasks, human behavior is guided by multiple levels of\nintentions that evolve over time, such as task sequence preferences and\ninteraction strategies. To adapt to these changing preferences and promptly\ncorrect any inaccurate estimations, collaborative robots must accurately track\nthese dynamic human intentions in real time. We propose a Hierarchical\nIntention Tracking (HIT) algorithm for collaborative robots to track dynamic\nand hierarchical human intentions effectively in real time. HIT represents\nhuman intentions as intention trees with arbitrary depth, and probabilistically\ntracks human intentions by Bayesian filtering, upward measurement propagation,\nand downward posterior propagation across all levels. We develop a HIT-based\nrobotic system that dynamically switches between Interaction-Task and\nVerification-Task trees for a collaborative assembly task, allowing the robot\nto effectively coordinate human intentions at three levels: task-level (subtask\ngoal locations), interaction-level (mode of engagement with the robot), and\nverification-level (confirming or correcting intention recognition). Our user\nstudy shows that our HIT-based collaborative robot system surpasses existing\ncollaborative robot solutions by achieving a balance between efficiency,\nphysical workload, and user comfort while ensuring safety and task completion.\nPost-experiment surveys further reveal that the HIT-based system enhances the\nuser trust and minimizes interruptions to user's task flow through its\neffective understanding of human intentions across multiple levels.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on intention tracking for collaborative robots, which is related to predicting human behavior and can be considered a form of trajectory prediction in a broader sense. It doesn't directly involve large language models, but the intention tracking aspect has some overlap with predicting future states or actions. The hierarchical intention tracking method could potentially be combined with LLMs for more complex reasoning.", "keywords": ["intention tracking", "human behavior prediction", "collaborative robots", "dynamic intentions"]}}
{"id": "2506.07062", "pdf": "https://arxiv.org/pdf/2506.07062", "abs": "https://arxiv.org/abs/2506.07062", "authors": ["Dongryung Lee", "Sejune Joo", "Kimin Lee", "Beomjoon Kim"], "title": "Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search", "categories": ["cs.RO", "cs.AI"], "comment": "The International Journal of Robotics Research (IJRR)", "summary": "The problem of relocating a set of objects to designated areas amidst movable\nobstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)\nproblem, a subclass of task and motion planning (TAMP). Traditional approaches\nto G-TAMP have relied either on domain-independent heuristics or on learning\nfrom planning experience to guide the search, both of which typically demand\nsignificant computational resources or data. In contrast, humans often use\ncommon sense to intuitively decide which objects to manipulate in G-TAMP\nproblems. Inspired by this, we propose leveraging Large Language Models (LLMs),\nwhich have common sense knowledge acquired from internet-scale data, to guide\ntask planning in G-TAMP problems. To enable LLMs to perform geometric\nreasoning, we design a predicate-based prompt that encodes geometric\ninformation derived from a motion planning algorithm. We then query the LLM to\ngenerate a task plan, which is then used to search for a feasible set of\ncontinuous parameters. Since LLMs are prone to mistakes, instead of committing\nto LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action\nspace and use the LLM to guide the search. Unlike the previous approach that\ncalls an LLM at every node and incurs high computational costs, we use it to\nwarm-start the MCTS with the nodes explored in completing the LLM's task plan.\nOn six different G-TAMP problems, we show our method outperforms previous LLM\nplanners and pure search algorithms. Code can be found at:\nhttps://github.com/iMSquared/prime-the-search", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses Large Language Models (LLMs) to guide geometric task and motion planning, which can be seen as a form of trajectory planning. While it doesn't directly predict trajectories in the traditional sense (e.g., pedestrian or vehicle trajectory prediction), it uses LLMs to improve the efficiency of planning a sequence of actions that results in a trajectory. The connection to trajectory prediction is therefore indirect but present.", "keywords": ["Large Language Models", "LLMs", "motion planning", "task planning", "geometric task and motion planning", "G-TAMP", "tree search"]}}
{"id": "2506.07150", "pdf": "https://arxiv.org/pdf/2506.07150", "abs": "https://arxiv.org/abs/2506.07150", "authors": ["Xintao Yan", "Erdao Liang", "Jiawei Wang", "Haojie Zhu", "Henry X. Liu"], "title": "Improving Traffic Signal Data Quality for the Waymo Open Motion Dataset", "categories": ["cs.RO"], "comment": null, "summary": "Datasets pertaining to autonomous vehicles (AVs) hold significant promise for\na range of research fields, including artificial intelligence (AI), autonomous\ndriving, and transportation engineering. Nonetheless, these datasets often\nencounter challenges related to the states of traffic signals, such as missing\nor inaccurate data. Such issues can compromise the reliability of the datasets\nand adversely affect the performance of models developed using them. This\nresearch introduces a fully automated approach designed to tackle these issues\nby utilizing available vehicle trajectory data alongside knowledge from the\ntransportation domain to effectively impute and rectify traffic signal\ninformation within the Waymo Open Motion Dataset (WOMD). The proposed method is\nrobust and flexible, capable of handling diverse intersection geometries and\ntraffic signal configurations in real-world scenarios. Comprehensive\nvalidations have been conducted on the entire WOMD, focusing on over 360,000\nrelevant scenarios involving traffic signals, out of a total of 530,000\nreal-world driving scenarios. In the original dataset, 71.7% of traffic signal\nstates are either missing or unknown, all of which were successfully imputed by\nour proposed method. Furthermore, in the absence of ground-truth signal states,\nthe accuracy of our approach is evaluated based on the rate of red-light\nviolations among vehicle trajectories. Results show that our method reduces the\nestimated red-light running rate from 15.7% in the original data to 2.9%,\nthereby demonstrating its efficacy in rectifying data inaccuracies. This paper\nsignificantly enhances the quality of AV datasets, contributing to the wider AI\nand AV research communities and benefiting various downstream applications. The\ncode and improved traffic signal data are open-sourced at\nhttps://github.com/michigan-traffic-lab/WOMD-Traffic-Signal-Data-Improvement", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u5173\u6ce8\u4e8e\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u4ea4\u901a\u4fe1\u53f7\u6570\u636e\u7684\u51c6\u786e\u6027\uff0c\u8fd9\u5bf9\u4e8e\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u8bba\u6587\u672c\u8eab\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u63d0\u9ad8\u6570\u636e\u96c6\u8d28\u91cf\u53ef\u4ee5\u95f4\u63a5\u63d0\u5347\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u8f68\u8ff9\u9884\u6d4b\u6548\u679c\u3002\u5173\u952e\u8bcd\u5305\u62ec\u8f68\u8ff9\u6570\u636e\u548c\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u3002", "keywords": ["vehicle trajectory", "autonomous vehicles", "motion dataset"]}}
{"id": "2506.06780", "pdf": "https://arxiv.org/pdf/2506.06780", "abs": "https://arxiv.org/abs/2506.06780", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations", "categories": ["cs.CV", "cs.LG"], "comment": "Extended abstract, presented at the CVPR Workshop on 4D Vision", "summary": "Tracking and forecasting the rotation of objects is fundamental in computer\nvision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor\nobservations can be noisy and sparse, (2) motion patterns can be governed by\ncomplex dynamics, and (3) application settings can demand long-term\nforecasting. This work proposes modeling continuous-time rotational object\ndynamics on $SO(3)$ using Neural Controlled Differential Equations guided by\nSavitzky-Golay paths. Unlike existing methods that rely on simplified motion\nassumptions, our method learns a general latent dynamical system of the\nunderlying object trajectory while respecting the geometric structure of\nrotations. Experimental results on real-world data demonstrate compelling\nforecasting capabilities compared to existing approaches.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on forecasting the rotation of objects, which is related to trajectory prediction. It doesn't explicitly mention or utilize large language models. However, the use of neural controlled differential equations for modeling continuous-time dynamics is a relevant technique in time-series forecasting.", "keywords": ["trajectory prediction", "forecasting", "rotation", "neural controlled differential equations"]}}
{"id": "2506.06910", "pdf": "https://arxiv.org/pdf/2506.06910", "abs": "https://arxiv.org/abs/2506.06910", "authors": ["Mahnaz Koupaee", "Xueying Bai", "Mudan Chen", "Greg Durrett", "Nathanael Chambers", "Niranjan Balasubramanian"], "title": "Causal Graph based Event Reasoning using Semantic Relation Experts", "categories": ["cs.AI"], "comment": null, "summary": "Understanding how events in a scenario causally connect with each other is\nimportant for effectively modeling and reasoning about events. But event\nreasoning remains a difficult challenge, and despite recent advances, Large\nLanguage Models (LLMs) still struggle to accurately identify causal connections\nbetween events. This struggle leads to poor performance on deeper reasoning\ntasks like event forecasting and timeline understanding. To address this\nchallenge, we investigate the generation of causal event graphs (e.g., A\nenables B) as a parallel mechanism to help LLMs explicitly represent causality\nduring inference. This paper evaluates both how to generate correct graphs as\nwell as how graphs can assist reasoning. We propose a collaborative approach to\ncausal graph generation where we use LLMs to simulate experts that focus on\nspecific semantic relations. The experts engage in multiple rounds of\ndiscussions which are then consolidated by a final expert. Then, to demonstrate\nthe utility of causal graphs, we use them on multiple downstream applications,\nand also introduce a new explainable event prediction task that requires a\ncausal chain of events in the explanation. These explanations are more\ninformative and coherent than baseline generations. Finally, our overall\napproach not finetuned on any downstream task, achieves competitive results\nwith state-of-the-art models on both forecasting and next event prediction\ntasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on event reasoning and utilizes Large Language Models (LLMs) to generate causal event graphs. While it doesn't directly address trajectory prediction, the downstream applications include event forecasting and next event prediction, which are related to predicting future states and could potentially be applicable to trajectory prediction. The use of LLMs is central to the paper.", "keywords": ["Large Language Models", "LLMs", "event forecasting", "event prediction", "causal graph"]}}
{"id": "2506.07348", "pdf": "https://arxiv.org/pdf/2506.07348", "abs": "https://arxiv.org/abs/2506.07348", "authors": ["Pablo Moraes", "M\u00f3nica Rodr\u00edguez", "Sebastian Barcelona", "Angel Da Silva", "Santiago Fernandez", "Hiago Sodre", "Igor Nunes", "Bruna Guterres", "Ricardo Grando"], "title": "UruBots Autonomous Cars Challenge Pro Team Description Paper for FIRA 2025", "categories": ["cs.RO", "cs.SY", "eess.IV", "eess.SY"], "comment": null, "summary": "This paper describes the development of an autonomous car by the UruBots team\nfor the 2025 FIRA Autonomous Cars Challenge (Pro). The project involves\nconstructing a compact electric vehicle, approximately the size of an RC car,\ncapable of autonomous navigation through different tracks. The design\nincorporates mechanical and electronic components and machine learning\nalgorithms that enable the vehicle to make real-time navigation decisions based\non visual input from a camera. We use deep learning models to process camera\nimages and control vehicle movements. Using a dataset of over ten thousand\nimages, we trained a Convolutional Neural Network (CNN) to drive the vehicle\neffectively, through two outputs, steering and throttle. The car completed the\ntrack in under 30 seconds, achieving a pace of approximately 0.4 meters per\nsecond while avoiding obstacles.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper describes an autonomous car project that uses deep learning models (specifically CNN) for real-time navigation and control based on visual input. While it focuses on autonomous navigation and control, implying trajectory prediction, it does not explicitly mention trajectory prediction techniques or large language models. The use of CNNs for processing images and controlling vehicle movements suggests a moderate level of relevance to trajectory prediction due to the inherent need for path planning and control.", "keywords": ["autonomous navigation", "autonomous car", "deep learning", "CNN", "real-time navigation", "obstacle avoidance"]}}
{"id": "2506.07454", "pdf": "https://arxiv.org/pdf/2506.07454", "abs": "https://arxiv.org/abs/2506.07454", "authors": ["Jared Strader", "Aaron Ray", "Jacob Arkin", "Mason B. Peterson", "Yun Chang", "Nathan Hughes", "Christopher Bradley", "Yi Xuan Jia", "Carlos Nieto-Granda", "Rajat Talak", "Chuchu Fan", "Luca Carlone", "Jonathan P. How", "Nicholas Roy"], "title": "Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 4 figures", "summary": "In this paper, we introduce a multi-robot system that integrates mapping,\nlocalization, and task and motion planning (TAMP) enabled by 3D scene graphs to\nexecute complex instructions expressed in natural language. Our system builds a\nshared 3D scene graph incorporating an open-set object-based map, which is\nleveraged for multi-robot 3D scene graph fusion. This representation supports\nreal-time, view-invariant relocalization (via the object-based map) and\nplanning (via the 3D scene graph), allowing a team of robots to reason about\ntheir surroundings and execute complex tasks. Additionally, we introduce a\nplanning approach that translates operator intent into Planning Domain\nDefinition Language (PDDL) goals using a Large Language Model (LLM) by\nleveraging context from the shared 3D scene graph and robot capabilities. We\nprovide an experimental assessment of the performance of our system on\nreal-world tasks in large-scale, outdoor environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses a multi-robot system that uses a Large Language Model (LLM) for planning. While it doesn't directly address trajectory prediction, the planning aspect, especially in the context of robot motion, has some relevance. The use of LLM also contributes to the relevance.", "keywords": ["Large Language Model", "LLM", "planning", "multi-robot", "motion planning"]}}
{"id": "2506.07047", "pdf": "https://arxiv.org/pdf/2506.07047", "abs": "https://arxiv.org/abs/2506.07047", "authors": ["Yu Xuejun", "Jianyuan Zhong", "Zijin Feng", "Pengyi Zhai", "Roozbeh Yousefzadeh", "Wei Chong Ng", "Haoxiong Liu", "Ziyi Shou", "Jing Xiong", "Yudong Zhou", "Claudia Beth Ong", "Austen Jeremy Sugiarto", "Yaoxi Zhang", "Wai Ming Tai", "Huan Cao", "Dongcai Lu", "Jiacheng Sun", "Qiang Xu", "Shen Xin", "Zhenguo Li"], "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on formal theorem proving using large language models. While it doesn't directly address trajectory prediction, it does heavily involve the use of LLMs for reasoning and problem-solving, which is a relevant aspect of the prompt.", "keywords": ["large language models", "LLMs", "foundation models"]}}
{"id": "2506.07540", "pdf": "https://arxiv.org/pdf/2506.07540", "abs": "https://arxiv.org/abs/2506.07540", "authors": ["Sreeja Roy-Singh", "Sarvesh Kolekar", "Daniel P. Bonny", "Kyle Foss"], "title": "Fractional Collisions: A Framework for Risk Estimation of Counterfactual Conflicts using Autonomous Driving Behavior Simulations", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We present a methodology for estimating collision risk from counterfactual\nsimulated scenarios built on sensor data from automated driving systems (ADS)\nor naturalistic driving databases. Two-agent conflicts are assessed by\ndetecting and classifying conflict type, identifying the agents' roles\n(initiator or responder), identifying the point of reaction of the responder,\nand modeling their human behavioral expectations as probabilistic\ncounterfactual trajectories. The states are used to compute velocity\ndifferentials at collision, which when combined with crash models, estimates\nseverity of loss in terms of probabilistic injury or property damage,\nhenceforth called fractional collisions. The probabilistic models may also be\nextended to include other uncertainties associated with the simulation,\nfeatures, and agents. We verify the effectiveness of the methodology in a\nsynthetic simulation environment using reconstructed trajectories from 300+\ncollision and near-collision scenes sourced from VTTI's SHRP2 database and\nNexar dashboard camera data. Our methodology predicted fractional collisions\nwithin 1% of ground truth collisions. We then evaluate agent-initiated\ncollision risk of an arbitrary ADS software release by replacing the\nnaturalistic responder in these synthetic reconstructions with an ADS simulator\nand comparing the outcome to human-response outcomes. Our ADS reduced\nnaturalistic collisions by 4x and fractional collision risk by ~62%. The\nframework's utility is also demonstrated on 250k miles of proprietary,\nopen-loop sensor data collected on ADS test vehicles, re-simulated with an\narbitrary ADS software release. The ADS initiated conflicts that caused 0.4\ninjury-causing and 1.7 property-damaging fractional collisions, and the ADS\nimproved collision risk in 96% of the agent-initiated conflicts.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e86\u6a21\u62df\u548c\u8f68\u8ff9\u9884\u6d4b\u6280\u672f\uff08counterfactual trajectories, reconstructed trajectories\uff09\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u63d0\u5230\u4e86\u884c\u4e3a\u5efa\u6a21\uff0c\u4f46\u6ca1\u6709\u4f7f\u7528\u6216\u8ba8\u8bbaLLM\u3002\u76f8\u5173\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u8f68\u8ff9\u9884\u6d4b\u65b9\u9762\u3002", "keywords": ["trajectory prediction", "autonomous driving", "behavior simulation", "collision risk", "counterfactual trajectories"]}}
{"id": "2506.07633", "pdf": "https://arxiv.org/pdf/2506.07633", "abs": "https://arxiv.org/abs/2506.07633", "authors": ["Ana Tanevska", "Ananthapathmanabhan Ratheesh Kumar", "Arabinda Ghosh", "Ernesto Casablanca", "Ginevra Castellano", "Sadegh Soudjani"], "title": "Blending Participatory Design and Artificial Awareness for Trustworthy Autonomous Vehicles", "categories": ["cs.RO"], "comment": "Submitted to IEEE RO-MAN 2025", "summary": "Current robotic agents, such as autonomous vehicles (AVs) and drones, need to\ndeal with uncertain real-world environments with appropriate situational\nawareness (SA), risk awareness, coordination, and decision-making. The SymAware\nproject strives to address this issue by designing an architecture for\nartificial awareness in multi-agent systems, enabling safe collaboration of\nautonomous vehicles and drones. However, these agents will also need to\ninteract with human users (drivers, pedestrians, drone operators), which in\nturn requires an understanding of how to model the human in the interaction\nscenario, and how to foster trust and transparency between the agent and the\nhuman.\n  In this work, we aim to create a data-driven model of a human driver to be\nintegrated into our SA architecture, grounding our research in the principles\nof trustworthy human-agent interaction. To collect the data necessary for\ncreating the model, we conducted a large-scale user-centered study on human-AV\ninteraction, in which we investigate the interaction between the AV's\ntransparency and the users' behavior.\n  The contributions of this paper are twofold: First, we illustrate in detail\nour human-AV study and its findings, and second we present the resulting Markov\nchain models of the human driver computed from the study's data. Our results\nshow that depending on the AV's transparency, the scenario's environment, and\nthe users' demographics, we can obtain significant differences in the model's\ntransitions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses autonomous vehicles and modeling human driver behavior using Markov chain models. While it doesn't explicitly mention large language models, it touches upon the broader theme of modeling human behavior in the context of autonomous systems, which is relevant to trajectory prediction. The connection to trajectory prediction is through the modeling of driver behavior and its potential use in predicting vehicle trajectories. The lack of direct involvement with LLMs lowers the relevance score.", "keywords": ["autonomous vehicles", "human driver behavior", "Markov chain models", "human-agent interaction", "situational awareness"]}}
{"id": "2506.07217", "pdf": "https://arxiv.org/pdf/2506.07217", "abs": "https://arxiv.org/abs/2506.07217", "authors": ["Zihan Deng", "Changyu Du", "Stavros Nousias", "Andr\u00e9 Borrmann"], "title": "BIMgent: Towards Autonomous Building Modeling via Computer-use Agents", "categories": ["cs.AI"], "comment": "ICML 2025 Workshop on Computer Use Agents", "summary": "Existing computer-use agents primarily focus on general-purpose desktop\nautomation tasks, with limited exploration of their application in highly\nspecialized domains. In particular, the 3D building modeling process in the\nArchitecture, Engineering, and Construction (AEC) sector involves open-ended\ndesign tasks and complex interaction patterns within Building Information\nModeling (BIM) authoring software, which has yet to be thoroughly addressed by\ncurrent studies. In this paper, we propose BIMgent, an agentic framework\npowered by multimodal large language models (LLMs), designed to enable\nautonomous building model authoring via graphical user interface (GUI)\noperations. BIMgent automates the architectural building modeling process,\nincluding multimodal input for conceptual design, planning of software-specific\nworkflows, and efficient execution of the authoring GUI actions. We evaluate\nBIMgent on real-world building modeling tasks, including both text-based\nconceptual design generation and reconstruction from existing building design.\nThe design quality achieved by BIMgent was found to be reasonable. Its\noperations achieved a 32% success rate, whereas all baseline models failed to\ncomplete the tasks (0% success rate). Results demonstrate that BIMgent\neffectively reduces manual workload while preserving design intent,\nhighlighting its potential for practical deployment in real-world architectural\nmodeling scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using large language models (LLMs) for automating building modeling tasks, specifically within Building Information Modeling (BIM) software. While it doesn't directly address trajectory prediction, the use of LLMs and the automation of a complex process involving a sequence of actions (GUI operations) shares some conceptual similarities with trajectory prediction. The agent's planning and execution of GUI actions could be seen as a form of 'trajectory' within the software environment. Therefore, there's a moderate level of relevance due to the use of LLMs and the sequential nature of the task.", "keywords": ["Large Language Models (LLMs)", "computer-use agents", "automation", "Building Information Modeling (BIM)", "GUI operations", "agentic framework"]}}
{"id": "2506.06918", "pdf": "https://arxiv.org/pdf/2506.06918", "abs": "https://arxiv.org/abs/2506.06918", "authors": ["Carl Brander", "Giovanni Cioffi", "Nico Messikommer", "Davide Scaramuzza"], "title": "Reading in the Dark with Foveated Event Vision", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025 Workshop on Event-based Vision", "summary": "Current smart glasses equipped with RGB cameras struggle to perceive the\nenvironment in low-light and high-speed motion scenarios due to motion blur and\nthe limited dynamic range of frame cameras. Additionally, capturing dense\nimages with a frame camera requires large bandwidth and power consumption,\nconsequently draining the battery faster. These challenges are especially\nrelevant for developing algorithms that can read text from images. In this\nwork, we propose a novel event-based Optical Character Recognition (OCR)\napproach for smart glasses. By using the eye gaze of the user, we foveate the\nevent stream to significantly reduce bandwidth by around 98% while exploiting\nthe benefits of event cameras in high-dynamic and fast scenes. Our proposed\nmethod performs deep binary reconstruction trained on synthetic data and\nleverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our\nresults demonstrate the ability to read text in low light environments where\nRGB cameras struggle while using up to 2400 times less bandwidth than a\nwearable RGB camera.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper uses multimodal LLMs for OCR, which is relevant to large language models. While the paper does not directly address trajectory prediction, the use of event cameras in high-speed motion scenarios could potentially be relevant for future applications in that area. The connection is somewhat weak but present.", "keywords": ["LLMs", "multimodal LLMs"]}}
{"id": "2506.07223", "pdf": "https://arxiv.org/pdf/2506.07223", "abs": "https://arxiv.org/abs/2506.07223", "authors": ["Yangqing Zheng", "Shunqi Mao", "Dingxin Zhang", "Weidong Cai"], "title": "LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments", "categories": ["cs.AI"], "comment": "Accepted by the CVPR 2025 Embodied AI Workshop", "summary": "In the realm of embodied intelligence, the evolution of large language models\n(LLMs) has markedly enhanced agent decision making. Consequently, researchers\nhave begun exploring agent performance in dynamically changing high-risk\nscenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under\nthese extreme conditions, the delay in decision making emerges as a crucial yet\ninsufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that\ntranslates inference delays in decision-making into equivalent simulation\nframes, thus aligning cognitive and physical costs under a single FPS-based\nmetric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action\nRatio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we\npresent the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a\nlightweight LLM-guided feedback module with a rule-based agent to enable\nimmediate reactive behaviors and asynchronous reflective refinements in situ.\nExperiments on HAZARD show that RRARA substantially outperforms existing\nbaselines in latency-sensitive scenarios.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using LLMs to enhance agent decision-making in dynamic environments. While it doesn't explicitly mention trajectory prediction, the context of embodied agents operating in changing environments implies a need for understanding and predicting the consequences of actions, which is related to trajectory prediction. The paper also heavily features LLMs.", "keywords": ["LLMs", "large language models", "embodied agent", "decision-making", "dynamic environments"]}}
{"id": "2506.07961", "pdf": "https://arxiv.org/pdf/2506.07961", "abs": "https://arxiv.org/abs/2506.07961", "authors": ["Peiyan Li", "Yixiang Chen", "Hongtao Wu", "Xiao Ma", "Xiangnan Wu", "Yan Huang", "Liang Wang", "Tao Kong", "Tieniu Tan"], "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models", "categories": ["cs.RO", "cs.AI"], "comment": "In Submission", "summary": "Recently, leveraging pre-trained vision-language models (VLMs) for building\nvision-language-action (VLA) models has emerged as a promising approach to\neffective robot manipulation learning. However, only few methods incorporate 3D\nsignals into VLMs for action prediction, and they do not fully leverage the\nspatial structure inherent in 3D data, leading to low sample efficiency. In\nthis paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D\ninputs to multiple 2D images, ensuring input alignment with the VLM backbone,\nand (2) utilizes 2D heatmaps for action prediction, unifying the input and\noutput spaces within a consistent 2D image space. In addition, we propose a\nscalable pre-training method that equips the VLM backbone with the capability\nto predict 2D heatmaps before downstream policy learning. Extensive experiments\nshow the proposed method is able to learn 3D manipulation efficiently and\neffectively. BridgeVLA outperforms state-of-the-art baseline methods across\nthree simulation benchmarks. In RLBench, it improves the average success rate\nfrom 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better\nperformance in challenging generalization settings, boosting the average\nsuccess rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing\nbaseline methods in terms of average success rate. In real-robot experiments,\nBridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It\ngeneralizes robustly in multiple out-of-distribution settings, including visual\ndisturbances and unseen instructions. Remarkably, it is able to achieve a\nsuccess rate of 96.8% on 10+ tasks with only 3 trajectories per task,\nhighlighting its extraordinary sample efficiency. Project\nWebsite:https://bridgevla.github.io/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes Vision-Language Models (VLMs) for robot manipulation, which involves predicting actions based on visual and textual inputs. While not directly trajectory prediction, action prediction is a related field. The paper also prominently features VLMs, fitting the large language model aspect. However, the focus is more on manipulation than trajectory prediction specifically.", "keywords": ["vision-language models", "VLMs", "action prediction", "robot manipulation"]}}
{"id": "2506.07091", "pdf": "https://arxiv.org/pdf/2506.07091", "abs": "https://arxiv.org/abs/2506.07091", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui Jia"], "title": "SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model", "categories": ["cs.CV"], "comment": null, "summary": "Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation\nof complex, interactive indoor scenes tailored to user prompt remains a\nformidable challenge. While existing methods achieve indoor scene synthesis,\nthey struggle with rigid editing constraints, physical incoherence, excessive\nhuman effort, single-room limitations, and suboptimal material quality. To\naddress these limitations, we propose SceneLCM, an end-to-end framework that\nsynergizes Large Language Model (LLM) for layout design with Latent Consistency\nModel(LCM) for scene optimization. Our approach decomposes scene generation\ninto four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D\nspatial reasoning to convert textual descriptions into parametric blueprints(3D\nlayout). And an iterative programmatic validation mechanism iteratively refines\nlayout parameters through LLM-mediated dialogue loops; (2) Furniture\nGeneration. SceneLCM employs Consistency Trajectory Sampling(CTS), a\nconsistency distillation sampling loss guided by LCM, to form fast,\nsemantically rich, and high-quality representations. We also offer two\ntheoretical justification to demonstrate that our CTS loss is equivalent to\nconsistency loss and its distillation error is bounded by the truncation error\nof the Euler solver; (3) Environment Optimization. We use a multiresolution\ntexture field to encode the appearance of the scene, and optimize via CTS loss.\nTo maintain cross-geometric texture coherence, we introduce a normal-aware\ncross-attention decoder to predict RGB by cross-attending to the anchors\nlocations in geometrically heterogeneous instance. (4)Physically Editing.\nSceneLCM supports physically editing by integrating physical simulation,\nachieved persistent physical realism. Extensive experiments validate SceneLCM's\nsuperiority over state-of-the-art techniques, showing its wide-ranging\npotential for diverse applications.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on indoor scene generation using a Large Language Model (LLM) for layout design and a Latent Consistency Model (LCM) for scene optimization. While it doesn't directly deal with trajectory prediction, the use of LLMs and consistency models connects it to the broader theme of large models. The mention of \"Consistency Trajectory Sampling\" (CTS) suggests a potential, though indirect, link to trajectory-related concepts, even though it's used in a different context here.", "keywords": ["Large Language Model", "LLM", "Latent Consistency Model", "Consistency Trajectory Sampling"]}}
{"id": "2506.07896", "pdf": "https://arxiv.org/pdf/2506.07896", "abs": "https://arxiv.org/abs/2506.07896", "authors": ["Shoko Oka"], "title": "Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages, Additional resources available on GitHub repository", "summary": "Recent advancements in large language models (LLMs) have revitalized\nphilosophical debates surrounding artificial intelligence. Two of the most\nfundamental challenges - namely, the Frame Problem and the Symbol Grounding\nProblem - have historically been viewed as unsolvable within traditional\nsymbolic AI systems. This study investigates whether modern LLMs possess the\ncognitive capacities required to address these problems. To do so, I designed\ntwo benchmark tasks reflecting the philosophical core of each problem,\nadministered them under zero-shot conditions to 13 prominent LLMs (both closed\nand open-source), and assessed the quality of the models' outputs across five\ntrials each. Responses were scored along multiple criteria, including\ncontextual reasoning, semantic coherence, and information filtering. The\nresults demonstrate that while open-source models showed variability in\nperformance due to differences in model size, quantization, and instruction\ntuning, several closed models consistently achieved high scores. These findings\nsuggest that select modern LLMs may be acquiring capacities sufficient to\nproduce meaningful and stable responses to these long-standing theoretical\nchallenges.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on evaluating Large Language Models (LLMs) on the Frame and Symbol Grounding Problems. While it heavily involves LLMs, it doesn't directly address trajectory prediction. However, the Frame Problem can be relevant to reasoning about changes in dynamic environments, which is indirectly related to trajectory prediction. Therefore, the relevance score is moderate.", "keywords": ["Large Language Models", "LLMs", "Frame Problem", "Symbol Grounding Problem"]}}
{"id": "2506.07915", "pdf": "https://arxiv.org/pdf/2506.07915", "abs": "https://arxiv.org/abs/2506.07915", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement", "categories": ["cs.AI", "cs.CL", "cs.SY", "eess.SY"], "comment": "12 pages, 4 Figures, 3 Tables, submitted to the IEEE for possible\n  publication", "summary": "In dynamic environments, the rapid obsolescence of pre-existing environmental\nknowledge creates a gap between an agent's internal model and the evolving\nreality of its operational context. This disparity between prior and updated\nenvironmental valuations fundamentally limits the effectiveness of autonomous\ndecision-making. To bridge this gap, the contextual bias of human domain\nstakeholders, who naturally accumulate insights through direct, real-time\nobservation, becomes indispensable. However, translating their nuanced, and\ncontext-rich input into actionable intelligence for autonomous systems remains\nan open challenge. To address this, we propose LUCIFER (Language Understanding\nand Context-Infused Framework for Exploration and Behavior Refinement), a\ndomain-agnostic framework that integrates a hierarchical decision-making\narchitecture with reinforcement learning (RL) and large language models (LLMs)\ninto a unified system. This architecture mirrors how humans decompose complex\ntasks, enabling a high-level planner to coordinate specialised sub-agents, each\nfocused on distinct objectives and temporally interdependent actions. Unlike\ntraditional applications where LLMs are limited to single role, LUCIFER\nintegrates them in two synergistic roles: as context extractors, structuring\nverbal stakeholder input into domain-aware representations that influence\ndecision-making through an attention space mechanism aligning LLM-derived\ninsights with the agent's learning process, and as zero-shot exploration\nfacilitators guiding the agent's action selection process during exploration.\nWe benchmark various LLMs in both roles and demonstrate that LUCIFER improves\nexploration efficiency and decision quality, outperforming flat,\ngoal-conditioned policies. Our findings show the potential of context-driven\ndecision-making, where autonomous systems leverage human contextual knowledge\nfor operational success.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes Large Language Models (LLMs) to improve decision-making in dynamic environments. While it doesn't directly focus on trajectory prediction, the context of 'dynamic environments' and 'autonomous decision-making' suggests potential applicability to trajectory prediction scenarios, especially in autonomous driving or robotics. The use of reinforcement learning further strengthens this connection, as RL is often used in trajectory prediction tasks. However, the primary focus is on leveraging LLMs for context understanding and exploration, rather than directly predicting trajectories. Therefore, the relevance is moderate.", "keywords": ["Large Language Models", "LLMs", "reinforcement learning", "dynamic environments", "autonomous decision-making", "exploration"]}}
{"id": "2506.07196", "pdf": "https://arxiv.org/pdf/2506.07196", "abs": "https://arxiv.org/abs/2506.07196", "authors": ["Mengya Xu", "Zhongzhen Huang", "Dillan Imans", "Yiru Ye", "Xiaofan Zhang", "Qi Dou"], "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 4 figures", "summary": "Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on surgical action planning, which can be seen as a form of trajectory prediction in a specific domain. It also explicitly uses and benchmarks multimodal large language models (MLLMs). Therefore, it has relevance to both trajectory prediction and large language models, although the connection to general trajectory prediction is somewhat indirect.", "keywords": ["surgical action planning", "multimodal large language models", "MLLMs", "action prediction"]}}
{"id": "2506.08012", "pdf": "https://arxiv.org/pdf/2506.08012", "abs": "https://arxiv.org/abs/2506.08012", "authors": ["Penghao Wu", "Shengnan Ma", "Bo Wang", "Jiaheng Yu", "Lewei Lu", "Ziwei Liu"], "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior", "categories": ["cs.AI", "cs.CV"], "comment": "Project Page at https://penghao-wu.github.io/GUI_Reflection/", "summary": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762\uff08GUI\uff09\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u6a21\u578b\u7684\u81ea\u53cd\u601d\u548c\u7ea0\u9519\u80fd\u529b\u3002\u867d\u7136GUI\u64cd\u4f5c\u53ef\u4ee5\u88ab\u89c6\u4e3a\u4e00\u79cd\u8f68\u8ff9\uff0c\u4f46\u8bba\u6587\u7684\u6838\u5fc3\u5728\u4e8e\u5229\u7528LLM\u8fdb\u884cGUI\u4ea4\u4e92\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["Large Language Models", "MLLMs", "GUI automation", "self-reflection"]}}
{"id": "2506.07310", "pdf": "https://arxiv.org/pdf/2506.07310", "abs": "https://arxiv.org/abs/2506.07310", "authors": ["Adam W. Harley", "Yang You", "Xinglong Sun", "Yang Zheng", "Nikhil Raghuraman", "Yunqi Gu", "Sheldon Liang", "Wen-Hsuan Chu", "Achal Dave", "Pavel Tokmakov", "Suya You", "Rares Ambrus", "Katerina Fragkiadaki", "Leonidas J. Guibas"], "title": "AllTracker: Efficient Dense Point Tracking at High Resolution", "categories": ["cs.CV"], "comment": null, "summary": "We introduce AllTracker: a model that estimates long-range point tracks by\nway of estimating the flow field between a query frame and every other frame of\na video. Unlike existing point tracking methods, our approach delivers\nhigh-resolution and dense (all-pixel) correspondence fields, which can be\nvisualized as flow maps. Unlike existing optical flow methods, our approach\ncorresponds one frame to hundreds of subsequent frames, rather than just the\nnext frame. We develop a new architecture for this task, blending techniques\nfrom existing work in optical flow and point tracking: the model performs\niterative inference on low-resolution grids of correspondence estimates,\npropagating information spatially via 2D convolution layers, and propagating\ninformation temporally via pixel-aligned attention layers. The model is fast\nand parameter-efficient (16 million parameters), and delivers state-of-the-art\npoint tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on\na 40G GPU). A benefit of our design is that we can train on a wider set of\ndatasets, and we find that doing so is crucial for top performance. We provide\nan extensive ablation study on our architecture details and training recipe,\nmaking it clear which details matter most. Our code and model weights are\navailable at https://alltracker.github.io .", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on point tracking in videos, which is related to trajectory prediction. While it doesn't explicitly mention trajectory prediction or large language models, the underlying techniques and the problem it addresses are relevant to the broader field of motion analysis and prediction. The use of attention mechanisms could potentially be scaled up and integrated with larger models, although this is not explicitly discussed in the abstract.", "keywords": ["point tracking", "flow field", "optical flow", "attention"]}}
{"id": "2506.06999", "pdf": "https://arxiv.org/pdf/2506.06999", "abs": "https://arxiv.org/abs/2506.06999", "authors": ["Arun Sharma", "Mingzhou Yang", "Majid Farhadloo", "Subhankar Ghosh", "Bharat Jayaprakash", "Shashi Shekhar"], "title": "Towards Physics-informed Diffusion for Anomaly Detection in Trajectories", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "Given trajectory data, a domain-specific study area, and a user-defined\nthreshold, we aim to find anomalous trajectories indicative of possible GPS\nspoofing (e.g., fake trajectory). The problem is societally important to curb\nillegal activities in international waters, such as unauthorized fishing and\nillicit oil transfers. The problem is challenging due to advances in AI\ngenerated in deep fakes generation (e.g., additive noise, fake trajectories)\nand lack of adequate amount of labeled samples for ground-truth verification.\nRecent literature shows promising results for anomalous trajectory detection\nusing generative models despite data sparsity. However, they do not consider\nfine-scale spatiotemporal dependencies and prior physical knowledge, resulting\nin higher false-positive rates. To address these limitations, we propose a\nphysics-informed diffusion model that integrates kinematic constraints to\nidentify trajectories that do not adhere to physical laws. Experimental results\non real-world datasets in the maritime and urban domains show that the proposed\nframework results in higher prediction accuracy and lower estimation error rate\nfor anomaly detection and trajectory generation methods, respectively. Our\nimplementation is available at\nhttps://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u5f02\u5e38\u68c0\u6d4b\uff0c\u4f7f\u7528\u4e86\u6269\u6563\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u751f\u6210\u548c\u5f02\u5e38\u8bc6\u522b\uff0c\u5e76\u878d\u5165\u4e86\u7269\u7406\u4fe1\u606f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u6269\u6563\u6a21\u578b\uff0c\u5e76\u4e14\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["trajectory", "anomaly detection", "diffusion model", "trajectory generation", "physics-informed"]}}
{"id": "2506.07375", "pdf": "https://arxiv.org/pdf/2506.07375", "abs": "https://arxiv.org/abs/2506.07375", "authors": ["Xunjie He", "Christina Dao Wen Lee", "Meiling Wang", "Chengran Yuan", "Zefan Huang", "Yufeng Yue", "Marcelo H. Ang Jr"], "title": "DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Collaborative perception plays a crucial role in enhancing environmental\nunderstanding by expanding the perceptual range and improving robustness\nagainst sensor failures, which primarily involves collaborative 3D detection\nand tracking tasks. The former focuses on object recognition in individual\nframes, while the latter captures continuous instance tracklets over time.\nHowever, existing works in both areas predominantly focus on the vehicle\nsuperclass, lacking effective solutions for both multi-class collaborative\ndetection and tracking. This limitation hinders their applicability in\nreal-world scenarios, which involve diverse object classes with varying\nappearances and motion patterns. To overcome these limitations, we propose a\nmulti-class collaborative detection and tracking framework tailored for diverse\nroad users. We first present a detector with a global spatial attention fusion\n(GSAF) module, enhancing multi-scale feature learning for objects of varying\nsizes. Next, we introduce a tracklet RE-IDentification (REID) module that\nleverages visual semantics with a vision foundation model to effectively reduce\nID SWitch (IDSW) errors, in cases of erroneous mismatches involving small\nobjects like pedestrians. We further design a velocity-based adaptive tracklet\nmanagement (VATM) module that adjusts the tracking interval dynamically based\non object motion. Extensive experiments on the V2X-Real and OPV2V datasets show\nthat our approach significantly outperforms existing state-of-the-art methods\nin both detection and tracking accuracy.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on collaborative detection and tracking, which is closely related to trajectory prediction. It uses a vision foundation model for tracklet RE-ID, connecting it to large models. However, it doesn't directly perform trajectory prediction, making the relevance moderate.", "keywords": ["tracking", "vision foundation model", "collaborative perception", "multi-class detection", "tracklet RE-ID"]}}
{"id": "2506.07489", "pdf": "https://arxiv.org/pdf/2506.07489", "abs": "https://arxiv.org/abs/2506.07489", "authors": ["Yahao Shi", "Yang Liu", "Yanmin Wu", "Xing Liu", "Chen Zhao", "Jie Luo", "Bin Zhou"], "title": "Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video", "categories": ["cs.CV"], "comment": "technical report", "summary": "We propose DriveAnyMesh, a method for driving mesh guided by monocular video.\nCurrent 4D generation techniques encounter challenges with modern rendering\nengines. Implicit methods have low rendering efficiency and are unfriendly to\nrasterization-based engines, while skeletal methods demand significant manual\neffort and lack cross-category generalization. Animating existing 3D assets,\ninstead of creating 4D assets from scratch, demands a deep understanding of the\ninput's 3D structure. To tackle these challenges, we present a 4D diffusion\nmodel that denoises sequences of latent sets, which are then decoded to produce\nmesh animations from point cloud trajectory sequences. These latent sets\nleverage a transformer-based variational autoencoder, simultaneously capturing\n3D shape and motion information. By employing a spatiotemporal,\ntransformer-based diffusion model, information is exchanged across multiple\nlatent frames, enhancing the efficiency and generalization of the generated\nresults. Our experimental results demonstrate that DriveAnyMesh can rapidly\nproduce high-quality animations for complex motions and is compatible with\nmodern rendering engines. This method holds potential for applications in both\nthe gaming and filming industries.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on mesh deformation from video using a diffusion model. While it doesn't explicitly mention trajectory prediction or large language models, the use of a transformer-based diffusion model and the mention of point cloud trajectory sequences suggest a potential connection to trajectory prediction. The absence of explicit mention of LLMs lowers the relevance score.", "keywords": ["diffusion model", "point cloud trajectory sequences", "transformer"]}}
{"id": "2506.07491", "pdf": "https://arxiv.org/pdf/2506.07491", "abs": "https://arxiv.org/abs/2506.07491", "authors": ["Yongsen Mao", "Junhao Zhong", "Chuan Fang", "Jia Zheng", "Rui Tang", "Hao Zhu", "Ping Tan", "Zihan Zhou"], "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "categories": ["cs.CV"], "comment": null, "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses a Large Language Model (SpatialLM) designed for processing 3D point cloud data and generating structured 3D scene understanding outputs. While it doesn't directly address trajectory prediction, the 3D scene understanding aspect is related to spatial reasoning which is a component of trajectory prediction. The use of a Large Language Model makes it relevant to the specified topics.", "keywords": ["Large Language Models", "LLMs", "3D scene understanding", "point cloud data", "SpatialLM"]}}
{"id": "2506.07497", "pdf": "https://arxiv.org/pdf/2506.07497", "abs": "https://arxiv.org/abs/2506.07497", "authors": ["Xiangyu Guo", "Zhanqian Wu", "Kaixin Xiong", "Ziyang Xu", "Lijun Zhou", "Gangwei Xu", "Shaoqing Xu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency", "categories": ["cs.CV"], "comment": null, "summary": "We present Genesis, a unified framework for joint generation of multi-view\ndriving videos and LiDAR sequences with spatio-temporal and cross-modal\nconsistency. Genesis employs a two-stage architecture that integrates a\nDiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR\ngenerator with NeRF-based rendering and adaptive sampling. Both modalities are\ndirectly coupled through a shared latent space, enabling coherent evolution\nacross visual and geometric domains. To guide the generation with structured\nsemantics, we introduce DataCrafter, a captioning module built on\nvision-language models that provides scene-level and instance-level\nsupervision. Extensive experiments on the nuScenes benchmark demonstrate that\nGenesis achieves state-of-the-art performance across video and LiDAR metrics\n(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including\nsegmentation and 3D detection, validating the semantic fidelity and practical\nutility of the generated data.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating driving scenes including videos and LiDAR sequences. While not directly predicting trajectories, the generation of driving scenes is highly relevant to trajectory prediction as the generated data can be used for training and evaluation of trajectory prediction models. The paper mentions the use of vision-language models for captioning, which falls under the umbrella of large language models. The generation of LiDAR sequences, particularly in a driving context, could implicitly involve trajectory information. The downstream tasks mentioned (segmentation and 3D detection) are also relevant to trajectory prediction as they provide the necessary scene understanding for accurate prediction.", "keywords": ["driving scene generation", "LiDAR", "vision-language models", "downstream tasks", "3D detection", "segmentation"]}}
{"id": "2506.07570", "pdf": "https://arxiv.org/pdf/2506.07570", "abs": "https://arxiv.org/abs/2506.07570", "authors": ["Yixuan Yang", "Zhen Luo", "Tongsheng Ding", "Junru Lu", "Mingqi Gao", "Jinyu Yang", "Victor Sanchez", "Feng Zheng"], "title": "LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automatic indoor layout generation has attracted increasing attention due to\nits potential in interior design, virtual environment construction, and\nembodied AI. Existing methods fall into two categories: prompt-driven\napproaches that leverage proprietary LLM services (e.g., GPT APIs) and\nlearning-based methods trained on layout data upon diffusion-based models.\nPrompt-driven methods often suffer from spatial inconsistency and high\ncomputational costs, while learning-based methods are typically constrained by\ncoarse relational graphs and limited datasets, restricting their generalization\nto diverse room categories. In this paper, we revisit LLM-based indoor layout\ngeneration and present 3D-SynthPlace, a large-scale dataset that combines\nsynthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,\nupgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000\nscenes, covering four common room types -- bedroom, living room, kitchen, and\nbathroom -- enriched with diverse objects and high-level spatial annotations.\nWe further introduce OptiScene, a strong open-source LLM optimized for indoor\nlayout generation, fine-tuned based on our 3D-SynthPlace dataset through our\ntwo-stage training. For the warum-up stage I, we adopt supervised fine-tuning\n(SFT), which is taught to first generate high-level spatial descriptions then\nconditionally predict concrete object placements. For the reinforcing stage II,\nto better align the generated layouts with human design preferences, we apply\nmulti-turn direct preference optimization (DPO), which significantly improving\nlayout quality and generation success rates. Extensive experiments demonstrate\nthat OptiScene outperforms traditional prompt-driven and learning-based\nbaselines. Moreover, OptiScene shows promising potential in interactive tasks\nsuch as scene editing and robot navigation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4e8e\u5ba4\u5185\u573a\u666f\u5e03\u5c40\u751f\u6210\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u6570\u636e\u5408\u6210\u548c\u504f\u597d\u4f18\u5316\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86\"robot navigation\"\uff0c\u8868\u660e\u8be5\u7814\u7a76\u6210\u679c\u53ef\u80fd\u5728\u673a\u5668\u4eba\u5bfc\u822a\u9886\u57df\u6709\u6240\u5e94\u7528\uff0c\u800c\u673a\u5668\u4eba\u5bfc\u822a\u901a\u5e38\u6d89\u53ca\u5230\u8f68\u8ff9\u9884\u6d4b\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLM", "robot navigation"]}}
{"id": "2506.07298", "pdf": "https://arxiv.org/pdf/2506.07298", "abs": "https://arxiv.org/abs/2506.07298", "authors": ["Yijia Dai", "Zhaolin Gao", "Yahya Satter", "Sarah Dean", "Jennifer J. Sun"], "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)$\\unicode{x2013}$their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum. We uncover novel\nscaling trends influenced by HMM properties, and offer theoretical conjectures\nfor these empirical observations. We also provide practical guidelines for\nscientists on using ICL as a diagnostic tool for complex data. On real-world\nanimal decision-making tasks, ICL achieves competitive performance with models\ndesigned by human experts. To our knowledge, this is the first demonstration\nthat ICL can learn and predict HMM-generated sequences$\\unicode{x2013}$an\nadvance that deepens our understanding of in-context learning in LLMs and\nestablishes its potential as a powerful tool for uncovering hidden structure in\ncomplex scientific data.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Large Language Models (LLMs) to learn and predict sequences generated by Hidden Markov Models (HMMs). While it doesn't directly address trajectory prediction, HMMs are sometimes used in trajectory modeling, and the paper explores the capabilities of LLMs in learning sequential patterns, which is relevant to trajectory prediction. The primary focus, however, is on LLMs and HMMs in a more general context.", "keywords": ["Large Language Models", "LLMs", "Hidden Markov Models", "HMMs", "in-context learning", "sequential data"]}}
{"id": "2506.06376", "pdf": "https://arxiv.org/pdf/2506.06376", "abs": "https://arxiv.org/abs/2506.06376", "authors": ["Heng Dong", "Kefei Duan", "Chongjie Zhang"], "title": "Enhancing Decision-Making of Large Language Models via Actor-Critic", "categories": ["cs.CL", "cs.AI"], "comment": "Forty-second International Conference on Machine Learning (ICML 2025)", "summary": "Large Language Models (LLMs) have achieved remarkable advancements in natural\nlanguage processing tasks, yet they encounter challenges in complex\ndecision-making scenarios that require long-term reasoning and alignment with\nhigh-level objectives. Existing methods either rely on short-term\nauto-regressive action generation or face limitations in accurately simulating\nrollouts and assessing outcomes, leading to sub-optimal decisions. This paper\nintroduces a novel LLM-based Actor-Critic framework, termed LAC, that\neffectively improves LLM policies with long-term action evaluations in a\nprincipled and scalable way. Our approach addresses two key challenges: (1)\nextracting robust action evaluations by computing Q-values via token logits\nassociated with positive/negative outcomes, enhanced by future trajectory\nrollouts and reasoning; and (2) enabling efficient policy improvement through a\ngradient-free mechanism. Experiments across diverse environments -- including\nhigh-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),\nand large action spaces (WebShop) -- demonstrate the framework's generality and\nsuperiority over state-of-the-art methods. Notably, our approach achieves\ncompetitive performance using 7B/8B parameter LLMs, even outperforming baseline\nmethods employing GPT-4 in complex tasks. These results underscore the\npotential of integrating structured policy optimization with LLMs' intrinsic\nknowledge to advance decision-making capabilities in multi-step environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5982\u4f55\u4f7f\u7528Actor-Critic\u6846\u67b6\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u51b3\u7b56\u5236\u5b9a\u65b9\u9762\u7684\u80fd\u529b\u3002\u867d\u7136\u63d0\u5230\u4e86\u201cfuture trajectory rollouts\u201d\uff0c\u4f46\u4e3b\u8981\u76ee\u7684\u662f\u4e3a\u4e86\u8bc4\u4f30\u884c\u52a8\u7684\u957f\u671f\u5f71\u54cd\uff0c\u800c\u975e\u8f68\u8ff9\u9884\u6d4b\u672c\u8eab\u3002\u56e0\u6b64\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u4f46\u4e3b\u8981\u7126\u70b9\u5728\u5927\u6a21\u578b\u4e0a\u3002", "keywords": ["Large Language Models", "LLMs", "Actor-Critic", "decision-making", "future trajectory rollouts"]}}
{"id": "2506.07328", "pdf": "https://arxiv.org/pdf/2506.07328", "abs": "https://arxiv.org/abs/2506.07328", "authors": ["Jintao Yan", "Tan Chen", "Yuxuan Sun", "Zhaojun Nan", "Sheng Zhou", "Zhisheng Niu"], "title": "Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification", "categories": ["cs.LG"], "comment": null, "summary": "Asynchronous Federated Learning (AFL) enables distributed model training\nacross multiple mobile devices, allowing each device to independently update\nits local model without waiting for others. However, device mobility introduces\nintermittent connectivity, which necessitates gradient sparsification and leads\nto model staleness, jointly affecting AFL convergence. This paper develops a\ntheoretical model to characterize the interplay among sparsification, model\nstaleness and mobility-induced contact patterns, and their joint impact on AFL\nconvergence. Based on the analysis, we propose a mobility-aware dynamic\nsparsification (MADS) algorithm that optimizes the sparsification degree based\non contact time and model staleness. Closed-form solutions are derived, showing\nthat under low-speed conditions, MADS increases the sparsification degree to\nenhance convergence, while under high-speed conditions, it reduces the\nsparsification degree to guarantee reliable uploads within limited contact\ntime. Experimental results validate the theoretical findings. Compared with the\nstate-of-the-art benchmarks, the MADS algorithm increases the image\nclassification accuracy on the CIFAR-10 dataset by 8.76% and reduces the\naverage displacement error in the Argoverse trajectory prediction dataset by\n9.46%.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on asynchronous federated learning and dynamic sparsification in mobile environments. While the primary focus isn't trajectory prediction or large language models, it does mention using the Argoverse trajectory prediction dataset for evaluation and the overall theme of mobility could potentially relate to trajectory prediction. There is no mention of large language models.", "keywords": ["trajectory prediction", "mobility"]}}
{"id": "2506.07407", "pdf": "https://arxiv.org/pdf/2506.07407", "abs": "https://arxiv.org/abs/2506.07407", "authors": ["Yihong Jin", "Ze Yang", "Juntian Liu", "Xinhe Xu"], "title": "Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of 2025 5th International Symposium on Computer\n  Technology and Information Science (ISCTIS 2025)", "summary": "With the rapid development of multi-cloud environments, it is increasingly\nimportant to ensure the security and reliability of intelligent monitoring\nsystems. In this paper, we propose an anomaly detection and early warning\nmechanism for intelligent monitoring system in multi-cloud environment based on\nLarge-Scale Language Model (LLM). On the basis of the existing monitoring\nframework, the proposed model innovatively introduces a multi-level feature\nextraction method, which combines the natural language processing ability of\nLLM with traditional machine learning methods to enhance the accuracy of\nanomaly detection and improve the real-time response efficiency. By introducing\nthe contextual understanding capabilities of LLMs, the model dynamically adapts\nto different cloud service providers and environments, so as to more\neffectively detect abnormal patterns and predict potential failures.\nExperimental results show that the proposed model is significantly better than\nthe traditional anomaly detection system in terms of detection accuracy and\nlatency, and significantly improves the resilience and active management\nability of cloud infrastructure.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on anomaly detection in multi-cloud environments using Large Language Models (LLMs). While it doesn't directly address trajectory prediction, the use of LLMs as the core technology increases its relevance to the 'Large Language Models' aspect of the prompt. The anomaly detection aspect could potentially be related to detecting anomalous trajectories, but this is not explicitly mentioned.", "keywords": ["Large Language Model", "LLM", "anomaly detection"]}}
{"id": "2506.07416", "pdf": "https://arxiv.org/pdf/2506.07416", "abs": "https://arxiv.org/abs/2506.07416", "authors": ["Jin Huang", "Yuchao Jin", "Le An", "Josh Park"], "title": "LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an efficient Vision-Language Model (VLM) pipeline\nspecifically optimized for deployment on embedded devices, such as those used\nin robotics and autonomous driving. The pipeline significantly reduces the\ncomputational overhead by jointly leveraging patch selection to filter\nirrelevant camera views, a token selection module to reduce input sequence\nlength for the LLM, and speculative decoding to accelerate token generation.\nEvaluation on the NVIDIA DRIVE Thor platform for automonous driving\napplication, our pipeline achieves $2.5\\times$ end-to-end latency reduction\nwithout compromising task accuracy. The speed-up further increases to\n$3.2\\times$ when applying FP8 post-training quantization. These results\ndemonstrate our pipeline as a viable solution for enabling real-time VLM\ndeployment in resource-constrained environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on optimizing Vision-Language Models (VLMs) for resource-constrained environments, specifically mentioning applications in robotics and autonomous driving. While it doesn't directly deal with trajectory prediction, the mention of autonomous driving suggests a potential connection, as trajectory prediction is a crucial component of autonomous driving systems. It also explicitly mentions the use of Large Language Models (LLMs) as part of the VLM pipeline.", "keywords": ["Large Language Models", "Vision-Language Model", "autonomous driving", "robotics"]}}
{"id": "2506.07585", "pdf": "https://arxiv.org/pdf/2506.07585", "abs": "https://arxiv.org/abs/2506.07585", "authors": ["Seokbin Yoon", "Keumjin Lee"], "title": "Aircraft Trajectory Dataset Augmentation in Latent Space", "categories": ["cs.LG"], "comment": null, "summary": "Aircraft trajectory modeling plays a crucial role in Air Traffic Management\n(ATM) and is important for various downstream tasks, including conflict\ndetection and landing time prediction. Dataset augmentation through the\naddition of synthetically generated trajectory data is necessary to develop a\nmore robust aircraft trajectory model and ensure that the trajectory dataset is\nsufficient and balanced. In this work, we propose a novel framework called\nATRADA for aircraft trajectory dataset augmentation. In the proposed framework,\na Transformer encoder learns the underlying patterns in the original trajectory\ndataset and converts each data point into a context vector in the learned\nlatent space. The converted dataset in the latent space is projected into\nreduced dimensions using principal component analysis (PCA), and a Gaussian\nmixture model (GMM) is applied to fit the probability distribution of the data\npoints in the reduced-dimensional space. Finally, new samples are drawn from\nthe fitted GMM, the dimension of the samples is reverted to the original\ndimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several\nexperiments demonstrate that the framework effectively generates new,\nhigh-quality synthetic aircraft trajectory data, which were compared to the\nresults of several baselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on aircraft trajectory modeling and dataset augmentation using a Transformer encoder. While it involves trajectory prediction and uses a Transformer-based architecture (which can be considered a precursor to large language models), it doesn't directly utilize or discuss large language models in the context of trajectory prediction. The connection to large language models is indirect through the Transformer architecture.", "keywords": ["trajectory prediction", "aircraft trajectory", "transformer", "dataset augmentation"]}}
{"id": "2506.07886", "pdf": "https://arxiv.org/pdf/2506.07886", "abs": "https://arxiv.org/abs/2506.07886", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on egocentric multimodal learning, which includes egocentric camera tracking (a form of trajectory prediction). It also mentions training a large, general-purpose model, which aligns with the concept of large models, although it is not explicitly a Large Language Model. The connection to trajectory prediction is more direct than to Large Language Models.", "keywords": ["egocentric camera tracking", "trajectory prediction", "large model", "foundation models"]}}
{"id": "2506.07981", "pdf": "https://arxiv.org/pdf/2506.07981", "abs": "https://arxiv.org/abs/2506.07981", "authors": ["Dmitrii Vorobev", "Artem Prosvetov", "Karim Elhadji Daou"], "title": "Real-time Localization of a Soccer Ball from a Single Camera", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "We propose a computationally efficient method for real-time three-dimensional\nfootball trajectory reconstruction from a single broadcast camera. In contrast\nto previous work, our approach introduces a multi-mode state model with $W$\ndiscrete modes to significantly accelerate optimization while preserving\ncentimeter-level accuracy -- even in cases of severe occlusion, motion blur,\nand complex backgrounds. The system operates on standard CPUs and achieves low\nlatency suitable for live broadcast settings. Extensive evaluation on a\nproprietary dataset of 6K-resolution Russian Premier League matches\ndemonstrates performance comparable to multi-camera systems, without the need\nfor specialized or costly infrastructure. This work provides a practical method\nfor accessible and accurate 3D ball tracking in professional football\nenvironments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on real-time 3D trajectory reconstruction of a soccer ball. While it does not explicitly mention large language models, it falls under the umbrella of trajectory prediction and movement tracking. Therefore, there is a moderate level of relevance.", "keywords": ["trajectory prediction", "real-time trajectory reconstruction", "motion tracking"]}}
{"id": "2506.07833", "pdf": "https://arxiv.org/pdf/2506.07833", "abs": "https://arxiv.org/abs/2506.07833", "authors": ["Michael K. Chen", "Xikun Zhang", "Jiaxing Huang", "Dacheng Tao"], "title": "Improving large language models with concept-aware fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on improving large language models (LLMs) through a novel fine-tuning method. While it doesn't directly address trajectory prediction, it is highly relevant to the 'Large Language Models' aspect of the task. The improvements to LLMs could potentially be applied to trajectory prediction tasks in the future, although this is not explicitly mentioned in the abstract.", "keywords": ["large language models", "LLMs", "fine-tuning", "next-token prediction", "concept-aware learning"]}}
{"id": "2506.06355", "pdf": "https://arxiv.org/pdf/2506.06355", "abs": "https://arxiv.org/abs/2506.06355", "authors": ["Lingyao Li", "Dawei Li", "Zhenhui Ou", "Xiaoran Xu", "Jingxiao Liu", "Zihui Ma", "Runlong Yu", "Min Deng"], "title": "LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment", "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.CV"], "comment": null, "summary": "Efficient simulation is essential for enhancing proactive preparedness for\nsudden-onset disasters such as earthquakes. Recent advancements in large\nlanguage models (LLMs) as world models show promise in simulating complex\nscenarios. This study examines multiple LLMs to proactively estimate perceived\nearthquake impacts. Leveraging multimodal datasets including geospatial,\nsocioeconomic, building, and street-level imagery data, our framework generates\nModified Mercalli Intensity (MMI) predictions at zip code and county scales.\nEvaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did\nYou Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced\nby a high correlation of 0.88 and a low RMSE of 0.77 as compared to real\nreports at the zip code level. Techniques such as RAG and ICL can improve\nsimulation performance, while visual inputs notably enhance accuracy compared\nto structured numerical data alone. These findings show the promise of LLMs in\nsimulating disaster impacts that can help strengthen pre-event planning.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u707e\u5bb3\u5f71\u54cd\u8bc4\u4f30\u7684\u6a21\u62df\uff0c\u6d89\u53caLLMs\u7684\u5e94\u7528\u3002\u867d\u7136\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6ca1\u6709\u76f4\u63a5\u5173\u7cfb\uff0c\u4f46LLMs\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u4e8b\u4ef6\u6a21\u62df\u7684\u6982\u5ff5\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u4e2d\u9884\u6d4b\u672a\u6765\u72b6\u6001\u7684\u601d\u60f3\u5b58\u5728\u4e00\u5b9a\u5173\u8054\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e2d\u7b49\u7a0b\u5ea6\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "world models", "simulation"]}}
{"id": "2506.06377", "pdf": "https://arxiv.org/pdf/2506.06377", "abs": "https://arxiv.org/abs/2506.06377", "authors": ["Giuseppe Arbia", "Luca Morandini", "Vincenzo Nardelli"], "title": "Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research", "categories": ["cs.CY", "cs.LG", "econ.EM", "stat.CO"], "comment": null, "summary": "This paper investigates Large Language Models (LLMs) ability to assess the\neconomic soundness and theoretical consistency of empirical findings in spatial\neconometrics. We created original and deliberately altered \"counterfactual\"\nsummaries from 28 published papers (2005-2024), which were evaluated by a\ndiverse set of LLMs. The LLMs provided qualitative assessments and structured\nbinary classifications on variable choice, coefficient plausibility, and\npublication suitability. The results indicate that while LLMs can expertly\nassess the coherence of variable choices (with top models like GPT-4o achieving\nan overall F1 score of 0.87), their performance varies significantly when\nevaluating deeper aspects such as coefficient plausibility and overall\npublication suitability. The results further revealed that the choice of LLM,\nthe specific characteristics of the paper and the interaction between these two\nfactors significantly influence the accuracy of the assessment, particularly\nfor nuanced judgments. These findings highlight LLMs' current strengths in\nassisting with initial, more surface-level checks and their limitations in\nperforming comprehensive, deep economic reasoning, suggesting a potential\nassistive role in peer review that still necessitates robust human oversight.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on evaluating the capabilities of Large Language Models (LLMs) in assessing spatial econometrics research. While it discusses LLMs extensively, it doesn't directly address trajectory prediction. However, the use of LLMs for evaluating research papers is relevant to the broader application of LLMs in research and analysis, making it moderately relevant. The connection to trajectory prediction is indirect, through the general application of LLMs.", "keywords": ["Large Language Models", "LLMs", "GPT-4o", "foundation models"]}}
{"id": "2506.07209", "pdf": "https://arxiv.org/pdf/2506.07209", "abs": "https://arxiv.org/abs/2506.07209", "authors": ["Lei Li", "Angela Dai"], "title": "HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://hoipage.github.io/ Video:\n  https://youtu.be/b1pJU9lKQTE", "summary": "We present HOI-PAGE, a new approach to synthesizing 4D human-object\ninteractions (HOIs) from text prompts in a zero-shot fashion, driven by\npart-level affordance reasoning. In contrast to prior works that focus on\nglobal, whole body-object motion for 4D HOI synthesis, we observe that\ngenerating realistic and diverse HOIs requires a finer-grained understanding --\nat the level of how human body parts engage with object parts. We thus\nintroduce Part Affordance Graphs (PAGs), a structured HOI representation\ndistilled from large language models (LLMs) that encodes fine-grained part\ninformation along with contact relations. We then use these PAGs to guide a\nthree-stage synthesis: first, decomposing input 3D objects into geometric\nparts; then, generating reference HOI videos from text prompts, from which we\nextract part-based motion constraints; finally, optimizing for 4D HOI motion\nsequences that not only mimic the reference dynamics but also satisfy\npart-level contact constraints. Extensive experiments show that our approach is\nflexible and capable of generating complex multi-object or multi-person\ninteraction sequences, with significantly improved realism and text alignment\nfor zero-shot 4D HOI generation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on generating human-object interactions (HOIs) from text prompts using part affordance guidance. It leverages large language models (LLMs) to create Part Affordance Graphs (PAGs). While not directly focused on trajectory prediction, the generation of 4D HOI motion sequences can be considered a form of action prediction or motion planning, which is related to trajectory prediction. The use of LLMs is a key element linking it to the large language model domain.", "keywords": ["Large Language Models", "LLMs", "Human-Object Interaction", "motion sequences", "action prediction"]}}
{"id": "2506.07232", "pdf": "https://arxiv.org/pdf/2506.07232", "abs": "https://arxiv.org/abs/2506.07232", "authors": ["Xinran Li", "Chenjia Bai", "Zijian Li", "Jiakun Zheng", "Ting Xiao", "Jun Zhang"], "title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) possess extensive knowledge bases and strong\nreasoning capabilities, making them promising tools for complex, multi-agent\nplanning in embodied environments. However, despite LLMs' advanced abilities\nand the sophisticated modular design of agentic methods, existing LLM-based\nplanning algorithms remain limited by weak adaptation capabilities to\nmulti-agent embodied scenarios. We address this limitation by introducing a\nframework that enables LLM agents to learn and evolve both before and during\ntest time, equipping them with environment-relevant knowledge for better\nplanning and enhanced communication for improved cooperation. Inspired by\ncentralized training with decentralized execution in multi-agent reinforcement\nlearning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)}\nparadigm for multi-agent LLMs adaptation. At the individual level, LLM agents\nlearn a local utility function from exploratory datasets to better comprehend\nthe embodied environment, which is then queried during test time to support\ninformed decision-making. At the team level, LLM agents collaboratively and\niteratively maintain and update a shared cooperation knowledge list based on\nnew experiences, using it to guide more effective communication. By combining\nindividual learning with team evolution, LIET enables comprehensive and\nflexible adaptation for LLM agents. Our experiments on Communicative\nWatch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate\nthat LIET, instantiated with both LLaMA and GPT-4o, outperforms existing\nbaselines and exhibits strong cooperative planning abilities.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on multi-agent systems in embodied environments, leveraging Large Language Models (LLMs) for planning and cooperation. While not directly addressing trajectory prediction, the planning aspect in embodied environments can be related to trajectory generation or path planning. The strong emphasis on LLMs contributes to the relevance.", "keywords": ["Large Language Models", "LLMs", "multi-agent", "planning", "embodied environments", "cooperation"]}}
{"id": "2506.07398", "pdf": "https://arxiv.org/pdf/2506.07398", "abs": "https://arxiv.org/abs/2506.07398", "authors": ["Guibin Zhang", "Muxin Fu", "Guancheng Wan", "Miao Yu", "Kun Wang", "Shuicheng Yan"], "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "categories": ["cs.MA", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5e76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u5176\u6838\u5fc3\u7ec4\u4ef6\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201cinteraction trajectories\u201d\uff0c\u6697\u793a\u4e86\u667a\u80fd\u4f53\u4e4b\u95f4\u4ea4\u4e92\u884c\u4e3a\u7684\u8f68\u8ff9\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u6b64\u5916\uff0c\u8bba\u6587\u7684\u6838\u5fc3\u662f\u5229\u7528LLM\u6765\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u80fd\u529b\u3002", "keywords": ["Large language model (LLM)", "multi-agent systems (MAS)", "interaction trajectories"]}}
{"id": "2506.07962", "pdf": "https://arxiv.org/pdf/2506.07962", "abs": "https://arxiv.org/abs/2506.07962", "authors": ["Elliot Kim", "Avi Garg", "Kenny Peng", "Nikhil Garg"], "title": "Correlated Errors in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Diversity in training data, architecture, and providers is assumed to\nmitigate homogeneity in LLMs. However, we lack empirical evidence on whether\ndifferent LLMs differ meaningfully. We conduct a large-scale empirical\nevaluation on over 350 LLMs overall, using two popular leaderboards and a\nresume-screening task. We find substantial correlation in model errors -- on\none leaderboard dataset, models agree 60% of the time when both models err. We\nidentify factors driving model correlation, including shared architectures and\nproviders. Crucially, however, larger and more accurate models have highly\ncorrelated errors, even with distinct architectures and providers. Finally, we\nshow the effects of correlation in two downstream tasks: LLM-as-judge\nevaluation and hiring -- the latter reflecting theoretical predictions\nregarding algorithmic monoculture.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u9519\u8bef\u76f8\u5173\u6027\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u6838\u5fc3\u5173\u6ce8\u70b9\u5728\u4e8eLLMs\u7684\u6027\u80fd\u8bc4\u4f30\u548c\u7279\u6027\u5206\u6790\uff0c\u4e0e\u5927\u6a21\u578b\u9886\u57df\u5bc6\u5207\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["Large Language Models", "LLMs", "foundation models"]}}
