{"id": "2509.10714", "pdf": "https://arxiv.org/pdf/2509.10714", "abs": "https://arxiv.org/abs/2509.10714", "authors": ["Tony Astolfi", "Vidya Silai", "Darby Huye", "Lan Liu", "Raja R. Sambasivan", "Johes Bater"], "title": "Dynamic read & write optimization with TurtleKV", "categories": ["cs.DB"], "comment": null, "summary": "High read and write performance is important for generic key/value stores,\nwhich are fundamental to modern applications and databases. Yet, achieving high\nperformance for both reads and writes is challenging due to traditionally\nlimited memory and the pick-any-two-out-of-three tradeoff between memory use,\nread performance, and write performance. Existing state-of-the-art approaches\nlimit memory usage and chose a primary dimension (reads or writes) for which to\noptimize their on-disk structures. They recover performance in the remaining\ndimension by other mechanisms. This approach limits databases' maximum\nperformance in the remaining dimension and their dynamic (online) tunability to\nrespond to changing workloads. We explore a different approach that dynamically\ntrades memory for read or write performance as needed. We present TurtleKV,\nwhich includes a novel unbiased data structure for on-disk storage. It includes\na knob that dynamically increases memory reserved for increasing read or write\nperformance. When evaluated on YCSB, TurtleKV achieves up to 8x the write\nthroughput of industry-leader RocksDB and up to 5x the read throughput while\nincurring similar space amplification. Compared to the state-of-the-art system\nSplinterDB, TurtleKV runs up to 40% better on point queries, up to 6x better on\nrange scans and achieves similar write performance, while incurring 50% less\nspace amplification.", "AI": {"tldr": "TurtleKV\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5185\u5b58\u4f7f\u7528\uff0c\u5728\u8bfb\u5199\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u952e\u503c\u5b58\u50a8\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u952e\u503c\u5b58\u50a8\u5728\u5185\u5b58\u3001\u8bfb\u6027\u80fd\u548c\u5199\u6027\u80fd\u4e4b\u95f4\u5b58\u5728tradeoff\uff0c\u96be\u4ee5\u517c\u987e\u9ad8\u8bfb\u5199\u6027\u80fd\u548c\u52a8\u6001\u9002\u5e94\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u504f\u6570\u636e\u7ed3\u6784\uff0c\u5141\u8bb8\u52a8\u6001\u5730\u5c06\u5185\u5b58\u5206\u914d\u7ed9\u8bfb\u6216\u5199\u64cd\u4f5c\uff0c\u4ece\u800c\u4f18\u5316\u6027\u80fd\u3002", "result": "\u5728YCSB\u6d4b\u8bd5\u4e2d\uff0cTurtleKV\u7684\u5199\u541e\u5410\u91cf\u6bd4RocksDB\u9ad88\u500d\uff0c\u8bfb\u541e\u5410\u91cf\u9ad85\u500d\uff0c\u7a7a\u95f4\u653e\u5927\u76f8\u4f3c\u3002\u4e0eSplinterDB\u76f8\u6bd4\uff0c\u70b9\u67e5\u8be2\u6027\u80fd\u63d0\u534740%\uff0c\u8303\u56f4\u626b\u63cf\u6027\u80fd\u63d0\u53476\u500d\uff0c\u5199\u6027\u80fd\u76f8\u4f3c\uff0c\u7a7a\u95f4\u653e\u5927\u51cf\u5c1150%\u3002", "conclusion": "TurtleKV\u901a\u8fc7\u52a8\u6001\u5185\u5b58\u7ba1\u7406\u548c\u65b0\u7684\u6570\u636e\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u548c\u7075\u6d3b\u7684\u952e\u503c\u5b58\u50a8\u3002", "summary_zh": "\u5bf9\u4e8e\u73b0\u4ee3\u5e94\u7528\u548c\u6570\u636e\u5e93\u81f3\u5173\u91cd\u8981\u7684\u901a\u7528\u952e\u503c\u5b58\u50a8\u6765\u8bf4\uff0c\u9ad8\u8bfb\u5199\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4f20\u7edf\u4e0a\u6709\u9650\u7684\u5185\u5b58\u4ee5\u53ca\u5728\u5185\u5b58\u4f7f\u7528\u3001\u8bfb\u6027\u80fd\u548c\u5199\u6027\u80fd\u4e4b\u95f4\u4e09\u9009\u4e8c\u7684\u6743\u8861\uff0c\u540c\u65f6\u5b9e\u73b0\u9ad8\u8bfb\u5199\u6027\u80fd\u6781\u5177\u6311\u6218\u3002\u76ee\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9650\u5236\u4e86\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u9009\u62e9\u4e00\u4e2a\u4e3b\u8981\u7ef4\u5ea6\uff08\u8bfb\u6216\u5199\uff09\u6765\u4f18\u5316\u5176\u78c1\u76d8\u7ed3\u6784\u3002\u5b83\u4eec\u901a\u8fc7\u5176\u4ed6\u673a\u5236\u6765\u6062\u590d\u5269\u4f59\u7ef4\u5ea6\u7684\u6027\u80fd\u3002\u8fd9\u79cd\u65b9\u6cd5\u9650\u5236\u4e86\u6570\u636e\u5e93\u5728\u5269\u4f59\u7ef4\u5ea6\u4e0a\u7684\u6700\u5927\u6027\u80fd\uff0c\u4ee5\u53ca\u5b83\u4eec\u52a8\u6001\uff08\u5728\u7ebf\uff09\u8c03\u6574\u4ee5\u54cd\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u5de5\u4f5c\u8d1f\u8f7d\u7684\u80fd\u529b\u3002\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6839\u636e\u9700\u8981\u52a8\u6001\u5730\u7528\u5185\u5b58\u6362\u53d6\u8bfb\u6216\u5199\u6027\u80fd\u3002\u6211\u4eec\u63d0\u51fa\u4e86TurtleKV\uff0c\u5b83\u5305\u62ec\u4e00\u79cd\u7528\u4e8e\u78c1\u76d8\u5b58\u50a8\u7684\u65b0\u578b\u65e0\u504f\u6570\u636e\u7ed3\u6784\u3002\u5b83\u5305\u62ec\u4e00\u4e2a\u65cb\u94ae\uff0c\u53ef\u4ee5\u52a8\u6001\u589e\u52a0\u4e3a\u63d0\u9ad8\u8bfb\u53d6\u6216\u5199\u5165\u6027\u80fd\u800c\u4fdd\u7559\u7684\u5185\u5b58\u3002\u5728YCSB\u4e0a\u8fdb\u884c\u8bc4\u4f30\u65f6\uff0cTurtleKV\u7684\u5199\u5165\u541e\u5410\u91cf\u662f\u884c\u4e1a\u9886\u5bfc\u8005RocksDB\u76848\u500d\uff0c\u8bfb\u53d6\u541e\u5410\u91cf\u662f\u51765\u500d\uff0c\u540c\u65f6\u4ea7\u751f\u7684\u7a7a\u95f4\u653e\u5927\u7387\u76f8\u4f3c\u3002\u4e0e\u6700\u5148\u8fdb\u7684\u7cfb\u7edfSplinterDB\u76f8\u6bd4\uff0cTurtleKV\u5728\u70b9\u67e5\u8be2\u4e0a\u7684\u6027\u80fd\u63d0\u9ad8\u4e8640%\uff0c\u5728\u8303\u56f4\u626b\u63cf\u4e0a\u7684\u6027\u80fd\u63d0\u9ad8\u4e866\u500d\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u76f8\u4f3c\u7684\u5199\u5165\u6027\u80fd\uff0c\u540c\u65f6\u7a7a\u95f4\u653e\u5927\u51cf\u5c11\u4e8650%\u3002"}}
{"id": "2509.11920", "pdf": "https://arxiv.org/pdf/2509.11920", "abs": "https://arxiv.org/abs/2509.11920", "authors": ["Kyle Deeds", "Timo Camillo Merkl", "Reinhard Pichler", "Dan Suciu"], "title": "The Space-Time Complexity of Sum-Product Queries", "categories": ["cs.DB"], "comment": null, "summary": "While extensive research on query evaluation has achieved consistent\nimprovements in the time complexity of algorithms, the space complexity of\nquery evaluation has been largely ignored. This is a particular challenge in\nsettings with strict pre-defined space constraints. In this paper, we examine\nthe combined space-time complexity of conjunctive queries (CQs) and, more\ngenerally, of sum-product queries (SPQs). We propose several classes of\nspace-efficient algorithms for evaluating SPQs, and we show that the optimal\ntime complexity is almost always achievable with asymptotically lower space\ncomplexity than traditional approaches.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.11929", "pdf": "https://arxiv.org/pdf/2509.11929", "abs": "https://arxiv.org/abs/2509.11929", "authors": ["Marcelo Arenas", "Timo Camillo Merkl", "Reinhard Pichler", "Cristian Riveros"], "title": "Query Answering under Volume-Based Diversity Functions", "categories": ["cs.DB"], "comment": null, "summary": "When query evaluation produces too many tuples, a new approach in query\nanswering is to retrieve a diverse subset of them. The standard approach for\nmeasuring the diversity of a set of tuples is to use a distance function\nbetween tuples, which measures the dissimilarity between them, to then\naggregate the pairwise distances of the set into a score (e.g., by using sum or\nmin aggregation). However, as we will point out in this work, the resulting\ndiversity measures may display some unintuitive behavior. Moreover, even in\nvery simple settings, finding a maximally diverse subset of the answers of\nfixed size is, in general, intractable and little is known about approximations\napart from some hand-picked distance-aggregator pairs.\n  In this work, we introduce a novel approach for computing the diversity of\ntuples based on volume instead of distance. We present a framework for defining\nvolume-based diversity functions and provide several examples of these measures\napplied to relational data. Although query answering of conjunctive queries\n(CQ) under this setting is intractable in general, we show that one can always\ncompute a (1-1/e)-approximation for any volume-based diversity function.\nFurthermore, in terms of combined complexity, we connect the evaluation of CQs\nunder volume-based diversity functions with the ranked enumeration of\nsolutions, finding general conditions under which a (1-1/e)-approximation can\nbe computed in polynomial time.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12086", "pdf": "https://arxiv.org/pdf/2509.12086", "abs": "https://arxiv.org/abs/2509.12086", "authors": ["Hui Li", "Shiyuan Deng", "Xiao Yan", "Xiangyu Zhi", "James Cheng"], "title": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation", "categories": ["cs.DB", "cs.DS", "cs.IR"], "comment": "13 pages, 12 figures, accepted by SIGMOD", "summary": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in\napplications such as search engines, recommender systems, and RAG for LLMs.\nVector quantization (VQ), a crucial technique for ANNS, is commonly used to\nreduce space overhead and accelerate distance computations. However, despite\nsignificant research advances, state-of-the-art VQ methods still face\nchallenges in balancing encoding efficiency and quantization accuracy. To\naddress these limitations, we propose a novel VQ method called SAQ. To improve\naccuracy, SAQ employs a new dimension segmentation technique to strategically\npartition PCA-projected vectors into segments along their dimensions. By\nprioritizing leading dimension segments with larger magnitudes, SAQ allocates\nmore bits to high-impact segments, optimizing the use of the available space\nquota. An efficient dynamic programming algorithm is developed to optimize\ndimension segmentation and bit allocation, ensuring minimal quantization error.\nTo speed up vector encoding, SAQ devises a code adjustment technique to first\nquantize each dimension independently and then progressively refine quantized\nvectors using a coordinate-descent-like approach to avoid exhaustive\nenumeration. Extensive experiments demonstrate SAQ's superiority over classical\nmethods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,\nExtended RabitQ). SAQ achieves up to 80% reduction in quantization error and\naccelerates encoding speed by over 80x compared to Extended RabitQ.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12189", "pdf": "https://arxiv.org/pdf/2509.12189", "abs": "https://arxiv.org/abs/2509.12189", "authors": ["Elena Botoeva", "Julien Corman"], "title": "Towards a Standard for JSON Document Databases", "categories": ["cs.DB"], "comment": null, "summary": "In this technical report, we present a formalisation of the MongoDB\naggregation framework. Our aim is to identify a fragment that could serve as\nthe starting point for an industry-wide standard for querying JSON document\ndatabases. We provide a syntax and formal semantics for a set of selected\noperators, We show how this fragment relates to known relational query\nlanguages. We explain how our semantics differs from the current implementation\nof MongoDB, and justify our choices. We provide a set of algebraic\ntransformations that can be used for query optimisation.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.10572", "pdf": "https://arxiv.org/pdf/2509.10572", "abs": "https://arxiv.org/abs/2509.10572", "authors": ["Ashlesha Akella", "Akshar Kaul", "Krishnasuri Narayanam", "Sameep Mehta"], "title": "Quality Assessment of Tabular Data using Large Language Models and Code Generation", "categories": ["cs.SE", "cs.AI", "cs.DB"], "comment": "EMNLP industry track submitted", "summary": "Reliable data quality is crucial for downstream analysis of tabular datasets,\nyet rule-based validation often struggles with inefficiency, human\nintervention, and high computational costs. We present a three-stage framework\nthat combines statistical inliner detection with LLM-driven rule and code\ngeneration. After filtering data samples through traditional clustering, we\niteratively prompt LLMs to produce semantically valid quality rules and\nsynthesize their executable validators through code-generating LLMs. To\ngenerate reliable quality rules, we aid LLMs with retrieval-augmented\ngeneration (RAG) by leveraging external knowledge sources and domain-specific\nfew-shot examples. Robust guardrails ensure the accuracy and consistency of\nboth rules and code snippets. Extensive evaluations on benchmark datasets\nconfirm the effectiveness of our approach.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.10793", "pdf": "https://arxiv.org/pdf/2509.10793", "abs": "https://arxiv.org/abs/2509.10793", "authors": ["Eli Baum", "Sam Buxbaum", "Nitin Mathai", "Muhammad Faisal", "Vasiliki Kalavri", "Mayank Varia", "John Liagouris"], "title": "ORQ: Complex Analytics on Private Data with Strong Security Guarantees", "categories": ["cs.CR", "cs.DB"], "comment": "14 pages, plus Appendix. To appear at SOSP 2025. Code published at\n  https://github.com/CASP-Systems-BU/orq", "summary": "We present ORQ, a system that enables collaborative analysis of large private\ndatasets using cryptographically secure multi-party computation (MPC). ORQ\nprotects data against semi-honest or malicious parties and can efficiently\nevaluate relational queries with multi-way joins and aggregations that have\nbeen considered notoriously expensive under MPC. To do so, ORQ eliminates the\nquadratic cost of secure joins by leveraging the fact that, in practice, the\nstructure of many real queries allows us to join records and apply the\naggregations \"on the fly\" while keeping the result size bounded. On the system\nside, ORQ contributes generic oblivious operators, a data-parallel vectorized\nquery engine, a communication layer that amortizes MPC network costs, and a\ndataflow API for expressing relational analytics -- all built from the ground\nup.\n  We evaluate ORQ in LAN and WAN deployments on a diverse set of workloads,\nincluding complex queries with multiple joins and custom aggregations. When\ncompared to state-of-the-art solutions, ORQ significantly reduces MPC execution\ntimes and can process one order of magnitude larger datasets. For our most\nchallenging workload, the full TPC-H benchmark, we report results entirely\nunder MPC with Scale Factor 10 -- a scale that had previously been achieved\nonly with information leakage or the use of trusted third parties.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
