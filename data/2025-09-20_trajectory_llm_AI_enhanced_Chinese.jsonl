{"id": "2509.14295", "pdf": "https://arxiv.org/pdf/2509.14295", "abs": "https://arxiv.org/abs/2509.14295", "authors": ["Fanqi Kong", "Ruijie Zhang", "Huaxiao Yin", "Guibin Zhang", "Xiaofei Zhang", "Ziang Chen", "Zhaowei Zhang", "Xiaoyuan Zhang", "Song-Chun Zhu", "Xue Feng"], "title": "AEGIS: Automated Error Generation and Identification for Multi-Agent Systems", "categories": ["cs.RO"], "comment": null, "summary": "As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "The paper focuses on error generation and identification in multi-agent systems. While it doesn't directly perform trajectory prediction, it addresses the reliability and safety of these systems, which often involve trajectory planning and execution. Furthermore, it explicitly uses an LLM-based adaptive manipulator for error injection, indicating a significant connection to large language models. The context of multi-agent systems implicitly links to trajectory prediction as agents need to plan and follow trajectories.", "keywords": ["Multi-Agent Systems", "LLM", "Large Language Models", "error generation", "trajectory"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86AEGIS\uff0c\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u548c\u8bc6\u522b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9519\u8bef\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5730\u6ce8\u5165\u53ef\u63a7\u548c\u53ef\u8ffd\u8e2a\u7684\u9519\u8bef\u6765\u521b\u5efa\u4e30\u5bcc\u7684\u6545\u969c\u6570\u636e\u96c6\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u53d8\u5f97\u8d8a\u6765\u8d8a\u81ea\u4e3b\u548c\u590d\u6742\uff0c\u7406\u89e3\u5b83\u4eec\u7684\u9519\u8bef\u6a21\u5f0f\u5bf9\u4e8e\u786e\u4fdd\u5176\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5177\u6709\u7cbe\u786e\u7684\u3001\u5e26\u6709ground-truth\u9519\u8bef\u6807\u7b7e\u7684\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u4e25\u91cd\u963b\u788d\u4e86\u8fd9\u65b9\u9762\u7684\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5c06\u53ef\u63a7\u548c\u53ef\u8ffd\u8e2a\u7684\u9519\u8bef\u7cfb\u7edf\u5730\u6ce8\u5165\u5230\u6700\u521d\u6210\u529f\u7684\u8f68\u8ff9\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u9002\u5e94\u64cd\u7eb5\u5668\u6267\u884c\u8bf8\u5982prompt\u6ce8\u5165\u548c\u54cd\u5e94\u635f\u574f\u7b49\u590d\u6742\u653b\u51fb\uff0c\u4ee5\u8bf1\u5bfc\u7279\u5b9a\u7684\u3001\u9884\u5b9a\u4e49\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u4ece\u800c\u521b\u5efa\u4e00\u4e2a\u4e30\u5bcc\u7684\u771f\u5b9e\u6545\u969c\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u63a2\u7d22\u7528\u4e8e\u9519\u8bef\u8bc6\u522b\u4efb\u52a1\u7684\u4e09\u79cd\u4e0d\u540c\u7684\u5b66\u4e60\u8303\u5f0f\uff1a\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u6570\u636e\u96c6\u7684\u4ef7\u503c\u3002\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u5728AEGIS\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6240\u6709\u4e09\u79cd\u5b66\u4e60\u8303\u5f0f\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u4e00\u4e9b\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u4e13\u6709\u7cfb\u7edf\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u8d8a\u3002", "conclusion": "\u672c\u6587\u9a8c\u8bc1\u4e86\u81ea\u52a8\u6570\u636e\u751f\u6210\u6846\u67b6\u662f\u5f00\u53d1\u66f4\u5065\u58ee\u548c\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5173\u952e\u8d44\u6e90\u3002", "summary_zh": "\u968f\u7740\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u53d8\u5f97\u8d8a\u6765\u8d8a\u81ea\u4e3b\u548c\u590d\u6742\uff0c\u7406\u89e3\u5b83\u4eec\u7684\u9519\u8bef\u6a21\u5f0f\u5bf9\u4e8e\u786e\u4fdd\u5176\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7f3a\u4e4f\u5177\u6709\u7cbe\u786e\u7684\u3001\u5e26\u6709ground-truth\u9519\u8bef\u6807\u7b7e\u7684\u5927\u89c4\u6a21\u591a\u6837\u5316\u6570\u636e\u96c6\u4e25\u91cd\u963b\u788d\u4e86\u8fd9\u65b9\u9762\u7684\u7814\u7a76\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u74f6\u9888\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86AEGIS\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u81ea\u52a8\u9519\u8bef\u751f\u6210\u548c\u8bc6\u522b\u7684\u65b0\u6846\u67b6\u3002\u901a\u8fc7\u5c06\u53ef\u63a7\u548c\u53ef\u8ffd\u8e2a\u7684\u9519\u8bef\u7cfb\u7edf\u5730\u6ce8\u5165\u5230\u6700\u521d\u6210\u529f\u7684\u8f68\u8ff9\u4e2d\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u4e30\u5bcc\u7684\u771f\u5b9e\u6545\u969c\u6570\u636e\u96c6\u3002\u8fd9\u662f\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u81ea\u9002\u5e94\u64cd\u7eb5\u5668\u6765\u5b9e\u73b0\u7684\uff0c\u8be5\u64cd\u7eb5\u5668\u6267\u884c\u8bf8\u5982prompt\u6ce8\u5165\u548c\u54cd\u5e94\u635f\u574f\u7b49\u590d\u6742\u653b\u51fb\uff0c\u4ee5\u8bf1\u5bfc\u7279\u5b9a\u7684\u3001\u9884\u5b9a\u4e49\u7684\u9519\u8bef\u6a21\u5f0f\u3002\u6211\u4eec\u901a\u8fc7\u63a2\u7d22\u7528\u4e8e\u9519\u8bef\u8bc6\u522b\u4efb\u52a1\u7684\u4e09\u79cd\u4e0d\u540c\u7684\u5b66\u4e60\u8303\u5f0f\uff1a\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u6570\u636e\u96c6\u7684\u4ef7\u503c\u3002\u6211\u4eec\u5168\u9762\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728AEGIS\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u6240\u6709\u4e09\u79cd\u5b66\u4e60\u8303\u5f0f\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u6211\u4eec\u7684\u4e00\u4e9b\u5fae\u8c03\u6a21\u578b\u7684\u6027\u80fd\u4e0e\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u4e13\u6709\u7cfb\u7edf\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u8d8a\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u81ea\u52a8\u6570\u636e\u751f\u6210\u6846\u67b6\u662f\u5f00\u53d1\u66f4\u5065\u58ee\u548c\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5173\u952e\u8d44\u6e90\u3002\u6211\u4eec\u7684\u9879\u76ee\u7f51\u7ad9\u662fhttps://kfq20.github.io/AEGIS-Website\u3002"}}
{"id": "2509.14380", "pdf": "https://arxiv.org/pdf/2509.14380", "abs": "https://arxiv.org/abs/2509.14380", "authors": ["Seoyeon Choi", "Kanghyun Ryu", "Jonghoon Ock", "Negar Mehr"], "title": "CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks", "categories": ["cs.RO"], "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for\nlearning coordination in multi-agent systems. However, applying MARL to\nrobotics still remains challenging due to high-dimensional continuous joint\naction spaces, complex reward design, and non-stationary transitions inherent\nto decentralized settings. On the other hand, humans learn complex coordination\nthrough staged curricula, where long-horizon behaviors are progressively built\nupon simpler skills. Motivated by this, we propose CRAFT: Coaching\nReinforcement learning Autonomously using Foundation models for multi-robot\ncoordination Tasks, a framework that leverages the reasoning capabilities of\nfoundation models to act as a \"coach\" for multi-robot coordination. CRAFT\nautomatically decomposes long-horizon coordination tasks into sequences of\nsubtasks using the planning capability of Large Language Models (LLMs). In what\nfollows, CRAFT trains each subtask using reward functions generated by LLM, and\nrefines them through a Vision Language Model (VLM)-guided reward-refinement\nloop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation\ntasks, demonstrating its capability to learn complex coordination behaviors. In\naddition, we validate the multi-quadruped navigation policy in real hardware\nexperiments.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper is related to both trajectory prediction and large language models. It uses large language models (LLMs) to decompose multi-robot coordination tasks, which implicitly involves predicting the trajectories of the robots. The use of reinforcement learning and vision language models further connects to the problem of learning and refining these trajectories.", "keywords": ["Large Language Models", "LLMs", "Foundation Models", "Multi-robot coordination", "Reinforcement Learning", "Vision Language Model", "navigation"]}, "AI": {"tldr": "CRAFT\u6846\u67b6\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u89e3\u590d\u6742\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u9886\u57df\u9762\u4e34\u9ad8\u7ef4\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u3001\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u548c\u975e\u5e73\u7a33\u73af\u5883\u7b49\u6311\u6218\uff1b\u4eba\u7c7b\u901a\u8fc7\u5206\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u590d\u6742\u534f\u4f5c\uff0c\u53d7\u6b64\u542f\u53d1\uff0cCRAFT\u6846\u67b6\u65e8\u5728\u5229\u7528\u5927\u578b\u6a21\u578b\u4f5c\u4e3a\u201c\u6559\u7ec3\u201d\u6765\u6307\u5bfc\u591a\u673a\u5668\u4eba\u534f\u4f5c\u3002", "method": "CRAFT\u6846\u67b6\u9996\u5148\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u957f\u65f6\u7a0b\u534f\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e8f\u5217\uff0c\u7136\u540e\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5956\u52b1\u51fd\u6570\u6765\u8bad\u7ec3\u6bcf\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u5956\u52b1\u4f18\u5316\u5faa\u73af\u6765\u6539\u8fdb\u5b83\u4eec\u3002", "result": "\u5728\u591a\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u548c\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cCRAFT\u6846\u67b6\u80fd\u591f\u5b66\u4e60\u590d\u6742\u7684\u534f\u4f5c\u884c\u4e3a\uff1b\u6b64\u5916\uff0c\u8fd8\u5728\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u591a\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u7b56\u7565\u3002", "conclusion": "CRAFT\u6846\u67b6\u5229\u7528\u5927\u578b\u6a21\u578b\u5206\u89e3\u4efb\u52a1\u548c\u4f18\u5316\u5956\u52b1\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u65b9\u6cd5\u3002", "summary_zh": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e3a\u5b66\u4e60\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u9ad8\u7ef4\u8fde\u7eed\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u3001\u590d\u6742\u7684\u5956\u52b1\u8bbe\u8ba1\u4ee5\u53ca\u5206\u6563\u8bbe\u7f6e\u4e2d\u56fa\u6709\u7684\u975e\u5e73\u7a33\u8f6c\u6362\uff0c\u5c06MARL\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u53e6\u4e00\u65b9\u9762\uff0c\u4eba\u7c7b\u901a\u8fc7\u5206\u9636\u6bb5\u7684\u8bfe\u7a0b\u5b66\u4e60\u590d\u6742\u7684\u534f\u4f5c\uff0c\u5176\u4e2d\u957f\u671f\u7684\u884c\u4e3a\u662f\u9010\u6b65\u5efa\u7acb\u5728\u66f4\u7b80\u5355\u7684\u6280\u80fd\u4e4b\u4e0a\u7684\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86CRAFT\uff1a\u4f7f\u7528\u57fa\u7840\u6a21\u578b\u81ea\u4e3b\u8fdb\u884c\u6559\u7ec3\u5f0f\u5f3a\u5316\u5b66\u4e60\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u4efb\u52a1\uff0c\u8be5\u6846\u67b6\u5229\u7528\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u6765\u5145\u5f53\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7684\u201c\u6559\u7ec3\u201d\u3002CRAFT\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u89c4\u5212\u80fd\u529b\uff0c\u81ea\u52a8\u5c06\u957f\u65f6\u7a0b\u534f\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e8f\u5217\u3002\u63a5\u4e0b\u6765\uff0cCRAFT\u4f7f\u7528LLM\u751f\u6210\u7684\u5956\u52b1\u51fd\u6570\u8bad\u7ec3\u6bcf\u4e2a\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5f15\u5bfc\u7684\u5956\u52b1\u4f18\u5316\u5faa\u73af\u6765\u6539\u8fdb\u5b83\u4eec\u3002\u6211\u4eec\u5728\u591a\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u548c\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8bc4\u4f30CRAFT\uff0c\u8bc1\u660e\u4e86\u5b83\u5b66\u4e60\u590d\u6742\u534f\u4f5c\u884c\u4e3a\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5728\u771f\u5b9e\u7684\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u591a\u8db3\u673a\u5668\u4eba\u5bfc\u822a\u7b56\u7565\u3002"}}
{"id": "2509.14548", "pdf": "https://arxiv.org/pdf/2509.14548", "abs": "https://arxiv.org/abs/2509.14548", "authors": ["Emily Sumner", "Deepak E. Gopinath", "Laporsha Dees", "Patricio Reyes Gomez", "Xiongyi Cui", "Andrew Silva", "Jean Costa", "Allison Morgan", "Mariah Schrum", "Tiffany L. Chen", "Avinash Balachandran", "Guy Rosman"], "title": "SimCoachCorpus: A naturalistic dataset with language and trajectories for embodied teaching", "categories": ["cs.RO", "cs.HC"], "comment": null, "summary": "Curated datasets are essential for training and evaluating AI approaches, but\nare often lacking in domains where language and physical action are deeply\nintertwined. In particular, few datasets capture how people acquire embodied\nskills through verbal instruction over time. To address this gap, we introduce\nSimCoachCorpus: a unique dataset of race car simulator driving that allows for\nthe investigation of rich interactive phenomena during guided and unguided\nmotor skill acquisition. In this dataset, 29 humans were asked to drive in a\nsimulator around a race track for approximately ninety minutes. Fifteen\nparticipants were given personalized one-on-one instruction from a professional\nperformance driving coach, and 14 participants drove without coaching. \\name\\\nincludes embodied features such as vehicle state and inputs, map (track\nboundaries and raceline), and cone landmarks. These are synchronized with\nconcurrent verbal coaching from a professional coach and additional feedback at\nthe end of each lap. We further provide annotations of coaching categories for\neach concurrent feedback utterance, ratings on students' compliance with\ncoaching advice, and self-reported cognitive load and emotional state of\nparticipants (gathered from surveys during the study). The dataset includes\nover 20,000 concurrent feedback utterances, over 400 terminal feedback\nutterances, and over 40 hours of vehicle driving data. Our naturalistic dataset\ncan be used for investigating motor learning dynamics, exploring linguistic\nphenomena, and training computational models of teaching. We demonstrate\napplications of this dataset for in-context learning, imitation learning, and\ntopic modeling. The dataset introduced in this work will be released publicly\nupon publication of the peer-reviewed version of this paper. Researchers\ninterested in early access may register at\nhttps://tinyurl.com/SimCoachCorpusForm.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u5305\u542b\u8f66\u8f86\u8f68\u8ff9\u6570\u636e\uff08race car simulator driving data\uff09\u548c\u8bed\u8a00\u6570\u636e\uff08verbal coaching\uff09\uff0c\u867d\u7136\u6ca1\u6709\u660e\u786e\u63d0\u53ca\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u7ed3\u5408\u8bed\u8a00\u548c\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u72ec\u7279\u6570\u636e\u96c6\u3002\u6570\u636e\u96c6\u53ef\u4ee5\u7528\u4e8eimitation learning\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u540c\u65f6\uff0c\u8bed\u8a00\u6570\u636e\u4e5f\u4e3a\u4f7f\u7528LLM\u8fdb\u884c\u6559\u5b66\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "keywords": ["trajectory", "race car simulator driving", "verbal coaching", "imitation learning", "language"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86 SimCoachCorpus \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d5b\u8f66\u6a21\u62df\u5668\u9a7e\u9a76\u6570\u636e\uff0c\u65e8\u5728\u7814\u7a76\u5728\u6307\u5bfc\u548c\u975e\u6307\u5bfc\u4e0b\u8fd0\u52a8\u6280\u80fd\u4e60\u5f97\u8fc7\u7a0b\u4e2d\u7684\u4ea4\u4e92\u73b0\u8c61\u3002", "motivation": "\u7f3a\u4e4f\u80fd\u591f\u6355\u6349\u4eba\u4eec\u5982\u4f55\u901a\u8fc7\u53e3\u5934\u6307\u5bfc\u968f\u65f6\u95f4\u63a8\u79fb\u83b7\u5f97\u5177\u8eab\u6280\u80fd\u7684\u6570\u636e\u96c6\u3002", "method": "\u6536\u96c6\u4e86 29 \u540d\u4eba\u7c7b\u5728\u8d5b\u8f66\u6a21\u62df\u5668\u4e2d\u9a7e\u9a76\u7684\u6570\u636e\uff0c\u5176\u4e2d 15 \u540d\u53c2\u4e0e\u8005\u63a5\u53d7\u4e86\u4e13\u4e1a\u9a7e\u9a76\u6559\u7ec3\u7684\u4e00\u5bf9\u4e00\u6307\u5bfc\uff0c14 \u540d\u53c2\u4e0e\u8005\u5728\u6ca1\u6709\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u9a7e\u9a76\u3002\u6570\u636e\u96c6\u5305\u542b\u8f66\u8f86\u72b6\u6001\u3001\u8f93\u5165\u3001\u5730\u56fe\u548c\u9525\u5f62\u5730\u6807\u7b49\u5177\u8eab\u7279\u5f81\uff0c\u5e76\u4e0e\u4e13\u4e1a\u6559\u7ec3\u7684\u540c\u6b65\u53e3\u5934\u6307\u5bfc\u4ee5\u53ca\u6bcf\u5708\u7ed3\u675f\u65f6\u7684\u989d\u5916\u53cd\u9988\u76f8\u7ed3\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6bcf\u4e2a\u5e76\u53d1\u53cd\u9988\u8bdd\u8bed\u7684\u6307\u5bfc\u7c7b\u522b\u6ce8\u91ca\u3001\u5b66\u751f\u5bf9\u6307\u5bfc\u5efa\u8bae\u7684\u4f9d\u4ece\u6027\u8bc4\u7ea7\u4ee5\u53ca\u53c2\u4e0e\u8005\u7684\u81ea\u6211\u62a5\u544a\u8ba4\u77e5\u8d1f\u8377\u548c\u60c5\u7eea\u72b6\u6001\u3002", "result": "\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7 20,000 \u6761\u5e76\u53d1\u53cd\u9988\u8bdd\u8bed\u3001\u8d85\u8fc7 400 \u6761\u6700\u7ec8\u53cd\u9988\u8bdd\u8bed\u4ee5\u53ca\u8d85\u8fc7 40 \u5c0f\u65f6\u7684\u8f66\u8f86\u9a7e\u9a76\u6570\u636e\u3002\u901a\u8fc7\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u4e3b\u9898\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u5c55\u793a\u4e86\u8be5\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "SimCoachCorpus \u6570\u636e\u96c6\u53ef\u7528\u4e8e\u7814\u7a76\u8fd0\u52a8\u5b66\u4e60\u52a8\u529b\u5b66\u3001\u63a2\u7d22\u8bed\u8a00\u73b0\u8c61\u4ee5\u53ca\u8bad\u7ec3\u6559\u5b66\u8ba1\u7b97\u6a21\u578b\u3002", "summary_zh": "\u4e3a\u4e86\u5f25\u8865\u8bed\u8a00\u548c\u7269\u7406\u884c\u4e3a\u6df1\u5ea6\u4ea4\u7ec7\u7684\u9886\u57df\u4e2d\u7f3a\u4e4f\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u672c\u6587\u4ecb\u7ecd\u4e86 SimCoachCorpus\uff1a\u4e00\u4e2a\u72ec\u7279\u7684\u8d5b\u8f66\u6a21\u62df\u5668\u9a7e\u9a76\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u5728\u6307\u5bfc\u548c\u975e\u6307\u5bfc\u4e0b\u8fd0\u52a8\u6280\u80fd\u4e60\u5f97\u8fc7\u7a0b\u4e2d\u7684\u4e30\u5bcc\u4ea4\u4e92\u73b0\u8c61\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b 29 \u540d\u4eba\u7c7b\u5728\u8d5b\u8f66\u6a21\u62df\u5668\u4e2d\u9a7e\u9a76\u7684\u6570\u636e\uff0c\u5176\u4e2d 15 \u540d\u53c2\u4e0e\u8005\u63a5\u53d7\u4e86\u4e13\u4e1a\u9a7e\u9a76\u6559\u7ec3\u7684\u4e00\u5bf9\u4e00\u6307\u5bfc\uff0c14 \u540d\u53c2\u4e0e\u8005\u5728\u6ca1\u6709\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\u9a7e\u9a76\u3002\u6570\u636e\u96c6\u5305\u542b\u8f66\u8f86\u72b6\u6001\u3001\u8f93\u5165\u3001\u5730\u56fe\u548c\u9525\u5f62\u5730\u6807\u7b49\u5177\u8eab\u7279\u5f81\uff0c\u5e76\u4e0e\u4e13\u4e1a\u6559\u7ec3\u7684\u540c\u6b65\u53e3\u5934\u6307\u5bfc\u4ee5\u53ca\u6bcf\u5708\u7ed3\u675f\u65f6\u7684\u989d\u5916\u53cd\u9988\u76f8\u7ed3\u5408\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6bcf\u4e2a\u5e76\u53d1\u53cd\u9988\u8bdd\u8bed\u7684\u6307\u5bfc\u7c7b\u522b\u6ce8\u91ca\u3001\u5b66\u751f\u5bf9\u6307\u5bfc\u5efa\u8bae\u7684\u4f9d\u4ece\u6027\u8bc4\u7ea7\u4ee5\u53ca\u53c2\u4e0e\u8005\u7684\u81ea\u6211\u62a5\u544a\u8ba4\u77e5\u8d1f\u8377\u548c\u60c5\u7eea\u72b6\u6001\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc7 20,000 \u6761\u5e76\u53d1\u53cd\u9988\u8bdd\u8bed\u3001\u8d85\u8fc7 400 \u6761\u6700\u7ec8\u53cd\u9988\u8bdd\u8bed\u4ee5\u53ca\u8d85\u8fc7 40 \u5c0f\u65f6\u7684\u8f66\u8f86\u9a7e\u9a76\u6570\u636e\u3002\u8be5\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u7814\u7a76\u8fd0\u52a8\u5b66\u4e60\u52a8\u529b\u5b66\u3001\u63a2\u7d22\u8bed\u8a00\u73b0\u8c61\u4ee5\u53ca\u8bad\u7ec3\u6559\u5b66\u8ba1\u7b97\u6a21\u578b\u3002"}}
{"id": "2509.14252", "pdf": "https://arxiv.org/pdf/2509.14252", "abs": "https://arxiv.org/abs/2509.14252", "authors": ["Hai Huang", "Yann LeCun", "Randall Balestriero"], "title": "LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) pretraining, finetuning, and evaluation rely on\ninput-space reconstruction and generative capabilities. Yet, it has been\nobserved in vision that embedding-space training objectives, e.g., with Joint\nEmbedding Predictive Architectures (JEPAs), are far superior to their\ninput-space counterpart. That mismatch in how training is achieved between\nlanguage and vision opens up a natural question: {\\em can language training\nmethods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is\na testimony of the challenge in designing such objectives for language. In this\nwork, we propose a first step in that direction where we develop LLM-JEPA, a\nJEPA based solution for LLMs applicable both to finetuning and pretraining.\nThus far, LLM-JEPA is able to outperform the standard LLM training objectives\nby a significant margin across models, all while being robust to overfiting.\nThose findings are observed across numerous datasets (NL-RX, GSM8K, Spider,\nRottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo\nfamilies. Code: https://github.com/rbalestr-lab/llm-jepa.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper discusses Large Language Models (LLMs) and proposes a novel training method (LLM-JEPA) inspired by Joint Embedding Predictive Architectures (JEPAs) which are used in vision. While it doesn't directly address trajectory prediction, the use of predictive architectures and the focus on LLMs justifies a moderate relevance score.", "keywords": ["Large Language Models", "LLMs", "Joint Embedding Predictive Architectures", "JEPAs", "pretraining", "finetuning", "predictive architectures"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LLM-JEPA\uff0c\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPA\uff09\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5b83\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u65b9\u9762\u5747\u4f18\u4e8e\u6807\u51c6LLM\u8bad\u7ec3\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4f9d\u8d56\u4e8e\u8f93\u5165\u7a7a\u95f4\u91cd\u5efa\u548c\u751f\u6210\u80fd\u529b\uff0c\u4f46\u89c6\u89c9\u9886\u57df\u7684\u7ecf\u9a8c\u8868\u660e\uff0c\u5d4c\u5165\u7a7a\u95f4\u8bad\u7ec3\u76ee\u6807\uff08\u5982JEPA\uff09\u66f4\u4f18\u8d8a\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u662f\u5426\u80fd\u5c06\u89c6\u89c9\u9886\u57df\u7684\u8bad\u7ec3\u6280\u5de7\u5e94\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u89e3\u51b3\u4e3a\u8bed\u8a00\u6a21\u578b\u8bbe\u8ba1JEPA\u5f0f\u76ee\u6807\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86LLM-JEPA\uff0c\u4e00\u79cd\u57fa\u4e8eJEPA\u7684\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5fae\u8c03\u548c\u9884\u8bad\u7ec3\u3002", "result": "LLM-JEPA\u5728\u591a\u4e2a\u6570\u636e\u96c6\uff08NL-RX, GSM8K, Spider, RottenTomatoes\uff09\u548c\u591a\u79cd\u6a21\u578b\uff08Llama3, OpenELM, Gemma2, Olmo\uff09\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u6807\u51c6LLM\u8bad\u7ec3\u76ee\u6807\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4e0d\u6613\u8fc7\u62df\u5408\u3002", "conclusion": "LLM-JEPA\u662f\u4e00\u79cd\u6709\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u8bad\u7ec3\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u5e76\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9884\u8bad\u7ec3\u3001\u5fae\u8c03\u548c\u8bc4\u4f30\u4f9d\u8d56\u4e8e\u8f93\u5165\u7a7a\u95f4\u91cd\u5efa\u548c\u751f\u6210\u80fd\u529b\u3002\u7136\u800c\uff0c\u5728\u89c6\u89c9\u9886\u57df\u5df2\u7ecf\u89c2\u5bdf\u5230\uff0c\u5d4c\u5165\u7a7a\u95f4\u8bad\u7ec3\u76ee\u6807\uff0c\u4f8b\u5982\u4f7f\u7528\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff08JEPA\uff09\uff0c\u8fdc\u4f18\u4e8e\u5176\u8f93\u5165\u7a7a\u95f4\u5bf9\u5e94\u65b9\u6cd5\u3002\u8bed\u8a00\u548c\u89c6\u89c9\u4e4b\u95f4\u8bad\u7ec3\u65b9\u5f0f\u7684\u8fd9\u79cd\u4e0d\u5339\u914d\u5f15\u51fa\u4e86\u4e00\u4e2a\u81ea\u7136\u7684\u95ee\u9898\uff1a{\\em \u8bed\u8a00\u8bad\u7ec3\u65b9\u6cd5\u80fd\u5426\u4ece\u89c6\u89c9\u65b9\u6cd5\u4e2d\u5b66\u4e60\u4e00\u4e9b\u6280\u5de7\uff1f} \u7f3a\u4e4fJEPA\u98ce\u683c\u7684LLM\u8bc1\u660e\u4e86\u4e3a\u8bed\u8a00\u8bbe\u8ba1\u6b64\u7c7b\u76ee\u6807\u7684\u6311\u6218\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u671d\u8fd9\u4e2a\u65b9\u5411\u8fc8\u51fa\u7684\u7b2c\u4e00\u6b65\uff0c\u6211\u4eec\u5f00\u53d1\u4e86LLM-JEPA\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8eJEPA\u7684LLM\u89e3\u51b3\u65b9\u6848\uff0c\u65e2\u9002\u7528\u4e8e\u5fae\u8c03\u4e5f\u9002\u7528\u4e8e\u9884\u8bad\u7ec3\u3002\u5230\u76ee\u524d\u4e3a\u6b62\uff0cLLM-JEPA\u80fd\u591f\u5728\u6a21\u578b\u4e2d\u663e\u8457\u4f18\u4e8e\u6807\u51c6LLM\u8bad\u7ec3\u76ee\u6807\uff0c\u540c\u65f6\u5bf9\u8fc7\u5ea6\u62df\u5408\u5177\u6709\u9c81\u68d2\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u5728\u4f17\u591a\u6570\u636e\u96c6\uff08NL-RX\u3001GSM8K\u3001Spider\u3001RottenTomatoes\uff09\u548c\u6765\u81eaLlama3\u3001OpenELM\u3001Gemma2\u548cOlmo\u7cfb\u5217\u7684\u5404\u79cd\u6a21\u578b\u4e2d\u89c2\u5bdf\u5230\u3002\u4ee3\u7801\uff1ahttps://github.com/rbalestr-lab/llm-jepa\u3002"}}
{"id": "2509.14801", "pdf": "https://arxiv.org/pdf/2509.14801", "abs": "https://arxiv.org/abs/2509.14801", "authors": ["Julian F. Schumann", "Anna M\u00e9sz\u00e1ros", "Jens Kober", "Arkady Zgonnikov"], "title": "STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models", "categories": ["cs.LG"], "comment": null, "summary": "While trajectory prediction plays a critical role in enabling safe and\neffective path-planning in automated vehicles, standardized practices for\nevaluating such models remain underdeveloped. Recent efforts have aimed to\nunify dataset formats and model interfaces for easier comparisons, yet existing\nframeworks often fall short in supporting heterogeneous traffic scenarios,\njoint prediction models, or user documentation. In this work, we introduce STEP\n-- a new benchmarking framework that addresses these limitations by providing a\nunified interface for multiple datasets, enforcing consistent training and\nevaluation conditions, and supporting a wide range of prediction models. We\ndemonstrate the capabilities of STEP in a number of experiments which reveal 1)\nthe limitations of widely-used testing procedures, 2) the importance of joint\nmodeling of agents for better predictions of interactions, and 3) the\nvulnerability of current state-of-the-art models against both distribution\nshifts and targeted attacks by adversarial agents. With STEP, we aim to shift\nthe focus from the ``leaderboard'' approach to deeper insights about model\nbehavior and generalization in complex multi-agent settings.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u91cd\u70b9\u5728\u4e8e\u8bc4\u4f30\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u5185\u5bb9\u662f\u8f68\u8ff9\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "benchmarking", "trajectory prediction models", "path-planning", "joint prediction models", "multi-agent"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.14303", "pdf": "https://arxiv.org/pdf/2509.14303", "abs": "https://arxiv.org/abs/2509.14303", "authors": ["Hao Jiang", "Zhipeng Zhang", "Yu Gao", "Zhigang Sun", "Yiru Wang", "Yuwen Heng", "Shuo Wang", "Jinhao Chai", "Zhuo Chen", "Hao Zhao", "Hao Sun", "Xi Zhang", "Anqing Jiang", "Chuan Hu"], "title": "FlowDrive: Energy Flow Field for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Recent advances in end-to-end autonomous driving leverage multi-view images\nto construct BEV representations for motion planning. In motion planning,\nautonomous vehicles need considering both hard constraints imposed by\ngeometrically occupied obstacles (e.g., vehicles, pedestrians) and soft,\nrule-based semantics with no explicit geometry (e.g., lane boundaries, traffic\npriors). However, existing end-to-end frameworks typically rely on BEV features\nlearned in an implicit manner, lacking explicit modeling of risk and guidance\npriors for safe and interpretable planning. To address this, we propose\nFlowDrive, a novel framework that introduces physically interpretable\nenergy-based flow fields-including risk potential and lane attraction fields-to\nencode semantic priors and safety cues into the BEV space. These flow-aware\nfeatures enable adaptive refinement of anchor trajectories and serve as\ninterpretable guidance for trajectory generation. Moreover, FlowDrive decouples\nmotion intent prediction from trajectory denoising via a conditional diffusion\nplanner with feature-level gating, alleviating task interference and enhancing\nmultimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that\nFlowDrive achieves state-of-the-art performance with an EPDMS of 86.3,\nsurpassing prior baselines in both safety and planning quality. The project is\navailable at https://astrixdrive.github.io/FlowDrive.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on end-to-end autonomous driving with a focus on motion planning and trajectory generation. While it doesn't explicitly mention or utilize large language models, it directly addresses trajectory prediction and planning, making it moderately relevant. The approach uses energy-based flow fields and a conditional diffusion planner for trajectory generation, which are related to the trajectory prediction aspect.", "keywords": ["autonomous driving", "motion planning", "trajectory generation", "trajectory denoising", "BEV representations"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.14565", "pdf": "https://arxiv.org/pdf/2509.14565", "abs": "https://arxiv.org/abs/2509.14565", "authors": ["Li Gao", "Hongyang Sun", "Liu Liu", "Yunhao Li", "Yang Cai"], "title": "DiffVL: Diffusion-Based Visual Localization on 2D Maps via BEV-Conditioned GPS Denoising", "categories": ["cs.CV"], "comment": null, "summary": "Accurate visual localization is crucial for autonomous driving, yet existing\nmethods face a fundamental dilemma: While high-definition (HD) maps provide\nhigh-precision localization references, their costly construction and\nmaintenance hinder scalability, which drives research toward\nstandard-definition (SD) maps like OpenStreetMap. Current SD-map-based\napproaches primarily focus on Bird's-Eye View (BEV) matching between images and\nmaps, overlooking a ubiquitous signal-noisy GPS. Although GPS is readily\navailable, it suffers from multipath errors in urban environments. We propose\nDiffVL, the first framework to reformulate visual localization as a GPS\ndenoising task using diffusion models. Our key insight is that noisy GPS\ntrajectory, when conditioned on visual BEV features and SD maps, implicitly\nencode the true pose distribution, which can be recovered through iterative\ndiffusion refinement. DiffVL, unlike prior BEV-matching methods (e.g.,\nOrienterNet) or transformer-based registration approaches, learns to reverse\nGPS noise perturbations by jointly modeling GPS, SD map, and visual signals,\nachieving sub-meter accuracy without relying on HD maps. Experiments on\nmultiple datasets demonstrate that our method achieves state-of-the-art\naccuracy compared to BEV-matching baselines. Crucially, our work proves that\ndiffusion models can enable scalable localization by treating noisy GPS as a\ngenerative prior-making a paradigm shift from traditional matching-based\nmethods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper is related to trajectory prediction because it focuses on visual localization, which is a key component for predicting the future trajectory of autonomous vehicles. The paper uses diffusion models, which can be considered a type of generative model, but it doesn't explicitly use or discuss large language models. While it doesn't directly combine trajectory prediction and large language models, the use of diffusion models for localization tasks in autonomous driving makes it somewhat relevant to the intersection of these fields.", "keywords": ["visual localization", "autonomous driving", "diffusion models", "GPS denoising", "BEV"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faDiffVL\uff0c\u4e00\u4e2a\u5229\u7528\u6269\u6563\u6a21\u578b\u5c06\u89c6\u89c9\u5b9a\u4f4d\u91cd\u65b0\u5b9a\u4e49\u4e3aGPS\u53bb\u566a\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u5728\u6807\u51c6\u5730\u56fe\u4e0a\u4e9a\u7c73\u7ea7\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9ad8\u7cbe\u5730\u56fe\uff0c\u4f46\u5176\u6784\u5efa\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u6602\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u800c\u57fa\u4e8e\u6807\u51c6\u5730\u56fe\u7684\u65b9\u6cd5\u5ffd\u7565\u4e86\u666e\u904d\u5b58\u5728\u7684\u566a\u58f0GPS\u4fe1\u53f7\u3002\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u5728\u6807\u51c6\u5730\u56fe\u4e0a\u5229\u7528\u566a\u58f0GPS\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u89c6\u89c9\u5b9a\u4f4d\u7684\u95ee\u9898\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51faDiffVL\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u89c6\u89c9BEV\u7279\u5f81\u548c\u6807\u51c6\u5730\u56fe\u5bf9\u566a\u58f0GPS\u8f68\u8ff9\u8fdb\u884c\u6761\u4ef6\u5904\u7406\uff0c\u9690\u5f0f\u5730\u7f16\u7801\u771f\u5b9e\u59ff\u6001\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6269\u6563\u7ec6\u5316\u6765\u6062\u590d\u771f\u5b9e\u59ff\u6001\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiffVL\u65b9\u6cd5\u76f8\u6bd4\u4e8eBEV\u5339\u914d\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\uff0c\u5e76\u4e14\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u9ad8\u7cbe\u5730\u56fe\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e9a\u7c73\u7ea7\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5c06\u566a\u58f0GPS\u4f5c\u4e3a\u751f\u6210\u5148\u9a8c\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5b9a\u4f4d\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4ece\u4f20\u7edf\u7684\u57fa\u4e8e\u5339\u914d\u7684\u65b9\u6cd5\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "summary_zh": "\u7cbe\u786e\u7684\u89c6\u89c9\u5b9a\u4f4d\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9762\u4e34\u4e00\u4e2a\u6839\u672c\u7684\u56f0\u5883\uff1a\u867d\u7136\u9ad8\u7cbe\u5730\u56fe\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9a\u4f4d\u53c2\u8003\uff0c\u4f46\u5176\u9ad8\u6602\u7684\u6784\u5efa\u548c\u7ef4\u62a4\u6210\u672c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u8fd9\u63a8\u52a8\u4e86\u5bf9\u50cfOpenStreetMap\u8fd9\u6837\u7684\u6807\u51c6\u5730\u56fe\u7684\u7814\u7a76\u3002\u76ee\u524d\u57fa\u4e8e\u6807\u51c6\u5730\u56fe\u7684\u65b9\u6cd5\u4e3b\u8981\u4fa7\u91cd\u4e8e\u56fe\u50cf\u548c\u5730\u56fe\u4e4b\u95f4\u7684\u9e1f\u77b0\u56fe\uff08BEV\uff09\u5339\u914d\uff0c\u5ffd\u7565\u4e86\u666e\u904d\u5b58\u5728\u7684\u4fe1\u53f7\u566a\u58f0GPS\u3002\u867d\u7136GPS\u5f88\u5bb9\u6613\u83b7\u5f97\uff0c\u4f46\u5b83\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u4f1a\u53d7\u5230\u591a\u5f84\u8bef\u5dee\u7684\u5f71\u54cd\u3002\u6211\u4eec\u63d0\u51fa\u4e86DiffVL\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5c06\u89c6\u89c9\u5b9a\u4f4d\u91cd\u65b0\u5b9a\u4e49\u4e3aGPS\u53bb\u566a\u4efb\u52a1\u7684\u6846\u67b6\u3002\u6211\u4eec\u7684\u5173\u952e\u89c1\u89e3\u662f\uff0c\u5f53\u4ee5\u89c6\u89c9BEV\u7279\u5f81\u548c\u6807\u51c6\u5730\u56fe\u4e3a\u6761\u4ef6\u65f6\uff0c\u566a\u58f0GPS\u8f68\u8ff9\u9690\u5f0f\u5730\u7f16\u7801\u4e86\u771f\u5b9e\u7684\u59ff\u6001\u5206\u5e03\uff0c\u53ef\u4ee5\u901a\u8fc7\u8fed\u4ee3\u6269\u6563\u7ec6\u5316\u6765\u6062\u590d\u3002\u4e0e\u5148\u524d\u7684BEV\u5339\u914d\u65b9\u6cd5\uff08\u4f8b\u5982\uff0cOrienterNet\uff09\u6216\u57fa\u4e8eTransformer\u7684\u914d\u51c6\u65b9\u6cd5\u4e0d\u540c\uff0cDiffVL\u901a\u8fc7\u8054\u5408\u5efa\u6a21GPS\u3001\u6807\u51c6\u5730\u56fe\u548c\u89c6\u89c9\u4fe1\u53f7\u6765\u5b66\u4e60\u53cd\u8f6cGPS\u566a\u58f0\u6270\u52a8\uff0c\u4ece\u800c\u5728\u4e0d\u4f9d\u8d56\u9ad8\u7cbe\u5730\u56fe\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e9a\u7c73\u7ea7\u7684\u7cbe\u5ea6\u3002\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0eBEV\u5339\u914d\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u8bc1\u660e\u4e86\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5c06\u566a\u58f0GPS\u89c6\u4e3a\u751f\u6210\u5148\u9a8c\u6765\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5b9a\u4f4d\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4ece\u4f20\u7edf\u7684\u57fa\u4e8e\u5339\u914d\u7684\u65b9\u6cd5\u7684\u8303\u5f0f\u8f6c\u53d8\u3002"}}
{"id": "2509.14412", "pdf": "https://arxiv.org/pdf/2509.14412", "abs": "https://arxiv.org/abs/2509.14412", "authors": ["Artem Lykov", "Oleg Kobzarev", "Dzmitry Tsetserukou"], "title": "GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot", "categories": ["cs.RO"], "comment": null, "summary": "We present GestOS, a gesture-based operating system for high-level control of\nheterogeneous robot teams. Unlike prior systems that map gestures to fixed\ncommands or single-agent actions, GestOS interprets hand gestures semantically\nand dynamically distributes tasks across multiple robots based on their\ncapabilities, current state, and supported instruction sets. The system\ncombines lightweight visual perception with large language model (LLM)\nreasoning: hand poses are converted into structured textual descriptions, which\nthe LLM uses to infer intent and generate robot-specific commands. A robot\nselection module ensures that each gesture-triggered task is matched to the\nmost suitable agent in real time. This architecture enables context-aware,\nadaptive control without requiring explicit user specification of targets or\ncommands. By advancing gesture interaction from recognition to intelligent\norchestration, GestOS supports scalable, flexible, and user-friendly\ncollaboration with robotic systems in dynamic environments.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper uses Large Language Models (LLMs) for robot control based on hand gestures. While it doesn't directly address trajectory prediction, the control of robots inherently involves planning and potentially predicting their future movements. The connection to trajectory prediction is indirect but present in the broader context of robot control.", "keywords": ["Large Language Models", "LLM", "robot control", "gesture interpretation"]}, "AI": {"tldr": "GestOS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u624b\u52bf\u7684\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u63a8\u7406\u5b9e\u73b0\u5bf9\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u8bed\u4e49\u5316\u548c\u52a8\u6001\u4efb\u52a1\u5206\u914d\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u624b\u52bf\u63a7\u5236\u7cfb\u7edf\u65e0\u6cd5\u6709\u6548\u63a7\u5236\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u901a\u5e38\u5c06\u624b\u52bf\u6620\u5c04\u5230\u56fa\u5b9a\u547d\u4ee4\u6216\u5355\u667a\u80fd\u4f53\u52a8\u4f5c\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u89c6\u89c9\u611f\u77e5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\uff0c\u5c06\u624b\u52bf\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0cLLM\u7528\u4e8e\u63a8\u65ad\u610f\u56fe\u5e76\u751f\u6210\u673a\u5668\u4eba\u7279\u5b9a\u547d\u4ee4\u3002\u673a\u5668\u4eba\u9009\u62e9\u6a21\u5757\u786e\u4fdd\u6bcf\u4e2a\u624b\u52bf\u89e6\u53d1\u7684\u4efb\u52a1\u4e0e\u6700\u5408\u9002\u7684\u4ee3\u7406\u5b9e\u65f6\u5339\u914d\u3002", "result": "GestOS\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u65e0\u9700\u7528\u6237\u660e\u786e\u6307\u5b9a\u76ee\u6807\u6216\u547d\u4ee4\u3002", "conclusion": "GestOS\u901a\u8fc7\u5c06\u624b\u52bf\u4ea4\u4e92\u4ece\u8bc6\u522b\u63d0\u5347\u5230\u667a\u80fd\u7f16\u6392\uff0c\u652f\u6301\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e0e\u673a\u5668\u4eba\u7cfb\u7edf\u8fdb\u884c\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u4e14\u7528\u6237\u53cb\u597d\u7684\u534f\u4f5c\u3002", "summary_zh": "\u6211\u4eec\u63d0\u51fa\u4e86GestOS\uff0c\u4e00\u4e2a\u57fa\u4e8e\u624b\u52bf\u7684\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7528\u4e8e\u5bf9\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u8fdb\u884c\u9ad8\u7ea7\u63a7\u5236\u3002\u4e0e\u5148\u524d\u5c06\u624b\u52bf\u6620\u5c04\u5230\u56fa\u5b9a\u547d\u4ee4\u6216\u5355\u667a\u80fd\u4f53\u52a8\u4f5c\u7684\u7cfb\u7edf\u4e0d\u540c\uff0cGestOS\u5728\u8bed\u4e49\u4e0a\u89e3\u91ca\u624b\u52bf\uff0c\u5e76\u6839\u636e\u5176\u80fd\u529b\u3001\u5f53\u524d\u72b6\u6001\u548c\u652f\u6301\u7684\u6307\u4ee4\u96c6\u5728\u591a\u4e2a\u673a\u5668\u4eba\u4e4b\u95f4\u52a8\u6001\u5206\u914d\u4efb\u52a1\u3002\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86\u8f7b\u91cf\u7ea7\u89c6\u89c9\u611f\u77e5\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\uff1a\u624b\u90e8\u59ff\u52bf\u88ab\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u6587\u672c\u63cf\u8ff0\uff0cLLM\u4f7f\u7528\u8fd9\u4e9b\u63cf\u8ff0\u6765\u63a8\u65ad\u610f\u56fe\u5e76\u751f\u6210\u673a\u5668\u4eba\u7279\u5b9a\u7684\u547d\u4ee4\u3002\u673a\u5668\u4eba\u9009\u62e9\u6a21\u5757\u786e\u4fdd\u6bcf\u4e2a\u624b\u52bf\u89e6\u53d1\u7684\u4efb\u52a1\u5728\u5b9e\u65f6\u5339\u914d\u5230\u6700\u5408\u9002\u7684\u4ee3\u7406\u3002\u8fd9\u79cd\u67b6\u6784\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u81ea\u9002\u5e94\u63a7\u5236\uff0c\u800c\u65e0\u9700\u7528\u6237\u660e\u786e\u6307\u5b9a\u76ee\u6807\u6216\u547d\u4ee4\u3002\u901a\u8fc7\u5c06\u624b\u52bf\u4ea4\u4e92\u4ece\u8bc6\u522b\u63d0\u5347\u5230\u667a\u80fd\u7f16\u6392\uff0cGestOS\u652f\u6301\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e0e\u673a\u5668\u4eba\u7cfb\u7edf\u8fdb\u884c\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u4e14\u7528\u6237\u53cb\u597d\u7684\u534f\u4f5c\u3002"}}
{"id": "2509.14547", "pdf": "https://arxiv.org/pdf/2509.14547", "abs": "https://arxiv.org/abs/2509.14547", "authors": ["Yi Lin", "Lujin Zhao", "Yijie Shi"], "title": "(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration", "categories": ["cs.AI"], "comment": null, "summary": "Recent studies have shown that carefully designed workflows coordinating\nlarge language models(LLMs) significantly enhance task-solving capabilities\ncompared to using a single model. While an increasing number of works focus on\nautonomous workflow construction, most existing approaches rely solely on\nhistorical experience, leading to limitations in efficiency and adaptability.\nWe argue that while historical experience is valuable, workflow construction\nshould also flexibly respond to the unique characteristics of each task. To\nthis end, we propose an a priori dynamic framework for automated workflow\nconstruction. Our framework first leverages Q-table learning to optimize the\ndecision space, guiding agent decisions and enabling effective use of\nhistorical experience. At the same time, agents evaluate the current task\nprogress and make a priori decisions regarding the next executing agent,\nallowing the system to proactively select the more suitable workflow structure\nfor each given task. Additionally, we incorporate mechanisms such as cold-start\ninitialization, early stopping, and pruning to further improve system\nefficiency. Experimental evaluations on four benchmark datasets demonstrate the\nfeasibility and effectiveness of our approach. Compared to state-of-the-art\nbaselines, our method achieves an average improvement of 4.05%, while reducing\nworkflow construction and inference costs to only 30.68%-48.31% of those\nrequired by existing methods.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on workflow construction for large language models (LLMs) using a multi-agent system. While it doesn't directly address trajectory prediction, it heavily involves LLMs and dynamic workflow, which could potentially be applied to trajectory prediction tasks in the future. The connection is not immediate, but the core techniques are relevant to the use of LLMs.", "keywords": ["large language models", "LLMs", "multi-agent collaboration", "workflow construction", "dynamic workflow"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.14726", "pdf": "https://arxiv.org/pdf/2509.14726", "abs": "https://arxiv.org/abs/2509.14726", "authors": ["Fangguo Zhao", "Xin Guan", "Shuo Li"], "title": "Rethinking Reference Trajectories in Agile Drone Racing: A Unified Reference-Free Model-Based Controller via MPPI", "categories": ["cs.RO"], "comment": null, "summary": "While model-based controllers have demonstrated remarkable performance in\nautonomous drone racing, their performance is often constrained by the reliance\non pre-computed reference trajectories. Conventional approaches, such as\ntrajectory tracking, demand a dynamically feasible, full-state reference,\nwhereas contouring control relaxes this requirement to a geometric path but\nstill necessitates a reference. Recent advancements in reinforcement learning\n(RL) have revealed that many model-based controllers optimize surrogate\nobjectives, such as trajectory tracking, rather than the primary racing goal of\ndirectly maximizing progress through gates. Inspired by these findings, this\nwork introduces a reference-free method for time-optimal racing by\nincorporating this gate progress objective, derived from RL reward shaping,\ndirectly into the Model Predictive Path Integral (MPPI) formulation. The\nsampling-based nature of MPPI makes it uniquely capable of optimizing the\ndiscontinuous and non-differentiable objective in real-time. We also establish\na unified framework that leverages MPPI to systematically and fairly compare\nthree distinct objective functions with a consistent dynamics model and\nparameter set: classical trajectory tracking, contouring control, and the\nproposed gate progress objective. We compare the performance of these three\nobjectives when solved via both MPPI and a traditional gradient-based solver.\nOur results demonstrate that the proposed reference-free approach achieves\ncompetitive racing performance, rivaling or exceeding reference-based methods.\nVideos are available at https://zhaofangguo.github.io/racing_mppi/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u65e0\u4eba\u673a\u654f\u6377\u7ade\u901f\u4e2d\u7684\u8f68\u8ff9\u63a7\u5236\uff0c\u4f7f\u7528\u4e86MPPI\u65b9\u6cd5\u8fdb\u884c\u8f68\u8ff9\u4f18\u5316\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\u548c\u8def\u5f84\u89c4\u5212\uff08\u901a\u8fc7MPPI\uff09\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u76f8\u5173\u6027\u4e3b\u8981\u4f53\u73b0\u5728\u8f68\u8ff9\u9884\u6d4b\u7684\u80cc\u666f\u4e0a\u3002", "keywords": ["trajectory prediction", "MPPI", "path planning", "model-based control", "drone racing"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.14935", "pdf": "https://arxiv.org/pdf/2509.14935", "abs": "https://arxiv.org/abs/2509.14935", "authors": ["Punith Reddy Vanteddu", "Davide Gorbani", "Giuseppe L'Erario", "Hosameldin Awadalla Omer Mohamed", "Fabio Bergonti", "Daniele Pucci"], "title": "CAD-Driven Co-Design for Flight-Ready Jet-Powered Humanoids", "categories": ["cs.RO"], "comment": null, "summary": "This paper presents a CAD-driven co-design framework for optimizing\njet-powered aerial humanoid robots to execute dynamically constrained\ntrajectories. Starting from the iRonCub-Mk3 model, a Design of Experiments\n(DoE) approach is used to generate 5,000 geometrically varied and mechanically\nfeasible designs by modifying limb dimensions, jet interface geometry (e.g.,\nangle and offset), and overall mass distribution. Each model is constructed\nthrough CAD assemblies to ensure structural validity and compatibility with\nsimulation tools. To reduce computational cost and enable parameter sensitivity\nanalysis, the models are clustered using K-means, with representative centroids\nselected for evaluation. A minimum-jerk trajectory is used to assess flight\nperformance, providing position and velocity references for a momentum-based\nlinearized Model Predictive Control (MPC) strategy. A multi-objective\noptimization is then conducted using the NSGA-II algorithm, jointly exploring\nthe space of design centroids and MPC gain parameters. The objectives are to\nminimize trajectory tracking error and mechanical energy expenditure. The\nframework outputs a set of flight-ready humanoid configurations with validated\ncontrol parameters, offering a structured method for selecting and implementing\nfeasible aerial humanoid designs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u98de\u884c\u4eba\u5f62\u673a\u5668\u4eba\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u5176\u4e2d\u6d89\u53ca\u8f68\u8ff9\u4f18\u5316\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u4f18\u5316\u548c\u63a7\u5236\u7b56\u7565\u5728\u5e7f\u4e49\u4e0a\u4e0eAI\u6a21\u578b\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "Model Predictive Control", "MPC", "trajectory tracking", "optimization"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
