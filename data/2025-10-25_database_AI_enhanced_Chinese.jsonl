{"id": "2510.20082", "pdf": "https://arxiv.org/pdf/2510.20082", "abs": "https://arxiv.org/abs/2510.20082", "authors": ["Yuanyuan Tian"], "title": "Query Optimization in the Wild: Realities and Trends", "categories": ["cs.DB"], "comment": "6 pages, 3 figures. This paper is based on an invited talk given by\n  Yuanyuan Tian at the Special EDBT/ICDT Joint Event on Theory & Practice of\n  Query Processing in EDBT 2026\n  (https://edbticdt2025.upc.edu/?contents=special_event.html)", "summary": "For nearly half a century, the core design of query optimizers in industrial\ndatabase systems has remained remarkably stable, relying on foundational\nprinciples from System R and the Volcano/Cascades framework. However, the rise\nof cloud computing, massive data volumes, and unified data platforms has\nexposed the limitations of this traditional, monolithic architecture. Taking an\nindustrial perspective, this paper reviews the past and present of query\noptimization in production systems and identifies the challenges they face\ntoday. Then this paper highlights three key trends gaining momentum in the\nindustry that promise to address these challenges. First, a tighter feedback\nloop between query optimization and query execution is being used to improve\nthe robustness of query performance. Second, the scope of optimization is\nexpanding from a single query to entire workloads through the convergence of\nquery optimization and workload optimization. Third, and perhaps most\ntransformatively, the industry is moving from monolithic designs to composable\narchitectures that foster agility and cross-engine collaboration. Together,\nthese trends chart a clear path toward a more dynamic, holistic, and adaptable\nfuture for query optimization in practice.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.20110", "pdf": "https://arxiv.org/pdf/2510.20110", "abs": "https://arxiv.org/abs/2510.20110", "authors": ["Ming Sheng", "Shuliang Wang", "Yong Zhang", "Yi Luo", "Xianbo Liu", "Zeming Li"], "title": "UREM: A High-performance Unified and Resilient Enhancement Method for Multi- and High-Dimensional Indexes", "categories": ["cs.DB"], "comment": "12 pages,12 Figures", "summary": "Numerous multi- or high-dimensional indexes with distinct advantages have\nbeen proposed on various platforms to meet application requirements. To achieve\nhigher-performance queries, most indexes employ enhancement methods, including\nstructure-oriented and layout-oriented enhancement methods. Existing\nstructure-oriented methods tailored to specific indexes work well under static\nworkloads but lack generality and degrade under dynamic workloads. The\nlayout-oriented methods exhibit good generality and perform well under dynamic\nworkloads, but exhibit suboptimal performance under static workloads.\nTherefore, it is an open challenge to develop a unified and resilient\nenhancement method that can improve query performance for different indexes\nadaptively under different scenarios. In this paper, we propose UREM, which is\nthe first high-performance Unified and Resilient Enhancement Method designed\nfor both multi- and high-dimensional indexes, capable of adapting to different\nscenarios. Specifically, UREM (1) can be uniformly applied with different\nindexes on various platforms; (2) enhances the query performance of indexes by\nlayout optimization under static workloads; (3) enables indexes to stabilize\nperformance when queries shift through partial layout reorganization. We\nevaluate UREM on 20 widely used indexes. Experimental results demonstrate that\nUREM improves the query performance of multi- and high-dimensional indexes by\nup to 5.73x and 9.18x under static workloads, and by an average of 5.72x and\n9.47x under dynamic workloads. Moreover, some traditional indexes enhanced by\nUREM even achieve performance comparable to or even surpassing that of recent\nadvanced indexes.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.20296", "pdf": "https://arxiv.org/pdf/2510.20296", "abs": "https://arxiv.org/abs/2510.20296", "authors": ["Wenqi Jiang"], "title": "RAG-Stack: Co-Optimizing RAG Quality and Performance From the Vector Database Perspective", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Retrieval-augmented generation (RAG) has emerged as one of the most prominent\napplications of vector databases. By integrating documents retrieved from a\ndatabase into the prompt of a large language model (LLM), RAG enables more\nreliable and informative content generation. While there has been extensive\nresearch on vector databases, many open research problems remain once they are\nconsidered in the wider context of end-to-end RAG pipelines. One practical yet\nchallenging problem is how to jointly optimize both system performance and\ngeneration quality in RAG, which is significantly more complex than it appears\ndue to the numerous knobs on both the algorithmic side (spanning models and\ndatabases) and the systems side (from software to hardware). In this paper, we\npresent RAG-Stack, a three-pillar blueprint for quality-performance\nco-optimization in RAG systems. RAG-Stack comprises: (1) RAG-IR, an\nintermediate representation that serves as an abstraction layer to decouple\nquality and performance aspects; (2) RAG-CM, a cost model for estimating system\nperformance given an RAG-IR; and (3) RAG-PE, a plan exploration algorithm that\nsearches for high-quality, high-performance RAG configurations. We believe this\nthree-pillar blueprint will become the de facto paradigm for RAG\nquality-performance co-optimization in the years to come.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.20308", "pdf": "https://arxiv.org/pdf/2510.20308", "abs": "https://arxiv.org/abs/2510.20308", "authors": ["Manuel Sch\u00f6nberger", "Immanuel Trummer", "Wolfgang Mauerer"], "title": "Hybrid Mixed Integer Linear Programming for Large-Scale Join Order Optimisation", "categories": ["cs.DB"], "comment": null, "summary": "Finding optimal join orders is among the most crucial steps to be performed\nby query optimisers. Though extensively studied in data management research,\nthe problem remains far from solved: While query optimisers rely on exhaustive\nsearch methods to determine ideal solutions for small problems, such methods\nreach their limits once queries grow in size. Yet, large queries become\nincreasingly common in real-world scenarios, and require suitable methods to\ngenerate efficient execution plans. While a variety of heuristics have been\nproposed for large-scale query optimisation, they suffer from degrading\nsolution quality as queries grow in size, or feature highly sub-optimal\nworst-case behavior, as we will show.\n  We propose a novel method based on the paradigm of mixed integer linear\nprogramming (MILP): By deriving a novel MILP model capable of optimising\narbitrary bushy tree structures, we address the limitations of existing MILP\nmethods for join ordering, and can rely on highly optimised MILP solvers to\nderive efficient tree structures that elude competing methods. To ensure\noptimisation efficiency, we embed our MILP method into a hybrid framework,\nwhich applies MILP solvers precisely where they provide the greatest advantage\nover competitors, while relying on more efficient methods for less complex\noptimisation steps. Thereby, our approach gracefully scales to extremely large\nquery sizes joining up to 100 relations, and consistently achieves the most\nrobust plan quality among a large variety of competing join ordering methods.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.20582", "pdf": "https://arxiv.org/pdf/2510.20582", "abs": "https://arxiv.org/abs/2510.20582", "authors": ["Maxime Andr\u00e9", "Marco Raglianti", "Souhaila Serbout", "Anthony Cleve", "Michele Lanza"], "title": "An Empirical Study on Database Usage in Microservices", "categories": ["cs.DB"], "comment": "Submitted to Journal Systems and Software, 17 pages", "summary": "Microservices architectures are an integral part of modern software\ndevelopment. Their adoption brings significant changes to database management.\nInstead of relying on a single database, a microservices architecture is\ntypically composed of multiple, smaller, heterogeneous, and distributed DBs. In\nthese data-intensive systems, the variety and combination of database\ncategories and technologies play a crucial role in storing and managing data.\nWhile data management in microservices is a major challenge, research\nliterature is scarce.\n  We present an empirical study on how databases are used in microservices. On\nthe dataset we collected (and released as open data for future research),\nconsidering 15 years of microservices, we examine ca. 1,000 GitHub projects\nthat use databases selected among 180 technologies from 14 categories. We\nperform a comprehensive analysis of current practices, providing researchers\nand practitioners with empirical evidence to better understand database usage\nin microservices. We report 18 findings and 9 recommendations. We show that\nmicroservices predominantly use Relational, Key-Value, Document, and Search\ndatabases. Notably, 52% of microservices combine multiple database categories.\nComplexity correlates with database count, with older systems favoring\nRelational databases and newer ones increasingly adopting Key-Value and\nDocument technologies. Niche databases (e.g., EventStoreDB, PostGIS), while not\nwidespread, are often combined with a mainstream one.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.20600", "pdf": "https://arxiv.org/pdf/2510.20600", "abs": "https://arxiv.org/abs/2510.20600", "authors": ["Dildar Ali", "Suman Banerjee", "Yamuna Prasad"], "title": "Balanced Popularity in Multi-Product Billboard Advertisement", "categories": ["cs.DB"], "comment": "13 Pages", "summary": "The billboard advertisement has emerged as an effective out-of-home\nadvertisement technique where the objective is to choose a limited number of\nslots to play some advertisement content (e.g., animation, video, etc.) with\nthe hope that the content will be visible to a large number of travelers, and\nthis will be helpful to earn more revenue. In this paper, we study a variant of\nthe influential slot selection problem where the advertiser wants to promote\nmultiple products. Formally, we call this problem the \\textsc{Multi-Product\nInfluence Maximization Problem for the Balanced Popularity} Problem. The input\nto our problem is a trajectory and a billboard database, as well as a budget\nfor each product. The goal here is to choose a subset of slots for each product\nsuch that the aggregated influence of all the products gets maximized subject\nto the following two constraints: total selection cost for each product is less\nthan or equal to the allocated budget for that product, and the difference\nbetween the influence for any two products is less than or equal to a given\nthreshold. We show that the problem is NP-hard to solve optimally. We formulate\nthis problem as a linear programming problem and use linear programming\nrelaxation with randomized rounding. Further, we propose a greedy-based\nheuristic with balance correction to solve this problem. We conduct a number of\nexperiments with real-world trajectory and billboard datasets, and the results\nare reported. From the reported results, we observe that the proposed solution\napproaches lead to more influence compared to many baseline methods.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.20681", "pdf": "https://arxiv.org/pdf/2510.20681", "abs": "https://arxiv.org/abs/2510.20681", "authors": ["Xinhe Mu", "Zhaoqi Zhou", "Zaijiu Shang", "Chuan Zhou", "Gang Fu", "Guiying Yan", "Guoliang Li", "Zhiming Ma"], "title": "Downsizing Diffusion Models for Cardinality Estimation", "categories": ["cs.DB"], "comment": null, "summary": "Inspired by the performance of score-based diffusion models in estimating\ncomplex text, video, and image distributions with thousands of dimensions, we\nintroduce Accelerated Diffusion Cardest (ADC), the first joint distribution\ncardinality estimator based on a downsized diffusion model.\n  To calculate the pointwise density value of data distributions, ADC's density\nestimator uses a formula that evaluates log-likelihood by integrating the score\nfunction, a gradient mapping which ADC has learned to efficiently approximate\nusing its lightweight score estimator. To answer ranged queries, ADC's\nselectivity estimator first predicts their selectivity using a Gaussian Mixture\nModel (GMM), then uses importance sampling Monte Carlo to correct its\npredictions with more accurate pointwise density values calculated by the\ndensity estimator. ADC+ further trains a decision tree to identify the\nhigh-volume, high-selectivity queries that the GMM alone can predict very\naccurately, in which case it skips the correction phase to prevent Monte Carlo\nfrom adding more variance. Doing so lowers median Q-error and cuts per-query\nlatency by 25 percent, making ADC+ usually twice as fast as Naru, arguably the\nstate-of-the-art joint distribution cardinality estimator.\n  Numerical experiments using well-established benchmarks show that on all\nreal-world datasets tested, ADC+ is capable of rivaling Naru and outperforming\nMSCN, DeepDB, LW-Tree, and LW-NN using around 66 percent their storage space,\nbeing at least 3 times as accurate as MSCN on 95th and 99th percentile error.\nFurthermore, on a synthetic dataset where attributes exhibit complex,\nmultilateral correlations, ADC and ADC+ are considerably robust while almost\nevery other learned model suffered significant accuracy declines. In this case,\nADC+ performs better than any other tested model, being 10 times as accurate as\nNaru on 95th and 99th percentile error.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19954", "pdf": "https://arxiv.org/pdf/2510.19954", "abs": "https://arxiv.org/abs/2510.19954", "authors": ["Joseph Meyer", "Divyansha Lachi", "Reza Mohammadi", "Roshan Reddy Upendra", "Eva L. Dyer", "Mark Li", "Tom Palczewski"], "title": "RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs", "categories": ["cs.AI", "cs.DB", "cs.LG"], "comment": "6 pages", "summary": "Relational multi-table data is common in domains such as e-commerce,\nhealthcare, and scientific research, and can be naturally represented as\nheterogeneous temporal graphs with multi-modal node attributes. Existing graph\nneural networks (GNNs) rely on schema-specific feature encoders, requiring\nseparate modules for each node type and feature column, which hinders\nscalability and parameter sharing. We introduce RELATE (Relational Encoder for\nLatent Aggregation of Typed Entities), a schema-agnostic, plug-and-play feature\nencoder that can be used with any general purpose GNN. RELATE employs shared\nmodality-specific encoders for categorical, numerical, textual, and temporal\nattributes, followed by a Perceiver-style cross-attention module that\naggregates features into a fixed-size, permutation-invariant node\nrepresentation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,\nwhere it achieves performance within 3% of schema-specific encoders while\nreducing parameter counts by up to 5x. This design supports varying schemas and\nenables multi-dataset pretraining for general-purpose GNNs, paving the way\ntoward foundation models for relational graph data.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.20467", "pdf": "https://arxiv.org/pdf/2510.20467", "abs": "https://arxiv.org/abs/2510.20467", "authors": ["Yiwen Peng", "Thomas Bonald", "Fabian M. Suchanek"], "title": "FLORA: Unsupervised Knowledge Graph Alignment by Fuzzy Logic", "categories": ["cs.AI", "cs.DB"], "comment": null, "summary": "Knowledge graph alignment is the task of matching equivalent entities (that\nis, instances and classes) and relations across two knowledge graphs. Most\nexisting methods focus on pure entity-level alignment, computing the similarity\nof entities in some embedding space. They lack interpretable reasoning and need\ntraining data to work. In this paper, we propose FLORA, a simple yet effective\nmethod that (1) is unsupervised, i.e., does not require training data, (2)\nprovides a holistic alignment for entities and relations iteratively, (3) is\nbased on fuzzy logic and thus delivers interpretable results, (4) provably\nconverges, (5) allows dangling entities, i.e., entities without a counterpart\nin the other KG, and (6) achieves state-of-the-art results on major benchmarks.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
