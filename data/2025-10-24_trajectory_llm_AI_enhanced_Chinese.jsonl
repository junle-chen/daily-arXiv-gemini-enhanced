{"id": "2510.19001", "pdf": "https://arxiv.org/pdf/2510.19001", "abs": "https://arxiv.org/abs/2510.19001", "authors": ["Seungjun Yu", "Junsung Park", "Youngsun Lim", "Hyunjung Shim"], "title": "Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "We present a two-phase vision-language QA system for autonomous driving that\nanswers high-level perception, prediction, and planning questions. In Phase-1,\na large multimodal LLM (Qwen2.5-VL-32B) is conditioned on six-camera inputs, a\nshort temporal window of history, and a chain-of-thought prompt with few-shot\nexemplars. A self-consistency ensemble (multiple sampled reasoning chains)\nfurther improves answer reliability. In Phase-2, we augment the prompt with\nnuScenes scene metadata (object annotations, ego-vehicle state, etc.) and\ncategory-specific question instructions (separate prompts for perception,\nprediction, planning tasks). In experiments on a driving QA benchmark, our\napproach significantly outperforms the baseline Qwen2.5 models. For example,\nusing 5 history frames and 10-shot prompting in Phase-1 yields 65.1% overall\naccuracy (vs.62.61% with zero-shot); applying self-consistency raises this to\n66.85%. Phase-2 achieves 67.37% overall. Notably, the system maintains 96%\naccuracy under severe visual corruption. These results demonstrate that\ncarefully engineered prompts and contextual grounding can greatly enhance\nhigh-level driving QA with pretrained vision-language models.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-VL-32B\uff09\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00\u95ee\u7b54\u95ee\u9898\uff0c\u6d89\u53ca\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u4efb\u52a1\u3002\u867d\u7136\u4e3b\u8981\u4fa7\u91cd\u4e8e\u95ee\u7b54\uff0c\u4f46\u6458\u8981\u660e\u786e\u63d0\u5230\u4e86\u201cprediction\u201d\u4efb\u52a1\uff0c\u5e76\u4e14\u5e94\u7528\u573a\u666f\u662f\u81ea\u52a8\u9a7e\u9a76\uff0c\u56e0\u6b64\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u540c\u65f6\u4e5f\u4f7f\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u56e0\u6b64\u7efc\u5408\u6765\u770b\uff0c\u76f8\u5173\u6027\u8f83\u9ad8\u3002", "keywords": ["large language models", "LLMs", "trajectory prediction", "prediction", "autonomous driving", "vision-language models"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u89c6\u89c9\u8bed\u8a00\u95ee\u7b54\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\uff0c\u80fd\u591f\u56de\u7b54\u9ad8\u5c42\u6b21\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u6b21\u7684\u7406\u89e3\u548c\u63a8\u7406\u95ee\u7b54\u7684\u95ee\u9898\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e24\u4e2a\u9636\u6bb5\uff1a\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-VL-32B\uff09\u7ed3\u5408\u591a\u76f8\u673a\u8f93\u5165\u3001\u5386\u53f2\u4fe1\u606f\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u8fdb\u884c\u95ee\u7b54\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u589e\u52a0nuScenes\u573a\u666f\u5143\u6570\u636e\u548c\u7279\u5b9a\u7c7b\u522b\u7684\u95ee\u9898\u6307\u4ee4\u6765\u589e\u5f3a\u63d0\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9a7e\u9a76\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4f8b\u5982\uff0c\u5728\u7b2c\u4e00\u9636\u6bb5\u4f7f\u75285\u5e27\u5386\u53f2\u548c10-shot\u63d0\u793a\u53ef\u4ee5\u8fbe\u523065.1%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u4f7f\u7528\u81ea\u6d3d\u6027\u53ef\u4ee5\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u523066.85%\uff0c\u7b2c\u4e8c\u9636\u6bb5\u53ef\u4ee5\u8fbe\u523067.37%\u7684\u603b\u4f53\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u5728\u4e25\u91cd\u7684\u89c6\u89c9\u635f\u574f\u4e0b\u4ecd\u80fd\u4fdd\u630196%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u4e0a\u4e0b\u6587 grounding \u53ef\u4ee5\u6781\u5927\u5730\u63d0\u9ad8\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u6b21\u9a7e\u9a76\u95ee\u7b54\u7684\u80fd\u529b\u3002", "summary_zh": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u4e24\u9636\u6bb5\u89c6\u89c9\u8bed\u8a00\u95ee\u7b54\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u56de\u7b54\u9ad8\u5c42\u6b21\u7684\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u95ee\u9898\u3002\u5728\u7b2c\u4e00\u9636\u6bb5\uff0c\u4e00\u4e2a\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-VL-32B\uff09\u4ee5\u516d\u4e2a\u6444\u50cf\u5934\u8f93\u5165\u3001\u4e00\u6bb5\u5386\u53f2\u65f6\u95f4\u7a97\u53e3\u4ee5\u53ca\u5e26\u6709\u5c11\u91cf\u793a\u4f8b\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u4e3a\u6761\u4ef6\u3002\u81ea\u6d3d\u6027\u96c6\u6210\uff08\u591a\u4e2a\u91c7\u6837\u7684\u63a8\u7406\u94fe\uff09\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7b54\u6848\u7684\u53ef\u9760\u6027\u3002\u5728\u7b2c\u4e8c\u9636\u6bb5\uff0c\u6211\u4eec\u4f7f\u7528nuScenes\u573a\u666f\u5143\u6570\u636e\uff08\u5bf9\u8c61\u6ce8\u91ca\u3001\u81ea\u8f66\u72b6\u6001\u7b49\uff09\u548c\u7c7b\u522b\u7279\u5b9a\u7684\u95ee\u9898\u6307\u4ee4\uff08\u9488\u5bf9\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u4efb\u52a1\u7684\u5355\u72ec\u63d0\u793a\uff09\u6765\u6269\u5145\u63d0\u793a\u3002\u5728\u9a7e\u9a76\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u660e\u663e\u4f18\u4e8e\u57fa\u7ebf Qwen2.5 \u6a21\u578b\u3002\u4f8b\u5982\uff0c\u5728\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528 5 \u4e2a\u5386\u53f2\u5e27\u548c 10-shot \u63d0\u793a\u53ef\u4ea7\u751f 65.1% \u7684\u603b\u4f53\u51c6\u786e\u7387\uff08\u800c\u96f6\u6837\u672c\u4e3a 62.61%\uff09\uff1b\u5e94\u7528\u81ea\u6d3d\u6027\u5c06\u5176\u63d0\u9ad8\u5230 66.85%\u3002\u7b2c\u4e8c\u9636\u6bb5\u8fbe\u5230 67.37% \u7684\u603b\u4f53\u51c6\u786e\u7387\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u7cfb\u7edf\u5728\u4e25\u91cd\u7684\u89c6\u89c9\u635f\u574f\u4e0b\u4ecd\u4fdd\u6301 96% \u7684\u51c6\u786e\u7387\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\u548c\u4e0a\u4e0b\u6587\u57fa\u7840\u53ef\u4ee5\u6781\u5927\u5730\u589e\u5f3a\u4f7f\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u6b21\u9a7e\u9a76\u95ee\u7b54\u7684\u80fd\u529b\u3002"}}
{"id": "2510.19128", "pdf": "https://arxiv.org/pdf/2510.19128", "abs": "https://arxiv.org/abs/2510.19128", "authors": ["Mehran Ghafarian Tamizi", "Homayoun Honari", "Amir Mehdi Soufi Enayati", "Aleksey Nozdryn-Plotnicki", "Homayoun Najjaran"], "title": "A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model", "categories": ["cs.RO", "cs.AI", "68T40 (Primary), 70Q05 (Secondary)"], "comment": "20 pages, 9 figures", "summary": "Path planning for a robotic system in high-dimensional cluttered environments\nneeds to be efficient, safe, and adaptable for different environments and\nhardware. Conventional methods face high computation time and require extensive\nparameter tuning, while prior learning-based methods still fail to generalize\neffectively. The primary goal of this research is to develop a path planning\nframework capable of generalizing to unseen environments and new robotic\nmanipulators without the need for retraining. We present GADGET (Generalizable\nand Adaptive Diffusion-Guided Environment-aware Trajectory generation), a\ndiffusion-based planning model that generates joint-space trajectories\nconditioned on voxelized scene representations as well as start and goal\nconfigurations. A key innovation is GADGET's hybrid dual-conditioning mechanism\nthat combines classifier-free guidance via learned scene encoding with\nclassifier-guided Control Barrier Function (CBF) safety shaping, integrating\nenvironment awareness with real-time collision avoidance directly in the\ndenoising process. This design supports zero-shot transfer to new environments\nand robotic embodiments without retraining. Experimental results show that\nGADGET achieves high success rates with low collision intensity in\nspherical-obstacle, bin-picking, and shelf environments, with CBF guidance\nfurther improving safety. Moreover, comparative evaluations indicate strong\nperformance relative to both sampling-based and learning-based baselines.\nFurthermore, GADGET provides transferability across Franka Panda, Kinova Gen3\n(6/7-DoF), and UR5 robots, and physical execution on a Kinova Gen3 demonstrates\nits ability to generate safe, collision-free trajectories in real-world\nsettings.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper focuses on path planning, which is related to trajectory prediction. It uses a diffusion model, a type of generative model, for trajectory generation. While not explicitly using a Large Language Model, the diffusion model aspect gives it some relevance. The core focus is on robotic path planning and generalization, rather than language models.", "keywords": ["path planning", "trajectory generation", "diffusion model", "robotics"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19655", "pdf": "https://arxiv.org/pdf/2510.19655", "abs": "https://arxiv.org/abs/2510.19655", "authors": ["Hongyu Ding", "Ziming Xu", "Yudong Fang", "You Wu", "Zixuan Chen", "Jieqi Shi", "Jing Huo", "Yifan Zhang", "Yang Gao"], "title": "LaViRA: Language-Vision-Robot Actions Translation for Zero-Shot Vision Language Navigation in Continuous Environments", "categories": ["cs.RO"], "comment": null, "summary": "Zero-shot Vision-and-Language Navigation in Continuous Environments (VLN-CE)\nrequires an agent to navigate unseen environments based on natural language\ninstructions without any prior training. Current methods face a critical\ntrade-off: either rely on environment-specific waypoint predictors that limit\nscene generalization, or underutilize the reasoning capabilities of large\nmodels during navigation. We introduce LaViRA, a simple yet effective zero-shot\nframework that addresses this dilemma by decomposing action into a\ncoarse-to-fine hierarchy: Language Action for high-level planning, Vision\nAction for perceptual grounding, and Robot Action for robust navigation. This\nmodular decomposition allows us to leverage the distinct strengths of different\nscales of Multimodal Large Language Models (MLLMs) at each stage, creating a\nsystem that is powerful in its reasoning, grounding and practical control.\nLaViRA significantly outperforms existing state-of-the-art methods on the\nVLN-CE benchmark, demonstrating superior generalization capabilities in unseen\nenvironments, while maintaining transparency and efficiency for real-world\ndeployment.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6d89\u53caVision-Language Navigation\uff0c\u5176\u4e2dNavigation\u9690\u542b\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u9700\u6c42\u3002\u540c\u65f6\uff0c\u8bba\u6587\u660e\u786e\u4f7f\u7528\u4e86Multimodal Large Language Models (MLLMs)\uff0c\u56e0\u6b64\u4e0e\u5927\u6a21\u578b\u4e5f\u76f8\u5173\u3002\u867d\u7136\u4e0d\u662f\u76f4\u63a5\u7684\u8f68\u8ff9\u9884\u6d4b\u7814\u7a76\uff0c\u4f46\u5176\u5bfc\u822a\u4efb\u52a1\u4e0e\u8f68\u8ff9\u9884\u6d4b\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4f7f\u7528\u4e86\u5927\u6a21\u578b\u3002", "keywords": ["Language-Vision Navigation", "Multimodal Large Language Models", "VLN-CE", "Robot Action"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19654", "pdf": "https://arxiv.org/pdf/2510.19654", "abs": "https://arxiv.org/abs/2510.19654", "authors": ["Zhida Zhao", "Talas Fu", "Yifan Wang", "Lijun Wang", "Huchuan Lu"], "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": "Accepted by NuerIPS 2025 (Poster)", "summary": "Despite remarkable progress in driving world models, their potential for\nautonomous systems remains largely untapped: the world models are mostly\nlearned for world simulation and decoupled from trajectory planning. While\nrecent efforts aim to unify world modeling and planning in a single framework,\nthe synergistic facilitation mechanism of world modeling for planning still\nrequires further exploration. In this work, we introduce a new driving paradigm\nnamed Policy World Model (PWM), which not only integrates world modeling and\ntrajectory planning within a unified architecture, but is also able to benefit\nplanning using the learned world knowledge through the proposed action-free\nfuture state forecasting scheme. Through collaborative state-action prediction,\nPWM can mimic the human-like anticipatory perception, yielding more reliable\nplanning performance. To facilitate the efficiency of video forecasting, we\nfurther introduce a dynamically enhanced parallel token generation mechanism,\nequipped with a context-guided tokenizer and an adaptive dynamic focal loss.\nDespite utilizing only front camera input, our method matches or exceeds\nstate-of-the-art approaches that rely on multi-view and multi-modal inputs.\nCode and model weights will be released at\nhttps://github.com/6550Zhao/Policy-World-Model.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "This paper is relevant to trajectory prediction as it focuses on world models for autonomous driving, which involves forecasting future states and trajectory planning. While it doesn't explicitly mention large language models, the concept of a 'Policy World Model' and the use of video forecasting and token generation share some conceptual overlap with sequence prediction tasks often tackled by LLMs. The focus on action prediction and state forecasting aligns with trajectory prediction tasks.", "keywords": ["trajectory planning", "world model", "state forecasting", "action prediction", "driving world model", "autonomous systems", "video forecasting"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9a7e\u9a76\u8303\u5f0f\uff0cPolicy World Model (PWM)\uff0c\u5b83\u901a\u8fc7\u65e0\u52a8\u4f5c\u7684\u672a\u6765\u72b6\u6001\u9884\u6d4b\u6765\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u548c\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u5229\u7528\u5b66\u4e60\u5230\u7684\u4e16\u754c\u77e5\u8bc6\u6765\u6539\u8fdb\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u7684\u4e16\u754c\u6a21\u578b\u4e3b\u8981\u7528\u4e8e\u4e16\u754c\u6a21\u62df\uff0c\u4e0e\u8f68\u8ff9\u89c4\u5212\u5206\u79bb\uff1b\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u548c\u89c4\u5212\u7684\u534f\u540c\u4fc3\u8fdb\u673a\u5236\u4ecd\u9700\u63a2\u7d22\u3002", "method": "\u63d0\u51fa Policy World Model (PWM)\uff0c\u901a\u8fc7\u534f\u4f5c\u72b6\u6001-\u52a8\u4f5c\u9884\u6d4b\u6a21\u4eff\u7c7b\u4eba\u9884\u6d4b\u611f\u77e5\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u589e\u5f3a\u7684\u5e76\u884c token \u751f\u6210\u673a\u5236\u3002", "result": "\u4ec5\u4f7f\u7528\u524d\u7f6e\u6444\u50cf\u5934\u8f93\u5165\uff0c\u8be5\u65b9\u6cd5\u5c31\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4f9d\u8d56\u591a\u89c6\u89d2\u548c\u591a\u6a21\u6001\u8f93\u5165\u7684\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002", "conclusion": "Policy World Model (PWM) \u80fd\u591f\u6709\u6548\u6574\u5408\u4e16\u754c\u5efa\u6a21\u548c\u8f68\u8ff9\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u4e16\u754c\u77e5\u8bc6\u63d0\u5347\u89c4\u5212\u6027\u80fd\u3002", "summary_zh": "\u5c3d\u7ba1\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u81ea\u4e3b\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u5229\u7528\uff1a\u4e16\u754c\u6a21\u578b\u4e3b\u8981\u7528\u4e8e\u4e16\u754c\u6a21\u62df\uff0c\u4e0e\u8f68\u8ff9\u89c4\u5212\u5206\u79bb\u3002\u867d\u7136\u6700\u8fd1\u7684\u7814\u7a76\u8bd5\u56fe\u5728\u5355\u4e00\u6846\u67b6\u5185\u7edf\u4e00\u4e16\u754c\u5efa\u6a21\u548c\u89c4\u5212\uff0c\u4f46\u4e16\u754c\u5efa\u6a21\u5bf9\u89c4\u5212\u7684\u534f\u540c\u4fc3\u8fdb\u673a\u5236\u4ecd\u6709\u5f85\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u9a7e\u9a76\u8303\u5f0f\uff0c\u540d\u4e3a Policy World Model (PWM)\uff0c\u5b83\u4e0d\u4ec5\u5728\u7edf\u4e00\u67b6\u6784\u5185\u96c6\u6210\u4e86\u4e16\u754c\u5efa\u6a21\u548c\u8f68\u8ff9\u89c4\u5212\uff0c\u800c\u4e14\u8fd8\u80fd\u591f\u901a\u8fc7\u63d0\u51fa\u7684\u65e0\u52a8\u4f5c\u7684\u672a\u6765\u72b6\u6001\u9884\u6d4b\u65b9\u6848\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u4e16\u754c\u77e5\u8bc6\u6765\u6539\u8fdb\u89c4\u5212\u3002\u901a\u8fc7\u534f\u4f5c\u72b6\u6001-\u52a8\u4f5c\u9884\u6d4b\uff0cPWM \u53ef\u4ee5\u6a21\u4eff\u7c7b\u4eba\u9884\u6d4b\u611f\u77e5\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u53ef\u9760\u7684\u89c4\u5212\u6027\u80fd\u3002\u4e3a\u4e86\u63d0\u9ad8\u89c6\u9891\u9884\u6d4b\u7684\u6548\u7387\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u79cd\u52a8\u6001\u589e\u5f3a\u7684\u5e76\u884c token \u751f\u6210\u673a\u5236\uff0c\u8be5\u673a\u5236\u914d\u5907\u4e86\u4e0a\u4e0b\u6587\u5f15\u5bfc\u7684 tokenizer \u548c\u81ea\u9002\u5e94\u52a8\u6001 focal loss\u3002\u5c3d\u7ba1\u4ec5\u4f7f\u7528\u524d\u7f6e\u6444\u50cf\u5934\u8f93\u5165\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5c31\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4f9d\u8d56\u591a\u89c6\u89d2\u548c\u591a\u6a21\u6001\u8f93\u5165\u7684\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u6a21\u578b\u6743\u91cd\u5c06\u5728 https://github.com/6550Zhao/Policy-World-Model \u4e0a\u53d1\u5e03\u3002"}}
{"id": "2510.19050", "pdf": "https://arxiv.org/pdf/2510.19050", "abs": "https://arxiv.org/abs/2510.19050", "authors": ["Wenqian Ye", "Guangtao Zheng", "Aidong Zhang"], "title": "Rectifying Shortcut Behaviors in Preference-based Reward Learning", "categories": ["cs.AI", "cs.LG"], "comment": "NeurIPS 2025", "summary": "In reinforcement learning from human feedback, preference-based reward models\nplay a central role in aligning large language models to human-aligned\nbehavior. However, recent studies show that these models are prone to reward\nhacking and often fail to generalize well due to over-optimization. They\nachieve high reward scores by exploiting shortcuts, that is, exploiting\nspurious features (e.g., response verbosity, agreeable tone, or sycophancy)\nthat correlate with human preference labels in the training data rather than\ngenuinely reflecting the intended objectives. In this paper, instead of probing\nthese issues one at a time, we take a broader view of the reward hacking\nproblem as shortcut behaviors and introduce a principled yet flexible approach\nto mitigate shortcut behaviors in preference-based reward learning. Inspired by\nthe invariant theory in the kernel perspective, we propose Preference-based\nReward Invariance for Shortcut Mitigation (PRISM), which learns group-invariant\nkernels with feature maps in a closed-form learning objective. Experimental\nresults in several benchmarks show that our method consistently improves the\naccuracy of the reward model on diverse out-of-distribution tasks and reduces\nthe dependency on shortcuts in downstream policy models, establishing a robust\nframework for preference-based alignment.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on reward learning for aligning large language models to human behavior. While it doesn't directly deal with trajectory prediction, it does involve large language models and their interaction with human preferences, which is a relevant area in the broader context of AI and robotics where trajectory prediction might be integrated. The core of the paper is about improving the robustness and generalizability of reward models used to train LLMs, preventing them from exploiting shortcuts.", "keywords": ["large language models", "reward learning", "reinforcement learning from human feedback", "preference-based reward models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19022", "pdf": "https://arxiv.org/pdf/2510.19022", "abs": "https://arxiv.org/abs/2510.19022", "authors": ["Aritra Bhowmik", "Denis Korzhenkov", "Cees G. M. Snoek", "Amirhossein Habibian", "Mohsen Ghafoorian"], "title": "MoAlign: Motion-Centric Representation Alignment for Video Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-video diffusion models have enabled high-quality video synthesis, yet\noften fail to generate temporally coherent and physically plausible motion. A\nkey reason is the models' insufficient understanding of complex motions that\nnatural videos often entail. Recent works tackle this problem by aligning\ndiffusion model features with those from pretrained video encoders. However,\nthese encoders mix video appearance and dynamics into entangled features,\nlimiting the benefit of such alignment. In this paper, we propose a\nmotion-centric alignment framework that learns a disentangled motion subspace\nfrom a pretrained video encoder. This subspace is optimized to predict\nground-truth optical flow, ensuring it captures true motion dynamics. We then\nalign the latent features of a text-to-video diffusion model to this new\nsubspace, enabling the generative model to internalize motion knowledge and\ngenerate more plausible videos. Our method improves the physical commonsense in\na state-of-the-art video diffusion model, while preserving adherence to textual\nprompts, as evidenced by empirical evaluations on VideoPhy, VideoPhy2, VBench,\nand VBench-2.0, along with a user study.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on improving the motion understanding of video diffusion models, which relates to generating physically plausible motion. While not directly about trajectory prediction, the emphasis on motion and dynamics brings it closer to the field. The use of diffusion models (related to large models) and the optimization using optical flow (related to motion prediction) contribute to the relevance. However, the paper doesn't explicitly mention trajectory prediction or leverage LLMs in a significant way.", "keywords": ["diffusion models", "motion", "optical flow", "video encoder", "text-to-video", "motion dynamics", "video diffusion models"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19205", "pdf": "https://arxiv.org/pdf/2510.19205", "abs": "https://arxiv.org/abs/2510.19205", "authors": ["Yaoyao Qian", "Yuanli Wang", "Jinda Zhang", "Yun Zong", "Meixu Chen", "Hanhan Zhou", "Jindan Huang", "Yifan Zeng", "Xinyu Hu", "Chan Hee Song", "Danqing Zhang"], "title": "WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation", "categories": ["cs.AI"], "comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Multi-Turn Interactions in Large Language Models", "summary": "Current evaluation of web agents largely reduces to binary success metrics or\nconformity to a single reference trajectory, ignoring the structural diversity\npresent in benchmark datasets. We present WebGraphEval, a framework that\nabstracts trajectories from multiple agents into a unified, weighted action\ngraph. This representation is directly compatible with benchmarks such as\nWebArena, leveraging leaderboard runs and newly collected trajectories without\nmodifying environments. The framework canonically encodes actions, merges\nrecurring behaviors, and applies structural analyses including reward\npropagation and success-weighted edge statistics. Evaluations across thousands\nof trajectories from six web agents show that the graph abstraction captures\ncross-model regularities, highlights redundancy and inefficiency, and\nidentifies critical decision points overlooked by outcome-based metrics. By\nframing web interaction as graph-structured data, WebGraphEval establishes a\ngeneral methodology for multi-path, cross-agent, and efficiency-aware\nevaluation of web agents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on evaluating web agents' trajectories using a graph representation. While it doesn't directly use Large Language Models for trajectory prediction, it analyzes trajectories generated by agents, which could potentially include agents powered by LLMs. The core is trajectory evaluation and representation, making it somewhat relevant to trajectory analysis.", "keywords": ["trajectory evaluation", "web agents", "graph representation", "action graph"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19150", "pdf": "https://arxiv.org/pdf/2510.19150", "abs": "https://arxiv.org/abs/2510.19150", "authors": ["Yunzhe Wang", "Soham Hans", "Volkan Ustun"], "title": "X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "8 pages, 5 figures", "summary": "Human team tactics emerge from each player's individual perspective and their\nability to anticipate, interpret, and adapt to teammates' intentions. While\nadvances in video understanding have improved the modeling of team interactions\nin sports, most existing work relies on third-person broadcast views and\noverlooks the synchronous, egocentric nature of multi-agent learning. We\nintroduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplay\nfootage from 45 professional-level matches of the popular e-sports game\nCounter-Strike 2, designed to facilitate research on multi-agent\ndecision-making in complex 3D environments. X-Ego-CS provides cross-egocentric\nvideo streams that synchronously capture all players' first-person perspectives\nalong with state-action trajectories. Building on this resource, we propose\nCross-Ego Contrastive Learning (CECL), which aligns teammates' egocentric\nvisual streams to foster team-level tactical situational awareness from an\nindividual's perspective. We evaluate CECL on a teammate-opponent location\nprediction task, demonstrating its effectiveness in enhancing an agent's\nability to infer both teammate and opponent positions from a single\nfirst-person view using state-of-the-art video encoders. Together, X-Ego-CS and\nCECL establish a foundation for cross-egocentric multi-agent benchmarking in\nesports. More broadly, our work positions gameplay understanding as a testbed\nfor multi-agent modeling and tactical learning, with implications for\nspatiotemporal reasoning and human-AI teaming in both virtual and real-world\ndomains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u56e2\u961f\u6218\u672f\u6001\u52bf\u611f\u77e5\uff0c\u6d89\u53ca\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e0b\u7684\u51b3\u7b56\u548c\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u961f\u53cb/\u5bf9\u624b\u4f4d\u7f6e\u9884\u6d4b\u4efb\u52a1\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff0c\u4e14\u63d0\u5230\u4e86\u65f6\u7a7a\u63a8\u7406\u548c\u4eba\u673a\u534f\u4f5c\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "multi-agent", "decision-making", "location prediction", "spatiotemporal reasoning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19364", "pdf": "https://arxiv.org/pdf/2510.19364", "abs": "https://arxiv.org/abs/2510.19364", "authors": ["Golnaz Raja", "Ruslan Agishev", "Milo\u0161 Pr\u00e1gr", "Joni Pajarinen", "Karel Zimmermann", "Arun Kumar Singh", "Reza Ghabcheloo"], "title": "ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling", "categories": ["cs.RO"], "comment": "This paper is submitted to IEEE International Conference on Robotics\n  and Automation (ICRA) 2026", "summary": "Uncertainty-aware robot motion prediction is crucial for downstream\ntraversability estimation and safe autonomous navigation in unstructured,\noff-road environments, where terrain is heterogeneous and perceptual\nuncertainty is high. Most existing methods assume deterministic or spatially\nindependent terrain uncertainties, ignoring the inherent local correlations of\n3D spatial data and often producing unreliable predictions. In this work, we\nintroduce an efficient probabilistic framework that explicitly models spatially\ncorrelated aleatoric uncertainty over terrain parameters as a probabilistic\nworld model and propagates this uncertainty through a differentiable physics\nengine for probabilistic trajectory forecasting. By leveraging structured\nconvolutional operators, our approach provides high-resolution multivariate\npredictions at manageable computational cost. Experimental evaluation on a\npublicly available dataset shows significantly improved uncertainty estimation\nand trajectory prediction accuracy over aleatoric uncertainty estimation\nbaselines.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on probabilistic trajectory forecasting for robots navigating rough terrain. While it doesn't directly involve large language models, it heavily utilizes trajectory prediction and probabilistic modeling, which are relevant to the task. The connection to trajectory prediction is stronger than any connection to large language models.", "keywords": ["trajectory prediction", "probabilistic trajectory forecasting", "robot motion prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19195", "pdf": "https://arxiv.org/pdf/2510.19195", "abs": "https://arxiv.org/abs/2510.19195", "authors": ["Kai Zeng", "Zhanqian Wu", "Kaixin Xiong", "Xiaobao Wei", "Xiangyu Guo", "Zhenxin Zhu", "Kalok Ho", "Lijun Zhou", "Bohan Zeng", "Ming Lu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wentao Zhang"], "title": "Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in driving world models enable controllable generation of\nhigh-quality RGB videos or multimodal videos. Existing methods primarily focus\non metrics related to generation quality and controllability. However, they\noften overlook the evaluation of downstream perception tasks, which are\n$\\mathbf{really\\ crucial}$ for the performance of autonomous driving. Existing\nmethods usually leverage a training strategy that first pretrains on synthetic\ndata and finetunes on real data, resulting in twice the epochs compared to the\nbaseline (real data only). When we double the epochs in the baseline, the\nbenefit of synthetic data becomes negligible. To thoroughly demonstrate the\nbenefit of synthetic data, we introduce Dream4Drive, a novel synthetic data\ngeneration framework designed for enhancing the downstream perception tasks.\nDream4Drive first decomposes the input video into several 3D-aware guidance\nmaps and subsequently renders the 3D assets onto these guidance maps. Finally,\nthe driving world model is fine-tuned to produce the edited, multi-view\nphotorealistic videos, which can be used to train the downstream perception\nmodels. Dream4Drive enables unprecedented flexibility in generating multi-view\ncorner cases at scale, significantly boosting corner case perception in\nautonomous driving. To facilitate future research, we also contribute a\nlarge-scale 3D asset dataset named DriveObj3D, covering the typical categories\nin driving scenarios and enabling diverse 3D-aware video editing. We conduct\ncomprehensive experiments to show that Dream4Drive can effectively boost the\nperformance of downstream perception models under various training epochs.\nProject: $\\href{https://wm-research.github.io/Dream4Drive/}{this\\ https\\ URL}$", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u9a7e\u9a76\u4e16\u754c\u6a21\u578b\u4f5c\u4e3a\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff0c\u4ee5\u63d0\u5347\u611f\u77e5\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u81ea\u52a8\u9a7e\u9a76\u7684 corner case \u611f\u77e5\u65b9\u9762\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u5229\u7528\u4e86\u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8fd9\u4e0e\u4f7f\u7528\u5927\u578b\u6a21\u578b\u8fdb\u884c\u6570\u636e\u589e\u5f3a\u6216\u6a21\u62df\u73af\u5883\u7684\u601d\u60f3\u76f8\u5173\u3002\u6b64\u5916\uff0c\u81ea\u52a8\u9a7e\u9a76\u7684\u611f\u77e5\u4efb\u52a1\u901a\u5e38\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7d27\u5bc6\u76f8\u8fde\u3002", "keywords": ["driving world model", "synthetic data", "perception tasks", "autonomous driving", "3D-aware video editing", "corner case perception"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19661", "pdf": "https://arxiv.org/pdf/2510.19661", "abs": "https://arxiv.org/abs/2510.19661", "authors": ["Xusen Guo", "Mingxing Peng", "Xixuan Hao", "Xingchen Zou", "Qiongyan Wang", "Sijie Ruan", "Yuxuan Liang"], "title": "AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing", "categories": ["cs.AI"], "comment": "13 pages, 10 pages", "summary": "Web-based participatory urban sensing has emerged as a vital approach for\nmodern urban management by leveraging mobile individuals as distributed\nsensors. However, existing urban sensing systems struggle with limited\ngeneralization across diverse urban scenarios and poor interpretability in\ndecision-making. In this work, we introduce AgentSense, a hybrid, training-free\nframework that integrates large language models (LLMs) into participatory urban\nsensing through a multi-agent evolution system. AgentSense initially employs\nclassical planner to generate baseline solutions and then iteratively refines\nthem to adapt sensing task assignments to dynamic urban conditions and\nheterogeneous worker preferences, while producing natural language explanations\nthat enhance transparency and trust. Extensive experiments across two\nlarge-scale mobility datasets and seven types of dynamic disturbances\ndemonstrate that AgentSense offers distinct advantages in adaptivity and\nexplainability over traditional methods. Furthermore, compared to single-agent\nLLM baselines, our approach outperforms in both performance and robustness,\nwhile delivering more reasonable and transparent explanations. These results\nposition AgentSense as a significant advancement towards deploying adaptive and\nexplainable urban sensing systems on the web.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper utilizes Large Language Models (LLMs) for urban sensing, which involves analyzing mobility data and adapting to dynamic urban conditions. While the core focus isn't directly on trajectory prediction itself, the use of mobility datasets and the mention of planning (classical planner) suggest a connection to movement and path-related concepts, albeit indirectly. The LLM aspect is prominent.", "keywords": ["LLMs", "Large Language Models", "mobility datasets", "planner", "urban sensing"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19220", "pdf": "https://arxiv.org/pdf/2510.19220", "abs": "https://arxiv.org/abs/2510.19220", "authors": ["Xiaoqing Lan", "Biqiao Xin", "Bingshu Wang", "Han Zhang", "Laixian Zhang"], "title": "Space Object Detection using Multi-frame Temporal Trajectory Completion Method", "categories": ["cs.CV"], "comment": null, "summary": "Space objects in Geostationary Earth Orbit (GEO) present significant\ndetection challenges in optical imaging due to weak signals, complex stellar\nbackgrounds, and environmental interference. In this paper, we enhance\nhigh-frequency features of GEO targets while suppressing background noise at\nthe single-frame level through wavelet transform. Building on this, we propose\na multi-frame temporal trajectory completion scheme centered on the Hungarian\nalgorithm for globally optimal cross-frame matching. To effectively mitigate\nmissing and false detections, a series of key steps including temporal matching\nand interpolation completion, temporal-consistency-based noise filtering, and\nprogressive trajectory refinement are designed in the post-processing pipeline.\nExperimental results on the public SpotGEO dataset demonstrate the\neffectiveness of the proposed method, achieving an F_1 score of 90.14%.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on trajectory completion for space object detection, which falls under the umbrella of trajectory prediction. While it doesn't directly involve large language models, the 'trajectory completion' aspect links it to trajectory prediction research.", "keywords": ["trajectory completion", "trajectory prediction", "object detection"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19255", "pdf": "https://arxiv.org/pdf/2510.19255", "abs": "https://arxiv.org/abs/2510.19255", "authors": ["Mingrui Zhao", "Sauradip Nag", "Kai Wang", "Aditya Vora", "Guangda Ji", "Peter Chun", "Ali Mahdavi-Amiri", "Hao Zhang"], "title": "Advances in 4D Representation: Geometry, Motion, and Interaction", "categories": ["cs.CV"], "comment": "21 pages. Project Page: https://mingrui-zhao.github.io/4DRep-GMI/", "summary": "We present a survey on 4D generation and reconstruction, a fast-evolving\nsubfield of computer graphics whose developments have been propelled by recent\nadvances in neural fields, geometric and motion deep learning, as well 3D\ngenerative artificial intelligence (GenAI). While our survey is not the first\nof its kind, we build our coverage of the domain from a unique and distinctive\nperspective of 4D representations\\/}, to model 3D geometry evolving over time\nwhile exhibiting motion and interaction. Specifically, instead of offering an\nexhaustive enumeration of many works, we take a more selective approach by\nfocusing on representative works to highlight both the desirable properties and\nensuing challenges of each representation under different computation,\napplication, and data scenarios. The main take-away message we aim to convey to\nthe readers is on how to select and then customize the appropriate 4D\nrepresentations for their tasks. Organizationally, we separate the 4D\nrepresentations based on three key pillars: geometry, motion, and interaction.\nOur discourse will not only encompass the most popular representations of\ntoday, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),\nbut also bring attention to relatively under-explored representations in the 4D\ncontext, such as structured models and long-range motions. Throughout our\nsurvey, we will reprise the role of large language models (LLMs) and video\nfoundational models (VFMs) in a variety of 4D applications, while steering our\ndiscussion towards their current limitations and how they can be addressed. We\nalso provide a dedicated coverage on what 4D datasets are currently available,\nas well as what is lacking, in driving the subfield forward. Project\npage:https://mingrui-zhao.github.io/4DRep-GMI/", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses 4D representation with geometry, motion and interaction, mentioning the role of large language models (LLMs) and video foundational models (VFMs) in 4D applications. It touches upon motion which is related to trajectory prediction, and explicitly mentions LLMs. However, trajectory prediction isn't the main focus, and the connection to LLMs seems more like leveraging them for 4D representation rather than directly researching their use in trajectory prediction.", "keywords": ["motion", "large language models", "LLMs", "video foundational models", "VFMs"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19072", "pdf": "https://arxiv.org/pdf/2510.19072", "abs": "https://arxiv.org/abs/2510.19072", "authors": ["Tomoki Arita", "Keisuke Okumura"], "title": "Local Guidance for Configuration-Based Multi-Agent Pathfinding", "categories": ["cs.MA", "cs.AI", "cs.RO"], "comment": "10 pages", "summary": "Guidance is an emerging concept that improves the empirical performance of\nreal-time, sub-optimal multi-agent pathfinding (MAPF) methods. It offers\nadditional information to MAPF algorithms to mitigate congestion on a global\nscale by considering the collective behavior of all agents across the entire\nworkspace. This global perspective helps reduce agents' waiting times, thereby\nimproving overall coordination efficiency. In contrast, this study explores an\nalternative approach: providing local guidance in the vicinity of each agent.\nWhile such localized methods involve recomputation as agents move and may\nappear computationally demanding, we empirically demonstrate that supplying\ninformative spatiotemporal cues to the planner can significantly improve\nsolution quality without exceeding a moderate time budget. When applied to\nLaCAM, a leading configuration-based solver, this form of guidance establishes\na new performance frontier for MAPF.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08MAPF\uff09\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\uff0c\u4f46\u6ca1\u6709\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u867d\u7136\u662f\u8def\u5f84\u89c4\u5212\uff0c\u4f46\u66f4\u591a\u7684\u662f\u5173\u4e8e\u591a\u667a\u80fd\u4f53\u7684\u534f\u540c\u800c\u975e\u5355\u4e00\u8f68\u8ff9\u7684\u9884\u6d4b\uff0c\u56e0\u6b64\u76f8\u5173\u6027\u4e0d\u662f\u7279\u522b\u9ad8\u3002\u5173\u952e\u8bcd\u8868\u660e\u5b83\u5c5e\u4e8e\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002", "keywords": ["multi-agent pathfinding", "MAPF", "pathfinding", "path planning"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19818", "pdf": "https://arxiv.org/pdf/2510.19818", "abs": "https://arxiv.org/abs/2510.19818", "authors": ["Jacob Berg", "Chuning Zhu", "Yanda Bao", "Ishan Durugkar", "Abhishek Gupta"], "title": "Semantic World Models", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": null, "summary": "Planning with world models offers a powerful paradigm for robotic control.\nConventional approaches train a model to predict future frames conditioned on\ncurrent frames and actions, which can then be used for planning. However, the\nobjective of predicting future pixels is often at odds with the actual planning\nobjective; strong pixel reconstruction does not always correlate with good\nplanning decisions. This paper posits that instead of reconstructing future\nframes as pixels, world models only need to predict task-relevant semantic\ninformation about the future. For such prediction the paper poses world\nmodeling as a visual question answering problem about semantic information in\nfuture frames. This perspective allows world modeling to be approached with the\nsame tools underlying vision language models. Thus vision language models can\nbe trained as \"semantic\" world models through a supervised finetuning process\non image-action-text data, enabling planning for decision-making while\ninheriting many of the generalization and robustness properties from the\npretrained vision-language models. The paper demonstrates how such a semantic\nworld model can be used for policy improvement on open-ended robotics tasks,\nleading to significant generalization improvements over typical paradigms of\nreconstruction-based action-conditional world modeling. Website available at\nhttps://weirdlabuw.github.io/swm.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using vision-language models as \"semantic\" world models for robotic control. While it doesn't directly address trajectory prediction in the traditional sense (e.g., predicting the future trajectory of pedestrians or vehicles), it does involve predicting future states and planning, which are related. The use of vision-language models connects it to the realm of large models, although the focus is more on vision-language models than general large language models.", "keywords": ["world models", "vision language models", "planning", "robotic control", "semantic information"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19345", "pdf": "https://arxiv.org/pdf/2510.19345", "abs": "https://arxiv.org/abs/2510.19345", "authors": ["Alvaro Perez-Diaz", "James C. Loach", "Danielle E. Toutoungi", "Lee Middleton"], "title": "Foundation Model Forecasts: Form and Function", "categories": ["cs.LG", "cs.AI"], "comment": "28 pages, 3 figures", "summary": "Time-series foundation models (TSFMs) achieve strong forecast accuracy, yet\naccuracy alone does not determine practical value. The form of a forecast --\npoint, quantile, parametric, or trajectory ensemble -- fundamentally constrains\nwhich operational tasks it can support. We survey recent TSFMs and find that\ntwo-thirds produce only point or parametric forecasts, while many operational\ntasks require trajectory ensembles that preserve temporal dependence. We\nestablish when forecast types can be converted and when they cannot: trajectory\nensembles convert to simpler forms via marginalization without additional\nassumptions, but the reverse requires imposing temporal dependence through\ncopulas or conformal methods. We prove that marginals cannot determine\npath-dependent event probabilities -- infinitely many joint distributions share\nidentical marginals but yield different answers to operational questions. We\nmap six fundamental forecasting tasks to minimal sufficient forecast types and\nprovide a task-aligned evaluation framework. Our analysis clarifies when\nforecast type, not accuracy, differentiates practical utility.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on time-series forecasting using foundation models. While it doesn't explicitly mention trajectory prediction in the context of agents or vehicles, the concept of forecasting trajectories (trajectory ensembles) is discussed. The connection to Large Language Models is through the use of foundation models for time-series data. Therefore, there is a moderate level of relevance.", "keywords": ["foundation models", "time-series forecasting", "trajectory ensembles"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2510.19376", "pdf": "https://arxiv.org/pdf/2510.19376", "abs": "https://arxiv.org/abs/2510.19376", "authors": ["Fabian Schaipp"], "title": "Optimization Benchmark for Diffusion Models on Dynamical Systems", "categories": ["cs.LG", "math.OC"], "comment": null, "summary": "The training of diffusion models is often absent in the evaluation of new\noptimization techniques. In this work, we benchmark recent optimization\nalgorithms for training a diffusion model for denoising flow trajectories. We\nobserve that Muon and SOAP are highly efficient alternatives to AdamW (18%\nlower final loss). We also revisit several recent phenomena related to the\ntraining of models for text or image applications in the context of diffusion\nmodel training. This includes the impact of the learning-rate schedule on the\ntraining dynamics, and the performance gap between Adam and SGD.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on optimizing diffusion models for denoising flow trajectories, which is related to trajectory prediction. While it mentions diffusion models, it does not directly involve large language models. The connection to trajectory prediction comes from the 'flow trajectories' aspect.", "keywords": ["diffusion models", "trajectory", "flow trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
