{"id": "2506.15831", "pdf": "https://arxiv.org/pdf/2506.15831", "abs": "https://arxiv.org/abs/2506.15831", "authors": ["Jongjun Park", "Fei Chiang", "Mostafa Milani"], "title": "Adaptive Anomaly Detection in the Presence of Concept Drift: Extended Report", "categories": ["cs.DB"], "comment": "Extended version (to be updated)", "summary": "Data changes to reflect evolving user behaviour, preferences, and changes in\nthe environment. Such changes may occur due to expected shifts in the data\ndistribution, i.e., concept drift, or unexpected anomalous changes. The\npresence of concept drift poses challenges for anomaly detection in time\nseries. While anomalies are caused by undesirable changes in the data,\ndifferentiating abnormal changes from varying normal behaviours is difficult\ndue to differing frequencies of occurrence, varying time intervals when normal\npatterns occur. Differentiating between concept drift and anomalies is critical\nfor accurate analysis as studies have shown that the compounding effects of\nerror propagation in downstream data analysis tasks lead to lower detection\naccuracy and increased overhead due to unnecessary model updates.\nUnfortunately, existing work has largely explored anomaly detection and concept\ndrift detection in isolation. We develop AnDri, a system for Anomaly detection\nin the presence of Drift, which adjusts the normal patterns temporally, and\ndistinguish abnormal subsequences and new concepts. Moreover, it introduces a\nnew clustering method, Adjacent Hierarchical Clustering (AHC), which groups\nsimilar subsequences while respecting their temporal locality.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.15848", "pdf": "https://arxiv.org/pdf/2506.15848", "abs": "https://arxiv.org/abs/2506.15848", "authors": ["Jiazhen Peng", "Zheng Qu", "Xiaoye Miao", "Rong Zhu"], "title": "Delta: A Learned Mixed Cost-based Query Optimization Framework", "categories": ["cs.DB"], "comment": null, "summary": "Query optimizer is a crucial module for database management systems. Existing\noptimizers exhibit two flawed paradigms: (1) cost-based optimizers use dynamic\nprogramming with cost models but face search space explosion and heuristic\npruning constraints; (2) value-based ones train value networks to enable\nefficient beam search, but incur higher training costs and lower accuracy. They\nalso lack mechanisms to detect queries where they may perform poorly. To\ndetermine more efficient plans, we propose Delta, a mixed cost-based query\noptimization framework that consists of a compatible query detector and a\ntwo-stage planner. Delta first employs a Mahalanobis distancebased detector to\npreemptively filter out incompatible queries where the planner might perform\npoorly. For compatible queries, Delta activates its two-stage mixed cost-based\nplanner. Stage I serves as a coarse-grained filter to generate high-quality\ncandidate plans based on the value network via beam search, relaxing precision\nrequirements and narrowing the search space. Stage II employs a fine-grained\nranker to determine the best plan from the candidate plans based on a learned\ncost model. Moreover, to reduce training costs, we reuse and augment the\ntraining data from stage I to train the model in stage II. Experimental results\non three workloads demonstrate that Delta identifies higher-quality plans,\nachieving an average 2.34x speedup over PostgreSQL and outperforming the\nstate-of-the-art learned methods by 2.21x.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.15986", "pdf": "https://arxiv.org/pdf/2506.15986", "abs": "https://arxiv.org/abs/2506.15986", "authors": ["Jiancheng Ruan", "Tingyang Chen", "Renchi Yang", "Xiangyu Ke", "Yunjun Gao"], "title": "Empowering Graph-based Approximate Nearest Neighbor Search with Adaptive Awareness Capabilities", "categories": ["cs.DB", "cs.IR"], "comment": "Accecpted by KDD2025", "summary": "Approximate Nearest Neighbor Search (ANNS) in high-dimensional spaces finds\nextensive applications in databases, information retrieval, recommender\nsystems, etc. While graph-based methods have emerged as the leading solution\nfor ANNS due to their superior query performance, they still face several\nchallenges, such as struggling with local optima and redundant computations.\nThese issues arise because existing methods (i) fail to fully exploit the\ntopological information underlying the proximity graph G, and (ii) suffer from\nsevere distribution mismatches between the base data and queries in practice.\n  To this end, this paper proposes GATE, high-tier proximity Graph with\nAdaptive Topology and Query AwarEness, as a lightweight and adaptive module\natop the graph-based indexes to accelerate ANNS. Specifically, GATE formulates\nthe critical problem to identify an optimal entry point in the proximity graph\nfor a given query, facilitating faster online search. By leveraging the\ninherent clusterability of high-dimensional data, GATE first extracts a small\nset of hub nodes V as candidate entry points. Then, resorting to a contrastive\nlearning-based two-tower model, GATE encodes both the structural semantics\nunderlying G and the query-relevant features into the latent representations of\nthese hub nodes V. A navigation graph index on V is further constructed to\nminimize the model inference overhead. Extensive experiments demonstrate that\nGATE achieves a 1.2-2.0X speed-up in query performance compared to\nstate-of-the-art graph-based indexes.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.15987", "pdf": "https://arxiv.org/pdf/2506.15987", "abs": "https://arxiv.org/abs/2506.15987", "authors": ["Alireza Heidari", "Wei Zhang"], "title": "Filter-Centric Vector Indexing: Geometric Transformation for Efficient Filtered Vector Search", "categories": ["cs.DB", "math.MG"], "comment": "9 pages", "summary": "The explosive growth of vector search applications demands efficient handling\nof combined vector similarity and attribute filtering; a challenge where\ncurrent approaches force an unsatisfying choice between performance and\naccuracy. We introduce Filter-Centric Vector Indexing (FCVI), a novel framework\nthat transforms this fundamental trade-off by directly encoding filter\nconditions into the vector space through a mathematically principled\ntransformation $\\psi(v, f, \\alpha)$. Unlike specialized solutions, FCVI works\nwith any existing vector index (HNSW, FAISS, ANNOY) while providing theoretical\nguarantees on accuracy. Our comprehensive evaluation demonstrates that FCVI\nachieves 2.6-3.0 times higher throughput than state-of-the-art methods while\nmaintaining comparable recall. More remarkably, FCVI exhibits exceptional\nstability under distribution shifts; maintaining consistent performance when\nfilter patterns or vector distributions change, unlike traditional approaches\nthat degrade significantly. This combination of performance, compatibility, and\nresilience positions FCVI as an immediately applicable solution for production\nvector search systems requiring flexible filtering capabilities.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16007", "pdf": "https://arxiv.org/pdf/2506.16007", "abs": "https://arxiv.org/abs/2506.16007", "authors": ["Peizhi Wu", "Rong Kang", "Tieying Zhang", "Jianjun Chen", "Ryan Marcus", "Zachary G. Ives"], "title": "Data-Agnostic Cardinality Learning from Imperfect Workloads", "categories": ["cs.DB", "cs.LG"], "comment": "14 pages. Technical Report (Extended Version)", "summary": "Cardinality estimation (CardEst) is a critical aspect of query optimization.\nTraditionally, it leverages statistics built directly over the data. However,\norganizational policies (e.g., regulatory compliance) may restrict global data\naccess. Fortunately, query-driven cardinality estimation can learn CardEst\nmodels using query workloads. However, existing query-driven models often\nrequire access to data or summaries for best performance, and they assume\nperfect training workloads with complete and balanced join templates (or join\ngraphs). Such assumptions rarely hold in real-world scenarios, in which join\ntemplates are incomplete and imbalanced. We present GRASP, a data-agnostic\ncardinality learning system designed to work under these real-world\nconstraints. GRASP's compositional design generalizes to unseen join templates\nand is robust to join template imbalance. It also introduces a new per-table\nCardEst model that handles value distribution shifts for range predicates, and\na novel learned count sketch model that captures join correlations across base\nrelations. Across three database instances, we demonstrate that GRASP\nconsistently outperforms existing query-driven models on imperfect workloads,\nboth in terms of estimation accuracy and query latency. Remarkably, GRASP\nachieves performance comparable to, or even surpassing, traditional approaches\nbuilt over the underlying data on the complex CEB-IMDb-full benchmark --\ndespite operating without any data access and using only 10% of all possible\njoin templates.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16379", "pdf": "https://arxiv.org/pdf/2506.16379", "abs": "https://arxiv.org/abs/2506.16379", "authors": ["Yan Zhou", "Chunwei Liu", "Bhuvan Urgaonkar", "Zhengle Wang", "Magnus Mueller", "Chao Zhang", "Songyue Zhang", "Pascal Pfeil", "Dominik Horn", "Zhengchun Liu", "Davide Pagano", "Tim Kraska", "Samuel Madden", "Ju Fan"], "title": "PBench: Workload Synthesizer with Real Statistics for Cloud Analytics Benchmarking", "categories": ["cs.DB"], "comment": null, "summary": "Cloud service providers commonly use standard benchmarks like TPC-H and\nTPC-DS to evaluate and optimize cloud data analytics systems. However, these\nbenchmarks rely on fixed query patterns and fail to capture the real execution\nstatistics of production cloud workloads. Although some cloud database vendors\nhave recently released real workload traces, these traces alone do not qualify\nas benchmarks, as they typically lack essential components like the original\nSQL queries and their underlying databases. To overcome this limitation, this\npaper introduces a new problem of workload synthesis with real statistics,\nwhich aims to generate synthetic workloads that closely approximate real\nexecution statistics, including key performance metrics and operator\ndistributions, in real cloud workloads. To address this problem, we propose\nPBench, a novel workload synthesizer that constructs synthetic workloads by\njudiciously selecting and combining workload components (i.e., queries and\ndatabases) from existing benchmarks. This paper studies the key challenges in\nPBench. First, we address the challenge of balancing performance metrics and\noperator distributions by introducing a multi-objective optimization-based\ncomponent selection method. Second, to capture the temporal dynamics of real\nworkloads, we design a timestamp assignment method that progressively refines\nworkload timestamps. Third, to handle the disparity between the original\nworkload and the candidate workload, we propose a component augmentation\napproach that leverages large language models (LLMs) to generate additional\nworkload components while maintaining statistical fidelity. We evaluate PBench\non real cloud workload traces, demonstrating that it reduces approximation\nerror by up to 6x compared to state-of-the-art methods.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16616", "pdf": "https://arxiv.org/pdf/2506.16616", "abs": "https://arxiv.org/abs/2506.16616", "authors": ["Soroush Omidvartehrani", "Davood Rafiei"], "title": "LDI: Localized Data Imputation", "categories": ["cs.DB"], "comment": null, "summary": "Missing values are a common challenge in real-world tabular data and can\nsignificantly impair downstream analysis. While Large Language Models (LLMs)\nhave recently shown promise in data imputation, existing methods often rely on\nbroad, unfiltered prompts that compromise accuracy, scalability, and\nexplainability. We introduce LDI (Localized Data Imputation), a novel framework\nthat improves both the accuracy and transparency of LLM-based imputation by\nselecting a compact, contextually relevant subset of attributes and tuples for\neach missing value. This localized prompting reduces noise, enables\ntraceability by revealing which data influenced each prediction, and is\neffective across both hosted LLMs and lightweight local models. Our extensive\nexperiments on four real-world datasets show that LDI outperforms\nstate-of-the-art methods, achieving up to 8% higher accuracy when using hosted\nLLMs. The gains are more substantial with lightweight local models, reaching\nnearly 17% and 97% accuracy on some datasets when using 3 and 10 examples,\nrespectively. In addition to higher accuracy, LDI offers improved\ninterpretability and robustness to data inconsistencies, making it well-suited\nfor high-stakes and privacy-sensitive applications.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16923", "pdf": "https://arxiv.org/pdf/2506.16923", "abs": "https://arxiv.org/abs/2506.16923", "authors": ["Omer Abramovich", "Daniel Deutch", "Nave Frost", "Ahmet Kara", "Dan Olteanu"], "title": "Advancing Fact Attribution for Query Answering: Aggregate Queries and Novel Algorithms", "categories": ["cs.DB"], "comment": null, "summary": "In this paper, we introduce a novel approach to computing the contribution of\ninput tuples to the result of the query, quantified by the Banzhaf and Shapley\nvalues. In contrast to prior algorithmic work that focuses on\nSelect-Project-Join-Union queries, ours is the first practical approach for\nqueries with aggregates. It relies on two novel optimizations that are\nessential for its practicality and significantly improve the runtime\nperformance already for queries without aggregates. The first optimization\nexploits the observation that many input tuples have the same contribution to\nthe query result, so it is enough to compute the contribution of one of them.\nThe second optimization uses the gradient of the query lineage to compute the\ncontributions of all tuples with the same complexity as for one of them.\nExperiments with a million instances over 3 databases show that our approach\nachieves up to 3 orders of magnitude runtime improvements over the\nstate-of-the-art for queries without aggregates, and that it is practical for\naggregate queries.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16976", "pdf": "https://arxiv.org/pdf/2506.16976", "abs": "https://arxiv.org/abs/2506.16976", "authors": ["Arthur Bernhardt", "Sajjad Tamimi", "Florian Stock", "Andreas Koch", "Ilia Petrov"], "title": "PUL: Pre-load in Software for Caches Wouldn't Always Play Along", "categories": ["cs.DB"], "comment": null, "summary": "Memory latencies and bandwidth are major factors, limiting system performance\nand scalability. Modern CPUs aim at hiding latencies by employing large caches,\nout-of-order execution, or complex hardware prefetchers. However,\nsoftware-based prefetching exhibits higher efficiency, improving with newer CPU\ngenerations.\n  In this paper we investigate software-based, post-Moore systems that offload\noperations to intelligent memories. We show that software-based prefetching has\neven higher potential in near-data processing settings by maximizing compute\nutilization through compute/IO interleaving.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16015", "pdf": "https://arxiv.org/pdf/2506.16015", "abs": "https://arxiv.org/abs/2506.16015", "authors": ["Craig S. Wright"], "title": "Bayesian Epistemology with Weighted Authority: A Formal Architecture for Truth-Promoting Autonomous Scientific Reasoning", "categories": ["cs.AI", "cs.CL", "cs.DB", "cs.LO", "math.LO", "68T27, 03B70, 68P20", "I.2.3; F.4.1; H.2.8"], "comment": "91 pages, 0 figures, includes mathematical appendix and formal\n  proofs. Designed as a foundational submission for a modular autonomous\n  epistemic reasoning system. Suitable for logic in computer science, AI\n  epistemology, and scientific informatics", "summary": "The exponential expansion of scientific literature has surpassed the\nepistemic processing capabilities of both human experts and current artificial\nintelligence systems. This paper introduces Bayesian Epistemology with Weighted\nAuthority (BEWA), a formally structured architecture that operationalises\nbelief as a dynamic, probabilistically coherent function over structured\nscientific claims. Each claim is contextualised, author-attributed, and\nevaluated through a system of replication scores, citation weighting, and\ntemporal decay. Belief updates are performed via evidence-conditioned Bayesian\ninference, contradiction processing, and epistemic decay mechanisms. The\narchitecture supports graph-based claim propagation, authorial credibility\nmodelling, cryptographic anchoring, and zero-knowledge audit verification. By\nformalising scientific reasoning into a computationally verifiable epistemic\nnetwork, BEWA advances the foundation for machine reasoning systems that\npromote truth utility, rational belief convergence, and audit-resilient\nintegrity across dynamic scientific domains.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16051", "pdf": "https://arxiv.org/pdf/2506.16051", "abs": "https://arxiv.org/abs/2506.16051", "authors": ["Zhiwei Li", "Carl Kesselman", "Tran Huy Nguyen", "Benjamin Yixing Xu", "Kyle Bolo", "Kimberley Yu"], "title": "From Data to Decision: Data-Centric Infrastructure for Reproducible ML in Collaborative eScience", "categories": ["cs.LG", "cs.DB", "cs.DL", "cs.HC"], "comment": null, "summary": "Reproducibility remains a central challenge in machine learning (ML),\nespecially in collaborative eScience projects where teams iterate over data,\nfeatures, and models. Current ML workflows are often dynamic yet fragmented,\nrelying on informal data sharing, ad hoc scripts, and loosely connected tools.\nThis fragmentation impedes transparency, reproducibility, and the adaptability\nof experiments over time. This paper introduces a data-centric framework for\nlifecycle-aware reproducibility, centered around six structured artifacts:\nDataset, Feature, Workflow, Execution, Asset, and Controlled Vocabulary. These\nartifacts formalize the relationships between data, code, and decisions,\nenabling ML experiments to be versioned, interpretable, and traceable over\ntime. The approach is demonstrated through a clinical ML use case of glaucoma\ndetection, illustrating how the system supports iterative exploration, improves\nreproducibility, and preserves the provenance of collaborative decisions across\nthe ML lifecycle.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16444", "pdf": "https://arxiv.org/pdf/2506.16444", "abs": "https://arxiv.org/abs/2506.16444", "authors": ["Kangqi Chen", "Andreas Kosmas Kakolyris", "Rakesh Nadig", "Manos Frouzakis", "Nika Mansouri Ghiasi", "Yu Liang", "Haiyu Mao", "Jisung Park", "Mohammad Sadrosadati", "Onur Mutlu"], "title": "REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing", "categories": ["cs.CL", "cs.AR", "cs.DB", "H.3.3; I.2.7"], "comment": "Extended version of our publication at the 52nd International\n  Symposium on Computer Architecture (ISCA-52), 2025", "summary": "Large Language Models (LLMs) face an inherent challenge: their knowledge is\nconfined to the data that they have been trained on. To overcome this issue,\nRetrieval-Augmented Generation (RAG) complements the static training-derived\nknowledge of LLMs with an external knowledge repository. RAG consists of three\nstages: indexing, retrieval, and generation. The retrieval stage of RAG becomes\na significant bottleneck in inference pipelines. In this stage, a user query is\nmapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS)\nalgorithm searches for similar vectors in the database to identify relevant\nitems. Due to the large database sizes, ANNS incurs significant data movement\noverheads between the host and the storage system. To alleviate these\noverheads, prior works propose In-Storage Processing (ISP) techniques that\naccelerate ANNS by performing computations inside storage. However, existing\nworks that leverage ISP for ANNS (i) employ algorithms that are not tailored to\nISP systems, (ii) do not accelerate data retrieval operations for data selected\nby ANNS, and (iii) introduce significant hardware modifications, limiting\nperformance and hindering their adoption. We propose REIS, the first ISP system\ntailored for RAG that addresses these limitations with three key mechanisms.\nFirst, REIS employs a database layout that links database embedding vectors to\ntheir associated documents, enabling efficient retrieval. Second, it enables\nefficient ANNS by introducing an ISP-tailored data placement technique that\ndistributes embeddings across the planes of the storage system and employs a\nlightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that\nuses the existing computational resources inside the storage system. Compared\nto a server-grade system, REIS improves the performance (energy efficiency) of\nretrieval by an average of 13x (55x).", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16654", "pdf": "https://arxiv.org/pdf/2506.16654", "abs": "https://arxiv.org/abs/2506.16654", "authors": ["Vijay Prakash Dwivedi", "Charilaos Kanatsoulis", "Shenyang Huang", "Jure Leskovec"], "title": "Relational Deep Learning: Challenges, Foundations and Next-Generation Architectures", "categories": ["cs.LG", "cs.AI", "cs.DB"], "comment": null, "summary": "Graph machine learning has led to a significant increase in the capabilities\nof models that learn on arbitrary graph-structured data and has been applied to\nmolecules, social networks, recommendation systems, and transportation, among\nother domains. Data in multi-tabular relational databases can also be\nconstructed as 'relational entity graphs' for Relational Deep Learning (RDL) -\na new blueprint that enables end-to-end representation learning without\ntraditional feature engineering. Compared to arbitrary graph-structured data,\nrelational entity graphs have key properties: (i) their structure is defined by\nprimary-foreign key relationships between entities in different tables, (ii)\nthe structural connectivity is a function of the relational schema defining a\ndatabase, and (iii) the graph connectivity is temporal and heterogeneous in\nnature. In this paper, we provide a comprehensive review of RDL by first\nintroducing the representation of relational databases as relational entity\ngraphs, and then reviewing public benchmark datasets that have been used to\ndevelop and evaluate recent GNN-based RDL models. We discuss key challenges\nincluding large-scale multi-table integration and the complexities of modeling\ntemporal dynamics and heterogeneous data, while also surveying foundational\nneural network methods and recent architectural advances specialized for\nrelational entity graphs. Finally, we explore opportunities to unify these\ndistinct modeling challenges, highlighting how RDL converges multiple\nsub-fields in graph machine learning towards the design of foundation models\nthat can transform the processing of relational data.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2506.16087", "pdf": "https://arxiv.org/pdf/2506.16087", "abs": "https://arxiv.org/abs/2506.16087", "authors": ["Tom Jeleniewski", "Hamied Nabizada", "Jonathan Reif", "Felix Gehlhoff", "Alexander Fay"], "title": "Consistency Verification in Ontology-Based Process Models with Parameter Interdependencies", "categories": ["cs.AI", "cs.DB"], "comment": "This paper is accepted at IEEE ETFA 2025 and will be published in the\n  conference proceedings", "summary": "The formalization of process knowledge using ontologies enables consistent\nmodeling of parameter interdependencies in manufacturing. These\ninterdependencies are typically represented as mathematical expressions that\ndefine relations between process parameters, supporting tasks such as\ncalculation, validation, and simulation. To support cross-context application\nand knowledge reuse, such expressions are often defined in a generic form and\napplied across multiple process contexts. This highlights the necessity of a\nconsistent and semantically coherent model to ensure the correctness of data\nretrieval and interpretation. Consequently, dedicated mechanisms are required\nto address key challenges such as selecting context-relevant data, ensuring\nunit compatibility between variables and data elements, and verifying the\ncompleteness of input data required for evaluating mathematical expressions.\nThis paper presents a set of verification mechanisms for a previously developed\nontology-based process model that integrates standardized process semantics,\ndata element definitions, and formal mathematical constructs. The approach\nincludes (i) SPARQL-based filtering to retrieve process-relevant data, (ii) a\nunit consistency check based on expected-unit annotations and semantic\nclassification, and (iii) a data completeness check to validate the\nevaluability of interdependencies. The applicability of the approach is\ndemonstrated with a use case from Resin Transfer Molding (RTM), supporting the\ndevelopment of machine-interpretable and verifiable engineering models.", "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
