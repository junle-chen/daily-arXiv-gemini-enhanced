{"id": "2512.04441", "pdf": "https://arxiv.org/pdf/2512.04441", "abs": "https://arxiv.org/abs/2512.04441", "authors": ["Bin Suna", "Yaoguang Caob", "Yan Wanga", "Rui Wanga", "Jiachen Shanga", "Xiejie Fenga", "Jiayi Lu", "Jia Shi", "Shichun Yang", "Xiaoyu Yane", "Ziying Song"], "title": "MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of \"context simulation - candidate generation - multi-objective trade-off\". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned \"what-if\" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.", "relevance_analysis": {"relevance_score": 0.95, "explanation": "This paper is highly relevant as it directly addresses trajectory planning in end-to-end autonomous driving, incorporating both trajectory generation and selection. It also explicitly utilizes a Vision-Language Model (VLM) for multi-objective evaluation, bridging trajectory prediction and large language models.", "keywords": ["trajectory planning", "trajectory generation", "trajectory selection", "autonomous driving", "Vision-Language Model", "VLM", "World Model", "E2E-AD"]}}
{"id": "2512.04221", "pdf": "https://arxiv.org/pdf/2512.04221", "abs": "https://arxiv.org/abs/2512.04221", "authors": ["Xiangyu Bai", "He Liang", "Bishoy Galoaa", "Utsav Nandi", "Shayda Moezzi", "Yuhang He", "Sarah Ostadabbas"], "title": "MoReGen: Multi-Agent Motion-Reasoning Engine for Code-based Text-to-Video Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "While text-to-video (T2V) generation has achieved remarkable progress in photorealism, generating intent-aligned videos that faithfully obey physics principles remains a core challenge. In this work, we systematically study Newtonian motion-controlled text-to-video generation and evaluation, emphasizing physical precision and motion coherence. We introduce MoReGen, a motion-aware, physics-grounded T2V framework that integrates multi-agent LLMs, physics simulators, and renderers to generate reproducible, physically accurate videos from text prompts in the code domain. To quantitatively assess physical validity, we propose object-trajectory correspondence as a direct evaluation metric and present MoReSet, a benchmark of 1,275 human-annotated videos spanning nine classes of Newtonian phenomena with scene descriptions, spatiotemporal relations, and ground-truth trajectories. Using MoReSet, we conduct experiments on existing T2V models, evaluating their physical validity through both our MoRe metrics and existing physics-based evaluators. Our results reveal that state-of-the-art models struggle to maintain physical validity, while MoReGen establishes a principled direction toward physically coherent video synthesis.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8e\u6587\u672c\u5230\u89c6\u9891\u7684\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u5e76\u4e14\u7279\u522b\u5173\u6ce8\u7269\u7406\u89c4\u5f8b\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u3002\u6458\u8981\u4e2d\u660e\u786e\u63d0\u5230\u4e86\u201cobject-trajectory correspondence\u201d \u548c \u201cground-truth trajectories\u201d\uff0c\u8868\u660e\u8be5\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u4e3b\u8981\u76ee\u6807\u662f\u89c6\u9891\u751f\u6210\uff0c\u4f46\u5b83\u5229\u7528\u8f68\u8ff9\u9884\u6d4b\u4f5c\u4e3a\u5b9e\u73b0\u7269\u7406\u5408\u7406\u6027\u7684\u624b\u6bb5\uff0c\u5e76\u4f7f\u7528\u8f68\u8ff9\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "Large Language Models", "LLMs", "motion", "object-trajectory correspondence", "ground-truth trajectories", "text-to-video generation"]}}
{"id": "2512.04459", "pdf": "https://arxiv.org/pdf/2512.04459", "abs": "https://arxiv.org/abs/2512.04459", "authors": ["Yingzi Ma", "Yulong Cao", "Wenhao Ding", "Shuibai Zhang", "Yan Wang", "Boris Ivanovic", "Ming Jiang", "Marco Pavone", "Chaowei Xiao"], "title": "dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "\u8be5\u8bba\u6587\u6807\u9898\u548c\u6458\u8981\u90fd\u660e\u786e\u8868\u660e\u5176\u7814\u7a76\u5185\u5bb9\u4e0e\u81ea\u52a8\u9a7e\u9a76\u76f8\u5173\uff0c\u800c\u81ea\u52a8\u9a7e\u9a76\u6d89\u53ca\u5230\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u3002\u540c\u65f6\uff0c\u8bba\u6587\u4e5f\u660e\u786e\u4f7f\u7528\u4e86Vision-Language Models (VLMs)\uff0c\u8fd9\u5c5e\u4e8e\u5927\u6a21\u578b\u8303\u7574\u3002\u8bba\u6587\u4fa7\u91cd\u4e8e\u901a\u8fc7\u6539\u8fdbVLM\u6765\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c4\u5212\u6027\u80fd\uff0c\u56e0\u6b64\u5177\u6709\u8f83\u9ad8\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "Large Language Models", "vision-language models", "VLMs", "autonomous driving", "planning"]}}
{"id": "2512.04532", "pdf": "https://arxiv.org/pdf/2512.04532", "abs": "https://arxiv.org/abs/2512.04532", "authors": ["Yu-Wei Zhan", "Xin Wang", "Hong Chen", "Tongtong Feng", "Wei Feng", "Ren Wang", "Guangyao Li", "Qing Li", "Wenwu Zhu"], "title": "PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper is highly relevant as it combines video understanding with large language models and physics-guided motion modeling. While not directly focused on trajectory prediction, the core of the work involves modeling object motion and physical dynamics, which is closely related. It uses a Video LLM and integrates motion-aware representations for improved reasoning.", "keywords": ["Video Large Language Models", "Video LLMs", "LLM", "motion modeling", "physical dynamics", "motion-aware representations", "Neural Ordinary Differential Equation", "Neural ODE", "video understanding"]}}
{"id": "2512.04381", "pdf": "https://arxiv.org/pdf/2512.04381", "abs": "https://arxiv.org/abs/2512.04381", "authors": ["Chengyang He", "Ge Sun", "Yue Bai", "Junkai Lu", "Jiadong Zhao", "Guillaume Sartoretti"], "title": "FALCON: Actively Decoupled Visuomotor Policies for Loco-Manipulation with Foundation-Model-Based Coordination", "categories": ["cs.RO"], "comment": null, "summary": "We present FoundAtion-model-guided decoupled LoCO-maNipulation visuomotor policies (FALCON), a framework for loco-manipulation that combines modular diffusion policies with a vision-language foundation model as the coordinator. Our approach explicitly decouples locomotion and manipulation into two specialized visuomotor policies, allowing each subsystem to rely on its own observations. This mitigates the performance degradation that arise when a single policy is forced to fuse heterogeneous, potentially mismatched observations from locomotion and manipulation. Our key innovation lies in restoring coordination between these two independent policies through a vision-language foundation model, which encodes global observations and language instructions into a shared latent embedding conditioning both diffusion policies. On top of this backbone, we introduce a phase-progress head that uses textual descriptions of task stages to infer discrete phase and continuous progress estimates without manual phase labels. To further structure the latent space, we incorporate a coordination-aware contrastive loss that explicitly encodes cross-subsystem compatibility between arm and base actions. We evaluate FALCON on two challenging loco-manipulation tasks requiring navigation, precise end-effector placement, and tight base-arm coordination. Results show that it surpasses centralized and decentralized baselines while exhibiting improved robustness and generalization to out-of-distribution scenarios.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u7ed3\u5408\u4e86loco-manipulation\uff08\u67d0\u79cd\u8f68\u8ff9\u63a7\u5236\uff09\u4e0eFoundation Model (\u5927\u578b\u6a21\u578b\u7684\u4e00\u79cd)\uff0c\u4f7f\u7528\u5927\u578b\u6a21\u578b\u4f5c\u4e3a\u534f\u8c03\u5668\u6765\u6307\u5bfc\u8fd0\u52a8\u7b56\u7565\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46loco-manipulation\u672c\u8eab\u6d89\u53ca\u5230\u8fd0\u52a8\u8f68\u8ff9\u7684\u89c4\u5212\u548c\u63a7\u5236\uff0c\u4e14\u4f7f\u7528\u4e86\u5927\u578b\u6a21\u578b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["loco-manipulation", "Foundation Model", "diffusion policies", "vision-language foundation model"]}}
{"id": "2512.04513", "pdf": "https://arxiv.org/pdf/2512.04513", "abs": "https://arxiv.org/abs/2512.04513", "authors": ["Yu-Wei Zhan", "Xin Wang", "Pengzhe Mao", "Tongtong Feng", "Ren Wang", "Wenwu Zhu"], "title": "BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models", "categories": ["cs.AI"], "comment": null, "summary": "Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u4e16\u754c\u6a21\u578b\u7684\u53cc\u5411\u8026\u5408\uff0c\u4ee5\u6784\u5efa\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u4e16\u754c\u6a21\u578b\u901a\u5e38\u6d89\u53ca\u5bf9\u73af\u5883\u52a8\u6001\u7684\u9884\u6d4b\uff0c\u8fd9\u4e0e\u8f68\u8ff9\u9884\u6d4b\u7684\u6982\u5ff5\u76f8\u5173\u3002\u6b64\u5916\uff0c\u8bba\u6587\u660e\u786e\u63d0\u5230\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u3002\u56e0\u6b64\uff0c\u8bba\u6587\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["Multimodal Large Language Models", "MLLMs", "World Models", "WMs", "dynamic prediction"]}}
{"id": "2512.05094", "pdf": "https://arxiv.org/pdf/2512.05094", "abs": "https://arxiv.org/abs/2512.05094", "authors": ["James Ni", "Zekai Wang", "Wei Lin", "Amir Bar", "Yann LeCun", "Trevor Darrell", "Jitendra Malik", "Roei Herzig"], "title": "From Generated Human Videos to Physically Plausible Robot Trajectories", "categories": ["cs.RO", "cs.CV"], "comment": "For project website, see https://genmimic.github.io", "summary": "Video generation models are rapidly improving in their ability to synthesize human actions in novel contexts, holding the potential to serve as high-level planners for contextual robot control. To realize this potential, a key research question remains open: how can a humanoid execute the human actions from generated videos in a zero-shot manner? This challenge arises because generated videos are often noisy and exhibit morphological distortions that make direct imitation difficult compared to real video. To address this, we introduce a two-stage pipeline. First, we lift video pixels into a 4D human representation and then retarget to the humanoid morphology. Second, we propose GenMimic-a physics-aware reinforcement learning policy conditioned on 3D keypoints, and trained with symmetry regularization and keypoint-weighted tracking rewards. As a result, GenMimic can mimic human actions from noisy, generated videos. We curate GenMimicBench, a synthetic human-motion dataset generated using two video generation models across a spectrum of actions and contexts, establishing a benchmark for assessing zero-shot generalization and policy robustness. Extensive experiments demonstrate improvements over strong baselines in simulation and confirm coherent, physically stable motion tracking on a Unitree G1 humanoid robot without fine-tuning. This work offers a promising path to realizing the potential of video generation models as high-level policies for robot control.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\uff08\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e00\u79cd\u5927\u6a21\u578b\uff0c\u867d\u7136\u6458\u8981\u4e2d\u6ca1\u6709\u660e\u786e\u63d0\u53caLLM\uff09\u751f\u6210\u7684\u89c6\u9891\u6765\u63a7\u5236\u673a\u5668\u4eba\u8fd0\u52a8\u8f68\u8ff9\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u63a7\u5236\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u52a8\u4f5c\uff0c\u8fdb\u800c\u5f71\u54cd\u8f68\u8ff9\u89c4\u5212\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "video generation models", "robot trajectories", "human actions", "reinforcement learning"]}}
{"id": "2512.04213", "pdf": "https://arxiv.org/pdf/2512.04213", "abs": "https://arxiv.org/abs/2512.04213", "authors": ["Bishoy Galoaa", "Xiangyu Bai", "Shayda Moezzi", "Utsav Nandi", "Sai Siddhartha Vivek Dhir Rangoju", "Somaieh Amraee", "Sarah Ostadabbas"], "title": "Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on multi-camera point tracking using a transformer-based architecture. While it uses transformers (related to large models, but not directly LLMs), the core problem is point tracking, which is a form of trajectory estimation. The connection to trajectory prediction is present but not explicit, and there is no direct involvement of Large Language Models.", "keywords": ["point tracking", "transformers", "multi-camera", "attention mechanism", "geometric constraints"]}}
{"id": "2512.04279", "pdf": "https://arxiv.org/pdf/2512.04279", "abs": "https://arxiv.org/abs/2512.04279", "authors": ["Feeza Khan Khanzada", "Jaerock Kwon"], "title": "Driving Beyond Privilege: Distilling Dense-Reward Knowledge into Sparse-Reward Policies", "categories": ["cs.RO"], "comment": null, "summary": "We study how to exploit dense simulator-defined rewards in vision-based autonomous driving without inheriting their misalignment with deployment metrics. In realistic simulators such as CARLA, privileged state (e.g., lane geometry, infractions, time-to-collision) can be converted into dense rewards that stabilize and accelerate model-based reinforcement learning, but policies trained directly on these signals often overfit and fail to generalize when evaluated on sparse objectives such as route completion and collision-free overtaking. We propose reward-privileged world model distillation, a two-stage framework in which a teacher DreamerV3-style agent is first trained with a dense privileged reward, and only its latent dynamics are distilled into a student trained solely on sparse task rewards. Teacher and student share the same observation space (semantic bird's-eye-view images); privileged information enters only through the teacher's reward, and the student does not imitate the teacher's actions or value estimates. Instead, the student's world model is regularized to match the teacher's latent dynamics while its policy is learned from scratch on sparse success/failure signals. In CARLA lane-following and overtaking benchmarks, sparse-reward students outperform both dense-reward teachers and sparse-from-scratch baselines. On unseen lane-following routes, reward-privileged distillation improves success by about 23 percent relative to the dense teacher while maintaining comparable or better safety. On overtaking, students retain near-perfect performance on training routes and achieve up to a 27x improvement in success on unseen routes, with improved lane keeping. These results show that dense rewards can be leveraged to learn richer dynamics models while keeping the deployed policy optimized strictly for sparse, deployment-aligned objectives.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u7279\u522b\u662f\u5982\u4f55\u5229\u7528\u5bc6\u96c6\u7684\u5956\u52b1\u51fd\u6570\u6765\u8bad\u7ec3\u7b56\u7565\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5b83\u4e0e\u8f68\u8ff9\u9884\u6d4b\uff08\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\uff09\u5bc6\u5207\u76f8\u5173\uff0c\u5e76\u4e14\u4f7f\u7528\u4e86\u4e16\u754c\u6a21\u578b\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u67d0\u79cd\u5f62\u5f0f\u7684\u6a21\u578b\u5b66\u4e60\u3002\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["trajectory prediction", "autonomous driving", "reinforcement learning", "world model", "driving"]}}
{"id": "2512.04282", "pdf": "https://arxiv.org/pdf/2512.04282", "abs": "https://arxiv.org/abs/2512.04282", "authors": ["Tasmiah Haque", "Srinjoy Das"], "title": "Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u89c6\u9891\u8fd0\u52a8\u4f20\u9012\u4e2d\u7684\u672a\u6765\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u901a\u8fc7\u6539\u8fdbGRU-Normalizing Flow\u6a21\u578b\u6765\u63d0\u9ad8\u9884\u6d4b\u7684\u591a\u6837\u6027\u548c\u51c6\u786e\u6027\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u65f6\u5e8f\u6570\u636e\u7684\u751f\u6210\u5f0f\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u5bc6\u5207\u76f8\u5173\u3002\u4f7f\u7528\u4e86GRU\u548cNormalizing Flow\u7b49\u6280\u672f\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u7684\u4e00\u79cd\u65b9\u6cd5\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "GRU", "Normalizing Flow", "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b", "\u52a8\u4f5c\u9884\u6d4b", "\u8fd0\u52a8\u4f20\u9012", "\u89c6\u9891\u8fd0\u52a8"]}}
{"id": "2512.04315", "pdf": "https://arxiv.org/pdf/2512.04315", "abs": "https://arxiv.org/abs/2512.04315", "authors": ["Yonghan Lee", "Tsung-Wei Huang", "Shiv Gehlot", "Jaehoon Choi", "Guan-Ming Su", "Dinesh Manocha"], "title": "SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on 4D Gaussian Splatting for dynamic scene reconstruction from unsynchronized videos, which involves tracking and aligning motion trajectories across multiple views. While it doesn't directly use or discuss large language models, it heavily relies on 4D track representation and motion alignment, which are related to trajectory prediction and understanding dynamic scenes. Therefore, the relevance is moderate.", "keywords": ["trajectory", "motion", "4D Gaussian Splatting", "motion alignment", "dynamic scene"]}}
{"id": "2512.04917", "pdf": "https://arxiv.org/pdf/2512.04917", "abs": "https://arxiv.org/abs/2512.04917", "authors": ["Matteo Masoni", "Vincenzo Palermo", "Marco Gabiccini", "Martino Gulisano", "Giorgio Previati", "Massimiliano Gobbi", "Francesco Comolli", "Gianpiero Mastinu", "Massimo Guiggiani"], "title": "On Disturbance-Aware Minimum-Time Trajectory Planning: Evidence from Tests on a Dynamic Driving Simulator", "categories": ["cs.RO", "eess.SY"], "comment": "18 pages, 11 figures, 5 tables", "summary": "This work investigates how disturbance-aware, robustness-embedded reference trajectories translate into driving performance when executed by professional drivers in a dynamic simulator. Three planned reference trajectories are compared against a free-driving baseline (NOREF) to assess trade-offs between lap time (LT) and steering effort (SE): NOM, the nominal time-optimal trajectory; TLC, a track-limit-robust trajectory obtained by tightening margins to the track edges; and FLC, a friction-limit-robust trajectory obtained by tightening against axle and tire saturation. All trajectories share the same minimum lap-time objective with a small steering-smoothness regularizer and are evaluated by two professional drivers using a high-performance car on a virtual track. The trajectories derive from a disturbance-aware minimum-lap-time framework recently proposed by the authors, where worst-case disturbance growth is propagated over a finite horizon and used to tighten tire-friction and track-limit constraints, preserving performance while providing probabilistic safety margins. LT and SE are used as performance indicators, while RMS lateral deviation, speed error, and drift angle characterize driving style. Results show a Pareto-like LT-SE trade-off: NOM yields the shortest LT but highest SE; TLC minimizes SE at the cost of longer LT; FLC lies near the efficient frontier, substantially reducing SE relative to NOM with only a small LT increase. Removing trajectory guidance (NOREF) increases both LT and SE, confirming that reference trajectories improve pace and control efficiency. Overall, the findings highlight reference-based and disturbance-aware planning, especially FLC, as effective tools for training and for achieving fast yet stable trajectories.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on trajectory planning for a driving simulator. While it doesn't directly involve large language models, it's relevant to trajectory prediction due to its focus on planning optimal trajectories and considering disturbances. The core of the paper revolves around generating and evaluating different trajectory planning strategies. However, the absence of any LLM-related concepts lowers the score.", "keywords": ["trajectory planning", "disturbance-aware", "minimum-time trajectory", "driving simulator", "reference trajectories"]}}
{"id": "2512.05107", "pdf": "https://arxiv.org/pdf/2512.05107", "abs": "https://arxiv.org/abs/2512.05107", "authors": ["Feng Xu", "Guangyao Zhai", "Xin Kong", "Tingzhong Fu", "Daniel F. N. Gordon", "Xueli An", "Benjamin Busam"], "title": "STARE-VLA: Progressive Stage-Aware Reinforcement for Fine-Tuning Vision-Language-Action Models", "categories": ["cs.RO"], "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models, powered by large language models and reinforcement learning-based fine-tuning, have shown remarkable progress in robotic manipulation. Existing methods often treat long-horizon actions as linguistic sequences and apply trajectory-level optimization methods such as Trajectory-wise Preference Optimization (TPO) or Proximal Policy Optimization (PPO), leading to coarse credit assignment and unstable training. However, unlike language, where a unified semantic meaning is preserved despite flexible sentence order, action trajectories progress through causally chained stages with different learning difficulties. This motivates progressive stage optimization. Thereby, we present Stage-Aware Reinforcement (STARE), a module that decomposes a long-horizon action trajectory into semantically meaningful stages and provides dense, interpretable, and stage-aligned reinforcement signals. Integrating STARE into TPO and PPO, we yield Stage-Aware TPO (STA-TPO) and Stage-Aware PPO (STA-PPO) for offline stage-wise preference and online intra-stage interaction, respectively. Further building on supervised fine-tuning as initialization, we propose the Imitation -> Preference -> Interaction (IPI), a serial fine-tuning pipeline for improving action accuracy in VLA models. Experiments on SimplerEnv and ManiSkill3 demonstrate substantial gains, achieving state-of-the-art success rates of 98.0 percent on SimplerEnv and 96.4 percent on ManiSkill3 tasks.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper discusses Vision-Language-Action (VLA) models, which are powered by large language models and reinforcement learning for robotic manipulation. While it doesn't directly focus on trajectory *prediction*, it involves optimizing action trajectories, which is related to planning and control. The use of large language models is also a key aspect. The connection to trajectory prediction is weaker than if it explicitly predicted trajectories, but the action trajectory optimization and LLM usage warrant a moderate relevance score.", "keywords": ["large language models", "action trajectories", "reinforcement learning", "Vision-Language-Action models", "robotic manipulation"]}}
{"id": "2512.04797", "pdf": "https://arxiv.org/pdf/2512.04797", "abs": "https://arxiv.org/abs/2512.04797", "authors": ["SIMA team", "Adrian Bolton", "Alexander Lerchner", "Alexandra Cordell", "Alexandre Moufarek", "Andrew Bolt", "Andrew Lampinen", "Anna Mitenkova", "Arne Olav Hallingstad", "Bojan Vujatovic", "Bonnie Li", "Cong Lu", "Daan Wierstra", "Daniel P. Sawyer", "Daniel Slater", "David Reichert", "Davide Vercelli", "Demis Hassabis", "Drew A. Hudson", "Duncan Williams", "Ed Hirst", "Fabio Pardo", "Felix Hill", "Frederic Besse", "Hannah Openshaw", "Harris Chan", "Hubert Soyer", "Jane X. Wang", "Jeff Clune", "John Agapiou", "John Reid", "Joseph Marino", "Junkyung Kim", "Karol Gregor", "Kaustubh Sridhar", "Kay McKinney", "Laura Kampis", "Lei M. Zhang", "Loic Matthey", "Luyu Wang", "Maria Abi Raad", "Maria Loks-Thompson", "Martin Engelcke", "Matija Kecman", "Matthew Jackson", "Maxime Gazeau", "Ollie Purkiss", "Oscar Knagg", "Peter Stys", "Piermaria Mendolicchio", "Raia Hadsell", "Rosemary Ke", "Ryan Faulkner", "Sarah Chakera", "Satinder Singh Baveja", "Shane Legg", "Sheleem Kashem", "Tayfun Terzi", "Thomas Keck", "Tim Harley", "Tim Scholtes", "Tyson Roberts", "Volodymyr Mnih", "Yulan Liu", "Zhengdong Wang", "Zoubin Ghahramani"], "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8fd9\u7bc7\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u5177\u8eab\u667a\u80fd\u4f53\u5728\u865a\u62df\u4e16\u754c\u4e2d\u7684\u884c\u4e3a\uff0c\u4f7f\u7528\u4e86Gemini\u5927\u6a21\u578b\u4f5c\u4e3a\u57fa\u7840\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u667a\u80fd\u4f53\u5728\u865a\u62df\u4e16\u754c\u4e2d\u7684\u884c\u52a8\u5fc5\u7136\u6d89\u53ca\u8def\u5f84\u89c4\u5212\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u8bba\u6587\u91cd\u70b9\u5728\u4e8e\u5927\u6a21\u578b\u5728\u667a\u80fd\u4f53\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u3002", "keywords": ["Large Language Models", "foundation models", "embodied agent", "virtual worlds", "Gemini", "reasoning", "interaction", "self-improvement"]}}
{"id": "2512.04425", "pdf": "https://arxiv.org/pdf/2512.04425", "abs": "https://arxiv.org/abs/2512.04425", "authors": ["Manar Alnaasan", "Md Selim Sarowar", "Sungho Kim"], "title": "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper utilizes large language models (LLMs) to provide explainable gait analysis for Parkinson's disease. While it doesn't directly address trajectory prediction, the gait analysis aspect is related to movement patterns, and the use of LLMs connects it to the large language model domain. The core focus is on disease recognition and explainability rather than predicting future trajectories.", "keywords": ["Large Language Models", "LLM", "gait analysis"]}}
{"id": "2512.04952", "pdf": "https://arxiv.org/pdf/2512.04952", "abs": "https://arxiv.org/abs/2512.04952", "authors": ["Yicheng Liu", "Shiduo Zhang", "Zibin Dong", "Baijun Ye", "Tianyuan Yuan", "Xiaopeng Yu", "Linqi Yin", "Chenhao Lu", "Junhao Shi", "Luca Jiang-Tao Yu", "Liangtao Zheng", "Tao Jiang", "Jingjing Gong", "Xipeng Qiu", "Hang Zhao"], "title": "FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on autoregressive vision-language-action (VLA) models for robotic manipulation. While it doesn't explicitly mention trajectory prediction or large language models by name, the action modeling aspect is related to trajectory prediction, especially in the context of robotic movement. The autoregressive nature and the use of vision and language models suggest a connection to the broader field of large models, even if it's not a direct application of LLMs as text generators. The 'action tokenization' and 'autoregressive decoding' components are relevant to predicting sequences of actions, which can be interpreted as trajectories.", "keywords": ["autoregressive", "vision-language-action models", "robotic manipulation", "action tokenization", "action modeling", "robot learning"]}}
{"id": "2512.04601", "pdf": "https://arxiv.org/pdf/2512.04601", "abs": "https://arxiv.org/abs/2512.04601", "authors": ["Joey Hong", "Kang Liu", "Zhan Ling", "Jiecao Chen", "Sergey Levine"], "title": "Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space", "categories": ["cs.LG", "cs.CL"], "comment": "22 pages, 4 figures", "summary": "Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on training LLM agents using an actor-critic method with natural language feedback. While it doesn't directly address trajectory prediction, the concept of 'long horizons' and 'action spaces' within an environment is somewhat related to planning and sequential decision-making, which are relevant in trajectory prediction. The primary focus is on LLMs and their interaction with environments, making it moderately relevant.", "keywords": ["Large Language Models", "LLM agents", "actor-critic", "policy gradient", "natural language", "long-horizon tasks"]}}
{"id": "2512.04487", "pdf": "https://arxiv.org/pdf/2512.04487", "abs": "https://arxiv.org/abs/2512.04487", "authors": ["Eunjong Lee", "Eunhee Kim", "Sanghoon Hong", "Eunho Jung", "Jihoon Kim"], "title": "Controllable Long-term Motion Generation with Extended Joint Targets", "categories": ["cs.CV"], "comment": "WACV 2026", "summary": "Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on controllable motion generation, which is related to trajectory prediction, particularly in the context of character animation. While it doesn't explicitly use Large Language Models, the use of a Transformer-based conditional VAE suggests a connection to modern sequence modeling techniques often associated with LLMs. The long-term motion generation aspect is also relevant to trajectory prediction.", "keywords": ["motion generation", "character animation", "Transformer", "long-term", "control", "trajectory prediction (implied)"]}}
{"id": "2512.04499", "pdf": "https://arxiv.org/pdf/2512.04499", "abs": "https://arxiv.org/abs/2512.04499", "authors": ["Yuduo Jin", "Brandon Haworth"], "title": "Back to Basics: Motion Representation Matters for Human Motion Generation Using Diffusion Model", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Diffusion models have emerged as a widely utilized and successful methodology in human motion synthesis. Task-oriented diffusion models have significantly advanced action-to-motion, text-to-motion, and audio-to-motion applications. In this paper, we investigate fundamental questions regarding motion representations and loss functions in a controlled study, and we enumerate the impacts of various decisions in the workflow of the generative motion diffusion model. To answer these questions, we conduct empirical studies based on a proxy motion diffusion model (MDM). We apply v loss as the prediction objective on MDM (vMDM), where v is the weighted sum of motion data and noise. We aim to enhance the understanding of latent data distributions and provide a foundation for improving the state of conditional motion diffusion models. First, we evaluate the six common motion representations in the literature and compare their performance in terms of quality and diversity metrics. Second, we compare the training time under various configurations to shed light on how to speed up the training process of motion diffusion models. Finally, we also conduct evaluation analysis on a large motion dataset. The results of our experiments indicate clear performance differences across motion representations in diverse datasets. Our results also demonstrate the impacts of distinct configurations on model training and suggest the importance and effectiveness of these decisions on the outcomes of motion diffusion models.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on human motion generation using diffusion models. While it doesn't explicitly mention trajectory prediction, motion generation is closely related, and diffusion models can be considered a type of large model. The paper explores motion representations and training techniques relevant to generative models, suggesting a moderate connection to the specified themes.", "keywords": ["diffusion models", "motion generation", "motion representation", "large model", "generative motion diffusion model"]}}
{"id": "2512.04871", "pdf": "https://arxiv.org/pdf/2512.04871", "abs": "https://arxiv.org/abs/2512.04871", "authors": ["Junjie Fan", "Hongye Zhao", "Linduo Wei", "Jiayu Rao", "Guijia Li", "Jiaxin Yuan", "Wenqi Xu", "Yong Qi"], "title": "STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Large Language Models (LLMs) for time series forecasting. While it doesn't directly address trajectory prediction, time series forecasting is a related field and the paper explicitly uses LLMs. The connection to trajectory prediction is weaker, but the use of LLMs makes it somewhat relevant.", "keywords": ["Large Language Models", "LLMs", "time series forecasting", "forecasting", "semantic abstraction"]}}
