# 每日 ArXiv 轨迹预测与大模型摘要速递: 2025-06-25

## 目录

- [cs.DB (10)](#cs-db)
- [cs.DL (1)](#cs-dl)
- [cs.DS (1)](#cs-ds)
- [机器学习 (Machine Learning) (2)](#cs-lg)

## cs.DB [cs.DB]
### [1] [DCMF: A Dynamic Context Monitoring and Caching Framework for Context Management Platforms](https://arxiv.org/abs/2506.17226)
*Ashish Manchanda, Prem Prakash Jayaraman, Abhik Banerjee, Kaneez Fizza, Arkady Zaslavsky*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: The rise of context-aware IoT applications has increased the demand for timely and accurate context information. Context is derived by aggregating and inferring from dynamic IoT data, making it highly volatile and posing challenges in maintaining freshness and real-time accessibility. Caching is a potential solution, but traditional policies struggle with the transient nature of context in IoT (e.g., ensuring real-time access for frequent queries or handling fast-changing data). To address this, we propose the Dynamic Context Monitoring Framework (DCMF) to enhance context caching in Context Management Platforms (CMPs) by dynamically evaluating and managing context. DCMF comprises two core components: the Context Evaluation Engine (CEE) and the Context Management Module (CMM). The CEE calculates the Probability of Access (PoA) using parameters such as Quality of Service (QoS), Quality of Context (QoC), Cost of Context (CoC), timeliness, and Service Level Agreements (SLAs), assigning weights to assess access likelihood. Based on this, the CMM applies a hybrid Dempster-Shafer approach to manage Context Freshness (CF), updating belief levels and confidence scores to determine whether to cache, evict, or refresh context items. We implemented DCMF in a Context-as-a-Service (CoaaS) platform and evaluated it using real-world smart city data, particularly traffic and roadwork scenarios. Results show DCMF achieves a 12.5% higher cache hit rate and reduces cache expiry by up to 60% compared to the m-CAC technique, ensuring timely delivery of relevant context and reduced latency. These results demonstrate DCMF's scalability and suitability for dynamic context-aware IoT environments.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.17226) | **Categories:** cs.DB

---

### [2] [Transient Concepts in Streaming Graphs](https://arxiv.org/abs/2506.17451)
*Aida Sheshbolouki, M. Tamer Ozsu*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Concept Drift (CD) occurs when a change in a hidden context can induce changes in a target concept. CD is a natural phenomenon in non-stationary settings such as data streams. Understanding, detection, and adaptation to CD in streaming data is (i) vital for effective and efficient analytics as reliable output depends on adaptation to fresh input, (ii) challenging as it requires efficient operations as well as effective performance evaluations, and (iii) impactful as it applies to a variety of use cases and is a crucial initial step for data management systems. Current works are mostly focused on passive CD detection as part of supervised adaptation, on independently generated data instances or graph snapshots, on target concepts as a function of data labels, on static data management, and on specific temporal order of data record. These methods do not always work. We revisit CD for the streaming graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for streaming graph CD detection and prediction. Both frameworks discern the change of generative source. SGDD detects the CDs due to the changes of generative parameters with significant delays such that it is difficult to evaluate the performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds ahead of their occurrence, without accessing the payloads of data records.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.17451) | **Categories:** cs.DB

---

### [3] [Lower Bounds for Conjunctive Query Evaluation](https://arxiv.org/abs/2506.17702)
*Stefan Mengel*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: In this tutorial, we will survey known results on the complexity of conjunctive query evaluation in different settings, ranging from Boolean queries over counting to more complex models like enumeration and direct access. A particular focus will be on showing how different relatively recent hypotheses from complexity theory connect to query answering and allow showing that known algorithms in several cases can likely not be improved.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.17702) | **Categories:** cs.DB, cs.CC

---

### [4] [Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks](https://arxiv.org/abs/2506.18013)
*Muhammad Farhan, Henning Koehler, Qing Wang*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Computing the shortest-path distance between any two given vertices in road networks is an important problem. A tremendous amount of research has been conducted to address this problem, most of which are limited to static road networks. Since road networks undergo various real-time traffic conditions, there is a pressing need to address this problem for dynamic road networks. Existing state-of-the-art methods incrementally maintain an indexing structure to reflect dynamic changes on road networks. However, these methods suffer from either slow query response time or poor maintenance performance, particularly when road networks are large. In this work, we propose an efficient solution \emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road networks from a novel perspective, which incorporates two hierarchies with different but complementary data structures to support efficient query and update processing. Specifically, our proposed solution is comprised of three main components: \emph{query hierarchy}, \emph{update hierarchy}, and \emph{hierarchical labelling}, where \emph{query hierarchy} enables efficient query answering by exploring only a small subset of vertices in the labels of two query vertices and \emph{update hierarchy} supports efficient maintenance of distance labelling under edge weight increase or decrease. We further develop dynamic algorithms to reflect dynamic changes by efficiently maintaining the update hierarchy and hierarchical labelling. We also propose a parallel variant of our dynamic algorithms by exploiting labelling structure. We evaluate our methods on 10 large road networks and it shows that our methods significantly outperform the state-of-the-art methods, i.e., achieving considerably faster construction and update time, while being consistently 2-4 times faster in terms of query processing and consuming only 10\%-20\% labelling space.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.18013) | **Categories:** cs.DB, cs.DS

---

### [5] [Floating-Point Data Transformation for Lossless Compression](https://arxiv.org/abs/2506.18062)
*Samirasadat Jamalidinan, Kazem Cheshmi*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Floating-point data is widely used across various domains. Depending on the required precision, each floating-point value can occupy several bytes. Lossless storage of this information is crucial due to its critical accuracy, as seen in applications such as medical imaging and language model weights. In these cases, data size is often significant, making lossless compression essential. Previous approaches either treat this data as raw byte streams for compression or fail to leverage all patterns within the dataset. However, because multiple bytes represent a single value and due to inherent patterns in floating-point representations, some of these bytes are correlated. To leverage this property, we propose a novel data transformation method called Typed Data Transformation (\DTT{}) that groups related bytes together to improve compression. We implemented and tested our approach on various datasets across both CPU and GPU. \DTT{} achieves a geometric mean compression ratio improvement of 1.16$\times$ over state-of-the-art compression tools such as zstd, while also improving both compression and decompression throughput by 1.18--3.79$\times$.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.18062) | **Categories:** cs.DB, cs.DC

---

### [6] [Learning Lineage Constraints for Data Science Operations](https://arxiv.org/abs/2506.18252)
*Jinjin Zhao*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Data science workflows often integrate functionalities from a diverse set of libraries and frameworks. Tasks such as debugging require data lineage that crosses library boundaries. The problem is that the way that "lineage" is represented is often intimately tied to particular data models and data manipulation paradigms. Inspired by the use of intermediate representations (IRs) in cross-library performance optimizations, this vision paper proposes a similar architecture for lineage - how do we specify logical lineage across libraries in a common parameterized way? In practice, cross-library workflows will contain both known operations and unknown operations, so a key design of XProv to link both materialized lineage graphs of data transformations and the aforementioned abstracted logical patterns. We further discuss early ideas on how to infer logical patterns when only the materialized graphs are available.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.18252) | **Categories:** cs.DB

---

### [7] [Fast Capture of Cell-Level Provenance in Numpy](https://arxiv.org/abs/2506.18255)
*Jinjin Zhao, Sanjay Krishnan*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Effective provenance tracking enhances reproducibility, governance, and data quality in array workflows. However, significant challenges arise in capturing this provenance, including: (1) rapidly evolving APIs, (2) diverse operation types, and (3) large-scale datasets. To address these challenges, this paper presents a prototype annotation system designed for arrays, which captures cell-level provenance specifically within the numpy library. With this prototype, we explore straightforward memory optimizations that substantially reduce annotation latency. We envision this provenance capture approach for arrays as part of a broader governance system for tracking for structured data workflows and diverse data science applications.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.18255) | **Categories:** cs.DB

---

### [8] [TableVault: Managing Dynamic Data Collections for LLM-Augmented Workflows](https://arxiv.org/abs/2506.18257)
*Jinjin Zhao, Sanjay Krishnan*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating and executing complex data tasks. However, their integration into more complex data workflows introduces significant management challenges. In response, we present TableVault - a data management system designed to handle dynamic data collections in LLM-augmented environments. TableVault meets the demands of these workflows by supporting concurrent execution, ensuring reproducibility, maintaining robust data versioning, and enabling composable workflow design. By merging established database methodologies with emerging LLM-driven requirements, TableVault offers a transparent platform that efficiently manages both structured data and associated data artifacts.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.18257) | **Categories:** cs.DB

---

### [9] [Patient Journey Ontology: Representing Medical Encounters for Enhanced Patient-Centric Applications](https://arxiv.org/abs/2506.18772)
*Hassan S. Al Khatib, Subash Neupane, Sudip Mittal, Shahram Rahimi, Nina Marhamati, Sean Bozorgzad*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: The healthcare industry is moving towards a patient-centric paradigm that requires advanced methods for managing and representing patient data. This paper presents a Patient Journey Ontology (PJO), a framework that aims to capture the entirety of a patient's healthcare encounters. Utilizing ontologies, the PJO integrates different patient data sources like medical histories, diagnoses, treatment pathways, and outcomes; it enables semantic interoperability and enhances clinical reasoning. By capturing temporal, sequential, and causal relationships between medical encounters, the PJO supports predictive analytics, enabling earlier interventions and optimized treatment plans. The ontology's structure, including its main classes, subclasses, properties, and relationships, as detailed in the paper, demonstrates its ability to provide a holistic view of patient care. Quantitative and qualitative evaluations by Subject Matter Experts (SMEs) demonstrate strong capabilities in patient history retrieval, symptom tracking, and provider interaction representation, while identifying opportunities for enhanced diagnosis-symptom linking. These evaluations reveal the PJO's reliability and practical applicability, demonstrating its potential to enhance patient outcomes and healthcare efficiency. This work contributes to the ongoing efforts of knowledge representation in healthcare, offering a reliable tool for personalized medicine, patient journey analysis and advancing the capabilities of Generative AI in healthcare applications.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.18772) | **Categories:** cs.DB

---

### [10] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema, Henry Herzog, Yawen Zhang, Hunter Pitelka, Favyen Bastani*

Main category: cs.DB

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal distance calculations from Anywhere on Earth (AoE). Existing global coastal datasets are only available at coarse resolution (e.g. 1-4 km) which limits their utility. Publicly available satellite imagery combined with computer vision enable much higher precision. We provide a global coastline dataset at 10 meter resolution, a 100+ fold improvement in precision over existing data. To handle the computational challenge of querying at such an increased scale, we introduce a new library: Layered Iterative Geospatial Hierarchical Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM to achieve millisecond online inference, making it well suited for real-time applications in resource-constrained environments.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.18842) | **Categories:** cs.DB, cs.CV, cs.LG

---


## cs.DL [cs.DL]
### [1] [Mapping the Evolution of Research Contributions using KnoVo](https://arxiv.org/abs/2506.17508)
*Sajratul Y. Rubaiat, Syed N. Sakib, Hasan M. Jamil*

Main category: cs.DL

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: This paper presents KnoVo (Knowledge Evolution), an intelligent framework designed for quantifying and analyzing the evolution of research novelty in the scientific literature. Moving beyond traditional citation analysis, which primarily measures impact, KnoVo determines a paper's novelty relative to both prior and subsequent work within its multilayered citation network. Given a target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to dynamically extract dimensions of comparison (e.g., methodology, application, dataset). The target paper is then compared to related publications along these same extracted dimensions. This comparative analysis, inspired by tournament selection, yields quantitative novelty scores reflecting the relative improvement, equivalence, or inferiority of the target paper in specific aspects. By aggregating these scores and visualizing their progression, for instance, through dynamic evolution graphs and comparative radar charts, KnoVo facilitates researchers not only to assess originality and identify similar work, but also to track knowledge evolution along specific research dimensions, uncover research gaps, and explore cross-disciplinary connections. We demonstrate these capabilities through a detailed analysis of 20 diverse papers from multiple scientific fields and report on the performance of various open-source LLMs within the KnoVo framework.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.17508) | **Categories:** cs.DL, cs.AI, cs.DB, cs.ET, cs.IR

---


## cs.DS [cs.DS]
### [1] [Contextual Pattern Mining and Counting](https://arxiv.org/abs/2506.17613)
*Ling Li, Daniel Gibney, Sharma V. Thankachan, Solon P. Pissis, Grigorios Loukides*

Main category: cs.DS

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Given a string $P$ of length $m$, a longer string $T$ of length $n>m$, and two integers $l\geq 0$ and $r\geq 0$, the context of $P$ in $T$ is the set of all string pairs $(L,R)$, with $|L|=l$ and $|R|=r$, such that the string $LPR$ occurs in $T$. We introduce two problems related to the notion of context: (1) the Contextual Pattern Mining (CPM) problem, which given $T$, $(m,l,r)$, and an integer $\tau>0$, asks for outputting the context of each substring $P$ of length $m$ of $T$, provided that the size of the context of $P$ is at least $\tau$; and (2) the Contextual Pattern Counting (CPC) problem, which asks for preprocessing $T$ so that the size of the context of a given query string $P$ of length $m$ can be found efficiently.   For CPM, we propose a linear-work algorithm that either uses only internal memory, or a bounded amount of internal memory and external memory, which allows much larger datasets to be handled. For CPC, we propose an $\widetilde{\mathcal{O}}(n)$-space index that can be constructed in $\widetilde{\mathcal{O}}n)$ time and answers queries in $\mathcal{O}(m)+\widetilde{\mathcal{O}}(1)$ time. We further improve the practical performance of the CPC index by optimizations that exploit the LZ77 factorization of $T$ and an upper bound on the query length. Using billion-letter datasets from different domains, we show that the external memory version of our CPM algorithm can deal with very large datasets using a small amount of internal memory while its runtime is comparable to that of the internal memory version. Interestingly, we also show that our optimized index for CPC outperforms an approach based on the state of the art for the reporting version of CPC [Navarro, SPIRE 2020] in terms of query time, index size, construction time, and construction space, often by more than an order of magnitude.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.17613) | **Categories:** cs.DS, cs.DB

---


## 机器学习 (Machine Learning) [cs.LG]
### [1] [SliceGX: Layer-wise GNN Explanation with Model-slicing](https://arxiv.org/abs/2506.17977)
*Tingting Zhu, Tingyang Chen, Yinghui Wu, Arijit Khan, Xiangyu Ke*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box models requires effective explanation methods. Existing GNN explanations typically apply input perturbations to identify subgraphs that are responsible for the occurrence of the final output of GNNs. However, such approaches lack finer-grained, layer-wise analysis of how intermediate representations contribute to the final result, capabilities that are crucial for model diagnosis and architecture optimization. This paper introduces SliceGX, a novel GNN explanation approach that generates explanations at specific GNN layers in a progressive manner. Given a GNN M, a set of selected intermediate layers, and a target layer, SliceGX automatically segments M into layer blocks ("model slice") and discovers high-quality explanatory subgraphs in each layer block that clarifies the occurrence of output of M at the targeted layer. Although finding such layer-wise explanations is computationally challenging, we develop efficient algorithms and optimization techniques that incrementally generate and maintain these subgraphs with provable approximation guarantees. Additionally, SliceGX offers a SPARQL-like query interface, providing declarative access and search capacities for the generated explanations. Through experiments on large real-world graphs and representative GNN architectures, we verify the effectiveness and efficiency of SliceGX, and illustrate its practical utility in supporting model debugging.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.17977) | **Categories:** cs.LG, cs.DB

---

### [2] [PuckTrick: A Library for Making Synthetic Data More Realistic](https://arxiv.org/abs/2506.18499)
*Alessandra Agostini, Andrea Maurino, Blerina Spahiu*

Main category: cs.LG

TL;DR: 达到API配额限制，请明天再试


<details>
  <summary>Details</summary>
Motivation: Error: API quota exceeded

Method: Error: API quota exceeded

Result: Error: API quota exceeded

Conclusion: 请联系管理员或等待明天API配额重置。

Abstract: The increasing reliance on machine learning (ML) models for decision-making requires high-quality training data. However, access to real-world datasets is often restricted due to privacy concerns, proprietary restrictions, and incomplete data availability. As a result, synthetic data generation (SDG) has emerged as a viable alternative, enabling the creation of artificial datasets that preserve the statistical properties of real data while ensuring privacy compliance. Despite its advantages, synthetic data is often overly clean and lacks real-world imperfections, such as missing values, noise, outliers, and misclassified labels, which can significantly impact model generalization and robustness. To address this limitation, we introduce Pucktrick, a Python library designed to systematically contaminate synthetic datasets by introducing controlled errors. The library supports multiple error types, including missing data, noisy values, outliers, label misclassification, duplication, and class imbalance, offering a structured approach to evaluating ML model resilience under real-world data imperfections. Pucktrick provides two contamination modes: one for injecting errors into clean datasets and another for further corrupting already contaminated datasets. Through extensive experiments on real-world financial datasets, we evaluate the impact of systematic data contamination on model performance. Our findings demonstrate that ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees.

</details>

[**[PDF]**](https://arxiv.org/pdf/2506.18499) | **Categories:** cs.LG, cs.AI, cs.DB, H.4.1; I.2.1

---
