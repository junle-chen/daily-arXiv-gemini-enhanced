{"id": "2509.12273", "pdf": "https://arxiv.org/pdf/2509.12273", "abs": "https://arxiv.org/abs/2509.12273", "authors": ["Liangqi Yuan", "Dong-Jun Han", "Christopher G. Brinton", "Sabine Brunswicker"], "title": "LLMAP: LLM-Assisted Multi-Objective Route Planning with User Preferences", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The rise of large language models (LLMs) has made natural language-driven\nroute planning an emerging research area that encompasses rich user objectives.\nCurrent research exhibits two distinct approaches: direct route planning using\nLLM-as-Agent and graph-based searching strategies. However, LLMs in the former\napproach struggle to handle extensive map data, while the latter shows limited\ncapability in understanding natural language preferences. Additionally, a more\ncritical challenge arises from the highly heterogeneous and unpredictable\nspatio-temporal distribution of users across the globe. In this paper, we\nintroduce a novel LLM-Assisted route Planning (LLMAP) system that employs an\nLLM-as-Parser to comprehend natural language, identify tasks, and extract user\npreferences and recognize task dependencies, coupled with a Multi-Step Graph\nconstruction with iterative Search (MSGS) algorithm as the underlying solver\nfor optimal route finding. Our multi-objective optimization approach adaptively\ntunes objective weights to maximize points of interest (POI) quality and task\ncompletion rate while minimizing route distance, subject to three key\nconstraints: user time limits, POI opening hours, and task dependencies. We\nconduct extensive experiments using 1,000 routing prompts sampled with varying\ncomplexity across 14 countries and 27 cities worldwide. The results demonstrate\nthat our approach achieves superior performance with guarantees across multiple\nconstraints.", "relevance_analysis": {"relevance_score": 0.9, "explanation": "The paper explicitly combines large language models (LLMs) with route planning, which is a form of trajectory prediction. The title and abstract clearly indicate the use of LLMs to assist in multi-objective route planning, demonstrating a strong connection to both trajectory prediction and large language models.", "keywords": ["large language models", "LLMs", "route planning", "trajectory prediction", "user preferences", "multi-objective optimization"]}, "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684LLM\u8f85\u52a9\u8def\u7ebf\u89c4\u5212\u7cfb\u7edf\uff08LLMAP\uff09\uff0c\u8be5\u7cfb\u7edf\u7ed3\u5408\u4e86LLM-as-Parser\u548c\u591a\u6b65\u56fe\u6784\u5efa\u8fed\u4ee3\u641c\u7d22\uff08MSGS\uff09\u7b97\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8003\u8651\u591a\u91cd\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u6700\u4f18\u8def\u7ebf\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2d\uff0cLLM-as-Agent\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u5927\u89c4\u6a21\u5730\u56fe\u6570\u636e\uff0c\u800c\u57fa\u4e8e\u56fe\u641c\u7d22\u7684\u65b9\u6cd5\u5728\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u504f\u597d\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u7528\u6237\u5728\u5168\u7403\u8303\u56f4\u5185\u5448\u73b0\u9ad8\u5ea6\u5f02\u6784\u548c\u4e0d\u53ef\u9884\u6d4b\u7684\u65f6\u7a7a\u5206\u5e03\uff0c\u8fd9\u5e26\u6765\u4e86\u66f4\u5927\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdLLM\u8f85\u52a9\u8def\u7ebf\u89c4\u5212\u7cfb\u7edf\uff08LLMAP\uff09\uff0c\u8be5\u7cfb\u7edf\u5229\u7528LLM-as-Parser\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u3001\u8bc6\u522b\u4efb\u52a1\u3001\u63d0\u53d6\u7528\u6237\u504f\u597d\u5e76\u8bc6\u522b\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u591a\u6b65\u56fe\u6784\u5efa\u8fed\u4ee3\u641c\u7d22\uff08MSGS\uff09\u7b97\u6cd5\u4f5c\u4e3a\u5e95\u5c42\u6c42\u89e3\u5668\uff0c\u4ee5\u5bfb\u627e\u6700\u4f18\u8def\u7ebf\u3002", "result": "\u5728\u5305\u542b\u5168\u740314\u4e2a\u56fd\u5bb6\u548c27\u4e2a\u57ce\u5e02\u76841000\u4e2a\u5177\u6709\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u8def\u7ebf\u89c4\u5212\u63d0\u793a\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684LLMAP\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u81ea\u7136\u8bed\u8a00\u8def\u7ebf\u89c4\u5212\u4e2d\u5b58\u5728\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u8003\u8651\u591a\u91cd\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6700\u4f18\u8def\u7ebf\u89c4\u5212\u3002", "summary_zh": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5174\u8d77\u4f7f\u5f97\u81ea\u7136\u8bed\u8a00\u9a71\u52a8\u7684\u8def\u7ebf\u89c4\u5212\u6210\u4e3a\u4e00\u4e2a\u65b0\u5174\u7684\u7814\u7a76\u9886\u57df\uff0c\u8be5\u9886\u57df\u6db5\u76d6\u4e86\u4e30\u5bcc\u7684\u7528\u6237\u76ee\u6807\u3002\u76ee\u524d\u7684\u7814\u7a76\u4e3b\u8981\u6709\u4e24\u79cd\u4e0d\u540c\u7684\u65b9\u6cd5\uff1a\u4f7f\u7528LLM-as-Agent\u7684\u76f4\u63a5\u8def\u7ebf\u89c4\u5212\u548c\u57fa\u4e8e\u56fe\u7684\u641c\u7d22\u7b56\u7565\u3002\u7136\u800c\uff0c\u524d\u4e00\u79cd\u65b9\u6cd5\u4e2d\u7684LLM\u96be\u4ee5\u5904\u7406\u5927\u91cf\u7684\u5730\u56fe\u6570\u636e\uff0c\u800c\u540e\u4e00\u79cd\u65b9\u6cd5\u5728\u7406\u89e3\u81ea\u7136\u8bed\u8a00\u504f\u597d\u65b9\u9762\u7684\u80fd\u529b\u6709\u9650\u3002\u6b64\u5916\uff0c\u4e00\u4e2a\u66f4\u4e25\u5cfb\u7684\u6311\u6218\u6765\u81ea\u4e8e\u5168\u7403\u7528\u6237\u9ad8\u5ea6\u5f02\u6784\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u65f6\u7a7a\u5206\u5e03\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u9896\u7684LLM\u8f85\u52a9\u8def\u7ebf\u89c4\u5212\uff08LLMAP\uff09\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u91c7\u7528LLM-as-Parser\u6765\u7406\u89e3\u81ea\u7136\u8bed\u8a00\uff0c\u8bc6\u522b\u4efb\u52a1\uff0c\u63d0\u53d6\u7528\u6237\u504f\u597d\uff0c\u5e76\u8bc6\u522b\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\uff0c\u540c\u65f6\u7ed3\u5408\u591a\u6b65\u56fe\u6784\u5efa\u8fed\u4ee3\u641c\u7d22\uff08MSGS\uff09\u7b97\u6cd5\u4f5c\u4e3a\u5e95\u5c42\u6c42\u89e3\u5668\uff0c\u4ee5\u5bfb\u627e\u6700\u4f18\u8def\u7ebf\u3002\u6211\u4eec\u7684\u591a\u76ee\u6807\u4f18\u5316\u65b9\u6cd5\u81ea\u9002\u5e94\u5730\u8c03\u6574\u76ee\u6807\u6743\u91cd\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u5174\u8da3\u70b9\uff08POI\uff09\u7684\u8d28\u91cf\u548c\u4efb\u52a1\u5b8c\u6210\u7387\uff0c\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8def\u7ebf\u8ddd\u79bb\uff0c\u5e76\u53d7\u9650\u4e8e\u4e09\u4e2a\u5173\u952e\u7ea6\u675f\uff1a\u7528\u6237\u65f6\u95f4\u9650\u5236\u3001POI\u5f00\u653e\u65f6\u95f4\u548c\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u3002\u6211\u4eec\u4f7f\u75281000\u4e2a\u5728\u4e16\u754c\u8303\u56f4\u518514\u4e2a\u56fd\u5bb6\u548c27\u4e2a\u57ce\u5e02\u91c7\u6837\u7684\u5177\u6709\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u8def\u7ebf\u89c4\u5212\u63d0\u793a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u591a\u4e2a\u7ea6\u675f\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2509.12423", "pdf": "https://arxiv.org/pdf/2509.12423", "abs": "https://arxiv.org/abs/2509.12423", "authors": ["Danielle Cohen", "Yoni Halpern", "Noam Kahlon", "Joel Oren", "Omri Berkovitch", "Sapir Caduri", "Ido Dagan", "Anatoly Efros"], "title": "Small Models, Big Results: Achieving Superior Intent Extraction through Decomposition", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Understanding user intents from UI interaction trajectories remains a\nchallenging, yet crucial, frontier in intelligent agent development. While\nmassive, datacenter-based, multi-modal large language models (MLLMs) possess\ngreater capacity to handle the complexities of such sequences, smaller models\nwhich can run on-device to provide a privacy-preserving, low-cost, and\nlow-latency user experience, struggle with accurate intent inference. We\naddress these limitations by introducing a novel decomposed approach: first, we\nperform structured interaction summarization, capturing key information from\neach user action. Second, we perform intent extraction using a fine-tuned model\noperating on the aggregated summaries. This method improves intent\nunderstanding in resource-constrained models, even surpassing the base\nperformance of large MLLMs.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u6458\u8981\u63d0\u5230\u4e86UI interaction trajectories\u548clarge language models (MLLMs)\u3002\u867d\u7136\u4e3b\u8981\u5173\u6ce8\u7684\u662f\u5c0f\u578b\u6a21\u578b\uff0c\u4f46\u4f7f\u7528\u4e86\u5927\u6a21\u578b\u4f5c\u4e3a\u5bf9\u6bd4\uff0c\u5e76\u4e14\u6d89\u53ca\u4e86\u4ece\u8f68\u8ff9\u4e2d\u63d0\u53d6\u610f\u56fe\u7684\u4efb\u52a1\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u90fd\u6709\u5173\u8054\u3002", "keywords": ["trajectory", "large language models", "MLLMs", "intent extraction", "UI interaction trajectories"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u4ea4\u4e92\u6458\u8981\u548c\u610f\u56fe\u63d0\u53d6\uff0c\u63d0\u9ad8\u4e86\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u5bf9\u7528\u6237\u754c\u9762\u4ea4\u4e92\u8f68\u8ff9\u7684\u610f\u56fe\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\u7684\u5c0f\u578b\u6a21\u578b\u96be\u4ee5\u51c6\u786e\u63a8\u65ad\u7528\u6237\u754c\u9762\u4ea4\u4e92\u8f68\u8ff9\u4e2d\u7684\u610f\u56fe\uff0c\u800c\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u529b\u66f4\u5f3a\uff0c\u4f46\u6210\u672c\u9ad8\u4e14\u65e0\u6cd5\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u6267\u884c\u7ed3\u6784\u5316\u4ea4\u4e92\u6458\u8981\uff0c\u6355\u6349\u6bcf\u4e2a\u7528\u6237\u52a8\u4f5c\u7684\u5173\u952e\u4fe1\u606f\uff0c\u7136\u540e\u4f7f\u7528\u5fae\u8c03\u6a21\u578b\u5bf9\u6c47\u603b\u7684\u6458\u8981\u8fdb\u884c\u610f\u56fe\u63d0\u53d6\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u4e2d\u7684\u610f\u56fe\u7406\u89e3\u80fd\u529b\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5206\u89e3\u65b9\u6cd5\uff0c\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u53ef\u4ee5\u5728\u7528\u6237\u610f\u56fe\u7406\u89e3\u65b9\u9762\u8fbe\u5230\u751a\u81f3\u8d85\u8fc7\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "summary_zh": "\u4ece\u7528\u6237\u754c\u9762\u4ea4\u4e92\u8f68\u8ff9\u4e2d\u7406\u89e3\u7528\u6237\u610f\u56fe\u4ecd\u7136\u662f\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u4f46\u81f3\u5173\u91cd\u8981\u7684\u524d\u6cbf\u9886\u57df\u3002\u867d\u7136\u5927\u578b\u6570\u636e\u4e2d\u5fc3\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5177\u6709\u66f4\u5f3a\u7684\u5904\u7406\u6b64\u7c7b\u5e8f\u5217\u590d\u6742\u6027\u7684\u80fd\u529b\uff0c\u4f46\u53ef\u4ee5\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\u4ee5\u63d0\u4f9b\u4fdd\u62a4\u9690\u79c1\u3001\u4f4e\u6210\u672c\u548c\u4f4e\u5ef6\u8fdf\u7528\u6237\u4f53\u9a8c\u7684\u5c0f\u578b\u6a21\u578b\uff0c\u96be\u4ee5\u51c6\u786e\u5730\u8fdb\u884c\u610f\u56fe\u63a8\u65ad\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u5206\u89e3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff1a\u9996\u5148\uff0c\u6211\u4eec\u6267\u884c\u7ed3\u6784\u5316\u4ea4\u4e92\u6458\u8981\uff0c\u4ece\u6bcf\u4e2a\u7528\u6237\u52a8\u4f5c\u4e2d\u6355\u83b7\u5173\u952e\u4fe1\u606f\u3002\u5176\u6b21\uff0c\u6211\u4eec\u4f7f\u7528\u5728\u805a\u5408\u6458\u8981\u4e0a\u8fd0\u884c\u7684\u5fae\u8c03\u6a21\u578b\u6267\u884c\u610f\u56fe\u63d0\u53d6\u3002\u8fd9\u79cd\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8d44\u6e90\u53d7\u9650\u6a21\u578b\u4e2d\u7684\u610f\u56fe\u7406\u89e3\u80fd\u529b\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u5927\u578b MLLM \u7684\u57fa\u672c\u6027\u80fd\u3002"}}
{"id": "2509.13132", "pdf": "https://arxiv.org/pdf/2509.13132", "abs": "https://arxiv.org/abs/2509.13132", "authors": ["Zhihao Zhang", "Chengyang Peng", "Minghao Zhu", "Ekim Yurtsever", "Keith A. Redmill"], "title": "An Uncertainty-Weighted Decision Transformer for Navigation in Dense, Complex Driving Scenarios", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Autonomous driving in dense, dynamic environments requires decision-making\nsystems that can exploit both spatial structure and long-horizon temporal\ndependencies while remaining robust to uncertainty. This work presents a novel\nframework that integrates multi-channel bird's-eye-view occupancy grids with\ntransformer-based sequence modeling for tactical driving in complex roundabout\nscenarios. To address the imbalance between frequent low-risk states and rare\nsafety-critical decisions, we propose the Uncertainty-Weighted Decision\nTransformer (UWDT). UWDT employs a frozen teacher transformer to estimate\nper-token predictive entropy, which is then used as a weight in the student\nmodel's loss function. This mechanism amplifies learning from uncertain,\nhigh-impact states while maintaining stability across common low-risk\ntransitions. Experiments in a roundabout simulator, across varying traffic\ndensities, show that UWDT consistently outperforms other baselines in terms of\nreward, collision rate, and behavioral stability. The results demonstrate that\nuncertainty-aware, spatial-temporal transformers can deliver safer and more\nefficient decision-making for autonomous driving in complex traffic\nenvironments.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4f7f\u7528\u4e86Transformer\u8fdb\u884c\u51b3\u7b56\uff0c\u5e76\u6d89\u53ca\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e0b\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46Transformer\u67b6\u6784\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u9884\u6d4b\u4e2d\u7684\u51b3\u7b56\u95ee\u9898\u3002", "keywords": ["trajectory prediction", "transformer", "autonomous driving", "decision-making", "sequence modeling"]}, "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u51b3\u7b56Transformer\uff08UWDT\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u590d\u6742\u73af\u5c9b\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u9ad8\u6548\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u3002", "motivation": "\u5728\u5bc6\u96c6\u3001\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u80fd\u591f\u5229\u7528\u7a7a\u95f4\u7ed3\u6784\u548c\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u5bf9\u4e0d\u786e\u5b9a\u6027\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u51b3\u7b56\u7cfb\u7edf\u3002", "method": "\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u591a\u901a\u9053\u9e1f\u77b0\u56fe\u5360\u7528\u6805\u683c\u548c\u57fa\u4e8eTransformer\u7684\u5e8f\u5217\u5efa\u6a21\uff0c\u5e76\u63d0\u51faUWDT\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u6559\u5e08Transformer\u4f30\u8ba1\u6bcf\u4e2atoken\u7684\u9884\u6d4b\u71b5\uff0c\u4f5c\u4e3a\u5b66\u751f\u6a21\u578b\u635f\u5931\u51fd\u6570\u7684\u6743\u91cd\u3002", "result": "\u5728\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u7684\u73af\u5c9b\u6a21\u62df\u5668\u5b9e\u9a8c\u4e2d\uff0cUWDT\u5728\u5956\u52b1\u3001\u78b0\u649e\u7387\u548c\u884c\u4e3a\u7a33\u5b9a\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u65f6\u7a7aTransformer\u53ef\u4ee5\u4e3a\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u51b3\u7b56\u3002", "summary_zh": "\u5728\u5bc6\u96c6\u3001\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u80fd\u591f\u5229\u7528\u7a7a\u95f4\u7ed3\u6784\u548c\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u540c\u65f6\u5bf9\u4e0d\u786e\u5b9a\u6027\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u51b3\u7b56\u7cfb\u7edf\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u591a\u901a\u9053\u9e1f\u77b0\u56fe\u5360\u7528\u6805\u683c\u548c\u57fa\u4e8eTransformer\u7684\u5e8f\u5217\u5efa\u6a21\uff0c\u7528\u4e8e\u590d\u6742\u73af\u5c9b\u573a\u666f\u4e2d\u7684\u6218\u672f\u9a7e\u9a76\u3002\u4e3a\u4e86\u89e3\u51b3\u9891\u7e41\u7684\u4f4e\u98ce\u9669\u72b6\u6001\u548c\u7f55\u89c1\u7684\u5b89\u5168\u6027\u5173\u952e\u51b3\u7b56\u4e4b\u95f4\u7684\u4e0d\u5e73\u8861\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u51b3\u7b56Transformer\uff08UWDT\uff09\u3002UWDT\u91c7\u7528\u51bb\u7ed3\u7684\u6559\u5e08Transformer\u6765\u4f30\u8ba1\u6bcf\u4e2atoken\u7684\u9884\u6d4b\u71b5\uff0c\u7136\u540e\u5c06\u5176\u7528\u4f5c\u5b66\u751f\u6a21\u578b\u635f\u5931\u51fd\u6570\u4e2d\u7684\u6743\u91cd\u3002\u8fd9\u79cd\u673a\u5236\u589e\u5f3a\u4e86\u4ece\u4e0d\u786e\u5b9a\u3001\u9ad8\u5f71\u54cd\u72b6\u6001\u7684\u5b66\u4e60\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e38\u89c1\u4f4e\u98ce\u9669\u8f6c\u6362\u7684\u7a33\u5b9a\u6027\u3002\u5728\u73af\u5c9b\u6a21\u62df\u5668\u4e2d\uff0c\u8de8\u4e0d\u540c\u4ea4\u901a\u5bc6\u5ea6\u7684\u5b9e\u9a8c\u8868\u660e\uff0cUWDT\u5728\u5956\u52b1\u3001\u78b0\u649e\u7387\u548c\u884c\u4e3a\u7a33\u5b9a\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u65f6\u7a7aTransformer\u53ef\u4ee5\u4e3a\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u51b3\u7b56\u3002"}}
{"id": "2508.12176", "pdf": "https://arxiv.org/pdf/2508.12176", "abs": "https://arxiv.org/abs/2508.12176", "authors": ["Zhiwei Zheng", "Dongyin Hu", "Mingmin Zhao"], "title": "Scalable RF Simulation in Generative 4D Worlds", "categories": ["cs.CV", "cs.AI", "eess.SP"], "comment": null, "summary": "Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving\nalternative to vision-based methods for indoor perception tasks. However,\ncollecting high-quality RF data in dynamic and diverse indoor environments\nremains a major challenge. To address this, we introduce WaveVerse, a\nprompt-based, scalable framework that simulates realistic RF signals from\ngenerated indoor scenes with human motions. WaveVerse introduces a\nlanguage-guided 4D world generator, which includes a state-aware causal\ntransformer for human motion generation conditioned on spatial constraints and\ntexts, and a phase-coherent ray tracing simulator that enables the simulation\nof accurate and coherent RF signals. Experiments demonstrate the effectiveness\nof our approach in conditioned human motion generation and highlight how phase\ncoherence is applied to beamforming and respiration monitoring. We further\npresent two case studies in ML-based high-resolution imaging and human activity\nrecognition, demonstrating that WaveVerse not only enables data generation for\nRF imaging for the first time, but also consistently achieves performance gain\nin both data-limited and data-adequate scenarios.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "The paper describes WaveVerse, a framework that includes a language-guided 4D world generator with a state-aware causal transformer for human motion generation. This relates to trajectory prediction. The 'language-guided' aspect and the use of a transformer model suggest a connection to large language models, although it's not explicitly stated that a pre-trained LLM is used. The focus is more on generative models and motion prediction conditioned on language and spatial constraints.", "keywords": ["human motion generation", "transformer", "language-guided", "4D world generator", "motion prediction"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12367", "pdf": "https://arxiv.org/pdf/2509.12367", "abs": "https://arxiv.org/abs/2509.12367", "authors": ["Daniel Lindmark", "Jonas Andersson", "Kenneth Bodin", "Tora Bodin", "Hugo B\u00f6rjesson", "Fredrik Nordfeldth", "Martin Servin"], "title": "An integrated process for design and control of lunar robotics using AI and simulation", "categories": ["cs.RO", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "We envision an integrated process for developing lunar construction\nequipment, where physical design and control are explored in parallel. In this\npaper, we describe a technical framework that supports this process. It relies\non OpenPLX, a readable/writable declarative language that links CAD-models and\nautonomous systems to high-fidelity, real-time 3D simulations of contacting\nmultibody dynamics, machine regolith interaction forces, and non-ideal sensors.\nTo demonstrate its capabilities, we present two case studies, including an\nautonomous lunar rover that combines a vision-language model for navigation\nwith a reinforcement learning-based control policy for locomotion.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u6708\u7403\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\uff0c\u4f46\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5bfc\u822a\uff0c\u5e76\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u3002\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5c5e\u4e8e\u5927\u6a21\u578b\u8303\u7574\uff0c\u800c\u5bfc\u822a\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["vision-language model", "navigation", "autonomous rover"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12263", "pdf": "https://arxiv.org/pdf/2509.12263", "abs": "https://arxiv.org/abs/2509.12263", "authors": ["Gautam Sreekumar", "Vishnu Naresh Boddeti"], "title": "InPhyRe Discovers: Large Multimodal Models Struggle in Inductive Physical Reasoning", "categories": ["cs.AI", "cs.LG"], "comment": "35 pages including appendix", "summary": "Large multimodal models (LMMs) encode universal physical laws observed during\ntraining, such as momentum conservation, as parametric knowledge. It allows\nLMMs to answer physical reasoning queries, such as the outcome of a potential\ncollision event from visual input. However, since parametric knowledge includes\nonly the physical laws seen during training, it is insufficient for reasoning\nwhen the inference scenario violates these physical laws. In contrast, humans\npossess the skill to adapt their physical reasoning to unseen physical\nenvironments from a few visual examples. This ability, which we refer to as\ninductive physical reasoning, is indispensable for LMMs if they are to replace\nhuman agents in safety-critical applications. Despite its importance, existing\nvisual benchmarks evaluate only the parametric knowledge in LMMs, and not\ninductive physical reasoning. To this end, we propose InPhyRe, the first visual\nquestion answering benchmark to measure inductive physical reasoning in LMMs.\nInPhyRe evaluates LMMs on their ability to predict the outcome of collision\nevents in algorithmically generated synthetic collision videos. By inspecting\n13 LMMs, InPhyRe informs us that (1) LMMs struggle to apply their limited\nparametric knowledge about universal physical laws to reasoning, (2) inductive\nphysical reasoning in LMMs is weak when demonstration samples violate universal\nphysical laws, and (3) inductive physical reasoning in LMMs suffers from\nlanguage bias and largely ignores the visual inputs, questioning the\ntrustworthiness of LMMs regarding visual inputs.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper discusses the physical reasoning capabilities of Large Multimodal Models (LMMs) in predicting collision events. While it doesn't directly focus on trajectory prediction in the traditional sense (e.g., predicting pedestrian or vehicle trajectories), it does involve predicting the outcome of physical interactions, which has some overlap. The primary focus is on LMMs and their limitations in inductive physical reasoning.", "keywords": ["Large Multimodal Models", "LMMs", "physical reasoning", "collision events", "visual question answering"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12437", "pdf": "https://arxiv.org/pdf/2509.12437", "abs": "https://arxiv.org/abs/2509.12437", "authors": ["Dingrui Wang", "Zhexiao Sun", "Zhouheng Li", "Cheng Wang", "Youlun Peng", "Hongyuan Ye", "Baha Zarrouki", "Wei Li", "Mattia Piccinini", "Lei Xie", "Johannes Betz"], "title": "Enhancing Physical Consistency in Lightweight World Models", "categories": ["cs.AI"], "comment": "8 pages", "summary": "A major challenge in deploying world models is the trade-off between size and\nperformance. Large world models can capture rich physical dynamics but require\nmassive computing resources, making them impractical for edge devices. Small\nworld models are easier to deploy but often struggle to learn accurate physics,\nleading to poor predictions. We propose the Physics-Informed BEV World Model\n(PIWM), a compact model designed to efficiently capture physical interactions\nin bird's-eye-view (BEV) representations. PIWM uses Soft Mask during training\nto improve dynamic object modeling and future prediction. We also introduce a\nsimple yet effective technique, Warm Start, for inference to enhance prediction\nquality with a zero-shot model. Experiments show that at the same parameter\nscale (400M), PIWM surpasses the baseline by 60.6% in weighted overall score.\nMoreover, even when compared with the largest baseline model (400M), the\nsmallest PIWM (130M Soft Mask) achieves a 7.4% higher weighted overall score\nwith a 28% faster inference speed.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on building a lightweight world model that can efficiently capture physical interactions and improve future prediction, which is related to trajectory prediction. It uses techniques like Soft Mask and Warm Start to enhance prediction quality. While it mentions the trade-off between size and performance of world models (implicitly relating to larger models), it doesn't explicitly involve large language models. The focus is more on physics-informed modeling and efficient computation for prediction.", "keywords": ["world model", "trajectory prediction", "future prediction", "BEV", "physics-informed"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12430", "pdf": "https://arxiv.org/pdf/2509.12430", "abs": "https://arxiv.org/abs/2509.12430", "authors": ["Mayank Patel", "Rahul Jain", "Asim Unmesh", "Karthik Ramani"], "title": "DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Understanding the motion of articulated mechanical assemblies from static\ngeometry remains a core challenge in 3D perception and design automation. Prior\nwork on everyday articulated objects such as doors and laptops typically\nassumes simplified kinematic structures or relies on joint annotations.\nHowever, in mechanical assemblies like gears, motion arises from geometric\ncoupling, through meshing teeth or aligned axes, making it difficult for\nexisting methods to reason about relational motion from geometry alone. To\naddress this gap, we introduce MechBench, a benchmark dataset of 693 diverse\nsynthetic gear assemblies with part-wise ground-truth motion trajectories.\nMechBench provides a structured setting to study coupled motion, where part\ndynamics are induced by contact and transmission rather than predefined joints.\nBuilding on this, we propose DYNAMO, a dependency-aware neural model that\npredicts per-part SE(3) motion trajectories directly from segmented CAD point\nclouds. Experiments show that DYNAMO outperforms strong baselines, achieving\naccurate and temporally consistent predictions across varied gear\nconfigurations. Together, MechBench and DYNAMO establish a novel systematic\nframework for data-driven learning of coupled mechanical motion in CAD\nassemblies.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u673a\u68b0\u7ec4\u4ef6\u7684\u8fd0\u52a8\u9884\u6d4b\uff0c\u7279\u522b\u662f\u9f7f\u8f6e\u7b49\u673a\u68b0\u7ed3\u6784\u7684\u8fd0\u52a8\u8f68\u8ff9\u9884\u6d4b\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u6d89\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u6838\u5fc3\u5728\u4e8e\u8fd0\u52a8\u9884\u6d4b\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9886\u57df\u76f8\u5173\u3002\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002", "keywords": ["trajectory prediction", "motion prediction", "SE(3) motion trajectories", "motion trajectories"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12289", "pdf": "https://arxiv.org/pdf/2509.12289", "abs": "https://arxiv.org/abs/2509.12289", "authors": ["Yuting Liu", "Qiang Zhou", "Hanzhe Li", "Chenqi Gong", "Jingjing Gu"], "title": "C3DE: Causal-Aware Collaborative Neural Controlled Differential Equation for Long-Term Urban Crowd Flow Prediction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Long-term urban crowd flow prediction suffers significantly from cumulative\nsampling errors, due to increased sequence lengths and sampling intervals,\nwhich inspired us to leverage Neural Controlled Differential Equations (NCDEs)\nto mitigate this issue. However, regarding the crucial influence of Points of\nInterest (POIs) evolution on long-term crowd flow, the multi-timescale\nasynchronous dynamics between crowd flow and POI distribution, coupled with\nlatent spurious causality, poses challenges to applying NCDEs for long-term\nurban crowd flow prediction. To this end, we propose Causal-aware Collaborative\nneural CDE (C3DE) to model the long-term dynamic of crowd flow. Specifically,\nwe introduce a dual-path NCDE as the backbone to effectively capture the\nasynchronous evolution of collaborative signals across multiple time scales.\nThen, we design a dynamic correction mechanism with the counterfactual-based\ncausal effect estimator to quantify the causal impact of POIs on crowd flow and\nminimize the accumulation of spurious correlations. Finally, we leverage a\npredictor for long-term prediction with the fused collaborative signals of POI\nand crowd flow. Extensive experiments on three real-world datasets demonstrate\nthe superior performance of C3DE, particularly in cities with notable flow\nfluctuations.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on long-term urban crowd flow prediction, which is a specific type of trajectory prediction. It uses Neural Controlled Differential Equations (NCDEs) for modeling the dynamics. While it doesn't directly involve large language models, the general area of crowd flow prediction and the use of neural networks for time series prediction are related to trajectory prediction. The focus on causal relationships is also a relevant aspect in trajectory prediction.", "keywords": ["crowd flow prediction", "trajectory prediction", "Neural Controlled Differential Equations", "time series prediction", "causal inference"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12838", "pdf": "https://arxiv.org/pdf/2509.12838", "abs": "https://arxiv.org/abs/2509.12838", "authors": ["Kento Murata", "Shoichi Hasegawa", "Tomochika Ishikawa", "Yoshinobu Hagiwara", "Akira Taniguchi", "Lotfi El Hafi", "Tadahiro Taniguchi"], "title": "Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "Submitted to AROB-ISBC 2026 (Journal Track option)", "summary": "It is crucial to efficiently execute instructions such as \"Find an apple and\na banana\" or \"Get ready for a field trip,\" which require searching for multiple\nobjects or understanding context-dependent commands. This study addresses the\nchallenging problem of determining which robot should be assigned to which part\nof a task when each robot possesses different situational on-site\nknowledge-specifically, spatial concepts learned from the area designated to it\nby the user. We propose a task planning framework that leverages large language\nmodels (LLMs) and spatial concepts to decompose natural language instructions\ninto subtasks and allocate them to multiple robots. We designed a novel\nfew-shot prompting strategy that enables LLMs to infer required objects from\nambiguous commands and decompose them into appropriate subtasks. In our\nexperiments, the proposed method achieved 47/50 successful assignments,\noutperforming random (28/50) and commonsense-based assignment (26/50).\nFurthermore, we conducted qualitative evaluations using two actual mobile\nmanipulators. The results demonstrated that our framework could handle\ninstructions, including those involving ad hoc categories such as \"Get ready\nfor a field trip,\" by successfully performing task decomposition, assignment,\nsequential planning, and execution.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on using Large Language Models (LLMs) for multi-robot task planning, specifically for object retrieval. While it involves robots moving (implying trajectories), the core contribution is in task decomposition and assignment using LLMs, not trajectory prediction itself. The connection to trajectory prediction is indirect.", "keywords": ["Large Language Models", "LLMs", "multi-robot task planning", "task decomposition", "natural language instructions"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.12982", "pdf": "https://arxiv.org/pdf/2509.12982", "abs": "https://arxiv.org/abs/2509.12982", "authors": ["Erblin Isaku", "Hassan Sartaj", "Shaukat Ali", "Beatriz Sanguino", "Tongtong Wang", "Guoyuan Li", "Houxiang Zhang", "Thomas Peyrucain"], "title": "Out of Distribution Detection in Self-adaptive Robots with AI-powered Digital Twins", "categories": ["cs.RO", "cs.AI", "cs.SE"], "comment": "15 pages, 4 figures, 3 tables", "summary": "Self-adaptive robots (SARs) in complex, uncertain environments must\nproactively detect and address abnormal behaviors, including\nout-of-distribution (OOD) cases. To this end, digital twins offer a valuable\nsolution for OOD detection. Thus, we present a digital twin-based approach for\nOOD detection (ODiSAR) in SARs. ODiSAR uses a Transformer-based digital twin to\nforecast SAR states and employs reconstruction error and Monte Carlo dropout\nfor uncertainty quantification. By combining reconstruction error with\npredictive variance, the digital twin effectively detects OOD behaviors, even\nin previously unseen conditions. The digital twin also includes an\nexplainability layer that links potential OOD to specific SAR states, offering\ninsights for self-adaptation. We evaluated ODiSAR by creating digital twins of\ntwo industrial robots: one navigating an office environment, and another\nperforming maritime ship navigation. In both cases, ODiSAR forecasts SAR\nbehaviors (i.e., robot trajectories and vessel motion) and proactively detects\nOOD events. Our results showed that ODiSAR achieved high detection performance\n-- up to 98\\% AUROC, 96\\% TNR@TPR95, and 95\\% F1-score -- while providing\ninterpretable insights to support self-adaptation.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on out-of-distribution detection in self-adaptive robots using digital twins. It forecasts robot states (trajectories) using a Transformer-based model. While it doesn't directly use large language models, the Transformer architecture is a key component of many LLMs, and the forecasting of trajectories aligns with trajectory prediction. The maritime ship navigation example involves trajectory prediction.", "keywords": ["trajectory prediction", "Transformer", "digital twins", "robot trajectories", "vessel motion", "forecasting"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.13024", "pdf": "https://arxiv.org/pdf/2509.13024", "abs": "https://arxiv.org/abs/2509.13024", "authors": ["Haohan Min", "Zhoujian Li", "Yu Yang", "Jinyu Chen", "Shenghai Yuan"], "title": "DVDP: An End-to-End Policy for Mobile Robot Visual Docking with RGB-D Perception", "categories": ["cs.RO"], "comment": null, "summary": "Automatic docking has long been a significant challenge in the field of\nmobile robotics. Compared to other automatic docking methods, visual docking\nmethods offer higher precision and lower deployment costs, making them an\nefficient and promising choice for this task. However, visual docking methods\nimpose strict requirements on the robot's initial position at the start of the\ndocking process. To overcome the limitations of current vision-based methods,\nwe propose an innovative end-to-end visual docking method named DVDP(direct\nvisual docking policy). This approach requires only a binocular RGB-D camera\ninstalled on the mobile robot to directly output the robot's docking path,\nachieving end-to-end automatic docking. Furthermore, we have collected a\nlarge-scale dataset of mobile robot visual automatic docking dataset through a\ncombination of virtual and real environments using the Unity 3D platform and\nactual mobile robot setups. We developed a series of evaluation metrics to\nquantify the performance of the end-to-end visual docking method. Extensive\nexperiments, including benchmarks against leading perception backbones adapted\ninto our framework, demonstrate that our method achieves superior performance.\nFinally, real-world deployment on the SCOUT Mini confirmed DVDP's efficacy,\nwith our model generating smooth, feasible docking trajectories that meet\nphysical constraints and reach the target pose.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u79fb\u52a8\u673a\u5668\u4eba\u7684\u89c6\u89c9\u5bf9\u63a5\uff0c\u6d89\u53ca\u8def\u5f84\u89c4\u5212\u548c\u8f68\u8ff9\u751f\u6210\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7aef\u5230\u7aef\u7684\u7b56\u7565\u548c\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u53ef\u80fd\u4e3a\u672a\u6765\u7ed3\u5408\u5927\u578b\u6a21\u578b\u8fdb\u884c\u8f68\u8ff9\u9884\u6d4b\u63d0\u4f9b\u57fa\u7840\u3002", "keywords": ["\u8f68\u8ff9\u9884\u6d4b", "\u79fb\u52a8\u673a\u5668\u4eba", "\u8def\u5f84\u89c4\u5212", "\u7aef\u5230\u7aef", "\u89c6\u89c9\u5bf9\u63a5"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
{"id": "2509.13164", "pdf": "https://arxiv.org/pdf/2509.13164", "abs": "https://arxiv.org/abs/2509.13164", "authors": ["Jiawei Wang", "Haowei Sun", "Xintao Yan", "Shuo Feng", "Jun Gao", "Henry X. Liu"], "title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End Autonomous Driving", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": "8 pages, 6 figures. Codes and videos are available at\n  https://wjiawei.com/terasim-world-web/", "summary": "Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical~data synthesis framework for training and evaluation of E2E autonomous\ndriving systems.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u7684\u4eff\u771f\u6570\u636e\u751f\u6210\uff0c\u63d0\u5230\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\uff0c\u5176\u4e2d\u6d89\u53caagent\u884c\u4e3a\u6a21\u62df\uff0c\u53ef\u4ee5\u770b\u4f5c\u662f\u8f68\u8ff9\u9884\u6d4b\u7684\u4e00\u79cd\u5e94\u7528\u573a\u666f\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u5927\u6a21\u578b\uff0c\u4f46\u662f\u63d0\u5230\u4e86\u89c6\u9891\u751f\u6210\u6a21\u578b Cosmos-Drive\uff0c\u53ef\u4ee5\u88ab\u8ba4\u4e3a\u662f\u4e0e\u5927\u6a21\u578b\u76f8\u5173\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["autonomous driving", "agent behaviors", "Cosmos-Drive", "simulation"]}, "AI": {"tldr": "\u8fbe\u5230API\u914d\u989d\u9650\u5236\uff0c\u8bf7\u660e\u5929\u518d\u8bd5", "motivation": "Error: API quota exceeded", "method": "Error: API quota exceeded", "result": "Error: API quota exceeded", "conclusion": "\u8bf7\u8054\u7cfb\u7ba1\u7406\u5458\u6216\u7b49\u5f85\u660e\u5929API\u914d\u989d\u91cd\u7f6e\u3002"}}
