{"id": "2506.04263", "pdf": "https://arxiv.org/pdf/2506.04263", "abs": "https://arxiv.org/abs/2506.04263", "authors": ["Alan Mitkiy", "James Smith", "Hana Satou", "Hiroshi Tanaka", "Emily Johnson", "F Monkey"], "title": "Dynamic Epsilon Scheduling: A Multi-Factor Adaptive Perturbation Budget for Adversarial Training", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Adversarial training is among the most effective strategies for defending\ndeep neural networks against adversarial examples. A key limitation of existing\nadversarial training approaches lies in their reliance on a fixed perturbation\nbudget, which fails to account for instance-specific robustness\ncharacteristics. While prior works such as IAAT and MMA introduce\ninstance-level adaptations, they often rely on heuristic or static\napproximations of data robustness. In this paper, we propose Dynamic Epsilon\nScheduling (DES), a novel framework that adaptively adjusts the adversarial\nperturbation budget per instance and per training iteration. DES integrates\nthree key factors: (1) the distance to the decision boundary approximated via\ngradient-based proxies, (2) prediction confidence derived from softmax entropy,\nand (3) model uncertainty estimated via Monte Carlo dropout. By combining these\ncues into a unified scheduling strategy, DES tailors the perturbation budget\ndynamically to guide more effective adversarial learning. Experimental results\non CIFAR-10 and CIFAR-100 show that our method consistently improves both\nadversarial robustness and standard accuracy compared to fixed-epsilon\nbaselines and prior adaptive methods. Moreover, we provide theoretical insights\ninto the stability and convergence of our scheduling policy. This work opens a\nnew avenue for instance-aware, data-driven adversarial training methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u8c03\u6574\u5bf9\u6297\u6270\u52a8\u9884\u7b97\u7684\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u6270\u52a8\u9884\u7b97\uff0c\u65e0\u6cd5\u8003\u8651\u5b9e\u4f8b\u7279\u5b9a\u7684\u9c81\u68d2\u6027\u7279\u5f81\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a8\u6001Epsilon\u8c03\u5ea6\uff08DES\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u4ee3\u7406\u3001\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7b49\u56e0\u7d20\uff0c\u81ea\u9002\u5e94\u5730\u8c03\u6574\u6bcf\u4e2a\u5b9e\u4f8b\u548c\u6bcf\u4e2a\u8bad\u7ec3\u8fed\u4ee3\u7684\u5bf9\u6297\u6270\u52a8\u9884\u7b97\u3002", "result": "\u5728CIFAR-10\u548cCIFAR-100\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u56fa\u5b9aepsilon\u57fa\u7ebf\u548c\u5148\u524d\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u63d0\u9ad8\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6807\u51c6\u51c6\u786e\u7387\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6270\u52a8\u9884\u7b97\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "summary_zh": "\u5bf9\u6297\u8bad\u7ec3\u662f\u9632\u5fa1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u514d\u53d7\u5bf9\u6297\u6837\u672c\u653b\u51fb\u7684\u6700\u6709\u6548\u7b56\u7565\u4e4b\u4e00\u3002\u73b0\u6709\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u7684\u4e00\u4e2a\u5173\u952e\u9650\u5236\u5728\u4e8e\u5b83\u4eec\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u6270\u52a8\u9884\u7b97\uff0c\u8fd9\u65e0\u6cd5\u89e3\u91ca\u7279\u5b9a\u5b9e\u4f8b\u7684\u9c81\u68d2\u6027\u7279\u5f81\u3002\u867d\u7136\u8bf8\u5982IAAT\u548cMMA\u4e4b\u7c7b\u7684\u5de5\u4f5c\u5f15\u5165\u4e86\u5b9e\u4f8b\u7ea7\u522b\u7684\u81ea\u9002\u5e94\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u4e8e\u6570\u636e\u9c81\u68d2\u6027\u7684\u542f\u53d1\u5f0f\u6216\u9759\u6001\u8fd1\u4f3c\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001Epsilon\u8c03\u5ea6\uff08DES\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u53ef\u4ee5\u81ea\u9002\u5e94\u5730\u8c03\u6574\u6bcf\u4e2a\u5b9e\u4f8b\u548c\u6bcf\u4e2a\u8bad\u7ec3\u8fed\u4ee3\u7684\u5bf9\u6297\u6270\u52a8\u9884\u7b97\u3002DES\u96c6\u6210\u4e86\u4e09\u4e2a\u5173\u952e\u56e0\u7d20\uff1a\uff081\uff09\u901a\u8fc7\u57fa\u4e8e\u68af\u5ea6\u7684\u4ee3\u7406\u8fd1\u4f3c\u7684\u5230\u51b3\u7b56\u8fb9\u754c\u7684\u8ddd\u79bb\uff0c\uff082\uff09\u4ecesoftmax\u71b5\u5f97\u51fa\u7684\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u4ee5\u53ca\uff083\uff09\u901a\u8fc7\u8499\u7279\u5361\u6d1bdropout\u4f30\u8ba1\u7684\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u7ebf\u7d22\u6574\u5408\u5230\u7edf\u4e00\u7684\u8c03\u5ea6\u7b56\u7565\u4e2d\uff0cDES\u53ef\u4ee5\u52a8\u6001\u5730\u8c03\u6574\u6270\u52a8\u9884\u7b97\uff0c\u4ee5\u6307\u5bfc\u66f4\u6709\u6548\u7684\u5bf9\u6297\u5b66\u4e60\u3002\u5728CIFAR-10\u548cCIFAR-100\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u56fa\u5b9aepsilon\u57fa\u7ebf\u548c\u5148\u524d\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6301\u7eed\u63d0\u9ad8\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6807\u51c6\u51c6\u786e\u7387\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u6709\u5173\u8c03\u5ea6\u7b56\u7565\u7684\u7a33\u5b9a\u6027\u548c\u6536\u655b\u6027\u7684\u7406\u8bba\u89c1\u89e3\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5b9e\u4f8b\u611f\u77e5\u3001\u6570\u636e\u9a71\u52a8\u7684\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u5f00\u8f9f\u4e86\u4e00\u6761\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.04277", "pdf": "https://arxiv.org/pdf/2506.04277", "abs": "https://arxiv.org/abs/2506.04277", "authors": ["Yi Lu", "Jiawang Cao", "Yongliang Wu", "Bozheng Li", "Licheng Tang", "Yangguang Ji", "Chong Wu", "Jay Wu", "Wenbo Zhu"], "title": "RSVP: Reasoning Segmentation via Visual Prompting and Multi-modal Chain-of-Thought", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted as ACL 2025 Main", "summary": "Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nreasoning capability while lack explicit mechanisms for visual grounding and\nsegmentation, creating a gap between cognitive reasoning and visual perception.\nTo bridge this gap, we introduce Reasoning Segmentation via Visual Prompting\n(RSVP), a novel framework that unifies multi-step multimodal reasoning with\ngrounded visual understanding. RSVP is a two-stage structuralized framework\nthat integrates reasoning-driven localization with segmentation refinement. In\nthe reasoning stage, RSVP employs multimodal chain-of-thought visual prompts to\nhelp MLLMs understand queries and infer targets, generating interpretable\nregion proposals that enhance visual grounding. In segmentation stage, RSVP\nrefines these proposals with a Vision-Language Segmentation Module (VLSM),\nseamlessly integrates textual and visual cues to produce precise segmentation\nmasks. By explicitly modelling the interaction between multimodal reasoning and\nsegmentation, RSVP introduces a new paradigm for interpretable reasoning\nsegmentation. It exploits MLLMs' inherent localization capabilities, enabling\nthe models to not only reason about objects but also generate structured visual\nrepresentations. Our extensive experiments demonstrate that RSVP achieves\nstate-of-the-art performance, surpasses state-of-the-art methods by up to +6.5\ngIoU and +9.2 cIoU on ReasonSeg, and achieves 49.7 mAP on SegInW under\nzero-shot settings. These results validate RSVP as an effective and scalable\nframework for integrating cognitive reasoning with structured visual\nunderstanding.", "AI": {"tldr": "RSVP\uff1a\u4e00\u4e2a\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u63a8\u7406\u4e0e\u5206\u5272\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u5206\u5272\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u663e\u5f0f\u7684\u89c6\u89c9\u57fa\u7840\u548c\u5206\u5272\u673a\u5236\uff0c\u8fd9\u5728\u8ba4\u77e5\u63a8\u7406\u548c\u89c6\u89c9\u611f\u77e5\u4e4b\u95f4\u9020\u6210\u4e86\u5dee\u8ddd\u3002", "method": "RSVP\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u63a8\u7406\u9a71\u52a8\u7684\u5b9a\u4f4d\u548c\u5206\u5272\u7ec6\u5316\u3002", "result": "RSVP\u5728ReasonSeg\u4e0a\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9ad8\u8fbe+6.5 gIoU\u548c+9.2 cIoU\uff0c\u5e76\u5728zero-shot\u8bbe\u7f6e\u4e0b\u5728SegInW\u4e0a\u5b9e\u73b0\u4e8649.7 mAP\u3002", "conclusion": "RSVP\u901a\u8fc7\u663e\u5f0f\u5730\u5efa\u6a21\u591a\u6a21\u6001\u63a8\u7406\u548c\u5206\u5272\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u5206\u5272\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u4f8b\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "summary_zh": "\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLM)\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u663e\u8457\u7684\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u663e\u5f0f\u7684\u89c6\u89c9\u57fa\u7840\u548c\u5206\u5272\u673a\u5236\uff0c\u8fd9\u5728\u8ba4\u77e5\u63a8\u7406\u548c\u89c6\u89c9\u611f\u77e5\u4e4b\u95f4\u9020\u6210\u4e86\u5dee\u8ddd\u3002\u4e3a\u4e86\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u901a\u8fc7\u89c6\u89c9\u63d0\u793a\u8fdb\u884c\u63a8\u7406\u5206\u5272(RSVP)\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u7edf\u4e00\u4e86\u591a\u6b65\u9aa4\u591a\u6a21\u6001\u63a8\u7406\u4e0e\u6709\u57fa\u7840\u7684\u89c6\u89c9\u7406\u89e3\u3002RSVP\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5b83\u96c6\u6210\u4e86\u63a8\u7406\u9a71\u52a8\u7684\u5b9a\u4f4d\u548c\u5206\u5272\u7ec6\u5316\u3002\u5728\u63a8\u7406\u9636\u6bb5\uff0cRSVP\u91c7\u7528\u591a\u6a21\u6001\u7684\u601d\u7ef4\u94fe\u89c6\u89c9\u63d0\u793a\uff0c\u4ee5\u5e2e\u52a9MLLM\u7406\u89e3\u67e5\u8be2\u548c\u63a8\u65ad\u76ee\u6807\uff0c\u751f\u6210\u53ef\u89e3\u91ca\u7684\u533a\u57df\u5efa\u8bae\uff0c\u4ece\u800c\u589e\u5f3a\u89c6\u89c9\u57fa\u7840\u3002\u5728\u5206\u5272\u9636\u6bb5\uff0cRSVP\u4f7f\u7528\u89c6\u89c9-\u8bed\u8a00\u5206\u5272\u6a21\u5757(VLSM)\u7ec6\u5316\u8fd9\u4e9b\u5efa\u8bae\uff0c\u65e0\u7f1d\u5730\u96c6\u6210\u6587\u672c\u548c\u89c6\u89c9\u7ebf\u7d22\uff0c\u4ee5\u751f\u6210\u7cbe\u786e\u7684\u5206\u5272\u63a9\u7801\u3002\u901a\u8fc7\u663e\u5f0f\u5730\u5efa\u6a21\u591a\u6a21\u6001\u63a8\u7406\u548c\u5206\u5272\u4e4b\u95f4\u7684\u4ea4\u4e92\uff0cRSVP\u4e3a\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u5206\u5272\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u4f8b\u3002\u5b83\u5229\u7528\u4e86MLLM\u56fa\u6709\u7684\u5b9a\u4f4d\u80fd\u529b\uff0c\u4f7f\u6a21\u578b\u4e0d\u4ec5\u80fd\u591f\u63a8\u7406\u5bf9\u8c61\uff0c\u8fd8\u80fd\u591f\u751f\u6210\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u8868\u793a\u3002\u6211\u4eec\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRSVP\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728ReasonSeg\u4e0a\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u9ad8\u8fbe+6.5 gIoU\u548c+9.2 cIoU\uff0c\u5e76\u5728zero-shot\u8bbe\u7f6e\u4e0b\u5728SegInW\u4e0a\u5b9e\u73b0\u4e8649.7 mAP\u3002\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86RSVP\u4f5c\u4e3a\u4e00\u4e2a\u6709\u6548\u7684\u548c\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u8ba4\u77e5\u63a8\u7406\u4e0e\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u7406\u89e3\u3002"}}
{"id": "2506.04280", "pdf": "https://arxiv.org/pdf/2506.04280", "abs": "https://arxiv.org/abs/2506.04280", "authors": ["Ziming Cheng", "Binrui Xu", "Lisheng Gong", "Zuhe Song", "Tianshuo Zhou", "Shiqi Zhong", "Siyu Ren", "Mingxiang Chen", "Xiangchao Meng", "Yuxin Zhang", "Yanlin Li", "Lei Ren", "Wei Chen", "Zhiyuan Huang", "Mingjie Zhan", "Xiaojie Wang", "Fangxiang Feng"], "title": "Evaluating MLLMs with Multimodal Multi-image Reasoning Benchmark", "categories": ["cs.CV", "cs.AI", "68T50", "I.2.7"], "comment": "18 pages", "summary": "With enhanced capabilities and widespread applications, Multimodal Large\nLanguage Models (MLLMs) are increasingly required to process and reason over\nmultiple images simultaneously. However, existing MLLM benchmarks focus either\non single-image visual reasoning or on multi-image understanding tasks with\nonly final-answer evaluation, leaving the reasoning capabilities of MLLMs over\nmulti-image inputs largely underexplored. To address this gap, we introduce the\n$\\textbf{Multimodal Multi-image Reasoning Benchmark (MMRB)}$, the first\nbenchmark designed to evaluate structured visual reasoning across multiple\nimages. MMRB comprises $\\textbf{92 sub-tasks}$ covering spatial, temporal, and\nsemantic reasoning, with multi-solution, CoT-style annotations generated by\nGPT-4o and refined by human experts. A derivative subset is designed to\nevaluate multimodal reward models in multi-image scenarios. To support fast and\nscalable evaluation, we propose a sentence-level matching framework using\nopen-source LLMs. Extensive baseline experiments on $\\textbf{40 MLLMs}$,\nincluding 9 reasoning-specific models and 8 reward models, demonstrate that\nopen-source MLLMs still lag significantly behind commercial MLLMs in\nmulti-image reasoning tasks. Furthermore, current multimodal reward models are\nnearly incapable of handling multi-image reward ranking tasks.", "AI": {"tldr": "MMRB\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u63a8\u7406\u80fd\u529b\u4e0a\u663e\u8457\u843d\u540e\u4e8e\u5546\u4e1a\u6a21\u578b\uff0c\u4e14\u73b0\u6709\u5956\u52b1\u6a21\u578b\u96be\u4ee5\u5904\u7406\u591a\u56fe\u5956\u52b1\u6392\u5e8f\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u56fe\u89c6\u89c9\u63a8\u7406\u6216\u4ec5\u6709\u6700\u7ec8\u7b54\u6848\u8bc4\u4f30\u7684\u591a\u56fe\u7406\u89e3\u4efb\u52a1\uff0c\u5bf9\u591a\u56fe\u8f93\u5165\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u591a\u56fe\u63a8\u7406\u57fa\u51c6\uff08MMRB\uff09\uff0c\u5305\u542b92\u4e2a\u5b50\u4efb\u52a1\uff0c\u6db5\u76d6\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u8bed\u4e49\u63a8\u7406\uff0c\u5e76\u4f7f\u7528GPT-4o\u751f\u6210\u5e76\u7531\u4eba\u5de5\u4e13\u5bb6\u6539\u8fdb\u7684\u591a\u89e3\u3001CoT\u98ce\u683c\u6ce8\u91ca\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u53e5\u5b50\u7ea7\u5339\u914d\u6846\u67b6\uff0c\u4f7f\u7528\u5f00\u6e90LLM\u6765\u652f\u6301\u5feb\u901f\u548c\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u3002", "result": "\u5bf940\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec9\u4e2a\u63a8\u7406\u4e13\u7528\u6a21\u578b\u548c8\u4e2a\u5956\u52b1\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u57fa\u7ebf\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u7136\u663e\u8457\u843d\u540e\u4e8e\u5546\u4e1a\u6a21\u578b\u3002\u6b64\u5916\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u51e0\u4e4e\u65e0\u6cd5\u5904\u7406\u591a\u56fe\u5956\u52b1\u6392\u5e8f\u4efb\u52a1\u3002", "conclusion": "\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u56fe\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u843d\u540e\u4e8e\u5546\u4e1a\u6a21\u578b\uff0c\u4e14\u5f53\u524d\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u51e0\u4e4e\u65e0\u6cd5\u5904\u7406\u591a\u56fe\u5956\u52b1\u6392\u5e8f\u4efb\u52a1\u3002", "summary_zh": "\u968f\u7740\u80fd\u529b\u7684\u589e\u5f3a\u548c\u5e94\u7528\u7684\u666e\u53ca\uff0c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u8d8a\u6765\u8d8a\u9700\u8981\u5728\u591a\u4e2a\u56fe\u50cf\u4e0a\u8fdb\u884c\u5904\u7406\u548c\u63a8\u7406\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684MLLM\u57fa\u51c6\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u56fe\u50cf\u89c6\u89c9\u63a8\u7406\u6216\u4ec5\u6709\u6700\u7ec8\u7b54\u6848\u8bc4\u4f30\u7684\u591a\u56fe\u50cf\u7406\u89e3\u4efb\u52a1\uff0c\u8fd9\u4f7f\u5f97MLLM\u5728\u591a\u56fe\u50cf\u8f93\u5165\u4e0a\u7684\u63a8\u7406\u80fd\u529b\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u672a\u88ab\u63a2\u7d22\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u51fa\u4e86\u591a\u6a21\u6001\u591a\u56fe\u50cf\u63a8\u7406\u57fa\u51c6\uff08MMRB\uff09\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u65e8\u5728\u8bc4\u4f30\u8de8\u591a\u4e2a\u56fe\u50cf\u7684\u7ed3\u6784\u5316\u89c6\u89c9\u63a8\u7406\u7684\u57fa\u51c6\u3002MMRB\u5305\u542b92\u4e2a\u5b50\u4efb\u52a1\uff0c\u6db5\u76d6\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u8bed\u4e49\u63a8\u7406\uff0c\u5177\u6709\u7531GPT-4o\u751f\u6210\u5e76\u7531\u4eba\u5de5\u4e13\u5bb6\u6539\u8fdb\u7684\u591a\u89e3\u3001CoT\u98ce\u683c\u7684\u6ce8\u91ca\u3002\u4e00\u4e2a\u884d\u751f\u7684\u5b50\u96c6\u65e8\u5728\u8bc4\u4f30\u591a\u56fe\u50cf\u573a\u666f\u4e2d\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u3002\u4e3a\u4e86\u652f\u6301\u5feb\u901f\u548c\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7528\u5f00\u6e90LLM\u7684\u53e5\u5b50\u7ea7\u5339\u914d\u6846\u67b6\u3002\u5bf940\u4e2aMLLM\uff08\u5305\u62ec9\u4e2a\u63a8\u7406\u4e13\u7528\u6a21\u578b\u548c8\u4e2a\u5956\u52b1\u6a21\u578b\uff09\u7684\u5e7f\u6cdb\u57fa\u7ebf\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u591a\u56fe\u50cf\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5f00\u6e90MLLM\u4ecd\u7136\u663e\u8457\u843d\u540e\u4e8e\u5546\u4e1aMLLM\u3002\u6b64\u5916\uff0c\u5f53\u524d\u7684\u591a\u6a21\u6001\u5956\u52b1\u6a21\u578b\u51e0\u4e4e\u65e0\u6cd5\u5904\u7406\u591a\u56fe\u50cf\u5956\u52b1\u6392\u5e8f\u4efb\u52a1\u3002"}}
{"id": "2506.04351", "pdf": "https://arxiv.org/pdf/2506.04351", "abs": "https://arxiv.org/abs/2506.04351", "authors": ["Maksym Ivashechkin", "Oscar Mendez", "Richard Bowden"], "title": "HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D human generation is an important problem with a wide range of applications\nin computer vision and graphics. Despite recent progress in generative AI such\nas diffusion models or rendering methods like Neural Radiance Fields or\nGaussian Splatting, controlling the generation of accurate 3D humans from text\nprompts remains an open challenge. Current methods struggle with fine detail,\naccurate rendering of hands and faces, human realism, and controlability over\nappearance. The lack of diversity, realism, and annotation in human image data\nalso remains a challenge, hindering the development of a foundational 3D human\nmodel. We present a weakly supervised pipeline that tries to address these\nchallenges. In the first step, we generate a photorealistic human image dataset\nwith controllable attributes such as appearance, race, gender, etc using a\nstate-of-the-art image diffusion model. Next, we propose an efficient mapping\napproach from image features to 3D point clouds using a transformer-based\narchitecture. Finally, we close the loop by training a point-cloud diffusion\nmodel that is conditioned on the same text prompts used to generate the\noriginal samples. We demonstrate orders-of-magnitude speed-ups in 3D human\ngeneration compared to the state-of-the-art approaches, along with\nsignificantly improved text-prompt alignment, realism, and rendering quality.\nWe will make the code and dataset available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u53ef\u63a7\u76843D\u4eba\u4f53\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5728\u751f\u6210\u7cbe\u786e\u76843D\u4eba\u4f53\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5305\u62ec\u7ec6\u8282\u4e0d\u8db3\u3001\u624b\u548c\u9762\u90e8\u6e32\u67d3\u4e0d\u51c6\u786e\u3001\u4eba\u4f53\u903c\u771f\u5ea6\u4e0d\u8db3\u4ee5\u53ca\u5916\u89c2\u63a7\u5236\u6027\u5dee\u3002\u6b64\u5916\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u3001\u903c\u771f\u5ea6\u548c\u6807\u6ce8\u7684\u4eba\u4f53\u56fe\u50cf\u6570\u636e\u4e5f\u963b\u788d\u4e863D\u4eba\u4f53\u6a21\u578b\u7684\u53d1\u5c55\u3002", "method": "\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a1) \u4f7f\u7528\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u4e14\u53ef\u63a7\u7684\u4eba\u4f53\u56fe\u50cf\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u7684\u56fe\u50cf\u7279\u5f81\u52303D\u70b9\u4e91\u7684\u6620\u5c04\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8eTransformer\u67b6\u6784\uff1b3) \u8bad\u7ec3\u4e00\u4e2a\u4ee5\u6587\u672c\u63d0\u793a\u4e3a\u6761\u4ef6\u7684\u70b9\u4e91\u6269\u6563\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u57283D\u4eba\u4f53\u751f\u6210\u901f\u5ea6\u4e0a\u63d0\u9ad8\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u4e14\u5728\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u3001\u903c\u771f\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u7684\u4eba\u4f53\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5c06\u56fe\u50cf\u7279\u5f81\u6620\u5c04\u52303D\u70b9\u4e91\uff0c\u6700\u540e\u8bad\u7ec3\u4e00\u4e2a\u4ee5\u6587\u672c\u63d0\u793a\u4e3a\u6761\u4ef6\u7684\u70b9\u4e91\u6269\u6563\u6a21\u578b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u5feb\u901f\u3001\u66f4\u9ad8\u8d28\u91cf\u76843D\u4eba\u4f53\u751f\u6210\u3002", "summary_zh": "\u4e09\u7ef4\u4eba\u4f53\u751f\u6210\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u56fe\u5f62\u5b66\u4e2d\u4e00\u4e2a\u91cd\u8981\u7684\u95ee\u9898\uff0c\u6709\u7740\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u5c3d\u7ba1\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08\u5982\u6269\u6563\u6a21\u578b\uff09\u6216\u795e\u7ecf\u8f90\u5c04\u573a\u6216\u9ad8\u65af\u6e85\u5c04\u7b49\u6e32\u67d3\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u65b0\u8fdb\u5c55\uff0c\u4f46\u4ece\u6587\u672c\u63d0\u793a\u63a7\u5236\u7cbe\u786e\u7684\u4e09\u7ef4\u4eba\u4f53\u7684\u751f\u6210\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002\u76ee\u524d\u7684\u65b9\u6cd5\u5728\u7cbe\u7ec6\u7ec6\u8282\u3001\u624b\u548c\u9762\u90e8\u7684\u7cbe\u786e\u6e32\u67d3\u3001\u4eba\u4f53\u903c\u771f\u5ea6\u4ee5\u53ca\u5916\u89c2\u7684\u53ef\u63a7\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u7f3a\u4e4f\u591a\u6837\u6027\u3001\u903c\u771f\u5ea6\u548c\u6807\u6ce8\u7684\u4eba\u4f53\u56fe\u50cf\u6570\u636e\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u963b\u788d\u4e86\u57fa\u7840\u4e09\u7ef4\u4eba\u4f53\u6a21\u578b\u7684\u53d1\u5c55\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5f31\u76d1\u7763\u6d41\u6c34\u7ebf\uff0c\u8bd5\u56fe\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002\u7b2c\u4e00\u6b65\uff0c\u6211\u4eec\u4f7f\u7528\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u751f\u6210\u5177\u6709\u53ef\u63a7\u5c5e\u6027\uff08\u5982\u5916\u89c2\u3001\u79cd\u65cf\u3001\u6027\u522b\u7b49\uff09\u7684\u903c\u771f\u4eba\u4f53\u56fe\u50cf\u6570\u636e\u96c6\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4ece\u56fe\u50cf\u7279\u5f81\u5230\u4e09\u7ef4\u70b9\u4e91\u7684\u6620\u5c04\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u4ee5\u7528\u4e8e\u751f\u6210\u539f\u59cb\u6837\u672c\u7684\u76f8\u540c\u6587\u672c\u63d0\u793a\u4e3a\u6761\u4ef6\u7684\u70b9\u4e91\u6269\u6563\u6a21\u578b\u6765\u95ed\u73af\u3002\u6211\u4eec\u8bc1\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4e09\u7ef4\u4eba\u4f53\u751f\u6210\u7684\u901f\u5ea6\u63d0\u9ad8\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u4e14\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u3001\u903c\u771f\u5ea6\u548c\u6e32\u67d3\u8d28\u91cf\u4e5f\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\u3002\u6211\u4eec\u5c06\u63d0\u4f9b\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2506.04353", "pdf": "https://arxiv.org/pdf/2506.04353", "abs": "https://arxiv.org/abs/2506.04353", "authors": ["Ankit Pal", "Jung-Oh Lee", "Xiaoman Zhang", "Malaikannan Sankarasubbu", "Seunghyeon Roh", "Won Jung Kim", "Meesun Lee", "Pranav Rajpurkar"], "title": "ReXVQA: A Large-scale Visual Question Answering Benchmark for Generalist Chest X-ray Understanding", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.CL", "cs.LG"], "comment": null, "summary": "We present ReXVQA, the largest and most comprehensive benchmark for visual\nquestion answering (VQA) in chest radiology, comprising approximately 696,000\nquestions paired with 160,000 chest X-rays studies across training, validation,\nand test sets. Unlike prior efforts that rely heavily on template based\nqueries, ReXVQA introduces a diverse and clinically authentic task suite\nreflecting five core radiological reasoning skills: presence assessment,\nlocation analysis, negation detection, differential diagnosis, and geometric\nreasoning. We evaluate eight state-of-the-art multimodal large language models,\nincluding MedGemma-4B-it, Qwen2.5-VL, Janus-Pro-7B, and Eagle2-9B. The\nbest-performing model (MedGemma) achieves 83.24% overall accuracy. To bridge\nthe gap between AI performance and clinical expertise, we conducted a\ncomprehensive human reader study involving 3 radiology residents on 200\nrandomly sampled cases. Our evaluation demonstrates that MedGemma achieved\nsuperior performance (83.84% accuracy) compared to human readers (best\nradiology resident: 77.27%), representing a significant milestone where AI\nperformance exceeds expert human evaluation on chest X-ray interpretation. The\nreader study reveals distinct performance patterns between AI models and human\nexperts, with strong inter-reader agreement among radiologists while showing\nmore variable agreement patterns between human readers and AI models. ReXVQA\nestablishes a new standard for evaluating generalist radiological AI systems,\noffering public leaderboards, fine-grained evaluation splits, structured\nexplanations, and category-level breakdowns. This benchmark lays the foundation\nfor next-generation AI systems capable of mimicking expert-level clinical\nreasoning beyond narrow pathology classification. Our dataset will be\nopen-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXVQA", "AI": {"tldr": "ReXVQA\u662f\u4e00\u4e2a\u65b0\u7684\u80f8\u90e8X\u5149\u7247\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0cAI\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u8d85\u8d8a\u4e86\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u73b0\u6709\u7684VQA\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6a21\u677f\u67e5\u8be2\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u4e34\u5e8a\u771f\u5b9e\u6027\u3002", "method": "\u63d0\u51fa\u4e86ReXVQA\uff0c\u4e00\u4e2a\u5305\u542b\u7ea6696,000\u4e2a\u95ee\u9898\u548c160,000\u5f20\u80f8\u90e8X\u5149\u7247\u7814\u7a76\u7684\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u3002", "result": "MedGemma\u6a21\u578b\u53d6\u5f97\u4e8683.24%\u7684\u603b\u4f53\u51c6\u786e\u7387\uff0c\u8d85\u8fc7\u4e86\u653e\u5c04\u79d1\u4f4f\u9662\u533b\u5e08\u768477.27%\u3002", "conclusion": "AI\u6a21\u578b\u5728\u80f8\u90e8X\u5149\u7247\u5224\u8bfb\u65b9\u9762\u8d85\u8d8a\u4e86\u4eba\u7c7b\u4e13\u5bb6\uff0c\u4f46AI\u6a21\u578b\u548c\u4eba\u7c7b\u4e13\u5bb6\u4e4b\u95f4\u5b58\u5728\u4e0d\u540c\u7684\u8868\u73b0\u6a21\u5f0f\u3002", "summary_zh": "\u6211\u4eec\u63d0\u51fa\u4e86ReXVQA\uff0c\u8fd9\u662f\u80f8\u90e8\u653e\u5c04\u5b66\u4e2d\u6700\u5927\u3001\u6700\u5168\u9762\u7684\u89c6\u89c9\u95ee\u7b54\uff08VQA\uff09\u57fa\u51c6\uff0c\u5305\u542b\u7ea6696,000\u4e2a\u95ee\u9898\uff0c\u5e76\u914d\u6709\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u96c6\u4e2d160,000\u4e2a\u80f8\u90e8X\u5149\u7247\u7814\u7a76\u3002\u4e0e\u5148\u524d\u4e25\u91cd\u4f9d\u8d56\u57fa\u4e8e\u6a21\u677f\u7684\u67e5\u8be2\u7684\u5de5\u4f5c\u4e0d\u540c\uff0cReXVQA\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u4e14\u4e34\u5e8a\u771f\u5b9e\u7684Task\u5957\u4ef6\uff0c\u53cd\u6620\u4e86\u4e94\u4e2a\u6838\u5fc3\u653e\u5c04\u5b66\u63a8\u7406\u6280\u80fd\uff1a\u5b58\u5728\u8bc4\u4f30\u3001\u4f4d\u7f6e\u5206\u6790\u3001\u5426\u5b9a\u68c0\u6d4b\u3001\u9274\u522b\u8bca\u65ad\u548c\u51e0\u4f55\u63a8\u7406\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u516b\u4e2a\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ecMedGemma-4B-it\u3001Qwen2.5-VL\u3001Janus-Pro-7B\u548cEagle2-9B\u3002\u6027\u80fd\u6700\u4f73\u7684\u6a21\u578b\uff08MedGemma\uff09\u5b9e\u73b0\u4e8683.24%\u7684\u603b\u4f53\u51c6\u786e\u7387\u3002\u4e3a\u4e86\u5f25\u5408AI\u6027\u80fd\u548c\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6211\u4eec\u5bf9200\u4e2a\u968f\u673a\u62bd\u6837\u7684\u75c5\u4f8b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u4eba\u7c7b\u8bfb\u8005\u7814\u7a76\uff0c\u6d89\u53ca3\u540d\u653e\u5c04\u79d1\u4f4f\u9662\u533b\u5e08\u3002\u6211\u4eec\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u4eba\u7c7b\u8bfb\u8005\uff08\u6700\u4f73\u653e\u5c04\u79d1\u4f4f\u9662\u533b\u5e08\uff1a77.27%\uff09\u76f8\u6bd4\uff0cMedGemma\u53d6\u5f97\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0883.84%\u7684\u51c6\u786e\u7387\uff09\uff0c\u8fd9\u4ee3\u8868\u7740\u4e00\u4e2a\u91cd\u8981\u7684\u91cc\u7a0b\u7891\uff0c\u5373AI\u6027\u80fd\u8d85\u8fc7\u4e86\u80f8\u90e8X\u5149\u7247\u89e3\u8bfb\u65b9\u9762\u7684\u4e13\u5bb6\u4eba\u7c7b\u8bc4\u4f30\u3002\u8bfb\u8005\u7814\u7a76\u63ed\u793a\u4e86AI\u6a21\u578b\u548c\u4eba\u7c7b\u4e13\u5bb6\u4e4b\u95f4\u4e0d\u540c\u7684\u6027\u80fd\u6a21\u5f0f\uff0c\u653e\u5c04\u79d1\u533b\u751f\u4e4b\u95f4\u5177\u6709\u5f88\u5f3a\u7684\u8bfb\u8005\u95f4\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u663e\u793a\u4e86\u4eba\u7c7b\u8bfb\u8005\u548cAI\u6a21\u578b\u4e4b\u95f4\u66f4\u53ef\u53d8\u7684\u4e00\u81f4\u6027\u6a21\u5f0f\u3002ReXVQA\u4e3a\u8bc4\u4f30\u901a\u7528\u653e\u5c04AI\u7cfb\u7edf\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u6807\u51c6\uff0c\u63d0\u4f9b\u516c\u5171\u6392\u884c\u699c\u3001\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u62c6\u5206\u3001\u7ed3\u6784\u5316\u89e3\u91ca\u548c\u7c7b\u522b\u7ea7\u522b\u7ec6\u5206\u3002\u8be5\u57fa\u51c6\u4e3a\u4e0b\u4e00\u4ee3AI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u591f\u6a21\u4eff\u8d85\u51fa\u72ed\u7a84\u75c5\u7406\u5206\u7c7b\u7684\u4e13\u5bb6\u7ea7\u4e34\u5e8a\u63a8\u7406\u3002\u6211\u4eec\u7684\u6570\u636e\u96c6\u5c06\u5728https://huggingface.co/datasets/rajpurkarlab/ReXVQA\u4e0a\u5f00\u6e90\u3002"}}
{"id": "2506.04363", "pdf": "https://arxiv.org/pdf/2506.04363", "abs": "https://arxiv.org/abs/2506.04363", "authors": ["Delong Chen", "Willy Chung", "Yejin Bang", "Ziwei Ji", "Pascale Fung"], "title": "WorldPrediction: A Benchmark for High-level World Modeling and Long-horizon Procedural Planning", "categories": ["cs.CV"], "comment": null, "summary": "Humans are known to have an internal \"world model\" that enables us to carry\nout action planning based on world states. AI agents need to have such a world\nmodel for action planning as well. It is not clear how current AI models,\nespecially generative models, are able to learn such world models and carry out\nprocedural planning in diverse environments. We introduce WorldPrediction, a\nvideo-based benchmark for evaluating world modeling and procedural planning\ncapabilities of different AI models. In contrast to prior benchmarks that focus\nprimarily on low-level world modeling and robotic motion planning,\nWorldPrediction is the first benchmark that emphasizes actions with temporal\nand semantic abstraction. Given initial and final world states, the task is to\ndistinguish the proper action (WorldPrediction-WM) or the properly ordered\nsequence of actions (WorldPrediction-PP) from a set of counterfactual\ndistractors. This discriminative task setup enable us to evaluate different\ntypes of world models and planners and realize a thorough comparison across\ndifferent hypothesis. The benchmark represents states and actions using visual\nobservations. In order to prevent models from exploiting low-level continuity\ncues in background scenes, we provide \"action equivalents\" - identical actions\nobserved in different contexts - as candidates for selection. This benchmark is\ngrounded in a formal framework of partially observable semi-MDP, ensuring\nbetter reliability and robustness of the evaluation. We conduct extensive human\nfiltering and validation on our benchmark and show that current frontier models\nbarely achieve 57% accuracy on WorldPrediction-WM and 38% on WorldPrediction-PP\nwhereas humans are able to solve both tasks perfectly.", "AI": {"tldr": "WorldPrediction\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u6a21\u578b\u7684\u4e16\u754c\u5efa\u6a21\u548c\u7a0b\u5e8f\u89c4\u5212\u80fd\u529b\uff0c\u7ed3\u679c\u8868\u660e\u5f53\u524d\u6a21\u578b\u4e0e\u4eba\u7c7b\u6c34\u5e73\u5dee\u8ddd\u5f88\u5927\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\uff0c\u7279\u522b\u662f\u751f\u6210\u6a21\u578b\uff0c\u5982\u4f55\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u5e76\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u7a0b\u5e8f\u89c4\u5212\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5f15\u5165WorldPrediction\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540cAI\u6a21\u578b\u7684\u4e16\u754c\u5efa\u6a21\u548c\u7a0b\u5e8f\u89c4\u5212\u80fd\u529b\u3002", "result": "\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u5728WorldPrediction-WM\u4e0a\u4ec5\u8fbe\u523057%\u7684\u51c6\u786e\u7387\uff0c\u5728WorldPrediction-PP\u4e0a\u4ec5\u8fbe\u523038%\uff0c\u800c\u4eba\u7c7b\u53ef\u4ee5\u5b8c\u7f8e\u89e3\u51b3\u8fd9\u4e24\u9879\u4efb\u52a1\u3002", "conclusion": "\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u5728WorldPrediction-WM\u4e0a\u4ec5\u8fbe\u523057%\u7684\u51c6\u786e\u7387\uff0c\u5728WorldPrediction-PP\u4e0a\u4ec5\u8fbe\u523038%\uff0c\u800c\u4eba\u7c7b\u53ef\u4ee5\u5b8c\u7f8e\u89e3\u51b3\u8fd9\u4e24\u9879\u4efb\u52a1\u3002", "summary_zh": "\u4eba\u7c7b\u62e5\u6709\u4e00\u4e2a\u5185\u5728\u7684\u201c\u4e16\u754c\u6a21\u578b\u201d\uff0c\u4f7f\u6211\u4eec\u80fd\u591f\u6839\u636e\u4e16\u754c\u72b6\u6001\u8fdb\u884c\u884c\u52a8\u89c4\u5212\u3002\u4eba\u5de5\u667a\u80fd\u4f53\u4e5f\u9700\u8981\u62e5\u6709\u8fd9\u6837\u7684\u4e16\u754c\u6a21\u578b\u6765\u8fdb\u884c\u884c\u52a8\u89c4\u5212\u3002\u76ee\u524d\u7684\u4eba\u5de5\u667a\u80fd\u6a21\u578b\uff0c\u7279\u522b\u662f\u751f\u6210\u6a21\u578b\uff0c\u5982\u4f55\u5b66\u4e60\u8fd9\u79cd\u4e16\u754c\u6a21\u578b\u5e76\u5728\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u7a0b\u5e8f\u89c4\u5212\u5c1a\u4e0d\u6e05\u695a\u3002\u6211\u4eec\u5f15\u5165\u4e86WorldPrediction\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u89c6\u9891\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540cAI\u6a21\u578b\u7684\u4e16\u754c\u5efa\u6a21\u548c\u7a0b\u5e8f\u89c4\u5212\u80fd\u529b\u3002\u4e0e\u4e4b\u524d\u4e3b\u8981\u5173\u6ce8\u4f4e\u7ea7\u4e16\u754c\u5efa\u6a21\u548c\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u57fa\u51c6\u76f8\u6bd4\uff0cWorldPrediction\u662f\u7b2c\u4e00\u4e2a\u5f3a\u8c03\u5177\u6709\u65f6\u95f4\u548c\u8bed\u4e49\u62bd\u8c61\u7684\u52a8\u4f5c\u7684\u57fa\u51c6\u3002\u7ed9\u5b9a\u521d\u59cb\u548c\u6700\u7ec8\u7684\u4e16\u754c\u72b6\u6001\uff0c\u4efb\u52a1\u662f\u4ece\u4e00\u7ec4\u53cd\u4e8b\u5b9e\u7684\u5e72\u6270\u9879\u4e2d\u533a\u5206\u51fa\u6b63\u786e\u7684\u52a8\u4f5c\uff08WorldPrediction-WM\uff09\u6216\u6b63\u786e\u6392\u5e8f\u7684\u52a8\u4f5c\u5e8f\u5217\uff08WorldPrediction-PP\uff09\u3002\u8fd9\u79cd\u5224\u522b\u6027\u4efb\u52a1\u8bbe\u7f6e\u4f7f\u6211\u4eec\u80fd\u591f\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684\u4e16\u754c\u6a21\u578b\u548c\u89c4\u5212\u5668\uff0c\u5e76\u5b9e\u73b0\u5bf9\u4e0d\u540c\u5047\u8bbe\u7684\u5168\u9762\u6bd4\u8f83\u3002\u8be5\u57fa\u51c6\u4f7f\u7528\u89c6\u89c9\u89c2\u5bdf\u6765\u8868\u793a\u72b6\u6001\u548c\u52a8\u4f5c\u3002\u4e3a\u4e86\u9632\u6b62\u6a21\u578b\u5229\u7528\u80cc\u666f\u573a\u666f\u4e2d\u7684\u4f4e\u7ea7\u8fde\u7eed\u6027\u7ebf\u7d22\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u201c\u52a8\u4f5c\u7b49\u4ef7\u7269\u201d\u2014\u2014\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u89c2\u5bdf\u5230\u7684\u76f8\u540c\u52a8\u4f5c\u2014\u2014\u4f5c\u4e3a\u9009\u62e9\u7684\u5019\u9009\u5bf9\u8c61\u3002\u8be5\u57fa\u51c6\u57fa\u4e8e\u90e8\u5206\u53ef\u89c2\u5bdf\u534aMDP\u7684\u5f62\u5f0f\u6846\u67b6\uff0c\u786e\u4fdd\u4e86\u8bc4\u4f30\u7684\u66f4\u597d\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002\u6211\u4eec\u5bf9\u57fa\u51c6\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u4eba\u5de5\u8fc7\u6ee4\u548c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u524d\u6cbf\u6a21\u578b\u5728WorldPrediction-WM\u4e0a\u7684\u51c6\u786e\u7387\u4ec5\u4e3a57%\uff0c\u5728WorldPrediction-PP\u4e0a\u7684\u51c6\u786e\u7387\u4ec5\u4e3a38%\uff0c\u800c\u4eba\u7c7b\u80fd\u591f\u5b8c\u7f8e\u5730\u89e3\u51b3\u8fd9\u4e24\u9879\u4efb\u52a1\u3002"}}
{"id": "2506.04365", "pdf": "https://arxiv.org/pdf/2506.04365", "abs": "https://arxiv.org/abs/2506.04365", "authors": ["Liam Salass", "Jerrin Bright", "Amir Nazemi", "Yuhao Chen", "John Zelek", "David Clausi"], "title": "Ice Hockey Puck Localization Using Contextual Cues", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Puck detection in ice hockey broadcast videos poses significant challenges\ndue to the puck's small size, frequent occlusions, motion blur, broadcast\nartifacts, and scale inconsistencies due to varying camera zoom and broadcast\ncamera viewpoints. Prior works focus on appearance-based or motion-based cues\nof the puck without explicitly modelling the cues derived from player\nbehaviour. Players consistently turn their bodies and direct their gaze toward\nthe puck. Motivated by this strong contextual cue, we propose Puck Localization\nUsing Contextual Cues (PLUCC), a novel approach for scale-aware and\ncontext-driven single-frame puck detections. PLUCC consists of three\ncomponents: (a) a contextual encoder, which utilizes player orientations and\npositioning as helpful priors; (b) a feature pyramid encoder, which extracts\nmultiscale features from the dual encoders; and (c) a gating decoder that\ncombines latent features with a channel gating mechanism. For evaluation, in\naddition to standard average precision, we propose Rink Space Localization\nError (RSLE), a scale-invariant homography-based metric for removing\nperspective bias from rink space evaluation. The experimental results of PLUCC\non the PuckDataset dataset demonstrated state-of-the-art detection performance,\nsurpassing previous baseline methods by an average precision improvement of\n12.2% and RSLE average precision of 25%. Our research demonstrates the critical\nrole of contextual understanding in improving puck detection performance, with\nbroad implications for automated sports analysis.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7403\u5458\u884c\u4e3a\u4e0a\u4e0b\u6587\u7ebf\u7d22\u8fdb\u884c\u51b0\u7403\u68c0\u6d4b\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5728PuckDataset\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u51b0\u7403\u4f53\u79ef\u5c0f\u3001\u9891\u7e41\u906e\u6321\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u5e7f\u64ad\u4f2a\u5f71\u4ee5\u53ca\u56e0\u76f8\u673a\u53d8\u7126\u548c\u5e7f\u64ad\u76f8\u673a\u89c6\u89d2\u53d8\u5316\u800c\u5bfc\u81f4\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u51b0\u7403\u5e7f\u64ad\u89c6\u9891\u4e2d\u7684\u51b0\u7403\u68c0\u6d4b\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u4fa7\u91cd\u4e8e\u57fa\u4e8e\u5916\u89c2\u6216\u57fa\u4e8e\u8fd0\u52a8\u7684\u51b0\u7403\u7ebf\u7d22\uff0c\u800c\u6ca1\u6709\u660e\u786e\u5730\u5bf9\u6765\u81ea\u7403\u5458\u884c\u4e3a\u7684\u7ebf\u7d22\u8fdb\u884c\u5efa\u6a21\u3002\u7403\u5458\u4eec\u4f1a\u6301\u7eed\u5730\u8f6c\u52a8\u8eab\u4f53\uff0c\u5e76\u5c06\u89c6\u7ebf \u043d\u0430\u043f\u0440\u0430\u0432 \u043a \u0448\u0430\u0439\u0431\u0435.", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPLUCC\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e0a\u4e0b\u6587\u7ebf\u7d22\u8fdb\u884c\u5c3a\u5ea6\u611f\u77e5\u7684\u5355\u5e27\u51b0\u7403\u68c0\u6d4b\u3002PLUCC\u5305\u62ec\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\uff1a(a) \u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff0c\u5b83\u5229\u7528\u7403\u5458\u7684\u671d\u5411\u548c\u4f4d\u7f6e\u4f5c\u4e3a\u6709\u7528\u7684\u5148\u9a8c\u77e5\u8bc6\uff1b(b) \u7279\u5f81\u91d1\u5b57\u5854\u7f16\u7801\u5668\uff0c\u5b83\u4ece\u53cc\u7f16\u7801\u5668\u4e2d\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff1b(c) \u95e8\u63a7\u89e3\u7801\u5668\uff0c\u5b83\u7ed3\u5408\u4e86\u6f5c\u5728\u7279\u5f81\u548c\u901a\u9053\u95e8\u63a7\u673a\u5236\u3002", "result": "\u5728PuckDataset\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684PLUCC\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u68c0\u6d4b\u6027\u80fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad8\u4e8612.2%\uff0cRSLE\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad8\u4e8625%\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u7406\u89e3\u5728\u63d0\u9ad8\u51b0\u7403\u68c0\u6d4b\u6027\u80fd\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5bf9\u81ea\u52a8\u5316\u4f53\u80b2\u5206\u6790\u5177\u6709\u5e7f\u6cdb\u7684\u5f71\u54cd\u3002", "summary_zh": "\u51b0\u7403\u5e7f\u64ad\u89c6\u9891\u4e2d\u7684\u51b0\u7403\u68c0\u6d4b\u7531\u4e8e\u51b0\u7403\u4f53\u79ef\u5c0f\u3001\u9891\u7e41\u906e\u6321\u3001\u8fd0\u52a8\u6a21\u7cca\u3001\u5e7f\u64ad\u4f2a\u5f71\u4ee5\u53ca\u56e0\u76f8\u673a\u53d8\u7126\u548c\u5e7f\u64ad\u76f8\u673a\u89c6\u89d2\u53d8\u5316\u800c\u5bfc\u81f4\u7684\u4e0d\u4e00\u81f4\u6027\u800c\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002\u4ee5\u5f80\u7684\u7814\u7a76\u4fa7\u91cd\u4e8e\u57fa\u4e8e\u5916\u89c2\u6216\u57fa\u4e8e\u8fd0\u52a8\u7684\u51b0\u7403\u7ebf\u7d22\uff0c\u800c\u6ca1\u6709\u660e\u786e\u5730\u5bf9\u6765\u81ea\u7403\u5458\u884c\u4e3a\u7684\u7ebf\u7d22\u8fdb\u884c\u5efa\u6a21\u3002\u53d7\u8fd9\u79cd\u5f3a\u70c8\u7684\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u63a8\u52a8\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPuck Localization Using Contextual Cues (PLUCC) \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7528\u4e8e\u8fdb\u884c\u5c3a\u5ea6\u611f\u77e5\u548c\u4e0a\u4e0b\u6587\u9a71\u52a8\u7684\u5355\u5e27\u51b0\u7403\u68c0\u6d4b\u3002PLUCC\u5305\u62ec\u4e09\u4e2a\u7ec4\u6210\u90e8\u5206\uff1a(a) \u4e0a\u4e0b\u6587\u7f16\u7801\u5668\uff0c\u5b83\u5229\u7528\u7403\u5458\u7684\u671d\u5411\u548c\u4f4d\u7f6e\u4f5c\u4e3a\u6709\u7528\u7684\u5148\u9a8c\u77e5\u8bc6\uff1b(b) \u7279\u5f81\u91d1\u5b57\u5854\u7f16\u7801\u5668\uff0c\u5b83\u4ece\u53cc\u7f16\u7801\u5668\u4e2d\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff1b(c) \u95e8\u63a7\u89e3\u7801\u5668\uff0c\u5b83\u7ed3\u5408\u4e86\u6f5c\u5728\u7279\u5f81\u548c\u901a\u9053\u95e8\u63a7\u673a\u5236\u3002\u4e3a\u4e86\u8fdb\u884c\u8bc4\u4f30\uff0c\u9664\u4e86\u6807\u51c6\u7684\u5e73\u5747\u7cbe\u5ea6\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u51b0\u573a\u7a7a\u95f4\u5b9a\u4f4d\u8bef\u5dee (RSLE)\uff0c\u8fd9\u662f\u4e00\u79cd\u5c3a\u5ea6\u4e0d\u53d8\u7684\u57fa\u4e8e\u5355\u5e94\u6027\u7684\u5ea6\u91cf\uff0c\u7528\u4e8e\u6d88\u9664\u51b0\u573a\u7a7a\u95f4\u8bc4\u4f30\u4e2d\u7684\u900f\u89c6\u504f\u5dee\u3002PLUCC\u5728PuckDataset\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u68c0\u6d4b\u6027\u80fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad8\u4e8612.2%\uff0cRSLE\u5e73\u5747\u7cbe\u5ea6\u63d0\u9ad8\u4e8625%\uff0c\u8d85\u8fc7\u4e86\u4e4b\u524d\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u6211\u4eec\u7684\u7814\u7a76\u8868\u660e\uff0c\u4e0a\u4e0b\u6587\u7406\u89e3\u5728\u63d0\u9ad8\u51b0\u7403\u68c0\u6d4b\u6027\u80fd\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5bf9\u81ea\u52a8\u5316\u4f53\u80b2\u5206\u6790\u5177\u6709\u5e7f\u6cdb\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.04367", "pdf": "https://arxiv.org/pdf/2506.04367", "abs": "https://arxiv.org/abs/2506.04367", "authors": ["Jubayer Ahmed Bhuiyan Shawon", "Hasan Mahmud", "Kamrul Hasan"], "title": "Fine-Tuning Video Transformers for Word-Level Bangla Sign Language: A Comparative Analysis for Classification Tasks", "categories": ["cs.CV"], "comment": "16 pages, 8 figures, 6 tables", "summary": "Sign Language Recognition (SLR) involves the automatic identification and\nclassification of sign gestures from images or video, converting them into text\nor speech to improve accessibility for the hearing-impaired community. In\nBangladesh, Bangla Sign Language (BdSL) serves as the primary mode of\ncommunication for many individuals with hearing impairments. This study\nfine-tunes state-of-the-art video transformer architectures -- VideoMAE, ViViT,\nand TimeSformer -- on BdSLW60 (arXiv:2402.08635), a small-scale BdSL dataset\nwith 60 frequent signs. We standardized the videos to 30 FPS, resulting in\n9,307 user trial clips. To evaluate scalability and robustness, the models were\nalso fine-tuned on BdSLW401 (arXiv:2503.02360), a large-scale dataset with 401\nsign classes. Additionally, we benchmark performance against public datasets,\nincluding LSA64 and WLASL. Data augmentation techniques such as random\ncropping, horizontal flipping, and short-side scaling were applied to improve\nmodel robustness. To ensure balanced evaluation across folds during model\nselection, we employed 10-fold stratified cross-validation on the training set,\nwhile signer-independent evaluation was carried out using held-out test data\nfrom unseen users U4 and U8. Results show that video transformer models\nsignificantly outperform traditional machine learning and deep learning\napproaches. Performance is influenced by factors such as dataset size, video\nquality, frame distribution, frame rate, and model architecture. Among the\nmodels, the VideoMAE variant (MCG-NJU/videomae-base-finetuned-kinetics)\nachieved the highest accuracies of 95.5% on the frame rate corrected BdSLW60\ndataset and 81.04% on the front-facing signs of BdSLW401 -- demonstrating\nstrong potential for scalable and accurate BdSL recognition.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5fae\u8c03\u89c6\u9891Transformer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b5f\u52a0\u62c9\u8bed\u624b\u8bed\u8bc6\u522b\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728VideoMAE\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u679c\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u624b\u8bed\uff08BdSL\uff09\u662f\u5b5f\u52a0\u62c9\u56fd\u542c\u969c\u4eba\u58eb\u7684\u4e3b\u8981\u4ea4\u6d41\u65b9\u5f0f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8BdSL\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5728BdSLW60\u548cBdSLW401\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e86VideoMAE\u3001ViViT\u548cTimeSformer\u7b49\u89c6\u9891Transformer\u67b6\u6784\uff0c\u5e76\u91c7\u7528\u4e86\u6570\u636e\u589e\u5f3a\u548c\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\u7b49\u6280\u672f\u3002", "result": "VideoMAE\u6a21\u578b\u5728\u4fee\u6b63\u5e27\u7387\u7684BdSLW60\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8695.5%\u7684\u51c6\u786e\u7387\uff0c\u5728BdSLW401\u6570\u636e\u96c6\u7684\u524d\u5411\u624b\u52bf\u4e0a\u8fbe\u5230\u4e8681.04%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u89c6\u9891Transformer\u6a21\u578b\u5728\u5b5f\u52a0\u62c9\u8bed\u624b\u8bed\u8bc6\u522b\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cVideoMAE\u5728BdSLW60\u548cBdSLW401\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\u3002", "summary_zh": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u8bc6\u522b\u548c\u5206\u7c7b\u56fe\u50cf\u6216\u89c6\u9891\u4e2d\u7684\u624b\u52bf\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u6587\u672c\u6216\u8bed\u97f3\uff0c\u4ece\u800c\u63d0\u9ad8\u542c\u969c\u4eba\u58eb\u7684\u53ef\u8bbf\u95ee\u6027\u3002\u5728\u5b5f\u52a0\u62c9\u56fd\uff0c\u5b5f\u52a0\u62c9\u8bed\u624b\u8bed\uff08BdSL\uff09\u662f\u8bb8\u591a\u542c\u969c\u4eba\u58eb\u7684\u4e3b\u8981\u4ea4\u6d41\u65b9\u5f0f\u3002\u672c\u7814\u7a76\u5728BdSLW60\uff08\u4e00\u4e2a\u5305\u542b60\u4e2a\u5e38\u7528\u624b\u52bf\u7684\u5c0f\u89c4\u6a21BdSL\u6570\u636e\u96c6\uff09\u4e0a\u5fae\u8c03\u4e86\u6700\u5148\u8fdb\u7684\u89c6\u9891Transformer\u67b6\u6784\u2014\u2014VideoMAE\u3001ViViT\u548cTimeSformer\u3002\u6211\u4eec\u5c06\u89c6\u9891\u6807\u51c6\u5316\u4e3a30 FPS\uff0c\u751f\u6210\u4e869307\u4e2a\u7528\u6237\u8bd5\u9a8c\u7247\u6bb5\u3002\u4e3a\u4e86\u8bc4\u4f30\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u8fd8\u5728\u5305\u542b401\u4e2a\u624b\u52bf\u7c7b\u522b\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6BdSLW401\u4e0a\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u9488\u5bf9\u5305\u62ecLSA64\u548cWLASL\u5728\u5185\u7684\u516c\u5171\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u5e94\u7528\u4e86\u968f\u673a\u88c1\u526a\u3001\u6c34\u5e73\u7ffb\u8f6c\u548c\u77ed\u8fb9\u7f29\u653e\u7b49\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u4e3a\u4e86\u786e\u4fdd\u6a21\u578b\u9009\u62e9\u8fc7\u7a0b\u4e2d\u5404\u6298\u53e0\u4e4b\u95f4\u7684\u5e73\u8861\u8bc4\u4f30\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u96c6\u4e0a\u91c7\u7528\u4e8610\u6298\u5206\u5c42\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u540c\u65f6\u4f7f\u7528\u6765\u81ea\u672a\u89c1\u8fc7\u7528\u6237U4\u548cU8\u7684\u4fdd\u7559\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u4e0e\u8bf4\u8bdd\u4eba\u65e0\u5173\u7684\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u9891Transformer\u6a21\u578b\u660e\u663e\u4f18\u4e8e\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002\u6027\u80fd\u53d7\u6570\u636e\u96c6\u5927\u5c0f\u3001\u89c6\u9891\u8d28\u91cf\u3001\u5e27\u5206\u5e03\u3001\u5e27\u7387\u548c\u6a21\u578b\u67b6\u6784\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u5728\u8fd9\u4e9b\u6a21\u578b\u4e2d\uff0cVideoMAE\u53d8\u4f53\u5728\u5e27\u7387\u6821\u6b63\u7684BdSLW60\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u4e8695.5%\u7684\u6700\u9ad8\u51c6\u786e\u7387\uff0c\u5728BdSLW401\u7684\u524d\u5411\u624b\u52bf\u4e0a\u83b7\u5f97\u4e8681.04%\u7684\u6700\u9ad8\u51c6\u786e\u7387\u2014\u2014\u8bc1\u660e\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684BdSL\u8bc6\u522b\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.04379", "pdf": "https://arxiv.org/pdf/2506.04379", "abs": "https://arxiv.org/abs/2506.04379", "authors": ["Matthew W. Shinkle", "Mark D. Lescroart"], "title": "Visualizing and Controlling Cortical Responses Using Voxel-Weighted Activation Maximization", "categories": ["cs.CV", "cs.AI", "q-bio.NC"], "comment": "Accepted to the Mechanistic Interpretability for Vision (MIV)\n  Workshop at the 2025 Conference on Computer Vision and Pattern Recognition\n  (CVPR) conference", "summary": "Deep neural networks (DNNs) trained on visual tasks develop feature\nrepresentations that resemble those in the human visual system. Although\nDNN-based encoding models can accurately predict brain responses to visual\nstimuli, they offer limited insight into the specific features driving these\nresponses. Here, we demonstrate that activation maximization -- a technique\ndesigned to interpret vision DNNs -- can be applied to DNN-based encoding\nmodels of the human brain. We extract and adaptively downsample activations\nfrom multiple layers of a pretrained Inception V3 network, then use linear\nregression to predict fMRI responses. This yields a full image-computable model\nof brain responses. Next, we apply activation maximization to generate images\noptimized for predicted responses in individual cortical voxels. We find that\nthese images contain visual characteristics that qualitatively correspond with\nknown selectivity and enable exploration of selectivity across the visual\ncortex. We further extend our method to whole regions of interest (ROIs) of the\nbrain and validate its efficacy by presenting these images to human\nparticipants in an fMRI study. We find that the generated images reliably drive\nactivity in targeted regions across both low- and high-level visual areas and\nacross subjects. These results demonstrate that activation maximization can be\nsuccessfully applied to DNN-based encoding models. By addressing key\nlimitations of alternative approaches that require natively generative models,\nour approach enables flexible characterization and modulation of responses\nacross the human visual system.", "AI": {"tldr": "\u6fc0\u6d3b\u6700\u5927\u5316\u53ef\u4ee5\u5e94\u7528\u4e8e\u57fa\u4e8eDNN\u7684\u7f16\u7801\u6a21\u578b\uff0c\u4ee5\u7075\u6d3b\u5730\u8868\u5f81\u548c\u8c03\u8282\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u53cd\u5e94\u3002", "motivation": "\u57fa\u4e8eDNN\u7684\u7f16\u7801\u6a21\u578b\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u5927\u8111\u5bf9\u89c6\u89c9\u523a\u6fc0\u7684\u53cd\u5e94\uff0c\u4f46\u5bf9\u9a71\u52a8\u8fd9\u4e9b\u53cd\u5e94\u7684\u5177\u4f53\u7279\u5f81\u7684\u6d1e\u5bdf\u6709\u9650\u3002", "method": "\u63d0\u53d6\u5e76\u81ea\u9002\u5e94\u5730\u964d\u91c7\u6837\u9884\u8bad\u7ec3\u7684Inception V3\u7f51\u7edc\u591a\u5c42\u7684\u6fc0\u6d3b\u503c\uff0c\u7136\u540e\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u9884\u6d4bfMRI\u53cd\u5e94\u3002\u518d\u5e94\u7528\u6fc0\u6d3b\u6700\u5927\u5316\u751f\u6210\u9488\u5bf9\u76ae\u5c42\u4f53\u7d20\u9884\u6d4b\u53cd\u5e94\u4f18\u5316\u7684\u56fe\u50cf\u3002", "result": "\u751f\u6210\u7684\u56fe\u50cf\u5305\u542b\u4e0e\u5df2\u77e5\u9009\u62e9\u6027\u5b9a\u6027\u5bf9\u5e94\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a2\u7d22\u6574\u4e2a\u89c6\u89c9\u76ae\u5c42\u7684\u9009\u62e9\u6027\u3002\u5728fMRI\u7814\u7a76\u4e2d\uff0c\u751f\u6210\u7684\u56fe\u50cf\u53ef\u9760\u5730\u9a71\u52a8\u4e86\u4f4e\u7ea7\u548c\u9ad8\u7ea7\u89c6\u89c9\u533a\u57df\u4ee5\u53ca\u4e0d\u540c\u53d7\u8bd5\u8005\u7684\u76ee\u6807\u533a\u57df\u7684\u6d3b\u52a8\u3002", "conclusion": "\u6fc0\u6d3b\u6700\u5927\u5316\u53ef\u4ee5\u6210\u529f\u5e94\u7528\u4e8e\u57fa\u4e8eDNN\u7684\u7f16\u7801\u6a21\u578b\uff0c\u514b\u670d\u4e86\u9700\u8981\u539f\u751f\u751f\u6210\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u53cd\u5e94\u7684\u7075\u6d3b\u8868\u5f81\u548c\u8c03\u8282\u3002", "summary_zh": "\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4f1a\u4ea7\u751f\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u7279\u5f81\u8868\u793a\u3002\u867d\u7136\u57fa\u4e8eDNN\u7684\u7f16\u7801\u6a21\u578b\u53ef\u4ee5\u51c6\u786e\u9884\u6d4b\u5927\u8111\u5bf9\u89c6\u89c9\u523a\u6fc0\u7684\u53cd\u5e94\uff0c\u4f46\u5b83\u4eec\u5bf9\u9a71\u52a8\u8fd9\u4e9b\u53cd\u5e94\u7684\u5177\u4f53\u7279\u5f81\u7684\u6d1e\u5bdf\u6709\u9650\u3002\u672c\u6587\u8bc1\u660e\uff0c\u6fc0\u6d3b\u6700\u5927\u5316\u2014\u2014\u4e00\u79cd\u65e8\u5728\u89e3\u91ca\u89c6\u89c9DNN\u7684\u6280\u672f\u2014\u2014\u53ef\u4ee5\u5e94\u7528\u4e8e\u57fa\u4e8eDNN\u7684\u4eba\u8111\u7f16\u7801\u6a21\u578b\u3002\u6211\u4eec\u63d0\u53d6\u5e76\u81ea\u9002\u5e94\u5730\u964d\u91c7\u6837\u9884\u8bad\u7ec3\u7684Inception V3\u7f51\u7edc\u591a\u5c42\u7684\u6fc0\u6d3b\u503c\uff0c\u7136\u540e\u4f7f\u7528\u7ebf\u6027\u56de\u5f52\u9884\u6d4bfMRI\u53cd\u5e94\u3002\u8fd9\u4ea7\u751f\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u5927\u8111\u53cd\u5e94\u56fe\u50cf\u8ba1\u7b97\u6a21\u578b\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5e94\u7528\u6fc0\u6d3b\u6700\u5927\u5316\u6765\u751f\u6210\u9488\u5bf9\u5355\u4e2a\u76ae\u5c42\u4f53\u7d20\u9884\u6d4b\u53cd\u5e94\u4f18\u5316\u7684\u56fe\u50cf\u3002\u6211\u4eec\u53d1\u73b0\u8fd9\u4e9b\u56fe\u50cf\u5305\u542b\u4e0e\u5df2\u77e5\u9009\u62e9\u6027\u5b9a\u6027\u5bf9\u5e94\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a2\u7d22\u6574\u4e2a\u89c6\u89c9\u76ae\u5c42\u7684\u9009\u62e9\u6027\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5c06\u6211\u4eec\u7684\u65b9\u6cd5\u6269\u5c55\u5230\u6574\u4e2a\u5927\u8111\u611f\u5174\u8da3\u533a\u57df\uff08ROI\uff09\uff0c\u5e76\u901a\u8fc7\u5728fMRI\u7814\u7a76\u4e2d\u5411\u4eba\u7c7b\u53c2\u4e0e\u8005\u5c55\u793a\u8fd9\u4e9b\u56fe\u50cf\u6765\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u751f\u6210\u7684\u56fe\u50cf\u53ef\u9760\u5730\u9a71\u52a8\u4e86\u4f4e\u7ea7\u548c\u9ad8\u7ea7\u89c6\u89c9\u533a\u57df\u4ee5\u53ca\u4e0d\u540c\u53d7\u8bd5\u8005\u7684\u76ee\u6807\u533a\u57df\u7684\u6d3b\u52a8\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u6fc0\u6d3b\u6700\u5927\u5316\u53ef\u4ee5\u6210\u529f\u5e94\u7528\u4e8e\u57fa\u4e8eDNN\u7684\u7f16\u7801\u6a21\u578b\u3002\u901a\u8fc7\u89e3\u51b3\u9700\u8981\u539f\u751f\u751f\u6210\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6cd5\u7684\u5173\u952e\u9650\u5236\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u7075\u6d3b\u5730\u8868\u5f81\u548c\u8c03\u8282\u6574\u4e2a\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u7684\u53cd\u5e94\u3002"}}
{"id": "2506.04394", "pdf": "https://arxiv.org/pdf/2506.04394", "abs": "https://arxiv.org/abs/2506.04394", "authors": ["Qiuyu Tang", "Bonor Ayambem", "Mooi Choo Chuah", "Aparna Bharati"], "title": "Is Perturbation-Based Image Protection Disruptive to Image Editing?", "categories": ["cs.CV"], "comment": "6 pages, 8 figures, accepted by ICIP 2025", "summary": "The remarkable image generation capabilities of state-of-the-art diffusion\nmodels, such as Stable Diffusion, can also be misused to spread misinformation\nand plagiarize copyrighted materials. To mitigate the potential risks\nassociated with image editing, current image protection methods rely on adding\nimperceptible perturbations to images to obstruct diffusion-based editing. A\nfully successful protection for an image implies that the output of editing\nattempts is an undesirable, noisy image which is completely unrelated to the\nreference image. In our experiments with various perturbation-based image\nprotection methods across multiple domains (natural scene images and artworks)\nand editing tasks (image-to-image generation and style editing), we discover\nthat such protection does not achieve this goal completely. In most scenarios,\ndiffusion-based editing of protected images generates a desirable output image\nwhich adheres precisely to the guidance prompt. Our findings suggest that\nadding noise to images may paradoxically increase their association with given\ntext prompts during the generation process, leading to unintended consequences\nsuch as better resultant edits. Hence, we argue that perturbation-based methods\nmay not provide a sufficient solution for robust image protection against\ndiffusion-based editing.", "AI": {"tldr": "\u57fa\u4e8e\u6270\u52a8\u7684\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6709\u6548\u963b\u6b62\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56fe\u50cf\u7f16\u8f91\uff0c\u751a\u81f3\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002", "motivation": "\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff08\u5982 Stable Diffusion\uff09\u7684\u5353\u8d8a\u56fe\u50cf\u751f\u6210\u80fd\u529b\u4e5f\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u548c\u527d\u7a83\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u6750\u6599\u3002\u4e3a\u4e86\u51cf\u8f7b\u4e0e\u56fe\u50cf\u7f16\u8f91\u76f8\u5173\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5f53\u524d\u7684\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5411\u56fe\u50cf\u6dfb\u52a0\u96be\u4ee5\u5bdf\u89c9\u7684\u6270\u52a8\uff0c\u4ee5\u963b\u6b62\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u3002", "method": "\u5bf9\u5404\u79cd\u57fa\u4e8e\u6270\u52a8\u7684\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\uff08\u81ea\u7136\u573a\u666f\u56fe\u50cf\u548c\u827a\u672f\u54c1\uff09\u548c\u7f16\u8f91\u4efb\u52a1\uff08\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u548c\u98ce\u683c\u7f16\u8f91\uff09\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u53d7\u4fdd\u62a4\u56fe\u50cf\u7684\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u4f1a\u751f\u6210\u7b26\u5408\u6307\u5bfc\u63d0\u793a\u7684\u7406\u60f3\u8f93\u51fa\u56fe\u50cf\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5411\u56fe\u50cf\u6dfb\u52a0\u566a\u58f0\u53ef\u80fd\u4f1a\u81ea\u76f8\u77db\u76fe\u5730\u589e\u52a0\u56fe\u50cf\u4e0e\u7ed9\u5b9a\u6587\u672c\u63d0\u793a\u7684\u5173\u8054\uff0c\u4ece\u800c\u5bfc\u81f4\u610f\u60f3\u4e0d\u5230\u7684\u540e\u679c\uff0c\u4f8b\u5982\u66f4\u597d\u7684\u7ed3\u679c\u7f16\u8f91\u3002", "conclusion": "\u6270\u52a8\u65b9\u6cd5\u53ef\u80fd\u4e0d\u8db3\u4ee5\u9488\u5bf9\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u63d0\u4f9b\u5f3a\u5927\u7684\u56fe\u50cf\u4fdd\u62a4\u3002", "summary_zh": "\u6700\u5148\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff08\u5982 Stable Diffusion\uff09\u5177\u6709\u5353\u8d8a\u7684\u56fe\u50cf\u751f\u6210\u80fd\u529b\uff0c\u4f46\u4e5f\u53ef\u80fd\u88ab\u6ee5\u7528\u4e8e\u4f20\u64ad\u865a\u5047\u4fe1\u606f\u548c\u527d\u7a83\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u6750\u6599\u3002\u4e3a\u4e86\u51cf\u8f7b\u4e0e\u56fe\u50cf\u7f16\u8f91\u76f8\u5173\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u5f53\u524d\u7684\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5411\u56fe\u50cf\u6dfb\u52a0\u96be\u4ee5\u5bdf\u89c9\u7684\u6270\u52a8\uff0c\u4ee5\u963b\u6b62\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u3002\u5b8c\u5168\u6210\u529f\u7684\u56fe\u50cf\u4fdd\u62a4\u610f\u5473\u7740\u7f16\u8f91\u5c1d\u8bd5\u7684\u8f93\u51fa\u662f\u4e0d\u5e0c\u671b\u7684\u3001\u5608\u6742\u7684\u56fe\u50cf\uff0c\u4e0e\u53c2\u8003\u56fe\u50cf\u5b8c\u5168\u65e0\u5173\u3002\u5728\u5bf9\u5404\u79cd\u57fa\u4e8e\u6270\u52a8\u7684\u56fe\u50cf\u4fdd\u62a4\u65b9\u6cd5\u5728\u591a\u4e2a\u9886\u57df\uff08\u81ea\u7136\u573a\u666f\u56fe\u50cf\u548c\u827a\u672f\u54c1\uff09\u548c\u7f16\u8f91\u4efb\u52a1\uff08\u56fe\u50cf\u5230\u56fe\u50cf\u751f\u6210\u548c\u98ce\u683c\u7f16\u8f91\uff09\u4e2d\u8fdb\u884c\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u53d1\u73b0\u8fd9\u79cd\u4fdd\u62a4\u5e76\u6ca1\u6709\u5b8c\u5168\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u53d7\u4fdd\u62a4\u56fe\u50cf\u7684\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u4f1a\u751f\u6210\u7b26\u5408\u6307\u5bfc\u63d0\u793a\u7684\u7406\u60f3\u8f93\u51fa\u56fe\u50cf\u3002\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5411\u56fe\u50cf\u6dfb\u52a0\u566a\u58f0\u53ef\u80fd\u4f1a\u81ea\u76f8\u77db\u76fe\u5730\u589e\u52a0\u56fe\u50cf\u4e0e\u7ed9\u5b9a\u6587\u672c\u63d0\u793a\u7684\u5173\u8054\uff0c\u4ece\u800c\u5bfc\u81f4\u610f\u60f3\u4e0d\u5230\u7684\u540e\u679c\uff0c\u4f8b\u5982\u66f4\u597d\u7684\u7ed3\u679c\u7f16\u8f91\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u8ba4\u4e3a\u57fa\u4e8e\u6270\u52a8\u7684\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u4e3a\u9488\u5bf9\u57fa\u4e8e\u6269\u6563\u7684\u7f16\u8f91\u63d0\u4f9b\u5f3a\u5927\u7684\u56fe\u50cf\u4fdd\u62a4\u3002"}}
{"id": "2506.04401", "pdf": "https://arxiv.org/pdf/2506.04401", "abs": "https://arxiv.org/abs/2506.04401", "authors": ["Gustavo Perez", "Stella X. Yu"], "title": "Normalize Filters! Classical Wisdom for Deep Vision", "categories": ["cs.CV"], "comment": null, "summary": "Classical image filters, such as those for averaging or differencing, are\ncarefully normalized to ensure consistency, interpretability, and to avoid\nartifacts like intensity shifts, halos, or ringing. In contrast, convolutional\nfilters learned end-to-end in deep networks lack such constraints. Although\nthey may resemble wavelets and blob/edge detectors, they are not normalized in\nthe same or any way. Consequently, when images undergo atmospheric transfer,\ntheir responses become distorted, leading to incorrect outcomes. We address\nthis limitation by proposing filter normalization, followed by learnable\nscaling and shifting, akin to batch normalization. This simple yet effective\nmodification ensures that the filters are atmosphere-equivariant, enabling\nco-domain symmetry. By integrating classical filtering principles into deep\nlearning (applicable to both convolutional neural networks and\nconvolution-dependent vision transformers), our method achieves significant\nimprovements on artificial and natural intensity variation benchmarks. Our\nResNet34 could even outperform CLIP by a large margin. Our analysis reveals\nthat unnormalized filters degrade performance, whereas filter normalization\nregularizes learning, promotes diversity, and improves robustness and\ngeneralization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6ee4\u6ce2\u5668\u5f52\u4e00\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7ecf\u5178\u6ee4\u6ce2\u539f\u5219\u6574\u5408\u5230\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u56fe\u50cf\u5f3a\u5ea6\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u56fe\u50cf\u7ecf\u8fc7\u5927\u6c14\u4f20\u8f93\u65f6\uff0c\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u5377\u79ef\u6ee4\u6ce2\u5668\u7684\u54cd\u5e94\u4f1a\u5931\u771f\uff0c\u5bfc\u81f4\u4e0d\u6b63\u786e\u7684\u7ed3\u679c\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u6ee4\u6ce2\u5668\u5f52\u4e00\u5316\uff0c\u7136\u540e\u8fdb\u884c\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u548c\u79fb\u4f4d\uff0c\u7c7b\u4f3c\u4e8e\u6279\u91cf\u5f52\u4e00\u5316\u3002", "result": "\u901a\u8fc7\u5c06\u7ecf\u5178\u6ee4\u6ce2\u539f\u5219\u6574\u5408\u5230\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4eba\u5de5\u548c\u81ea\u7136\u5f3a\u5ea6\u53d8\u5316\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\u3002\u6211\u4eec\u7684 ResNet34 \u751a\u81f3\u53ef\u4ee5\u5927\u5927\u4f18\u4e8e CLIP\u3002", "conclusion": "\u672a\u5f52\u4e00\u5316\u7684\u6ee4\u6ce2\u5668\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u6ee4\u6ce2\u5668\u5f52\u4e00\u5316\u53ef\u4ee5\u89c4\u8303\u5b66\u4e60\uff0c\u4fc3\u8fdb\u591a\u6837\u6027\uff0c\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "summary_zh": "\u7ecf\u5178\u7684\u56fe\u50cf\u6ee4\u6ce2\u5668\uff0c\u4f8b\u5982\u7528\u4e8e\u5e73\u5747\u6216\u5dee\u5206\u7684\u6ee4\u6ce2\u5668\uff0c\u90fd\u7ecf\u8fc7\u4ed4\u7ec6\u7684\u5f52\u4e00\u5316\uff0c\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u3001\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u907f\u514d\u8bf8\u5982\u5f3a\u5ea6\u504f\u79fb\u3001\u5149\u6655\u6216\u632f\u94c3\u4e4b\u7c7b\u7684\u4f2a\u5f71\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5728\u6df1\u5ea6\u7f51\u7edc\u4e2d\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u5377\u79ef\u6ee4\u6ce2\u5668\u7f3a\u4e4f\u8fd9\u79cd\u7ea6\u675f\u3002\u5c3d\u7ba1\u5b83\u4eec\u53ef\u80fd\u7c7b\u4f3c\u4e8e\u5c0f\u6ce2\u548c\u6591\u70b9/\u8fb9\u7f18\u68c0\u6d4b\u5668\uff0c\u4f46\u5b83\u4eec\u6ca1\u6709\u4ee5\u76f8\u540c\u6216\u4efb\u4f55\u65b9\u5f0f\u8fdb\u884c\u5f52\u4e00\u5316\u3002\u56e0\u6b64\uff0c\u5f53\u56fe\u50cf\u7ecf\u8fc7\u5927\u6c14\u4f20\u8f93\u65f6\uff0c\u5b83\u4eec\u7684\u54cd\u5e94\u4f1a\u5931\u771f\uff0c\u5bfc\u81f4\u4e0d\u6b63\u786e\u7684\u7ed3\u679c\u3002\u6211\u4eec\u901a\u8fc7\u63d0\u51fa\u6ee4\u6ce2\u5668\u5f52\u4e00\u5316\u6765\u89e3\u51b3\u6b64\u9650\u5236\uff0c\u7136\u540e\u8fdb\u884c\u53ef\u5b66\u4e60\u7684\u7f29\u653e\u548c\u79fb\u4f4d\uff0c\u7c7b\u4f3c\u4e8e\u6279\u91cf\u5f52\u4e00\u5316\u3002\u8fd9\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u4fee\u6539\u786e\u4fdd\u4e86\u6ee4\u6ce2\u5668\u662f\u5927\u6c14\u7b49\u53d8\u7684\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5171\u57df\u5bf9\u79f0\u6027\u3002\u901a\u8fc7\u5c06\u7ecf\u5178\u6ee4\u6ce2\u539f\u5219\u6574\u5408\u5230\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff08\u9002\u7528\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u5377\u79ef\u76f8\u5173\u7684\u89c6\u89c9 Transformer\uff09\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u4eba\u5de5\u548c\u81ea\u7136\u5f3a\u5ea6\u53d8\u5316\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u7740\u6539\u8fdb\u3002\u6211\u4eec\u7684 ResNet34 \u751a\u81f3\u53ef\u4ee5\u5927\u5927\u4f18\u4e8e CLIP\u3002\u6211\u4eec\u7684\u5206\u6790\u8868\u660e\uff0c\u672a\u5f52\u4e00\u5316\u7684\u6ee4\u6ce2\u5668\u4f1a\u964d\u4f4e\u6027\u80fd\uff0c\u800c\u6ee4\u6ce2\u5668\u5f52\u4e00\u5316\u53ef\u4ee5\u89c4\u8303\u5b66\u4e60\uff0c\u4fc3\u8fdb\u591a\u6837\u6027\uff0c\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.04421", "pdf": "https://arxiv.org/pdf/2506.04421", "abs": "https://arxiv.org/abs/2506.04421", "authors": ["Hermann Kumbong", "Xian Liu", "Tsung-Yi Lin", "Ming-Yu Liu", "Xihui Liu", "Ziwei Liu", "Daniel Y. Fu", "Christopher R\u00e9", "David W. Romero"], "title": "HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted to CVPR 2025. Project Page:\n  https://research.nvidia.com/labs/dir/hmar/", "summary": "Visual Auto-Regressive modeling (VAR) has shown promise in bridging the speed\nand quality gap between autoregressive image models and diffusion models. VAR\nreformulates autoregressive modeling by decomposing an image into successive\nresolution scales. During inference, an image is generated by predicting all\nthe tokens in the next (higher-resolution) scale, conditioned on all tokens in\nall previous (lower-resolution) scales. However, this formulation suffers from\nreduced image quality due to the parallel generation of all tokens in a\nresolution scale; has sequence lengths scaling superlinearly in image\nresolution; and requires retraining to change the sampling schedule.\n  We introduce Hierarchical Masked Auto-Regressive modeling (HMAR), a new image\ngeneration algorithm that alleviates these issues using next-scale prediction\nand masked prediction to generate high-quality images with fast sampling. HMAR\nreformulates next-scale prediction as a Markovian process, wherein the\nprediction of each resolution scale is conditioned only on tokens in its\nimmediate predecessor instead of the tokens in all predecessor resolutions.\nWhen predicting a resolution scale, HMAR uses a controllable multi-step masked\ngeneration procedure to generate a subset of the tokens in each step. On\nImageNet 256x256 and 512x512 benchmarks, HMAR models match or outperform\nparameter-matched VAR, diffusion, and autoregressive baselines. We develop\nefficient IO-aware block-sparse attention kernels that allow HMAR to achieve\nfaster training and inference times over VAR by over 2.5x and 1.75x\nrespectively, as well as over 3x lower inference memory footprint. Finally,\nHMAR yields additional flexibility over VAR; its sampling schedule can be\nchanged without further training, and it can be applied to image editing tasks\nin a zero-shot manner.", "AI": {"tldr": "HMAR\u901a\u8fc7\u5206\u5c42\u63a9\u7801\u81ea\u56de\u5f52\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u5feb\u901f\u91c7\u6837\u7684\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5728\u901f\u5ea6\u3001\u5185\u5b58\u5360\u7528\u548c\u7075\u6d3b\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21(VAR)\u5728\u5f25\u5408\u81ea\u56de\u5f52\u56fe\u50cf\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u7684\u901f\u5ea6\u548c\u8d28\u91cf\u5dee\u8ddd\u65b9\u9762\u663e\u793a\u51fa\u4e86\u5e0c\u671b\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5206\u8fa8\u7387\u5c3a\u5ea6\u4e2d\u6240\u6709token\u7684\u5e76\u884c\u751f\u6210\uff0c\u8fd9\u79cd\u516c\u5f0f\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u964d\u4f4e\uff1b\u5e8f\u5217\u957f\u5ea6\u4ee5\u8d85\u7ebf\u6027\u65b9\u5f0f\u7f29\u653e\u56fe\u50cf\u5206\u8fa8\u7387\uff1b\u5e76\u4e14\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u624d\u80fd\u6539\u53d8\u91c7\u6837\u8ba1\u5212\u3002", "method": "HMAR\u5c06\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5206\u8fa8\u7387\u5c3a\u5ea6\u7684\u9884\u6d4b\u4ec5\u4ee5\u5176\u76f4\u63a5\u524d\u7f6e\u5206\u8fa8\u7387\u4e2d\u7684token\u4e3a\u6761\u4ef6\uff0c\u800c\u4e0d\u662f\u4ee5\u6240\u6709\u524d\u7f6e\u5206\u8fa8\u7387\u4e2d\u7684token\u4e3a\u6761\u4ef6\u3002\u5728\u9884\u6d4b\u5206\u8fa8\u7387\u5c3a\u5ea6\u65f6\uff0cHMAR\u4f7f\u7528\u53ef\u63a7\u7684\u591a\u6b65\u63a9\u7801\u751f\u6210\u7a0b\u5e8f\uff0c\u4ee5\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u751f\u6210token\u7684\u5b50\u96c6\u3002", "result": "\u5728ImageNet 256x256\u548c512x512\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHMAR\u6a21\u578b\u4e0e\u53c2\u6570\u5339\u914d\u7684VAR\u3001\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5339\u914d\u6216\u4f18\u4e8e\u5b83\u4eec\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u9ad8\u6548\u7684IO\u611f\u77e5\u5757\u7a00\u758f\u6ce8\u610f\u5185\u6838\uff0c\u4f7fHMAR\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u6bd4VAR\u5206\u522b\u5feb2.5\u500d\u548c1.75\u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14\u63a8\u7406\u5185\u5b58\u5360\u7528\u964d\u4f4e\u4e863\u500d\u4ee5\u4e0a\u3002\u6700\u540e\uff0cHMAR\u6bd4VAR\u66f4\u7075\u6d3b\uff1b\u5b83\u7684\u91c7\u6837\u8ba1\u5212\u53ef\u4ee5\u5728\u6ca1\u6709\u8fdb\u4e00\u6b65\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6539\u53d8\uff0c\u5e76\u4e14\u5b83\u53ef\u4ee5\u4ee5\u96f6\u6837\u672c\u7684\u65b9\u5f0f\u5e94\u7528\u4e8e\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u3002", "conclusion": "HMAR\u6a21\u578b\u5728ImageNet 256x256\u548c512x512\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u4e0e\u53c2\u6570\u5339\u914d\u7684VAR\u3001\u6269\u6563\u6a21\u578b\u548c\u81ea\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u5206\u522b\u6bd4VAR\u5feb2.5\u500d\u548c1.75\u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14\u63a8\u7406\u5185\u5b58\u5360\u7528\u964d\u4f4e\u4e863\u500d\u4ee5\u4e0a\u3002\u6b64\u5916\uff0cHMAR\u8fd8\u6bd4VAR\u66f4\u7075\u6d3b\uff0c\u65e0\u9700\u8fdb\u4e00\u6b65\u8bad\u7ec3\u5373\u53ef\u66f4\u6539\u5176\u91c7\u6837\u8ba1\u5212\uff0c\u5e76\u4e14\u53ef\u4ee5\u4ee5\u96f6\u6837\u672c\u65b9\u5f0f\u5e94\u7528\u4e8e\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u3002", "summary_zh": "\u89c6\u89c9\u81ea\u56de\u5f52\u5efa\u6a21\uff08VAR\uff09\u5728\u5f25\u5408\u81ea\u56de\u5f52\u56fe\u50cf\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u4e4b\u95f4\u7684\u901f\u5ea6\u548c\u8d28\u91cf\u5dee\u8ddd\u65b9\u9762\u663e\u793a\u51fa\u524d\u666f\u3002VAR\u901a\u8fc7\u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u8fde\u7eed\u7684\u5206\u8fa8\u7387\u5c3a\u5ea6\u6765\u91cd\u65b0\u5236\u5b9a\u81ea\u56de\u5f52\u5efa\u6a21\u3002\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u4e2a\uff08\u66f4\u9ad8\u5206\u8fa8\u7387\uff09\u5c3a\u5ea6\u4e2d\u7684\u6240\u6709token\u6765\u751f\u6210\u56fe\u50cf\uff0c\u8be5\u9884\u6d4b\u4ee5\u6240\u6709\u5148\u524d\uff08\u8f83\u4f4e\u5206\u8fa8\u7387\uff09\u5c3a\u5ea6\u4e2d\u7684\u6240\u6709token\u4e3a\u6761\u4ef6\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5206\u8fa8\u7387\u5c3a\u5ea6\u4e2d\u6240\u6709token\u7684\u5e76\u884c\u751f\u6210\uff0c\u8fd9\u79cd\u516c\u5f0f\u5bfc\u81f4\u56fe\u50cf\u8d28\u91cf\u964d\u4f4e\uff1b\u5e8f\u5217\u957f\u5ea6\u4ee5\u8d85\u7ebf\u6027\u65b9\u5f0f\u7f29\u653e\u56fe\u50cf\u5206\u8fa8\u7387\uff1b\u5e76\u4e14\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u624d\u80fd\u6539\u53d8\u91c7\u6837\u8ba1\u5212\u3002\n\u6211\u4eec\u5f15\u5165\u4e86\u5206\u5c42\u63a9\u7801\u81ea\u56de\u5f52\u5efa\u6a21\uff08HMAR\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u751f\u6210\u7b97\u6cd5\uff0c\u5b83\u4f7f\u7528\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\u548c\u63a9\u7801\u9884\u6d4b\u6765\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ece\u800c\u4ee5\u5feb\u901f\u91c7\u6837\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002HMAR\u5c06\u4e0b\u4e00\u5c3a\u5ea6\u9884\u6d4b\u91cd\u65b0\u5b9a\u4e49\u4e3a\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5206\u8fa8\u7387\u5c3a\u5ea6\u7684\u9884\u6d4b\u4ec5\u4ee5\u5176\u76f4\u63a5\u524d\u7f6e\u5206\u8fa8\u7387\u4e2d\u7684token\u4e3a\u6761\u4ef6\uff0c\u800c\u4e0d\u662f\u4ee5\u6240\u6709\u524d\u7f6e\u5206\u8fa8\u7387\u4e2d\u7684token\u4e3a\u6761\u4ef6\u3002\u5728\u9884\u6d4b\u5206\u8fa8\u7387\u5c3a\u5ea6\u65f6\uff0cHMAR\u4f7f\u7528\u53ef\u63a7\u7684\u591a\u6b65\u63a9\u7801\u751f\u6210\u7a0b\u5e8f\uff0c\u4ee5\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u751f\u6210token\u7684\u5b50\u96c6\u3002\u5728ImageNet 256x256\u548c512x512\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHMAR\u6a21\u578b\u4e0e\u53c2\u6570\u5339\u914d\u7684VAR\u3001\u6269\u6563\u548c\u81ea\u56de\u5f52\u57fa\u7ebf\u6a21\u578b\u76f8\u5339\u914d\u6216\u4f18\u4e8e\u5b83\u4eec\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u9ad8\u6548\u7684IO\u611f\u77e5\u5757\u7a00\u758f\u6ce8\u610f\u5185\u6838\uff0c\u4f7fHMAR\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u6bd4VAR\u5206\u522b\u5feb2.5\u500d\u548c1.75\u500d\u4ee5\u4e0a\uff0c\u5e76\u4e14\u63a8\u7406\u5185\u5b58\u5360\u7528\u964d\u4f4e\u4e863\u500d\u4ee5\u4e0a\u3002\u6700\u540e\uff0cHMAR\u6bd4VAR\u66f4\u7075\u6d3b\uff1b\u5b83\u7684\u91c7\u6837\u8ba1\u5212\u53ef\u4ee5\u5728\u6ca1\u6709\u8fdb\u4e00\u6b65\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6539\u53d8\uff0c\u5e76\u4e14\u5b83\u53ef\u4ee5\u4ee5\u96f6\u6837\u672c\u7684\u65b9\u5f0f\u5e94\u7528\u4e8e\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\u3002"}}
{"id": "2506.04444", "pdf": "https://arxiv.org/pdf/2506.04444", "abs": "https://arxiv.org/abs/2506.04444", "authors": ["Zhaoyang Lv", "Maurizio Monge", "Ka Chen", "Yufeng Zhu", "Michael Goesele", "Jakob Engel", "Zhao Dong", "Richard Newcombe"], "title": "Photoreal Scene Reconstruction from an Egocentric Device", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.HC", "cs.MM"], "comment": "Paper accepted to SIGGRAPH Conference Paper 2025", "summary": "In this paper, we investigate the challenges associated with using egocentric\ndevices to photorealistic reconstruct the scene in high dynamic range. Existing\nmethodologies typically assume using frame-rate 6DoF pose estimated from the\ndevice's visual-inertial odometry system, which may neglect crucial details\nnecessary for pixel-accurate reconstruction. This study presents two\nsignificant findings. Firstly, in contrast to mainstream work treating RGB\ncamera as global shutter frame-rate camera, we emphasize the importance of\nemploying visual-inertial bundle adjustment (VIBA) to calibrate the precise\ntimestamps and movement of the rolling shutter RGB sensing camera in a high\nfrequency trajectory format, which ensures an accurate calibration of the\nphysical properties of the rolling-shutter camera. Secondly, we incorporate a\nphysical image formation model based into Gaussian Splatting, which effectively\naddresses the sensor characteristics, including the rolling-shutter effect of\nRGB cameras and the dynamic ranges measured by sensors. Our proposed\nformulation is applicable to the widely-used variants of Gaussian Splats\nrepresentation. We conduct a comprehensive evaluation of our pipeline using the\nopen-source Project Aria device under diverse indoor and outdoor lighting\nconditions, and further validate it on a Meta Quest3 device. Across all\nexperiments, we observe a consistent visual enhancement of +1 dB in PSNR by\nincorporating VIBA, with an additional +1 dB achieved through our proposed\nimage formation model. Our complete implementation, evaluation datasets, and\nrecording profile are available at\nhttp://www.projectaria.com/photoreal-reconstruction/", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u60ef\u6027\u675f\u8c03\u6574\u548c\u7269\u7406\u56fe\u50cf\u5f62\u6210\u6a21\u578b\u7684\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8bbe\u5907\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u4e09\u7ef4\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u4f7f\u7528\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8bbe\u5907\u8fdb\u884c\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u4e09\u7ef4\u91cd\u5efa\u7684\u65b9\u6cd5\u5ffd\u7565\u4e86\u50cf\u7d20\u7ea7\u7cbe\u786e\u91cd\u5efa\u6240\u9700\u7684\u5173\u952e\u7ec6\u8282\uff0c\u5e76\u4e14\u901a\u5e38\u5047\u8bbeRGB\u76f8\u673a\u4e3a\u5168\u5c40\u5feb\u95e8\u76f8\u673a\uff0c\u5ffd\u7565\u4e86\u6eda\u52a8\u5feb\u95e8\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u9ad8\u65af\u6e85\u5c04\u7684\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u89c6\u89c9\u60ef\u6027\u675f\u8c03\u6574(VIBA)\u548c\u57fa\u4e8e\u7269\u7406\u7684\u56fe\u50cf\u5f62\u6210\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u6eda\u52a8\u5feb\u95e8\u76f8\u673a\u7684\u65f6\u95f4\u6233\u548c\u8fd0\u52a8\u6821\u51c6\u95ee\u9898\uff0c\u5e76\u5904\u7406\u9ad8\u52a8\u6001\u8303\u56f4\u3002", "result": "\u5728\u5404\u79cd\u5ba4\u5185\u548c\u5ba4\u5916\u5149\u7167\u6761\u4ef6\u4e0b\uff0c\u4f7f\u7528Project Aria\u8bbe\u5907\u548cMeta Quest3\u8bbe\u5907\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408VIBA\u53ef\u4ee5\u7a33\u5b9a\u5730\u63d0\u9ad81 dB\u7684PSNR\uff0c\u800c\u4f7f\u7528\u63d0\u51fa\u7684\u56fe\u50cf\u5f62\u6210\u6a21\u578b\u53ef\u4ee5\u989d\u5916\u63d0\u9ad81 dB\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u60ef\u6027\u675f\u8c03\u6574(VIBA)\u548c\u57fa\u4e8e\u7269\u7406\u7684\u56fe\u50cf\u5f62\u6210\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u4e0b\u4f7f\u7528\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8bbe\u5907\u8fdb\u884c\u4e09\u7ef4\u91cd\u5efa\u7684\u8d28\u91cf\u3002", "summary_zh": "\u672c\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u8bbe\u5907\u8fdb\u884c\u9ad8\u52a8\u6001\u8303\u56f4\u573a\u666f\u7167\u7247\u7ea7\u771f\u5b9e\u611f\u91cd\u5efa\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u4f7f\u7528\u8bbe\u5907\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\u4f30\u8ba1\u7684\u5e27\u73876DoF\u4f4d\u59ff\uff0c\u8fd9\u53ef\u80fd\u4f1a\u5ffd\u7565\u50cf\u7d20\u7ea7\u7cbe\u786e\u91cd\u5efa\u6240\u9700\u7684\u5173\u952e\u7ec6\u8282\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u4e2a\u91cd\u8981\u7684\u53d1\u73b0\u3002\u9996\u5148\uff0c\u4e0e\u5c06RGB\u76f8\u673a\u89c6\u4e3a\u5168\u5c40\u5feb\u95e8\u76f8\u673a\u7684\u4f20\u7edf\u65b9\u6cd5\u4e0d\u540c\uff0c\u6211\u4eec\u5f3a\u8c03\u4e86\u4f7f\u7528\u89c6\u89c9\u60ef\u6027\u675f\u8c03\u6574(VIBA)\u6765\u6821\u51c6\u6eda\u52a8\u5feb\u95e8RGB\u4f20\u611f\u76f8\u673a\u5728\u9ad8\u901f\u7387\u8f68\u8ff9\u683c\u5f0f\u4e0b\u7684\u7cbe\u786e\u65f6\u95f4\u6233\u548c\u8fd0\u52a8\u7684\u91cd\u8981\u6027\uff0c\u8fd9\u786e\u4fdd\u4e86\u6eda\u52a8\u5feb\u95e8\u76f8\u673a\u7684\u7269\u7406\u7279\u6027\u7684\u7cbe\u786e\u6821\u51c6\u3002\u5176\u6b21\uff0c\u6211\u4eec\u5c06\u57fa\u4e8e\u7269\u7406\u7684\u56fe\u50cf\u5f62\u6210\u6a21\u578b\u878d\u5165\u5230\u9ad8\u65af\u6e85\u5c04\u4e2d\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u4f20\u611f\u5668\u7279\u6027\uff0c\u5305\u62ecRGB\u76f8\u673a\u7684\u6eda\u52a8\u5feb\u95e8\u6548\u5e94\u548c\u4f20\u611f\u5668\u6d4b\u91cf\u7684\u52a8\u6001\u8303\u56f4\u3002\u6211\u4eec\u63d0\u51fa\u7684\u516c\u5f0f\u9002\u7528\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u7684\u53d8\u4f53\u3002\u6211\u4eec\u4f7f\u7528\u5f00\u6e90\u7684Project Aria\u8bbe\u5907\u5728\u5404\u79cd\u5ba4\u5185\u548c\u5ba4\u5916\u7167\u660e\u6761\u4ef6\u4e0b\u5bf9\u6211\u4eec\u7684\u6d41\u7a0b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u5e76\u5728Meta Quest3\u8bbe\u5907\u4e0a\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5b83\u3002\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\uff0c\u901a\u8fc7\u7ed3\u5408VIBA\uff0c\u89c6\u89c9\u6548\u679c\u7a33\u5b9a\u5730\u63d0\u9ad8\u4e86+1 dB PSNR\uff0c\u800c\u901a\u8fc7\u6211\u4eec\u63d0\u51fa\u7684\u56fe\u50cf\u5f62\u6210\u6a21\u578b\uff0c\u89c6\u89c9\u6548\u679c\u53c8\u63d0\u9ad8\u4e86+1 dB\u3002\u6211\u4eec\u7684\u5b8c\u6574\u5b9e\u73b0\u3001\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u8bb0\u5f55\u914d\u7f6e\u6587\u4ef6\u53ef\u5728http://www.projectaria.com/photoreal-reconstruction/\u83b7\u5f97\u3002"}}
{"id": "2506.04496", "pdf": "https://arxiv.org/pdf/2506.04496", "abs": "https://arxiv.org/abs/2506.04496", "authors": ["Patrik Mesec", "Alan Jovi\u0107"], "title": "Towards Large-Scale Pose-Invariant Face Recognition Using Face Defrontalization", "categories": ["cs.CV"], "comment": "13 pages, 5 figures, 4 tables", "summary": "Face recognition under extreme head poses is a challenging task. Ideally, a\nface recognition system should perform well across different head poses, which\nis known as pose-invariant face recognition. To achieve pose invariance,\ncurrent approaches rely on sophisticated methods, such as face frontalization\nand various facial feature extraction model architectures. However, these\nmethods are somewhat impractical in real-life settings and are typically\nevaluated on small scientific datasets, such as Multi-PIE. In this work, we\npropose the inverse method of face frontalization, called face\ndefrontalization, to augment the training dataset of facial feature extraction\nmodel. The method does not introduce any time overhead during the inference\nstep. The method is composed of: 1) training an adapted face defrontalization\nFFWM model on a frontal-profile pairs dataset, which has been preprocessed\nusing our proposed face alignment method; 2) training a ResNet-50 facial\nfeature extraction model based on ArcFace loss on a raw and randomly\ndefrontalized large-scale dataset, where defrontalization was performed with\nour previously trained face defrontalization model. Our method was compared\nwith the existing approaches on four open-access datasets: LFW, AgeDB, CFP, and\nMulti-PIE. Defrontalization shows improved results compared to models without\ndefrontalization, while the proposed adjustments show clear superiority over\nthe state-of-the-art face frontalization FFWM method on three larger\nopen-access datasets, but not on the small Multi-PIE dataset for extreme poses\n(75 and 90 degrees). The results suggest that at least some of the current\nmethods may be overfitted to small datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u8138\u53bb\u6b63\u8138\u5316\u65b9\u6cd5\u6765\u6269\u589e\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u63d0\u9ad8\u59ff\u52bf\u4e0d\u53d8\u4eba\u8138\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u6781\u7aef\u5934\u90e8\u59ff\u52bf\u4e0b\u7684\u4eba\u8138\u8bc6\u522b\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5e94\u8be5\u5728\u4e0d\u540c\u7684\u5934\u90e8\u59ff\u52bf\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u8fd9\u88ab\u79f0\u4e3a\u59ff\u52bf\u4e0d\u53d8\u4eba\u8138\u8bc6\u522b\u3002\u4e3a\u4e86\u5b9e\u73b0\u59ff\u52bf\u4e0d\u53d8\u6027\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u4eba\u8138\u6b63\u8138\u5316\u548c\u5404\u79cd\u9762\u90e8\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u67b6\u6784\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u751f\u6d3b\u4e2d\u6709\u4e9b\u4e0d\u5207\u5b9e\u9645\uff0c\u5e76\u4e14\u901a\u5e38\u5728\u5c0f\u578b\u79d1\u5b66\u6570\u636e\u96c6\uff08\u5982Multi-PIE\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u8138\u53bb\u6b63\u8138\u5316\u65b9\u6cd5\uff0c\u4ee5\u6269\u589e\u9762\u90e8\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5728\u6b63\u9762-\u4fa7\u9762\u4eba\u8138\u5bf9\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u6539\u8fdb\u7684\u4eba\u8138\u53bb\u6b63\u8138\u5316FFWM\u6a21\u578b\uff0c\u8be5\u6570\u636e\u96c6\u5df2\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u4eba\u8138\u5bf9\u9f50\u65b9\u6cd5\u8fdb\u884c\u9884\u5904\u7406\uff1b2) \u5728\u539f\u59cb\u7684\u548c\u968f\u673a\u53bb\u6b63\u8138\u5316\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8eArcFace\u635f\u5931\u8bad\u7ec3\u4e00\u4e2aResNet-50\u9762\u90e8\u7279\u5f81\u63d0\u53d6\u6a21\u578b\uff0c\u5176\u4e2d\u53bb\u6b63\u8138\u5316\u662f\u4f7f\u7528\u6211\u4eec\u4e4b\u524d\u8bad\u7ec3\u7684\u4eba\u8138\u53bb\u6b63\u8138\u5316\u6a21\u578b\u6267\u884c\u7684\u3002", "result": "\u5728\u56db\u4e2a\u5f00\u653e\u83b7\u53d6\u6570\u636e\u96c6\uff08LFW\u3001AgeDB\u3001CFP\u548cMulti-PIE\uff09\u4e0a\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u53bb\u6b63\u8138\u5316\u663e\u793a\u51fa\u6bd4\u6ca1\u6709\u53bb\u6b63\u8138\u5316\u7684\u6a21\u578b\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u800c\u6240\u63d0\u51fa\u7684\u8c03\u6574\u663e\u793a\u51fa\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4eba\u8138\u6b63\u8138\u5316FFWM\u65b9\u6cd5\u3002", "conclusion": "\u5728\u4e09\u4e2a\u5927\u578b\u5f00\u653e\u6570\u636e\u96c6\u4e0a\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4eba\u8138\u6b63\u8138\u5316FFWM\u65b9\u6cd5\uff0c\u4f46\u5728\u7528\u4e8e\u6781\u7aef\u59ff\u52bf\uff0875\u5ea6\u548c90\u5ea6\uff09\u7684\u5c0f\u578bMulti-PIE\u6570\u636e\u96c6\u4e0a\u5219\u4e0d\u7136\u3002\u7ed3\u679c\u8868\u660e\uff0c\u81f3\u5c11\u67d0\u4e9b\u5f53\u524d\u65b9\u6cd5\u53ef\u80fd\u8fc7\u5ea6\u62df\u5408\u5230\u5c0f\u578b\u6570\u636e\u96c6\u3002", "summary_zh": "\u5728\u6781\u7aef\u5934\u90e8\u59ff\u52bf\u4e0b\u7684\u4eba\u8138\u8bc6\u522b\u662f\u4e00\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u5e94\u8be5\u5728\u4e0d\u540c\u7684\u5934\u90e8\u59ff\u52bf\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u8fd9\u88ab\u79f0\u4e3a\u59ff\u52bf\u4e0d\u53d8\u4eba\u8138\u8bc6\u522b\u3002\u4e3a\u4e86\u5b9e\u73b0\u59ff\u52bf\u4e0d\u53d8\u6027\uff0c\u5f53\u524d\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u590d\u6742\u7684\u65b9\u6cd5\uff0c\u4f8b\u5982\u4eba\u8138\u6b63\u8138\u5316\u548c\u5404\u79cd\u9762\u90e8\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u67b6\u6784\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u73b0\u5b9e\u751f\u6d3b\u4e2d\u6709\u4e9b\u4e0d\u5207\u5b9e\u9645\uff0c\u5e76\u4e14\u901a\u5e38\u5728\u5c0f\u578b\u79d1\u5b66\u6570\u636e\u96c6\uff08\u5982Multi-PIE\uff09\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4eba\u8138\u6b63\u8138\u5316\u7684\u9006\u65b9\u6cd5\uff0c\u79f0\u4e3a\u4eba\u8138\u53bb\u6b63\u8138\u5316\uff0c\u4ee5\u6269\u589e\u9762\u90e8\u7279\u5f81\u63d0\u53d6\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u6b65\u9aa4\u4e2d\u4e0d\u5f15\u5165\u4efb\u4f55\u65f6\u95f4\u5f00\u9500\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5728\u6b63\u9762-\u4fa7\u9762\u4eba\u8138\u5bf9\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e00\u4e2a\u6539\u8fdb\u7684\u4eba\u8138\u53bb\u6b63\u8138\u5316FFWM\u6a21\u578b\uff0c\u8be5\u6570\u636e\u96c6\u5df2\u4f7f\u7528\u6211\u4eec\u63d0\u51fa\u7684\u4eba\u8138\u5bf9\u9f50\u65b9\u6cd5\u8fdb\u884c\u9884\u5904\u7406\uff1b2) \u5728\u539f\u59cb\u7684\u548c\u968f\u673a\u53bb\u6b63\u8138\u5316\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u4e8eArcFace\u635f\u5931\u8bad\u7ec3\u4e00\u4e2aResNet-50\u9762\u90e8\u7279\u5f81\u63d0\u53d6\u6a21\u578b\uff0c\u5176\u4e2d\u53bb\u6b63\u8138\u5316\u662f\u4f7f\u7528\u6211\u4eec\u4e4b\u524d\u8bad\u7ec3\u7684\u4eba\u8138\u53bb\u6b63\u8138\u5316\u6a21\u578b\u6267\u884c\u7684\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u56db\u4e2a\u5f00\u653e\u83b7\u53d6\u6570\u636e\u96c6\uff08LFW\u3001AgeDB\u3001CFP\u548cMulti-PIE\uff09\u4e0a\u4e0e\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u53bb\u6b63\u8138\u5316\u663e\u793a\u51fa\u6bd4\u6ca1\u6709\u53bb\u6b63\u8138\u5316\u7684\u6a21\u578b\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u800c\u6240\u63d0\u51fa\u7684\u8c03\u6574\u663e\u793a\u51fa\u660e\u663e\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u4eba\u8138\u6b63\u8138\u5316FFWM\u65b9\u6cd5\uff0c\u4f46\u5728\u7528\u4e8e\u6781\u7aef\u59ff\u52bf\uff0875\u5ea6\u548c90\u5ea6\uff09\u7684\u5c0f\u578bMulti-PIE\u6570\u636e\u96c6\u4e0a\u5219\u4e0d\u7136\u3002\u7ed3\u679c\u8868\u660e\uff0c\u81f3\u5c11\u67d0\u4e9b\u5f53\u524d\u65b9\u6cd5\u53ef\u80fd\u8fc7\u5ea6\u62df\u5408\u5230\u5c0f\u578b\u6570\u636e\u96c6\u3002"}}
