{"id": "2507.03384", "pdf": "https://arxiv.org/pdf/2507.03384", "abs": "https://arxiv.org/abs/2507.03384", "authors": ["Suchen Liu", "Jun Gao", "Yinjun Han", "Yang Lin"], "title": "LLM4Hint: Leveraging Large Language Models for Hint Recommendation in Offline Query Optimization", "categories": ["cs.DB", "cs.AI"], "comment": null, "summary": "Query optimization is essential for efficient SQL query execution in DBMS,\nand remains attractive over time due to the growth of data volumes and advances\nin hardware. Existing traditional optimizers struggle with the cumbersome\nhand-tuning required for complex workloads, and the learning-based methods face\nlimitations in ensuring generalization. With the great success of Large\nLanguage Model (LLM) across diverse downstream tasks, this paper explores how\nLLMs can be incorporated to enhance the generalization of learned optimizers.\nThough promising, such an incorporation still presents challenges, mainly\nincluding high model inference latency, and the substantial fine-tuning cost\nand suboptimal performance due to inherent discrepancy between the token\nsequences in LLM and structured SQL execution plans with rich numerical\nfeatures.\n  In this paper, we focus on recurring queries in offline optimization to\nalleviate the issue of high inference latency, and propose \\textbf{LLM4Hint}\nthat leverages moderate-sized backbone LLMs to recommend query optimization\nhints. LLM4Hint achieves the goals through: (i) integrating a lightweight model\nto produce a soft prompt, which captures the data distribution in DBMS and the\nSQL predicates to provide sufficient optimization features while simultaneously\nreducing the context length fed to the LLM, (ii) devising a query rewriting\nstrategy using a larger commercial LLM, so as to simplify SQL semantics for the\nbackbone LLM and reduce fine-tuning costs, and (iii) introducing an explicit\nmatching prompt to facilitate alignment between the LLM and the lightweight\nmodel, which can accelerate convergence of the combined model. Experiments show\nthat LLM4Hint, by leveraging the LLM's stronger capability to understand the\nquery statement, can outperform the state-of-the-art learned optimizers in\nterms of both effectiveness and generalization."}
{"id": "2507.03919", "pdf": "https://arxiv.org/pdf/2507.03919", "abs": "https://arxiv.org/abs/2507.03919", "authors": ["Duy Le"], "title": "PFCS: Prime Factorization Cache System for Deterministic Data Relationship Discovery", "categories": ["cs.DB", "cs.CC"], "comment": "6 pages, 3 figures, 3 algorithms", "summary": "Cache systems fundamentally limit modern computing performance due to their\ninability to precisely capture data relationships. While achieving 85-92% hit\nrates, traditional systems rely on statistical heuristics that cannot guarantee\nrelationship discovery, leading to suboptimal prefetching and resource waste.\nWe present PFCS (Prime Factorization Cache System), which leverages the\nmathematical uniqueness of prime factorization to achieve deterministic\nrelationship discovery with zero false positives. PFCS assigns unique primes to\ndata elements and represents relationships as composite numbers, enabling the\nrecovery of perfect relationships through factorization. A comprehensive\nevaluation across database, ML, and HPC workloads demonstrates an average\nperformance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction\ncompared to state-of-the-art systems. The mathematical foundation provides\nformal guarantees impossible with approximation-based approaches, establishing\na new paradigm for cache system design"}
{"id": "2507.04256", "pdf": "https://arxiv.org/pdf/2507.04256", "abs": "https://arxiv.org/abs/2507.04256", "authors": ["Tang Qian", "Yifan Zhu", "Lu Chen", "Xiangyu Ke", "Jingwen Zhao", "Tianyi Li", "Yunjun Gao", "Christian S. Jensen"], "title": "OneDB: A Distributed Multi-Metric Data Similarity Search System", "categories": ["cs.DB"], "comment": null, "summary": "Increasingly massive volumes of multi-modal data are being accumulated in\nmany {real world} settings, including in health care and e-commerce. This\ndevelopment calls for effective general-purpose data management solutions for\nmulti-modal data. Such a solution must facilitate user-friendly and accurate\nretrieval of any multi-modal data according to diverse application\nrequirements. Further, such a solution must be capable of efficient and\nscalable retrieval.\n  To address this need, we present OneDB, a distributed multi-metric data\nsimilarity retrieval system. This system exploits the fact that data of diverse\nmodalities, such as text, images, and video, can be represented as metric data.\nThe system thus affords each data modality its own metric space with its own\ndistance function and then uses a multi-metric model to unify multi-modal data.\nThe system features several innovations: (i) an extended Spart SQL query\ninterface; (ii) lightweight means of learning appropriate weights of different\nmodalities when retrieving multi-modal data to enable accurate retrieval; (iii)\nsmart search-space pruning strategies that improve efficiency; (iv) two-layered\nindexing of data to ensure load-balancing during distributed processing; and\n(v) end-to-end system parameter autotuning.\n  Experiments on three real-life datasets and two synthetic datasets offer\nevidence that the system is capable of state-of-the-art performance: (i)\nefficient and effective weight learning; (ii) retrieval accuracy improvements\nof 12.63\\%--30.75\\% over the state-of-the-art vector similarity search system\nat comparable efficiency; (iii) accelerated search by 2.5--5.75x over\nstate-of-the-art single- or multi-metric solutions; (iv) demonstrated high\nscalability; and (v) parameter tuning that enables performance improvements of\n15+%."}
{"id": "2507.04687", "pdf": "https://arxiv.org/pdf/2507.04687", "abs": "https://arxiv.org/abs/2507.04687", "authors": ["Zhenwei Dai", "Chuan Lei", "Asterios Katsifodimos", "Xiao Qin", "Christos Faloutsos", "Huzefa Rangwala"], "title": "AKEGEN: A LLM-based Tabular Corpus Generator for Evaluating Dataset Discovery in Data Lakes", "categories": ["cs.DB"], "comment": "13 pages", "summary": "How to generate a large, realistic set of tables along with joinability\nrelationships, to stress-test dataset discovery methods? Dataset discovery\nmethods aim to automatically identify related data assets in a data lake. The\ndevelopment and evaluation of such solutions for customers from a wide range of\nbusiness domains, relies on diverse, high quality and domain-specific tabular\nbenchmarks. Large language models (LLMs) are trained on a wide variety of text\ndata, which can provide a strong foundation of general and domain-specific\nknowledge. In this paper, we ask the question -- \\textit{can we leverage LLMs\nto generate a tabular benchmark adequate for evaluating the dataset discovery\nsolutions?} In particular, we focus on the task of finding joinable tables\nwhich is the cornerstone of virtually every dataset discovery method. Current\ncorpora for evaluating dataset discovery methods are mainly based on subsets of\nopen data, and they suffer from three important issues: $i)$ they focus on very\ncommon and generic data types (e.g., address, id, name, etc.); $ii)$ they do\nnot contain human-annotated column pairs; instead, practitioners synthesize\nground truth using table splits (e.g., horizontal for table union search and\nvertical ones for joinability) and $iii)$ they do not focus on semantic column\nrelationships."}
{"id": "2507.04872", "pdf": "https://arxiv.org/pdf/2507.04872", "abs": "https://arxiv.org/abs/2507.04872", "authors": ["Cong Yu", "Tuo Shi", "Matthias Weidlich", "Bo Zhao"], "title": "SHARP: Shared State Reduction for Efficient Matching of Sequential Patterns", "categories": ["cs.DB"], "comment": null, "summary": "The detection of sequential patterns in data is a basic functionality of\nmodern data processing systems for complex event processing (CEP), OLAP, and\nretrieval-augmented generation (RAG). In practice, pattern matching is\nchallenging, since common applications rely on a large set of patterns that\nshall be evaluated with tight latency bounds. At the same time, matching needs\nto maintain state, i.e., intermediate results, that grows exponentially in the\ninput size. Hence, systems turn to best-effort processing, striving for maximal\nrecall under a latency bound. Existing techniques, however, consider each\npattern in isolation, neglecting the optimization potential induced by state\nsharing in pattern matching.\n  In this paper, we present SHARP, a library that employs state reduction to\nachieve efficient best-effort pattern matching. To this end, SHARP incorporates\nstate sharing between patterns through a new abstraction, coined\npattern-sharing degree (PSD). At runtime, this abstraction facilitates the\ncategorization and indexing of partial pattern matches. Based thereon, once a\nlatency bound is exceeded, SHARP realizes best-effort processing by selecting a\nsubset of partial matches for further processing in constant time. In\nexperiments with real-world data, SHARP achieves a recall of 97%, 96% and 73%\nfor pattern matching in CEP, OLAP, and RAG applications, under a bound of 50%\nof the average processing latency."}
{"id": "2507.04967", "pdf": "https://arxiv.org/pdf/2507.04967", "abs": "https://arxiv.org/abs/2507.04967", "authors": ["Bardia Mohammadi", "Laurent Bindschaedler"], "title": "The Case for Instance-Optimized LLMs in OLAP Databases", "categories": ["cs.DB", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) can enhance analytics systems with powerful data\nsummarization, cleaning, and semantic transformation capabilities. However,\ndeploying LLMs at scale -- processing millions to billions of rows -- remains\nprohibitively expensive in computation and memory. We present IOLM-DB, a novel\nsystem that makes LLM-enhanced database queries practical through\nquery-specific model optimization. Instead of using general-purpose LLMs,\nIOLM-DB generates lightweight, specialized models tailored to each query's\nspecific needs using representative data samples. IOLM-DB reduces model\nfootprints by up to 76% and increases throughput by up to 3.31$\\times$ while\nmaintaining accuracy through aggressive compression techniques, including\nquantization, sparsification, and structural pruning. We further show how our\napproach enables higher parallelism on existing hardware and seamlessly\nsupports caching and batching strategies to reduce overheads. Our prototype\ndemonstrates that leveraging LLM queries inside analytics systems is feasible\nat scale, opening new possibilities for future OLAP applications."}
{"id": "2507.03410", "pdf": "https://arxiv.org/pdf/2507.03410", "abs": "https://arxiv.org/abs/2507.03410", "authors": ["Hrishikesh Terdalkar", "Angela Bonifati", "Andrea Mauri"], "title": "Graph Repairs with Large Language Models: An Empirical Study", "categories": ["cs.CL", "cs.DB", "cs.ET"], "comment": "Accepted to the 8th GRADES-NDA 2025 @ SIGMOD/PODS 2025", "summary": "Property graphs are widely used in domains such as healthcare, finance, and\nsocial networks, but they often contain errors due to inconsistencies, missing\ndata, or schema violations. Traditional rule-based and heuristic-driven graph\nrepair methods are limited in their adaptability as they need to be tailored\nfor each dataset. On the other hand, interactive human-in-the-loop approaches\nmay become infeasible when dealing with large graphs, as the cost--both in\nterms of time and effort--of involving users becomes too high. Recent\nadvancements in Large Language Models (LLMs) present new opportunities for\nautomated graph repair by leveraging contextual reasoning and their access to\nreal-world knowledge. We evaluate the effectiveness of six open-source LLMs in\nrepairing property graphs. We assess repair quality, computational cost, and\nmodel-specific performance. Our experiments show that LLMs have the potential\nto detect and correct errors, with varying degrees of accuracy and efficiency.\nWe discuss the strengths, limitations, and challenges of LLM-driven graph\nrepair and outline future research directions for improving scalability and\ninterpretability."}
