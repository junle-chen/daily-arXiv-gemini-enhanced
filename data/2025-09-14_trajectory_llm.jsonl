{"id": "2509.09200", "pdf": "https://arxiv.org/pdf/2509.09200", "abs": "https://arxiv.org/abs/2509.09200", "authors": ["Ge Sun", "Jun Ma"], "title": "MGTraj: Multi-Granularity Goal-Guided Human Trajectory Prediction with Recursive Refinement Network", "categories": ["cs.CV"], "comment": null, "summary": "Accurate human trajectory prediction is crucial for robotics navigation and\nautonomous driving. Recent research has demonstrated that incorporating goal\nguidance significantly enhances prediction accuracy by reducing uncertainty and\nleveraging prior knowledge. Most goal-guided approaches decouple the prediction\ntask into two stages: goal prediction and subsequent trajectory completion\nbased on the predicted goal, which operate at extreme granularities:\ncoarse-grained goal prediction forecasts the overall intention, while\nfine-grained trajectory completion needs to generate the positions for all\nfuture timesteps. The potential utility of intermediate temporal granularity\nremains largely unexplored, which motivates multi-granularity trajectory\nmodeling. While prior work has shown that multi-granularity representations\ncapture diverse scales of human dynamics and motion patterns, effectively\nintegrating this concept into goal-guided frameworks remains challenging. In\nthis paper, we propose MGTraj, a novel Multi-Granularity goal-guided model for\nhuman Trajectory prediction. MGTraj recursively encodes trajectory proposals\nfrom coarse to fine granularity levels. At each level, a transformer-based\nrecursive refinement network (RRN) captures features and predicts progressive\nrefinements. Features across different granularities are integrated using a\nweight-sharing strategy, and velocity prediction is employed as an auxiliary\ntask to further enhance performance. Comprehensive experimental results in\nEHT/UCY and Stanford Drone Dataset indicate that MGTraj outperforms baseline\nmethods and achieves state-of-the-art performance among goal-guided methods.", "relevance_analysis": {"relevance_score": 0.8, "explanation": "This paper focuses on human trajectory prediction, which is a core aspect of trajectory prediction. While it doesn't directly involve Large Language Models, the use of transformers and the overall task fall within the broader scope of interest. The title and abstract clearly indicate a strong connection to trajectory prediction.", "keywords": ["trajectory prediction", "human trajectory prediction", "goal-guided", "transformer", "autonomous driving", "motion patterns"]}}
{"id": "2509.09210", "pdf": "https://arxiv.org/pdf/2509.09210", "abs": "https://arxiv.org/abs/2509.09210", "authors": ["Xing Gao", "Zherui Huang", "Weiyao Lin", "Xiao Sun"], "title": "ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting", "categories": ["cs.AI", "cs.RO"], "comment": null, "summary": "Accurate motion prediction of surrounding agents is crucial for the safe\nplanning of autonomous vehicles. Recent advancements have extended prediction\ntechniques from individual agents to joint predictions of multiple interacting\nagents, with various strategies to address complex interactions within future\nmotions of agents. However, these methods overlook the evolving nature of these\ninteractions. To address this limitation, we propose a novel progressive\nmulti-scale decoding strategy, termed ProgD, with the help of dynamic\nheterogeneous graph-based scenario modeling. In particular, to explicitly and\ncomprehensively capture the evolving social interactions in future scenarios,\ngiven their inherent uncertainty, we design a progressive modeling of scenarios\nwith dynamic heterogeneous graphs. With the unfolding of such dynamic\nheterogeneous graphs, a factorized architecture is designed to process the\nspatio-temporal dependencies within future scenarios and progressively\neliminate uncertainty in future motions of multiple agents. Furthermore, a\nmulti-scale decoding procedure is incorporated to improve on the future\nscenario modeling and consistent prediction of agents' future motion. The\nproposed ProgD achieves state-of-the-art performance on the INTERACTION\nmulti-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2\nmulti-world forecasting benchmark.", "relevance_analysis": {"relevance_score": 0.7, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\uff0c\u4f7f\u7528\u4e86\u52a8\u6001\u56fe\u7b49\u65b9\u6cd5\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u9ad8\u5ea6\u76f8\u5173\u3002\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u4f7f\u7528\u6216\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u5176\u7814\u7a76\u65b9\u5411\u4e0e\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u5bc6\u5207\u76f8\u5173\uff0c\u8fd9\u4e9b\u9886\u57df\u53ef\u80fd\u4f1a\u53d7\u76ca\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002\u56e0\u6b64\uff0c\u5177\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002", "keywords": ["motion prediction", "multi-agent", "trajectory prediction", "dynamic graphs", "autonomous vehicles"]}}
{"id": "2509.09074", "pdf": "https://arxiv.org/pdf/2509.09074", "abs": "https://arxiv.org/abs/2509.09074", "authors": ["Alice Kate Li", "Thales C Silva", "Victoria Edwards", "Vijay Kumar", "M. Ani Hsieh"], "title": "KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted to CoRL 2025 (Conference on Robot Learning). 15 pages 11\n  figures", "summary": "In this work, we propose a novel flow field-based motion planning method that\ndrives a robot from any initial state to a desired reference trajectory such\nthat it converges to the trajectory's end point. Despite demonstrated efficacy\nin using Koopman operator theory for modeling dynamical systems, Koopman does\nnot inherently enforce convergence to desired trajectories nor to specified\ngoals -- a requirement when learning from demonstrations (LfD). We present\nKoopMotion which represents motion flow fields as dynamical systems,\nparameterized by Koopman Operators to mimic desired trajectories, and leverages\nthe divergence properties of the learnt flow fields to obtain smooth motion\nfields that converge to a desired reference trajectory when a robot is placed\naway from the desired trajectory, and tracks the trajectory until the end\npoint. To demonstrate the effectiveness of our approach, we show evaluations of\nKoopMotion on the LASA human handwriting dataset and a 3D manipulator\nend-effector trajectory dataset, including spectral analysis. We also perform\nexperiments on a physical robot, verifying KoopMotion on a miniature autonomous\nsurface vehicle operating in a non-static fluid flow environment. Our approach\nis highly sample efficient in both space and time, requiring only 3\\% of the\nLASA dataset to generate dense motion plans. Additionally, KoopMotion provides\na significant improvement over baselines when comparing metrics that measure\nspatial and temporal dynamics modeling efficacy.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8fd0\u52a8\u89c4\u5212\uff0c\u7279\u522b\u662f\u4f7f\u7528Koopman\u7b97\u5b50\u5b66\u4e60\u8fd0\u52a8\u6d41\u573a\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u751f\u6210\u548c\u89c4\u5212\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u5173\u8054\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u6216\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["motion planning", "trajectory", "dynamical systems", "flow fields", "Koopman operator"]}}
{"id": "2509.09206", "pdf": "https://arxiv.org/pdf/2509.09206", "abs": "https://arxiv.org/abs/2509.09206", "authors": ["Farhad Nawaz", "Faizan M. Tariq", "Sangjae Bae", "David Isele", "Avinash Singh", "Nadia Figueroa", "Nikolai Matni", "Jovin D'sa"], "title": "Occupancy-aware Trajectory Planning for Autonomous Valet Parking in Uncertain Dynamic Environments", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Accurately reasoning about future parking spot availability and integrated\nplanning is critical for enabling safe and efficient autonomous valet parking\nin dynamic, uncertain environments. Unlike existing methods that rely solely on\ninstantaneous observations or static assumptions, we present an approach that\npredicts future parking spot occupancy by explicitly distinguishing between\ninitially vacant and occupied spots, and by leveraging the predicted motion of\ndynamic agents. We introduce a probabilistic spot occupancy estimator that\nincorporates partial and noisy observations within a limited Field-of-View\n(FoV) model and accounts for the evolving uncertainty of unobserved regions.\nCoupled with this, we design a strategy planner that adaptively balances\ngoal-directed parking maneuvers with exploratory navigation based on\ninformation gain, and intelligently incorporates wait-and-go behaviors at\npromising spots. Through randomized simulations emulating large parking lots,\nwe demonstrate that our framework significantly improves parking efficiency,\nsafety margins, and trajectory smoothness compared to existing approaches.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u81ea\u52a8\u4ee3\u5ba2\u6cca\u8f66\u573a\u666f\u3002\u867d\u7136\u6d89\u53ca\u8f68\u8ff9\u9884\u6d4b\uff08\u9884\u6d4b\u52a8\u6001agent\u7684\u8fd0\u52a8\uff09\uff0c\u4f46\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6ca1\u6709\u76f4\u63a5\u5173\u8054\u3002\u56e0\u6b64\uff0c\u76f8\u5173\u6027\u4e2d\u7b49\u3002", "keywords": ["trajectory planning", "trajectory prediction", "dynamic environments", "autonomous valet parking", "motion prediction"]}}
{"id": "2509.09332", "pdf": "https://arxiv.org/pdf/2509.09332", "abs": "https://arxiv.org/abs/2509.09332", "authors": ["Yuecheng Liu", "Dafeng Chi", "Shiguang Wu", "Zhanguang Zhang", "Yuzheng Zhuang", "Bowen Yang", "He Zhu", "Lingfeng Zhang", "Pengwei Xie", "David Gamaliel Arcos Bravo", "Yingxue Zhang", "Jianye Hao", "Xingyue Quan"], "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u6458\u8981\u63d0\u5230\u4e86\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b(MLLMs)\u5728\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u5e94\u7528\uff0c\u867d\u7136\u6ca1\u6709\u76f4\u63a5\u63d0\u53ca\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f46\u5176\u7814\u7a76\u7684\u5177\u8eab\u667a\u80fd\u89c4\u5212\u4e0e\u79fb\u52a8\u7269\u4f53\u7684\u8def\u5f84\u89c4\u5212\u5b58\u5728\u5173\u8054\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002\u5173\u952e\u8bcd\u5305\u62ec\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u53ca\u89c4\u5212\u76f8\u5173\u7684\u5185\u5bb9\u3002", "keywords": ["multimodal large language models", "MLLMs", "embodied intelligence", "planning", "embodied reasoning"]}}
{"id": "2509.09372", "pdf": "https://arxiv.org/pdf/2509.09372", "abs": "https://arxiv.org/abs/2509.09372", "authors": ["Yihao Wang", "Pengxiang Ding", "Lingxiao Li", "Can Cui", "Zirui Ge", "Xinyang Tong", "Wenxuan Song", "Han Zhao", "Wei Zhao", "Pengxu Hou", "Siteng Huang", "Yifan Tang", "Wenhui Wang", "Ru Zhang", "Jianyi Liu", "Donglin Wang"], "title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model", "categories": ["cs.RO"], "comment": null, "summary": "Vision-Language-Action (VLA) models typically bridge the gap between\nperceptual and action spaces by pre-training a large-scale Vision-Language\nModel (VLM) on robotic data. While this approach greatly enhances performance,\nit also incurs significant training costs. In this paper, we investigate how to\neffectively bridge vision-language (VL) representations to action (A). We\nintroduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA\nmodels on large-scale VLMs and extensive pre-training. To this end, we first\nsystematically analyze the effectiveness of various VL conditions and present\nkey findings on which conditions are essential for bridging perception and\naction spaces. Based on these insights, we propose a lightweight Policy module\nwith Bridge Attention, which autonomously injects the optimal condition into\nthe action space. In this way, our method achieves high performance using only\na 0.5B-parameter backbone, without any robotic data pre-training. Extensive\nexperiments on both simulated and real-world robotic benchmarks demonstrate\nthat VLA-Adapter not only achieves state-of-the-art level performance, but also\noffers the fast inference speed reported to date. Furthermore, thanks to the\nproposed advanced bridging paradigm, VLA-Adapter enables the training of a\npowerful VLA model in just 8 hours on a single consumer-grade GPU, greatly\nlowering the barrier to deploying the VLA model. Project page:\nhttps://vla-adapter.github.io/.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "This paper focuses on Vision-Language-Action models, which connect perception and action spaces. While it doesn't directly deal with trajectory prediction, the 'action' component could potentially involve predicting future actions or movements, making it somewhat related. It also utilizes large-scale Vision-Language Models (VLMs), which are a type of large model. The connection to trajectory prediction is weaker, but the use of VLMs and the focus on action justify a moderate relevance score.", "keywords": ["Vision-Language-Action models", "Large-scale Vision-Language Model (VLM)", "action", "foundation models"]}}
{"id": "2509.09284", "pdf": "https://arxiv.org/pdf/2509.09284", "abs": "https://arxiv.org/abs/2509.09284", "authors": ["Bingning Huang", "Tu Nguyen", "Matthieu Zimmer"], "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22 (MCTS) \u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7b56\u7565\u4f18\u5316\u3002\u867d\u7136MCTS\u53ef\u4ee5\u751f\u6210\u8f68\u8ff9\uff0c\u4f46\u8fd9\u91cc\u7684\u8f68\u8ff9\u4e3b\u8981\u7528\u4e8e\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff0c\u800c\u4e0d\u662f\u7269\u7406\u4e16\u754c\u7684\u8f68\u8ff9\u9884\u6d4b\u3002\u8bba\u6587\u660e\u786e\u6d89\u53ca\u5927\u6a21\u578b\u548c\u6811\u641c\u7d22\uff0c\u540e\u8005\u4e0e\u8f68\u8ff9\u751f\u6210\u76f8\u5173\uff0c\u56e0\u6b64\u5177\u6709\u4e00\u5b9a\u76f8\u5173\u6027\u3002", "keywords": ["large language models", "LLMs", "Monte Carlo Tree Search", "MCTS", "reinforcement learning", "policy optimization"]}}
{"id": "2509.09349", "pdf": "https://arxiv.org/pdf/2509.09349", "abs": "https://arxiv.org/abs/2509.09349", "authors": ["Ian Nell", "Shane Gilroy"], "title": "Classification of Driver Behaviour Using External Observation Techniques for Autonomous Vehicles", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.RO", "eess.IV"], "comment": null, "summary": "Road traffic accidents remain a significant global concern, with human error,\nparticularly distracted and impaired driving, among the leading causes. This\nstudy introduces a novel driver behavior classification system that uses\nexternal observation techniques to detect indicators of distraction and\nimpairment. The proposed framework employs advanced computer vision\nmethodologies, including real-time object tracking, lateral displacement\nanalysis, and lane position monitoring. The system identifies unsafe driving\nbehaviors such as excessive lateral movement and erratic trajectory patterns by\nimplementing the YOLO object detection model and custom lane estimation\nalgorithms. Unlike systems reliant on inter-vehicular communication, this\nvision-based approach enables behavioral analysis of non-connected vehicles.\nExperimental evaluations on diverse video datasets demonstrate the framework's\nreliability and adaptability across varying road and environmental conditions.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u8fdb\u884c\u9a7e\u9a76\u5458\u884c\u4e3a\u5206\u7c7b\uff0c\u5176\u4e2d\u6d89\u53ca\u8f66\u8f86\u8f68\u8ff9\u5206\u6790\u548c\u5bf9\u8c61\u8ddf\u8e2a\uff0c\u4e0e\u8f68\u8ff9\u9884\u6d4b\u6709\u4e00\u5b9a\u7684\u76f8\u5173\u6027\u3002\u4f46\u8bba\u6587\u4e2d\u6ca1\u6709\u63d0\u53ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "keywords": ["trajectory patterns", "object tracking", "lane position monitoring", "YOLO object detection"]}}
{"id": "2509.08858", "pdf": "https://arxiv.org/pdf/2509.08858", "abs": "https://arxiv.org/abs/2509.08858", "authors": ["Oriane Peter", "Kate Devlin"], "title": "Decentralising LLM Alignment: A Case for Context, Pluralism, and Participation", "categories": ["cs.CY", "cs.LG"], "comment": "Accepted at AIES 2025", "summary": "Large Language Models (LLMs) alignment methods have been credited with the\ncommercial success of products like ChatGPT, given their role in steering LLMs\ntowards user-friendly outputs. However, current alignment techniques\npredominantly mirror the normative preferences of a narrow reference group,\neffectively imposing their values on a wide user base. Drawing on theories of\nthe power/knowledge nexus, this work argues that current alignment practices\ncentralise control over knowledge production and governance within already\ninfluential institutions. To counter this, we propose decentralising alignment\nthrough three characteristics: context, pluralism, and participation.\nFurthermore, this paper demonstrates the critical importance of delineating the\ncontext-of-use when shaping alignment practices by grounding each of these\nfeatures in concrete use cases. This work makes the following contributions:\n(1) highlighting the role of context, pluralism, and participation in\ndecentralising alignment; (2) providing concrete examples to illustrate these\nstrategies; and (3) demonstrating the nuanced requirements associated with\napplying alignment across different contexts of use. Ultimately, this paper\npositions LLM alignment as a potential site of resistance against epistemic\ninjustice and the erosion of democratic processes, while acknowledging that\nthese strategies alone cannot substitute for broader societal changes.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "The paper focuses on aligning Large Language Models (LLMs), which is directly related to the 'Large Language Models' theme. While it doesn't directly address trajectory prediction, the core topic of LLM alignment is a significant area within the broader field of large models. The relevance score reflects the focus on LLMs, but the absence of any connection to trajectory prediction lowers the score.", "keywords": ["Large Language Models", "LLMs", "alignment", "foundation models"]}}
{"id": "2509.09496", "pdf": "https://arxiv.org/pdf/2509.09496", "abs": "https://arxiv.org/abs/2509.09496", "authors": ["Ha Linh Nguyen", "Tze Ho Elden Tse", "Angela Yao"], "title": "Improving Human Motion Plausibility with Body Momentum", "categories": ["cs.CV"], "comment": "Accepted at BMVC 2025", "summary": "Many studies decompose human motion into local motion in a frame attached to\nthe root joint and global motion of the root joint in the world frame, treating\nthem separately. However, these two components are not independent. Global\nmovement arises from interactions with the environment, which are, in turn,\ndriven by changes in the body configuration. Motion models often fail to\nprecisely capture this physical coupling between local and global dynamics,\nwhile deriving global trajectories from joint torques and external forces is\ncomputationally expensive and complex. To address these challenges, we propose\nusing whole-body linear and angular momentum as a constraint to link local\nmotion with global movement. Since momentum reflects the aggregate effect of\njoint-level dynamics on the body's movement through space, it provides a\nphysically grounded way to relate local joint behavior to global displacement.\nBuilding on this insight, we introduce a new loss term that enforces\nconsistency between the generated momentum profiles and those observed in\nground-truth data. Incorporating our loss reduces foot sliding and jitter,\nimproves balance, and preserves the accuracy of the recovered motion. Code and\ndata are available at the project page https://hlinhn.github.io/momentum_bmvc.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4eba\u4f53\u8fd0\u52a8\u7684\u5408\u7406\u6027\uff0c\u5e76\u4f7f\u7528\u52a8\u91cf\u4f5c\u4e3a\u7ea6\u675f\u6765\u5173\u8054\u5c40\u90e8\u8fd0\u52a8\u548c\u5168\u5c40\u8fd0\u52a8\u3002\u867d\u7136\u4e0e\u8f68\u8ff9\u9884\u6d4b\u76f8\u5173\uff08\u4eba\u4f53\u8fd0\u52a8\u53ef\u4ee5\u770b\u4f5c\u662f\u4e00\u79cd\u8f68\u8ff9\uff09\uff0c\u4f46\u6ca1\u6709\u76f4\u63a5\u63d0\u5230\u8f68\u8ff9\u9884\u6d4b\u7b97\u6cd5\u6216\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u8bba\u6587\u5173\u6ce8\u7684\u662f\u8fd0\u52a8\u5efa\u6a21\u548c\u7269\u7406\u7ea6\u675f\uff0c\u800c\u4e0d\u662f\u9884\u6d4b\u672a\u6765\u8f68\u8ff9\u6216\u5229\u7528\u5927\u6a21\u578b\u3002", "keywords": ["human motion", "global motion", "trajectory", "motion models"]}}
{"id": "2509.09530", "pdf": "https://arxiv.org/pdf/2509.09530", "abs": "https://arxiv.org/abs/2509.09530", "authors": ["Paul F. R. Wilson", "Matteo Ronchetti", "R\u00fcdiger G\u00f6bl", "Viktoria Markova", "Sebastian Rosenzweig", "Raphael Prevost", "Parvin Mousavi", "Oliver Zettinig"], "title": "DualTrack: Sensorless 3D Ultrasound needs Local and Global Context", "categories": ["cs.CV"], "comment": null, "summary": "Three-dimensional ultrasound (US) offers many clinical advantages over\nconventional 2D imaging, yet its widespread adoption is limited by the cost and\ncomplexity of traditional 3D systems. Sensorless 3D US, which uses deep\nlearning to estimate a 3D probe trajectory from a sequence of 2D US images, is\na promising alternative. Local features, such as speckle patterns, can help\npredict frame-to-frame motion, while global features, such as coarse shapes and\nanatomical structures, can situate the scan relative to anatomy and help\npredict its general shape. In prior approaches, global features are either\nignored or tightly coupled with local feature extraction, restricting the\nability to robustly model these two complementary aspects. We propose\nDualTrack, a novel dual-encoder architecture that leverages decoupled local and\nglobal encoders specialized for their respective scales of feature extraction.\nThe local encoder uses dense spatiotemporal convolutions to capture\nfine-grained features, while the global encoder utilizes an image backbone\n(e.g., a 2D CNN or foundation model) and temporal attention layers to embed\nhigh-level anatomical features and long-range dependencies. A lightweight\nfusion module then combines these features to estimate the trajectory.\nExperimental results on a large public benchmark show that DualTrack achieves\nstate-of-the-art accuracy and globally consistent 3D reconstructions,\noutperforming previous methods and yielding an average reconstruction error\nbelow 5 mm.", "relevance_analysis": {"relevance_score": 0.6, "explanation": "\u8be5\u8bba\u6587\u4e3b\u8981\u5173\u6ce8\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c3D\u8d85\u58f0\u63a2\u5934\u8f68\u8ff9\u7684\u4f30\u8ba1\uff0c\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u8303\u7574\u3002\u6458\u8981\u4e2d\u63d0\u5230\u4e86\u201c3D probe trajectory\u201d\u548c\u201cestimate the trajectory\u201d\uff0c\u8fd9\u4e9b\u90fd\u5c5e\u4e8e\u8f68\u8ff9\u9884\u6d4b\u7684\u5173\u952e\u8bcd\u3002\u6b64\u5916\uff0c\u8bba\u6587\u4e2d\u63d0\u5230\u4e86\u4f7f\u7528foundation model\u4f5c\u4e3a\u56fe\u50cfbackbone\uff0c\u8868\u660e\u4e0e\u5927\u6a21\u578b\u6709\u4e00\u5b9a\u7684\u5173\u8054\u3002\u4f46\u6574\u4f53\u800c\u8a00\uff0c\u8bba\u6587\u7684\u91cd\u70b9\u5728\u4e8e\u533b\u5b66\u56fe\u50cf\u5904\u7406\u548c\u8f68\u8ff9\u9884\u6d4b\uff0c\u4e0e\u5927\u6a21\u578b\u7684\u5173\u7cfb\u76f8\u5bf9\u8f83\u5f31\u3002", "keywords": ["trajectory prediction", "3D probe trajectory", "foundation model"]}}
